2023-06-22 14:58:47,261 - INFO - c: 1.0 and total_data_in_this_class: 258
2023-06-22 14:58:47,261 - INFO - c: 3.0 and total_data_in_this_class: 258
2023-06-22 14:58:47,261 - INFO - c: 4.0 and total_data_in_this_class: 283
2023-06-22 14:58:47,261 - INFO - c: 1.0 and total_data_in_this_class: 75
2023-06-22 14:58:47,261 - INFO - c: 3.0 and total_data_in_this_class: 75
2023-06-22 14:58:47,261 - INFO - c: 4.0 and total_data_in_this_class: 50
2023-06-22 14:58:47,308 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.0004558563232421875 sec
2023-06-22 14:58:47,313 - DEBUG - Initializing backend 'interpreter'
2023-06-22 14:58:47,316 - DEBUG - Backend 'interpreter' initialized
2023-06-22 14:58:47,316 - DEBUG - Initializing backend 'cpu'
2023-06-22 14:58:47,321 - DEBUG - Backend 'cpu' initialized
2023-06-22 14:58:47,321 - DEBUG - Initializing backend 'cuda'
2023-06-22 14:58:47,322 - INFO - Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
2023-06-22 14:58:47,322 - DEBUG - Initializing backend 'rocm'
2023-06-22 14:58:47,322 - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
2023-06-22 14:58:47,322 - DEBUG - Initializing backend 'tpu'
2023-06-22 14:58:47,323 - INFO - Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
2023-06-22 14:58:47,324 - DEBUG - Initializing backend 'plugin'
2023-06-22 14:58:47,324 - INFO - Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
2023-06-22 14:58:47,324 - WARNING - No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
2023-06-22 14:58:47,325 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005674362182617188 sec
2023-06-22 14:58:47,326 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 14:58:47,329 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.003182649612426758 sec
2023-06-22 14:58:47,330 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 14:58:47,356 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.025514602661132812 sec
2023-06-22 14:58:47,360 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003829002380371094 sec
2023-06-22 14:58:47,361 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 14:58:47,363 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0017876625061035156 sec
2023-06-22 14:58:47,363 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 14:58:47,377 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.014058351516723633 sec
2023-06-22 14:58:47,382 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00028824806213378906 sec
2023-06-22 14:58:47,385 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002608299255371094 sec
2023-06-22 14:58:47,386 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0007658004760742188 sec
2023-06-22 14:58:47,389 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002837181091308594 sec
2023-06-22 14:58:47,390 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002510547637939453 sec
2023-06-22 14:58:47,391 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0006110668182373047 sec
2023-06-22 14:58:47,393 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005290508270263672 sec
2023-06-22 14:58:47,394 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003635883331298828 sec
2023-06-22 14:58:47,395 - WARNING - Finished tracing + transforming fn for pjit in 0.0007252693176269531 sec
2023-06-22 14:58:47,397 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0006728172302246094 sec
2023-06-22 14:58:47,398 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002713203430175781 sec
2023-06-22 14:58:47,400 - WARNING - Finished tracing + transforming fn for pjit in 0.00048828125 sec
2023-06-22 14:58:47,402 - WARNING - Finished tracing + transforming fn for pjit in 0.0005710124969482422 sec
2023-06-22 14:58:47,403 - WARNING - Finished tracing + transforming fn for pjit in 0.0006363391876220703 sec
2023-06-22 14:58:47,405 - WARNING - Finished tracing + transforming fn for pjit in 0.0005857944488525391 sec
2023-06-22 14:58:47,407 - WARNING - Finished tracing + transforming fn for pjit in 0.000492095947265625 sec
2023-06-22 14:58:47,408 - WARNING - Finished tracing + transforming fn for pjit in 0.0004918575286865234 sec
2023-06-22 14:58:47,412 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004994869232177734 sec
2023-06-22 14:58:47,414 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006244182586669922 sec
2023-06-22 14:58:47,415 - WARNING - Finished tracing + transforming fn for pjit in 0.0005266666412353516 sec
2023-06-22 14:58:47,416 - WARNING - Finished tracing + transforming fn for pjit in 0.0005161762237548828 sec
2023-06-22 14:58:47,417 - WARNING - Finished tracing + transforming _uniform for pjit in 0.0062787532806396484 sec
2023-06-22 14:58:47,418 - WARNING - Finished tracing + transforming _normal_real for pjit in 0.0076634883880615234 sec
2023-06-22 14:58:47,419 - WARNING - Finished tracing + transforming _normal for pjit in 0.00862884521484375 sec
2023-06-22 14:58:47,421 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004932880401611328 sec
2023-06-22 14:58:47,423 - WARNING - Finished tracing + transforming fn for pjit in 0.0007035732269287109 sec
2023-06-22 14:58:47,425 - WARNING - Finished tracing + transforming fn for pjit in 0.0005123615264892578 sec
2023-06-22 14:58:47,425 - WARNING - Finished tracing + transforming _uniform for pjit in 0.005494356155395508 sec
2023-06-22 14:58:47,426 - WARNING - Finished tracing + transforming _normal_real for pjit in 0.006827592849731445 sec
2023-06-22 14:58:47,427 - WARNING - Finished tracing + transforming _normal for pjit in 0.0076482295989990234 sec
2023-06-22 14:58:47,428 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00035071372985839844 sec
2023-06-22 14:58:47,429 - WARNING - Finished tracing + transforming fn for pjit in 0.0004918575286865234 sec
2023-06-22 14:58:47,431 - WARNING - Finished tracing + transforming fn for pjit in 0.0005841255187988281 sec
2023-06-22 14:58:47,432 - WARNING - Finished tracing + transforming fn for pjit in 0.0005292892456054688 sec
2023-06-22 14:58:47,440 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0007684230804443359 sec
2023-06-22 14:58:47,441 - WARNING - Finished tracing + transforming _mean for pjit in 0.002047300338745117 sec
2023-06-22 14:58:47,443 - WARNING - Finished tracing + transforming fn for pjit in 0.0005035400390625 sec
2023-06-22 14:58:47,444 - WARNING - Finished tracing + transforming fn for pjit in 0.0004944801330566406 sec
2023-06-22 14:58:47,447 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0011069774627685547 sec
2023-06-22 14:58:47,449 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0008854866027832031 sec
2023-06-22 14:58:47,451 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006868839263916016 sec
2023-06-22 14:58:47,453 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005712509155273438 sec
2023-06-22 14:58:47,454 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004966259002685547 sec
2023-06-22 14:58:47,456 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005011558532714844 sec
2023-06-22 14:58:47,458 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006079673767089844 sec
2023-06-22 14:58:47,458 - WARNING - Finished tracing + transforming _where for pjit in 0.002063274383544922 sec
2023-06-22 14:58:47,460 - WARNING - Finished tracing + transforming fn for pjit in 0.0005769729614257812 sec
2023-06-22 14:58:47,461 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005638599395751953 sec
2023-06-22 14:58:47,463 - WARNING - Finished tracing + transforming fn for pjit in 0.0005114078521728516 sec
2023-06-22 14:58:47,464 - WARNING - Finished tracing + transforming fn for pjit in 0.0004930496215820312 sec
2023-06-22 14:58:47,466 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004832744598388672 sec
2023-06-22 14:58:47,468 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005841255187988281 sec
2023-06-22 14:58:47,469 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000392913818359375 sec
2023-06-22 14:58:47,471 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005776882171630859 sec
2023-06-22 14:58:47,473 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004961490631103516 sec
2023-06-22 14:58:47,474 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004911422729492188 sec
2023-06-22 14:58:47,476 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0014624595642089844 sec
2023-06-22 14:58:47,477 - WARNING - Finished tracing + transforming _where for pjit in 0.0027282238006591797 sec
2023-06-22 14:58:47,479 - WARNING - Finished tracing + transforming fn for pjit in 0.0005671977996826172 sec
2023-06-22 14:58:47,480 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005774497985839844 sec
2023-06-22 14:58:47,482 - WARNING - Finished tracing + transforming fn for pjit in 0.0004937648773193359 sec
2023-06-22 14:58:47,491 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006148815155029297 sec
2023-06-22 14:58:47,493 - WARNING - Finished tracing + transforming fn for pjit in 0.0006000995635986328 sec
2023-06-22 14:58:47,494 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005853176116943359 sec
2023-06-22 14:58:47,496 - WARNING - Finished tracing + transforming fn for pjit in 0.0004940032958984375 sec
2023-06-22 14:58:47,504 - WARNING - Finished tracing + transforming fn for pjit in 0.0004911422729492188 sec
2023-06-22 14:58:47,509 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004987716674804688 sec
2023-06-22 14:58:47,510 - WARNING - Finished tracing + transforming fn for pjit in 0.0006551742553710938 sec
2023-06-22 14:58:47,512 - WARNING - Finished tracing + transforming fn for pjit in 0.0005083084106445312 sec
2023-06-22 14:58:47,513 - WARNING - Finished tracing + transforming _uniform for pjit in 0.005480051040649414 sec
2023-06-22 14:58:47,514 - WARNING - Finished tracing + transforming _normal_real for pjit in 0.006825685501098633 sec
2023-06-22 14:58:47,514 - WARNING - Finished tracing + transforming _normal for pjit in 0.007659435272216797 sec
2023-06-22 14:58:47,517 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004937648773193359 sec
2023-06-22 14:58:47,519 - WARNING - Finished tracing + transforming fn for pjit in 0.0005183219909667969 sec
2023-06-22 14:58:47,520 - WARNING - Finished tracing + transforming fn for pjit in 0.0006880760192871094 sec
2023-06-22 14:58:47,521 - WARNING - Finished tracing + transforming _uniform for pjit in 0.005460977554321289 sec
2023-06-22 14:58:47,522 - WARNING - Finished tracing + transforming _normal_real for pjit in 0.006768703460693359 sec
2023-06-22 14:58:47,522 - WARNING - Finished tracing + transforming _normal for pjit in 0.007588386535644531 sec
2023-06-22 14:58:47,524 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003490447998046875 sec
2023-06-22 14:58:47,525 - WARNING - Finished tracing + transforming fn for pjit in 0.0005071163177490234 sec
2023-06-22 14:58:47,526 - WARNING - Finished tracing + transforming fn for pjit in 0.0005731582641601562 sec
2023-06-22 14:58:47,528 - WARNING - Finished tracing + transforming fn for pjit in 0.0005123615264892578 sec
2023-06-22 14:58:47,565 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.18329071998596191 sec
2023-06-22 14:58:47,567 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002567768096923828 sec
2023-06-22 14:58:47,569 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002493858337402344 sec
2023-06-22 14:58:47,570 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005807876586914062 sec
2023-06-22 14:58:47,573 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00044727325439453125 sec
2023-06-22 14:58:47,574 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00025010108947753906 sec
2023-06-22 14:58:47,575 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005860328674316406 sec
2023-06-22 14:58:47,577 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002522468566894531 sec
2023-06-22 14:58:47,579 - WARNING - Finished tracing + transforming fn for pjit in 0.0004949569702148438 sec
2023-06-22 14:58:47,580 - WARNING - Finished tracing + transforming fn for pjit in 0.0005488395690917969 sec
2023-06-22 14:58:47,582 - WARNING - Finished tracing + transforming fn for pjit in 0.0004811286926269531 sec
2023-06-22 14:58:47,583 - WARNING - Finished tracing + transforming fn for pjit in 0.0007107257843017578 sec
2023-06-22 14:58:47,586 - WARNING - Finished tracing + transforming fn for pjit in 0.0003752708435058594 sec
2023-06-22 14:58:47,588 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002684593200683594 sec
2023-06-22 14:58:47,589 - WARNING - Finished tracing + transforming fn for pjit in 0.0003833770751953125 sec
2023-06-22 14:58:47,591 - WARNING - Finished tracing + transforming fn for pjit in 0.0005261898040771484 sec
2023-06-22 14:58:47,596 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005865097045898438 sec
2023-06-22 14:58:47,597 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016040802001953125 sec
2023-06-22 14:58:47,599 - WARNING - Finished tracing + transforming fn for pjit in 0.0004177093505859375 sec
2023-06-22 14:58:47,600 - WARNING - Finished tracing + transforming fn for pjit in 0.0003705024719238281 sec
2023-06-22 14:58:47,601 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003752708435058594 sec
2023-06-22 14:58:47,604 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0007698535919189453 sec
2023-06-22 14:58:47,605 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046062469482421875 sec
2023-06-22 14:58:47,607 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004642009735107422 sec
2023-06-22 14:58:47,608 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038695335388183594 sec
2023-06-22 14:58:47,609 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004036426544189453 sec
2023-06-22 14:58:47,611 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006284713745117188 sec
2023-06-22 14:58:47,612 - WARNING - Finished tracing + transforming _where for pjit in 0.001729726791381836 sec
2023-06-22 14:58:47,613 - WARNING - Finished tracing + transforming fn for pjit in 0.00043272972106933594 sec
2023-06-22 14:58:47,614 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042748451232910156 sec
2023-06-22 14:58:47,615 - WARNING - Finished tracing + transforming fn for pjit in 0.0003800392150878906 sec
2023-06-22 14:58:47,616 - WARNING - Finished tracing + transforming fn for pjit in 0.0003879070281982422 sec
2023-06-22 14:58:47,618 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003750324249267578 sec
2023-06-22 14:58:47,619 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00048089027404785156 sec
2023-06-22 14:58:47,620 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000438690185546875 sec
2023-06-22 14:58:47,621 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044345855712890625 sec
2023-06-22 14:58:47,623 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003821849822998047 sec
2023-06-22 14:58:47,624 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004048347473144531 sec
2023-06-22 14:58:47,625 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004277229309082031 sec
2023-06-22 14:58:47,626 - WARNING - Finished tracing + transforming _where for pjit in 0.0013949871063232422 sec
2023-06-22 14:58:47,627 - WARNING - Finished tracing + transforming fn for pjit in 0.0004646778106689453 sec
2023-06-22 14:58:47,628 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005772113800048828 sec
2023-06-22 14:58:47,630 - WARNING - Finished tracing + transforming fn for pjit in 0.0004096031188964844 sec
2023-06-22 14:58:47,637 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005075931549072266 sec
2023-06-22 14:58:47,638 - WARNING - Finished tracing + transforming fn for pjit in 0.0004591941833496094 sec
2023-06-22 14:58:47,639 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004620552062988281 sec
2023-06-22 14:58:47,641 - WARNING - Finished tracing + transforming fn for pjit in 0.00040721893310546875 sec
2023-06-22 14:58:47,646 - WARNING - Finished tracing + transforming fn for pjit in 0.0003437995910644531 sec
2023-06-22 14:58:47,650 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00040912628173828125 sec
2023-06-22 14:58:47,650 - WARNING - Finished tracing + transforming fn for pjit in 0.0003802776336669922 sec
2023-06-22 14:58:47,652 - WARNING - Finished tracing + transforming fn for pjit in 0.0004086494445800781 sec
2023-06-22 14:58:47,678 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.1120765209197998 sec
2023-06-22 14:58:47,680 - WARNING - Finished tracing + transforming absolute for pjit in 0.0002815723419189453 sec
2023-06-22 14:58:47,681 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019550323486328125 sec
2023-06-22 14:58:47,683 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002532005310058594 sec
2023-06-22 14:58:47,683 - WARNING - Finished tracing + transforming _where for pjit in 0.001054525375366211 sec
2023-06-22 14:58:47,684 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006148815155029297 sec
2023-06-22 14:58:47,685 - WARNING - Finished tracing + transforming trace for pjit in 0.004292488098144531 sec
2023-06-22 14:58:47,686 - WARNING - Finished tracing + transforming fn for pjit in 0.0004258155822753906 sec
2023-06-22 14:58:47,698 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00018334388732910156 sec
2023-06-22 14:58:47,700 - WARNING - Finished tracing + transforming tril for pjit in 0.001064300537109375 sec
2023-06-22 14:58:47,701 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0028493404388427734 sec
2023-06-22 14:58:47,703 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0003178119659423828 sec
2023-06-22 14:58:47,703 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019049644470214844 sec
2023-06-22 14:58:47,706 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.002161741256713867 sec
2023-06-22 14:58:47,713 - WARNING - Finished tracing + transforming _solve for pjit in 0.01585221290588379 sec
2023-06-22 14:58:47,714 - WARNING - Finished tracing + transforming dot for pjit in 0.0005104541778564453 sec
2023-06-22 14:58:47,715 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004737377166748047 sec
2023-06-22 14:58:47,716 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00045680999755859375 sec
2023-06-22 14:58:47,717 - WARNING - Finished tracing + transforming _mean for pjit in 0.0014324188232421875 sec
2023-06-22 14:58:47,718 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00032973289489746094 sec
2023-06-22 14:58:47,719 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0003139972686767578 sec
2023-06-22 14:58:47,720 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037670135498046875 sec
2023-06-22 14:58:47,722 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005862712860107422 sec
2023-06-22 14:58:47,723 - WARNING - Finished tracing + transforming _mean for pjit in 0.0018384456634521484 sec
2023-06-22 14:58:47,724 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.3442537784576416 sec
2023-06-22 14:58:47,727 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 14:58:47,782 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.054582834243774414 sec
2023-06-22 14:58:47,782 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 14:58:47,948 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.16567468643188477 sec
2023-06-22 14:58:47,994 - INFO - initial test loss: 0.027373878899937528
2023-06-22 14:58:47,994 - INFO - initial test acc: 0.6449999809265137
2023-06-22 14:58:48,006 - WARNING - Finished tracing + transforming dot for pjit in 0.0008263587951660156 sec
2023-06-22 14:58:48,009 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0009629726409912109 sec
2023-06-22 14:58:48,012 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008399486541748047 sec
2023-06-22 14:58:48,013 - WARNING - Finished tracing + transforming _mean for pjit in 0.002857208251953125 sec
2023-06-22 14:58:48,016 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004470348358154297 sec
2023-06-22 14:58:48,018 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0005900859832763672 sec
2023-06-22 14:58:48,019 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0007894039154052734 sec
2023-06-22 14:58:48,022 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008792877197265625 sec
2023-06-22 14:58:48,023 - WARNING - Finished tracing + transforming _mean for pjit in 0.0029747486114501953 sec
2023-06-22 14:58:48,026 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.02706122398376465 sec
2023-06-22 14:58:48,045 - WARNING - Finished tracing + transforming fn for pjit in 0.0006127357482910156 sec
2023-06-22 14:58:48,047 - WARNING - Finished tracing + transforming fn for pjit in 0.0007715225219726562 sec
2023-06-22 14:58:48,049 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0007097721099853516 sec
2023-06-22 14:58:48,052 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006349086761474609 sec
2023-06-22 14:58:48,053 - WARNING - Finished tracing + transforming _where for pjit in 0.002488374710083008 sec
2023-06-22 14:58:48,074 - WARNING - Finished tracing + transforming fn for pjit in 0.0006043910980224609 sec
2023-06-22 14:58:48,076 - WARNING - Finished tracing + transforming fn for pjit in 0.0006241798400878906 sec
2023-06-22 14:58:48,078 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005326271057128906 sec
2023-06-22 14:58:48,080 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005419254302978516 sec
2023-06-22 14:58:48,081 - WARNING - Finished tracing + transforming _where for pjit in 0.0016942024230957031 sec
2023-06-22 14:58:48,144 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037980079650878906 sec
2023-06-22 14:58:48,224 - WARNING - Finished tracing + transforming fn for pjit in 0.0004775524139404297 sec
2023-06-22 14:58:48,225 - WARNING - Finished tracing + transforming fn for pjit in 0.00040078163146972656 sec
2023-06-22 14:58:48,226 - WARNING - Finished tracing + transforming square for pjit in 0.00030803680419921875 sec
2023-06-22 14:58:48,228 - WARNING - Finished tracing + transforming fn for pjit in 0.0003910064697265625 sec
2023-06-22 14:58:48,230 - WARNING - Finished tracing + transforming _power for pjit in 0.00045609474182128906 sec
2023-06-22 14:58:48,231 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006215572357177734 sec
2023-06-22 14:58:48,232 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004067420959472656 sec
2023-06-22 14:58:48,235 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002989768981933594 sec
2023-06-22 14:58:48,236 - WARNING - Finished tracing + transforming fn for pjit in 0.0004775524139404297 sec
2023-06-22 14:58:48,237 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000396728515625 sec
2023-06-22 14:58:48,238 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000408172607421875 sec
2023-06-22 14:58:48,239 - WARNING - Finished tracing + transforming fn for pjit in 0.0004899501800537109 sec
2023-06-22 14:58:48,240 - WARNING - Finished tracing + transforming fn for pjit in 0.00041747093200683594 sec
2023-06-22 14:58:48,241 - WARNING - Finished tracing + transforming square for pjit in 0.00030994415283203125 sec
2023-06-22 14:58:48,245 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003681182861328125 sec
2023-06-22 14:58:48,248 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00030732154846191406 sec
2023-06-22 14:58:48,249 - WARNING - Finished tracing + transforming fn for pjit in 0.0005402565002441406 sec
2023-06-22 14:58:48,250 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00040078163146972656 sec
2023-06-22 14:58:48,251 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040721893310546875 sec
2023-06-22 14:58:48,252 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2544825077056885 sec
2023-06-22 14:58:48,258 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 14:58:48,364 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10542488098144531 sec
2023-06-22 14:58:48,364 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 14:58:48,762 - WARNING - Finished XLA compilation of jit(update_fn) in 0.39749813079833984 sec
2023-06-22 14:58:51,500 - INFO - Distilling data from client: Client00
2023-06-22 14:58:51,501 - INFO - train loss: 0.0019305313485007483
2023-06-22 14:58:51,501 - INFO - train acc: 1.0
2023-06-22 14:58:51,563 - WARNING - Finished tracing + transforming fn for pjit in 0.0011630058288574219 sec
2023-06-22 14:58:51,565 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(float32[]), ShapedArray(int64[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 14:58:51,568 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.003216981887817383 sec
2023-06-22 14:58:51,569 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 14:58:51,583 - WARNING - Finished XLA compilation of jit(fn) in 0.013831615447998047 sec
2023-06-22 14:58:51,697 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.77      0.80        75
           3       0.71      0.61      0.66        75
           4       0.54      0.70      0.61        50

    accuracy                           0.69       200
   macro avg       0.69      0.70      0.69       200
weighted avg       0.71      0.69      0.70       200

2023-06-22 14:58:51,697 - INFO - test loss 0.024331815433810945
2023-06-22 14:58:51,697 - INFO - test acc 0.6949999928474426
2023-06-22 14:58:54,473 - INFO - Distilling data from client: Client00
2023-06-22 14:58:54,474 - INFO - train loss: 0.0010560636902159537
2023-06-22 14:58:54,474 - INFO - train acc: 1.0
2023-06-22 14:58:54,533 - WARNING - Finished tracing + transforming fn for pjit in 0.010128498077392578 sec
2023-06-22 14:58:54,538 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(float32[]), ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 14:58:54,547 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.008253812789916992 sec
2023-06-22 14:58:54,548 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 14:58:54,567 - WARNING - Finished XLA compilation of jit(fn) in 0.018259525299072266 sec
2023-06-22 14:58:54,577 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.72      0.78        75
           3       0.70      0.60      0.65        75
           4       0.46      0.66      0.54        50

    accuracy                           0.66       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.69      0.66      0.67       200

2023-06-22 14:58:54,578 - INFO - test loss 0.025278067331969432
2023-06-22 14:58:54,579 - INFO - test acc 0.6599999666213989
2023-06-22 14:58:57,386 - INFO - Distilling data from client: Client00
2023-06-22 14:58:57,387 - INFO - train loss: 0.0007134750942956146
2023-06-22 14:58:57,387 - INFO - train acc: 1.0
2023-06-22 14:58:57,550 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.80      0.83        75
           3       0.77      0.63      0.69        75
           4       0.52      0.72      0.61        50

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.74      0.71      0.72       200

2023-06-22 14:58:57,550 - INFO - test loss 0.02404176605049986
2023-06-22 14:58:57,550 - INFO - test acc 0.7149999737739563
2023-06-22 14:59:00,345 - INFO - Distilling data from client: Client00
2023-06-22 14:59:00,346 - INFO - train loss: 0.0005163245398404292
2023-06-22 14:59:00,346 - INFO - train acc: 1.0
2023-06-22 14:59:00,408 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.79      0.81        75
           3       0.78      0.63      0.70        75
           4       0.54      0.74      0.62        50

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.74      0.71      0.72       200

2023-06-22 14:59:00,415 - INFO - test loss 0.024376771659424604
2023-06-22 14:59:00,416 - INFO - test acc 0.7149999737739563
2023-06-22 14:59:03,224 - INFO - Distilling data from client: Client00
2023-06-22 14:59:03,225 - INFO - train loss: 0.000580899805134209
2023-06-22 14:59:03,225 - INFO - train acc: 1.0
2023-06-22 14:59:03,291 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.79      0.82        75
           3       0.75      0.63      0.68        75
           4       0.53      0.72      0.61        50

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.73      0.71      0.72       200

2023-06-22 14:59:03,292 - INFO - test loss 0.0247547124253464
2023-06-22 14:59:03,293 - INFO - test acc 0.7099999785423279
2023-06-22 14:59:06,176 - INFO - Distilling data from client: Client00
2023-06-22 14:59:06,177 - INFO - train loss: 0.0005589941339544236
2023-06-22 14:59:06,177 - INFO - train acc: 1.0
2023-06-22 14:59:06,237 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.80      0.83        75
           3       0.75      0.64      0.69        75
           4       0.52      0.70      0.60        50

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.74      0.71      0.72       200

2023-06-22 14:59:06,238 - INFO - test loss 0.024789688805508248
2023-06-22 14:59:06,238 - INFO - test acc 0.7149999737739563
2023-06-22 14:59:09,136 - INFO - Distilling data from client: Client00
2023-06-22 14:59:09,136 - INFO - train loss: 0.0004093347791918175
2023-06-22 14:59:09,136 - INFO - train acc: 1.0
2023-06-22 14:59:09,191 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.76      0.81        75
           3       0.75      0.64      0.69        75
           4       0.51      0.72      0.60        50

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.74      0.70      0.71       200

2023-06-22 14:59:09,192 - INFO - test loss 0.02473846878530619
2023-06-22 14:59:09,192 - INFO - test acc 0.7049999833106995
2023-06-22 14:59:12,344 - INFO - Distilling data from client: Client00
2023-06-22 14:59:12,344 - INFO - train loss: 0.00037653611590711026
2023-06-22 14:59:12,345 - INFO - train acc: 1.0
2023-06-22 14:59:12,402 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.77      0.81        75
           3       0.76      0.63      0.69        75
           4       0.53      0.74      0.62        50

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.74      0.71      0.72       200

2023-06-22 14:59:12,405 - INFO - test loss 0.02466133532202175
2023-06-22 14:59:12,406 - INFO - test acc 0.7099999785423279
2023-06-22 14:59:15,446 - INFO - Distilling data from client: Client00
2023-06-22 14:59:15,447 - INFO - train loss: 0.00047669215050349673
2023-06-22 14:59:15,447 - INFO - train acc: 1.0
2023-06-22 14:59:15,509 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.76      0.80        75
           3       0.77      0.64      0.70        75
           4       0.54      0.76      0.63        50

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.74      0.71      0.72       200

2023-06-22 14:59:15,510 - INFO - test loss 0.025009659819362925
2023-06-22 14:59:15,510 - INFO - test acc 0.7149999737739563
2023-06-22 14:59:18,500 - INFO - Distilling data from client: Client00
2023-06-22 14:59:18,501 - INFO - train loss: 0.0004449539843111149
2023-06-22 14:59:18,501 - INFO - train acc: 1.0
2023-06-22 14:59:18,559 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.77      0.82        75
           3       0.77      0.64      0.70        75
           4       0.51      0.72      0.60        50

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.70       200
weighted avg       0.74      0.71      0.72       200

2023-06-22 14:59:18,560 - INFO - test loss 0.024750237584142312
2023-06-22 14:59:18,561 - INFO - test acc 0.7099999785423279
2023-06-22 14:59:21,312 - INFO - Distilling data from client: Client00
2023-06-22 14:59:21,312 - INFO - train loss: 0.0003428891835144327
2023-06-22 14:59:21,312 - INFO - train acc: 1.0
2023-06-22 14:59:21,378 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.76      0.80        75
           3       0.74      0.60      0.66        75
           4       0.48      0.68      0.56        50

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.71      0.68      0.69       200

2023-06-22 14:59:21,378 - INFO - test loss 0.024810399500265083
2023-06-22 14:59:21,378 - INFO - test acc 0.6800000071525574
2023-06-22 14:59:24,214 - INFO - Distilling data from client: Client00
2023-06-22 14:59:24,214 - INFO - train loss: 0.0003817817370057086
2023-06-22 14:59:24,214 - INFO - train acc: 1.0
2023-06-22 14:59:24,280 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.76      0.80        75
           3       0.72      0.63      0.67        75
           4       0.51      0.70      0.59        50

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.72      0.69      0.70       200

2023-06-22 14:59:24,281 - INFO - test loss 0.025079745461764256
2023-06-22 14:59:24,281 - INFO - test acc 0.6949999928474426
2023-06-22 14:59:27,219 - INFO - Distilling data from client: Client00
2023-06-22 14:59:27,220 - INFO - train loss: 0.0002979554573652965
2023-06-22 14:59:27,221 - INFO - train acc: 1.0
2023-06-22 14:59:27,283 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.76      0.81        75
           3       0.77      0.63      0.69        75
           4       0.51      0.74      0.60        50

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.74      0.70      0.71       200

2023-06-22 14:59:27,284 - INFO - test loss 0.024765295783893713
2023-06-22 14:59:27,284 - INFO - test acc 0.7049999833106995
2023-06-22 14:59:30,287 - INFO - Distilling data from client: Client00
2023-06-22 14:59:30,288 - INFO - train loss: 0.0002975075337079318
2023-06-22 14:59:30,288 - INFO - train acc: 1.0
2023-06-22 14:59:30,479 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.81      0.84        75
           3       0.77      0.67      0.71        75
           4       0.51      0.66      0.57        50

    accuracy                           0.72       200
   macro avg       0.72      0.71      0.71       200
weighted avg       0.74      0.72      0.73       200

2023-06-22 14:59:30,479 - INFO - test loss 0.024705028400400154
2023-06-22 14:59:30,479 - INFO - test acc 0.7199999690055847
2023-06-22 14:59:33,337 - INFO - Distilling data from client: Client00
2023-06-22 14:59:33,338 - INFO - train loss: 0.00037271247603622875
2023-06-22 14:59:33,338 - INFO - train acc: 1.0
2023-06-22 14:59:33,395 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.79      0.81        75
           3       0.75      0.60      0.67        75
           4       0.52      0.72      0.61        50

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.72      0.70      0.70       200

2023-06-22 14:59:33,395 - INFO - test loss 0.02498468160999666
2023-06-22 14:59:33,396 - INFO - test acc 0.699999988079071
2023-06-22 14:59:36,271 - INFO - Distilling data from client: Client00
2023-06-22 14:59:36,271 - INFO - train loss: 0.00035677597885105857
2023-06-22 14:59:36,271 - INFO - train acc: 1.0
2023-06-22 14:59:36,321 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.79      0.81        75
           3       0.77      0.61      0.68        75
           4       0.51      0.72      0.60        50

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.73      0.70      0.71       200

2023-06-22 14:59:36,323 - INFO - test loss 0.025359239613699375
2023-06-22 14:59:36,323 - INFO - test acc 0.7049999833106995
2023-06-22 14:59:39,226 - INFO - Distilling data from client: Client00
2023-06-22 14:59:39,227 - INFO - train loss: 0.00030268340783187677
2023-06-22 14:59:39,227 - INFO - train acc: 1.0
2023-06-22 14:59:39,303 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.76      0.80        75
           3       0.76      0.64      0.70        75
           4       0.50      0.70      0.58        50

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.73      0.70      0.71       200

2023-06-22 14:59:39,307 - INFO - test loss 0.025014042368081597
2023-06-22 14:59:39,307 - INFO - test acc 0.699999988079071
2023-06-22 14:59:42,140 - INFO - Distilling data from client: Client00
2023-06-22 14:59:42,140 - INFO - train loss: 0.0003262364916078424
2023-06-22 14:59:42,140 - INFO - train acc: 1.0
2023-06-22 14:59:42,194 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.79      0.81        75
           3       0.78      0.61      0.69        75
           4       0.52      0.74      0.61        50

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.74      0.71      0.72       200

2023-06-22 14:59:42,194 - INFO - test loss 0.025247647934268973
2023-06-22 14:59:42,195 - INFO - test acc 0.7099999785423279
2023-06-22 14:59:44,995 - INFO - Distilling data from client: Client00
2023-06-22 14:59:44,997 - INFO - train loss: 0.00025180427941209167
2023-06-22 14:59:44,997 - INFO - train acc: 1.0
2023-06-22 14:59:45,046 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.77      0.81        75
           3       0.75      0.63      0.68        75
           4       0.51      0.70      0.59        50

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.72      0.70      0.71       200

2023-06-22 14:59:45,051 - INFO - test loss 0.025228582627632303
2023-06-22 14:59:45,051 - INFO - test acc 0.699999988079071
2023-06-22 14:59:47,956 - INFO - Distilling data from client: Client00
2023-06-22 14:59:47,957 - INFO - train loss: 0.0003098820571121916
2023-06-22 14:59:47,957 - INFO - train acc: 1.0
2023-06-22 14:59:48,021 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.77      0.82        75
           3       0.75      0.64      0.69        75
           4       0.52      0.72      0.61        50

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.74      0.71      0.72       200

2023-06-22 14:59:48,022 - INFO - test loss 0.02502765654370637
2023-06-22 14:59:48,022 - INFO - test acc 0.7099999785423279
2023-06-22 14:59:51,020 - INFO - Distilling data from client: Client00
2023-06-22 14:59:51,021 - INFO - train loss: 0.00024687412028981355
2023-06-22 14:59:51,021 - INFO - train acc: 1.0
2023-06-22 14:59:51,070 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.79      0.82        75
           3       0.75      0.60      0.67        75
           4       0.51      0.72      0.60        50

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.73      0.70      0.71       200

2023-06-22 14:59:51,071 - INFO - test loss 0.025135563912945347
2023-06-22 14:59:51,072 - INFO - test acc 0.699999988079071
2023-06-22 14:59:54,032 - INFO - Distilling data from client: Client00
2023-06-22 14:59:54,033 - INFO - train loss: 0.00019960389746959912
2023-06-22 14:59:54,033 - INFO - train acc: 1.0
2023-06-22 14:59:54,087 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.76      0.80        75
           3       0.73      0.63      0.68        75
           4       0.51      0.70      0.59        50

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.72      0.69      0.70       200

2023-06-22 14:59:54,089 - INFO - test loss 0.025067651604521037
2023-06-22 14:59:54,089 - INFO - test acc 0.6949999928474426
2023-06-22 14:59:56,923 - INFO - Distilling data from client: Client00
2023-06-22 14:59:56,924 - INFO - train loss: 0.0002606160207211559
2023-06-22 14:59:56,924 - INFO - train acc: 1.0
2023-06-22 14:59:56,981 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.77      0.81        75
           3       0.74      0.61      0.67        75
           4       0.48      0.66      0.55        50

    accuracy                           0.69       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.71      0.69      0.69       200

2023-06-22 14:59:56,983 - INFO - test loss 0.024657670920477052
2023-06-22 14:59:56,983 - INFO - test acc 0.6850000023841858
2023-06-22 14:59:59,998 - INFO - Distilling data from client: Client00
2023-06-22 14:59:59,998 - INFO - train loss: 0.0002062483716554861
2023-06-22 14:59:59,999 - INFO - train acc: 1.0
2023-06-22 15:00:00,061 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.73      0.78        75
           3       0.74      0.57      0.65        75
           4       0.49      0.74      0.59        50

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.67       200
weighted avg       0.71      0.68      0.68       200

2023-06-22 15:00:00,062 - INFO - test loss 0.025429474403114504
2023-06-22 15:00:00,063 - INFO - test acc 0.675000011920929
2023-06-22 15:00:03,091 - INFO - Distilling data from client: Client00
2023-06-22 15:00:03,091 - INFO - train loss: 0.0002784722300434452
2023-06-22 15:00:03,091 - INFO - train acc: 1.0
2023-06-22 15:00:03,155 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.76      0.79        75
           3       0.72      0.61      0.66        75
           4       0.54      0.72      0.62        50

    accuracy                           0.69       200
   macro avg       0.69      0.70      0.69       200
weighted avg       0.71      0.69      0.70       200

2023-06-22 15:00:03,155 - INFO - test loss 0.02501135605375871
2023-06-22 15:00:03,155 - INFO - test acc 0.6949999928474426
2023-06-22 15:00:03,163 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.0007507801055908203 sec
2023-06-22 15:00:03,166 - WARNING - Finished tracing + transforming fn for pjit in 0.0010242462158203125 sec
2023-06-22 15:00:03,168 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(int64[3]), ShapedArray(int64[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:00:03,173 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.00510716438293457 sec
2023-06-22 15:00:03,174 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:03,191 - WARNING - Finished XLA compilation of jit(fn) in 0.016426801681518555 sec
2023-06-22 15:00:03,192 - WARNING - Finished tracing + transforming jit(add) in 0.00047588348388671875 sec
2023-06-22 15:00:03,193 - DEBUG - Compiling add for with global shapes and types [ShapedArray(int64[3]), ShapedArray(int64[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:00:03,196 - WARNING - Finished jaxpr to MLIR module conversion jit(add) in 0.0024149417877197266 sec
2023-06-22 15:00:03,196 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:03,211 - WARNING - Finished XLA compilation of jit(add) in 0.014132261276245117 sec
2023-06-22 15:00:03,212 - WARNING - Finished tracing + transforming jit(select_n) in 0.0004444122314453125 sec
2023-06-22 15:00:03,213 - DEBUG - Compiling select_n for with global shapes and types [ShapedArray(bool[3]), ShapedArray(int64[3]), ShapedArray(int64[3])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:00:03,215 - WARNING - Finished jaxpr to MLIR module conversion jit(select_n) in 0.002257108688354492 sec
2023-06-22 15:00:03,216 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:03,237 - WARNING - Finished XLA compilation of jit(select_n) in 0.0203702449798584 sec
2023-06-22 15:00:03,241 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003733634948730469 sec
2023-06-22 15:00:03,245 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.00033974647521972656 sec
2023-06-22 15:00:03,247 - DEBUG - Compiling convert_element_type for with global shapes and types [ShapedArray(int64[3])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:00:03,250 - WARNING - Finished jaxpr to MLIR module conversion jit(convert_element_type) in 0.0021660327911376953 sec
2023-06-22 15:00:03,253 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:03,263 - WARNING - Finished XLA compilation of jit(convert_element_type) in 0.0095672607421875 sec
2023-06-22 15:00:03,264 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003635883331298828 sec
2023-06-22 15:00:03,265 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(int32[3])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:00:03,267 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0015881061553955078 sec
2023-06-22 15:00:03,267 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:03,275 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.007567167282104492 sec
2023-06-22 15:00:03,276 - WARNING - Finished tracing + transforming jit(gather) in 0.00036525726318359375 sec
2023-06-22 15:00:03,276 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[516,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:00:03,278 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0017561912536621094 sec
2023-06-22 15:00:03,279 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:03,292 - WARNING - Finished XLA compilation of jit(gather) in 0.012624979019165039 sec
2023-06-22 15:00:03,295 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00036454200744628906 sec
2023-06-22 15:00:03,295 - WARNING - Finished tracing + transforming jit(copy) in 0.00019884109497070312 sec
2023-06-22 15:00:03,296 - DEBUG - Compiling copy for with global shapes and types [ShapedArray(float32[3,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:00:03,297 - WARNING - Finished jaxpr to MLIR module conversion jit(copy) in 0.0014944076538085938 sec
2023-06-22 15:00:03,298 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:03,306 - WARNING - Finished XLA compilation of jit(copy) in 0.00785374641418457 sec
2023-06-22 15:00:03,310 - DEBUG - Loaded backend agg version unknown.
2023-06-22 15:00:03,313 - DEBUG - findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0.
2023-06-22 15:00:03,314 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerifDisplay.ttf', name='DejaVu Serif Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,314 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymReg.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,314 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymReg.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,314 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05
2023-06-22 15:00:03,314 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf', name='cmtt10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,314 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBolIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymReg.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBol.ttf', name='STIXGeneral', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFiveSymReg.ttf', name='STIXSizeFiveSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBolIta.ttf', name='STIXGeneral', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneral.ttf', name='STIXGeneral', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmmi10.ttf', name='cmmi10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymReg.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmss10.ttf', name='cmss10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymBol.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymBol.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,315 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymBol.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansDisplay.ttf', name='DejaVu Sans Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmex10.ttf', name='cmex10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmb10.ttf', name='cmb10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBol.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralItalic.ttf', name='STIXGeneral', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUni.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmr10.ttf', name='cmr10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymBol.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmsy10.ttf', name='cmsy10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,316 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSerifBoldItalic.ttf', name='FreeSerif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Sawasdee-Oblique.ttf', name='Sawasdee', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSerif-Bold.ttf', name='Liberation Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypo-Oblique.ttf', name='Tlwg Typo', style='oblique', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/LikhanNormal.ttf', name='Likhan', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-R.ttf', name='Ubuntu Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-gujarati/Lohit-Gujarati.ttf', name='Lohit Gujarati', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypo.ttf', name='Tlwg Typo', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Garuda-Bold.ttf', name='Garuda', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst-one/KacstOne.ttf', name='KacstOne', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/padauk/Padauk-Regular.ttf', name='Padauk', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 0.5349999999999999
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Rachana-Regular.ttf', name='Rachana', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,317 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuMathTeXGyre.ttf', name='DejaVu Math TeX Gyre', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf', name='Liberation Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/padauk/PadaukBook-Bold.ttf', name='Padauk Book', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Italic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstScreen.ttf', name='KacstScreen', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-LI.ttf', name='Ubuntu', style='italic', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/samyak-fonts/Samyak-Gujarati.ttf', name='Samyak Gujarati', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Karumbi.ttf', name='Karumbi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstQurn.ttf', name='KacstQurn', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Gubbi/Gubbi.ttf', name='Gubbi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Waree-Bold.ttf', name='Waree', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Loma-BoldOblique.ttf', name='Loma', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-BoldItalic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tibetan-machine/TibetanMachineUni.ttf', name='Tibetan Machine Uni', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/sinhala/lklug.ttf', name='LKLUG', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypewriter-Bold.ttf', name='Tlwg Typewriter', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,318 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 0.25
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationMono-Italic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoMono-Regular.ttf', name='Noto Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/samyak/Samyak-Devanagari.ttf', name='Samyak Devanagari', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-B.ttf', name='Ubuntu', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-BI.ttf', name='Ubuntu Mono', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-deva-extra/samanata.ttf', name='Samanata', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypo-BoldOblique.ttf', name='Tlwg Typo', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-oriya/Lohit-Odia.ttf', name='Lohit Odia', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc', name='Noto Sans CJK JP', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush-LightOblique.ttf', name='Umpush', style='oblique', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-BoldItalic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/malayalam/Manjari-Thin.otf', name='Manjari', style='normal', variant='normal', weight=100, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-MI.ttf', name='Ubuntu', style='italic', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari-Bold.ttf', name='Kinnari', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,319 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-orya-extra/utkal.ttf', name='ori1Uni', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Italic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush.ttf', name='Umpush', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeMono.ttf', name='FreeMono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/mry_KacstQurn.ttf', name='mry_KacstQurn', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-telu-extra/Pothana2000.ttf', name='Pothana2000', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Waree-Oblique.ttf', name='Waree', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstBook.ttf', name='KacstBook', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationMono-Bold.ttf', name='Liberation Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationMono-BoldItalic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSerif-Regular.ttf', name='Liberation Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi-Oblique.ttf', name='Norasi', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgMono-BoldOblique.ttf', name='Tlwg Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-devanagari/Lohit-Devanagari.ttf', name='Lohit Devanagari', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/AnjaliOldLipi.ttf', name='AnjaliOldLipi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', name='Liberation Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,320 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-B.ttf', name='Ubuntu Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeMonoBoldOblique.ttf', name='FreeMono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Dyuthi.ttf', name='Dyuthi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Garuda-Oblique.ttf', name='Garuda', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSans-Bold.ttf', name='Liberation Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc', name='Noto Serif CJK JP', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSans-Italic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', name='Liberation Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgMono.ttf', name='Tlwg Mono', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 10.25
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-telugu/Lohit-Telugu.ttf', name='Lohit Telugu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Keraleeyam.ttf', name='Keraleeyam', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf', name='Droid Sans Fallback', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush-Light.ttf', name='Umpush', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,321 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Italic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstDigital.ttf', name='KacstDigital', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/MuktiNarrow.ttf', name='Mukti Narrow', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-RI.ttf', name='Ubuntu', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf', name='Liberation Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/padauk/PadaukBook-Regular.ttf', name='Padauk Book', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Navilu/Navilu.ttf', name='Navilu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Purisa-Oblique.ttf', name='Purisa', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Loma.ttf', name='Loma', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstFarsi.ttf', name='KacstFarsi', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Sawasdee-BoldOblique.ttf', name='Sawasdee', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst-one/KacstOne-Bold.ttf', name='KacstOne', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/samyak-fonts/Samyak-Malayalam.ttf', name='Samyak Malayalam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Gargi/Gargi.ttf', name='Gargi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='condensed', size='scalable')) = 1.25
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25
2023-06-22 15:00:03,322 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf', name='Liberation Sans Narrow', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 10.25
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi.ttf', name='Norasi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypist-BoldOblique.ttf', name='Tlwg Typist', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSerif-BoldItalic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf', name='Liberation Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSansOblique.ttf', name='FreeSans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/mitra.ttf', name='Mitra Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari-BoldItalic.ttf', name='Kinnari', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Purisa-BoldOblique.ttf', name='Purisa', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-kalapi/Kalapi.ttf', name='Kalapi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-bengali/Lohit-Bengali.ttf', name='Lohit Bengali', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi-Bold.ttf', name='Norasi', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,323 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Garuda.ttf', name='Garuda', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeMonoOblique.ttf', name='FreeMono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/malayalam/Manjari-Regular.otf', name='Manjari', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf', name='Liberation Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstOffice.ttf', name='KacstOffice', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstPen.ttf', name='KacstPen', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-gujr-extra/aakar-medium.ttf', name='aakar', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ttf-khmeros-core/KhmerOSsys.ttf', name='Khmer OS System', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Uroob.ttf', name='Uroob', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Laksaman-BoldItalic.ttf', name='Laksaman', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationMono-Regular.ttf', name='Liberation Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-RI.ttf', name='Ubuntu Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,324 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstNaskh.ttf', name='KacstNaskh', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Sahadeva/sahadeva.ttf', name='Sahadeva', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeMonoBold.ttf', name='FreeMono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-gujr-extra/padmaa-Bold.1.1.ttf', name='padmaa-Bold.1.1', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypist.ttf', name='Tlwg Typist', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-M.ttf', name='Ubuntu', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Waree.ttf', name='Waree', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSerif-Italic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Chilanka-Regular.ttf', name='Chilanka', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/abyssinica/AbyssinicaSIL-R.ttf', name='Abyssinica SIL', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypewriter.ttf', name='Tlwg Typewriter', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSans-BoldItalic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSansBoldOblique.ttf', name='FreeSans', style='oblique', variant='normal', weight=600, stretch='normal', size='scalable')) = 11.24
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/malayalam/Manjari-Bold.otf', name='Manjari', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,325 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Purisa.ttf', name='Purisa', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-assamese/Lohit-Assamese.ttf', name='Lohit Assamese', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Loma-Oblique.ttf', name='Loma', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari-Italic.ttf', name='Kinnari', style='italic', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Loma-Bold.ttf', name='Loma', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstArt.ttf', name='KacstArt', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-gujr-extra/padmaa.ttf', name='padmaa', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-deva-extra/kalimati.ttf', name='Kalimati', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypist-Oblique.ttf', name='Tlwg Typist', style='oblique', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-telu-extra/vemana2000.ttf', name='Vemana2000', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-BI.ttf', name='Ubuntu', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush-BoldOblique.ttf', name='Umpush', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/ani.ttf', name='Ani', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,326 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-tamil-classical/Lohit-Tamil-Classical.ttf', name='Lohit Tamil Classical', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSans.ttf', name='FreeSans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSerif.ttf', name='FreeSerif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ttf-khmeros-core/KhmerOS.ttf', name='Khmer OS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstDecorative.ttf', name='KacstDecorative', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-BoldItalic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-gujr-extra/padmaa-Medium-0.5.ttf', name='padmaa', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypist-Bold.ttf', name='Tlwg Typist', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Rachana-Bold.ttf', name='Rachana', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypewriter-Oblique.ttf', name='Tlwg Typewriter', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstLetter.ttf', name='KacstLetter', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Italic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-kannada/Lohit-Kannada.ttf', name='Lohit Kannada', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Sawasdee.ttf', name='Sawasdee', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,327 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-gujr-extra/Rekha.ttf', name='Rekha', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-tamil/Lohit-Tamil.ttf', name='Lohit Tamil', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSansBold.ttf', name='FreeSans', style='normal', variant='normal', weight=600, stretch='normal', size='scalable')) = 10.24
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush-Bold.ttf', name='Umpush', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari-Oblique.ttf', name='Kinnari', style='oblique', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Nakula/nakula.ttf', name='Nakula', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSerifBold.ttf', name='FreeSerif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-punjabi/Lohit-Gurmukhi.ttf', name='Lohit Gurmukhi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/noto/NotoSerifCJK-Bold.ttc', name='Noto Serif CJK JP', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Waree-BoldOblique.ttf', name='Waree', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypewriter-BoldOblique.ttf', name='Tlwg Typewriter', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/openoffice/opens___.ttf', name='OpenSymbol', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc', name='Noto Sans CJK JP', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-L.ttf', name='Ubuntu', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Garuda-BoldOblique.ttf', name='Garuda', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,328 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi-BoldItalic.ttf', name='Norasi', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi-BoldOblique.ttf', name='Norasi', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypo-Bold.ttf', name='Tlwg Typo', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Laksaman-Italic.ttf', name='Laksaman', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/RaghuMalayalamSans-Regular.ttf', name='RaghuMalayalam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgMono-Bold.ttf', name='Tlwg Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-guru-extra/Saab.ttf', name='Saab', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstTitleL.ttf', name='KacstTitleL', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush-Oblique.ttf', name='Umpush', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Laksaman-Bold.ttf', name='Laksaman', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstTitle.ttf', name='KacstTitle', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-C.ttf', name='Ubuntu Condensed', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 10.25
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Laksaman.ttf', name='Laksaman', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/pagul/Pagul.ttf', name='Pagul', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-R.ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,329 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/JamrulNormal.ttf', name='Jamrul', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/samyak-fonts/Samyak-Tamil.ttf', name='Samyak Tamil', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Sarai/Sarai.ttf', name='Sarai', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/MuktiNarrowBold.ttf', name='Mukti Narrow', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-malayalam/Lohit-Malayalam.ttf', name='Lohit Malayalam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lao/Phetsarath_OT.ttf', name='Phetsarath OT', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgMono-Oblique.ttf', name='Tlwg Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Purisa-Bold.ttf', name='Purisa', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Bold.ttf', name='Liberation Sans Narrow', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/padauk/Padauk-Bold.ttf', name='Padauk', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='condensed', size='scalable')) = 1.535
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-deva-extra/chandas1-2.ttf', name='Chandas', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSans-Regular.ttf', name='Liberation Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari-BoldOblique.ttf', name='Kinnari', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-ExtraLight.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=200, stretch='normal', size='scalable')) = 0.24
2023-06-22 15:00:03,330 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Meera.ttf', name='Meera', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,331 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstPoster.ttf', name='KacstPoster', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,331 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05
2023-06-22 15:00:03,331 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSerifItalic.ttf', name='FreeSerif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,331 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi-Italic.ttf', name='Norasi', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,331 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Sawasdee-Bold.ttf', name='Sawasdee', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,331 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Suruma.ttf', name='Suruma', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,331 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari.ttf', name='Kinnari', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,331 - DEBUG - findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.
2023-06-22 15:00:03,342 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:00:03,354 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:00:03,365 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:00:03,376 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:00:03,387 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:00:03,398 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:00:03,400 - WARNING - Finished tracing + transforming _unstack for pjit in 0.0011932849884033203 sec
2023-06-22 15:00:03,401 - DEBUG - Compiling _unstack for with global shapes and types [ShapedArray(float32[3,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:00:03,404 - WARNING - Finished jaxpr to MLIR module conversion jit(_unstack) in 0.0027539730072021484 sec
2023-06-22 15:00:03,404 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:03,423 - WARNING - Finished XLA compilation of jit(_unstack) in 0.01883673667907715 sec
2023-06-22 15:00:03,438 - WARNING - Finished tracing + transforming jit(transpose) in 0.000339508056640625 sec
2023-06-22 15:00:03,438 - DEBUG - Compiling transpose for with global shapes and types [ShapedArray(float32[3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:00:03,440 - WARNING - Finished jaxpr to MLIR module conversion jit(transpose) in 0.0017414093017578125 sec
2023-06-22 15:00:03,440 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:03,453 - WARNING - Finished XLA compilation of jit(transpose) in 0.012080907821655273 sec
2023-06-22 15:00:03,454 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0003342628479003906 sec
2023-06-22 15:00:03,455 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:00:03,468 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:00:03,480 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:00:03,784 - DEBUG - findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=16.0.
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerifDisplay.ttf', name='DejaVu Serif Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymReg.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymReg.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf', name='cmtt10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBolIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymReg.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBol.ttf', name='STIXGeneral', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFiveSymReg.ttf', name='STIXSizeFiveSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,785 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBolIta.ttf', name='STIXGeneral', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneral.ttf', name='STIXGeneral', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmmi10.ttf', name='cmmi10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymReg.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmss10.ttf', name='cmss10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymBol.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymBol.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymBol.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansDisplay.ttf', name='DejaVu Sans Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmex10.ttf', name='cmex10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmb10.ttf', name='cmb10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBol.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05
2023-06-22 15:00:03,786 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralItalic.ttf', name='STIXGeneral', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUni.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmr10.ttf', name='cmr10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymBol.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmsy10.ttf', name='cmsy10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSerifBoldItalic.ttf', name='FreeSerif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Sawasdee-Oblique.ttf', name='Sawasdee', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,787 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSerif-Bold.ttf', name='Liberation Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypo-Oblique.ttf', name='Tlwg Typo', style='oblique', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/LikhanNormal.ttf', name='Likhan', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-R.ttf', name='Ubuntu Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-gujarati/Lohit-Gujarati.ttf', name='Lohit Gujarati', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypo.ttf', name='Tlwg Typo', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Garuda-Bold.ttf', name='Garuda', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst-one/KacstOne.ttf', name='KacstOne', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/padauk/Padauk-Regular.ttf', name='Padauk', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 0.5349999999999999
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Rachana-Regular.ttf', name='Rachana', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuMathTeXGyre.ttf', name='DejaVu Math TeX Gyre', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf', name='Liberation Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/padauk/PadaukBook-Bold.ttf', name='Padauk Book', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Italic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstScreen.ttf', name='KacstScreen', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-LI.ttf', name='Ubuntu', style='italic', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/samyak-fonts/Samyak-Gujarati.ttf', name='Samyak Gujarati', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Karumbi.ttf', name='Karumbi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstQurn.ttf', name='KacstQurn', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Gubbi/Gubbi.ttf', name='Gubbi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Waree-Bold.ttf', name='Waree', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Loma-BoldOblique.ttf', name='Loma', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-BoldItalic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tibetan-machine/TibetanMachineUni.ttf', name='Tibetan Machine Uni', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/sinhala/lklug.ttf', name='LKLUG', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypewriter-Bold.ttf', name='Tlwg Typewriter', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 0.25
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationMono-Italic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoMono-Regular.ttf', name='Noto Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/samyak/Samyak-Devanagari.ttf', name='Samyak Devanagari', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-B.ttf', name='Ubuntu', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-BI.ttf', name='Ubuntu Mono', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-deva-extra/samanata.ttf', name='Samanata', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypo-BoldOblique.ttf', name='Tlwg Typo', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-oriya/Lohit-Odia.ttf', name='Lohit Odia', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc', name='Noto Sans CJK JP', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush-LightOblique.ttf', name='Umpush', style='oblique', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-BoldItalic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/malayalam/Manjari-Thin.otf', name='Manjari', style='normal', variant='normal', weight=100, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-MI.ttf', name='Ubuntu', style='italic', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari-Bold.ttf', name='Kinnari', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-orya-extra/utkal.ttf', name='ori1Uni', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Italic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush.ttf', name='Umpush', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeMono.ttf', name='FreeMono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/mry_KacstQurn.ttf', name='mry_KacstQurn', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-telu-extra/Pothana2000.ttf', name='Pothana2000', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Waree-Oblique.ttf', name='Waree', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstBook.ttf', name='KacstBook', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationMono-Bold.ttf', name='Liberation Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationMono-BoldItalic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSerif-Regular.ttf', name='Liberation Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi-Oblique.ttf', name='Norasi', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgMono-BoldOblique.ttf', name='Tlwg Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-devanagari/Lohit-Devanagari.ttf', name='Lohit Devanagari', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/AnjaliOldLipi.ttf', name='AnjaliOldLipi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', name='Liberation Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-B.ttf', name='Ubuntu Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeMonoBoldOblique.ttf', name='FreeMono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Dyuthi.ttf', name='Dyuthi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Garuda-Oblique.ttf', name='Garuda', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSans-Bold.ttf', name='Liberation Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc', name='Noto Serif CJK JP', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSans-Italic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', name='Liberation Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgMono.ttf', name='Tlwg Mono', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 10.25
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-telugu/Lohit-Telugu.ttf', name='Lohit Telugu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Keraleeyam.ttf', name='Keraleeyam', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf', name='Droid Sans Fallback', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush-Light.ttf', name='Umpush', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Italic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25
2023-06-22 15:00:03,792 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstDigital.ttf', name='KacstDigital', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/MuktiNarrow.ttf', name='Mukti Narrow', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-RI.ttf', name='Ubuntu', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf', name='Liberation Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/padauk/PadaukBook-Regular.ttf', name='Padauk Book', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Navilu/Navilu.ttf', name='Navilu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Purisa-Oblique.ttf', name='Purisa', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Loma.ttf', name='Loma', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstFarsi.ttf', name='KacstFarsi', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Sawasdee-BoldOblique.ttf', name='Sawasdee', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst-one/KacstOne-Bold.ttf', name='KacstOne', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/samyak-fonts/Samyak-Malayalam.ttf', name='Samyak Malayalam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Gargi/Gargi.ttf', name='Gargi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='condensed', size='scalable')) = 1.25
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25
2023-06-22 15:00:03,793 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf', name='Liberation Sans Narrow', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 10.25
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi.ttf', name='Norasi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypist-BoldOblique.ttf', name='Tlwg Typist', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSerif-BoldItalic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf', name='Liberation Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSansOblique.ttf', name='FreeSans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/mitra.ttf', name='Mitra Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari-BoldItalic.ttf', name='Kinnari', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Purisa-BoldOblique.ttf', name='Purisa', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-kalapi/Kalapi.ttf', name='Kalapi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-bengali/Lohit-Bengali.ttf', name='Lohit Bengali', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi-Bold.ttf', name='Norasi', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Garuda.ttf', name='Garuda', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeMonoOblique.ttf', name='FreeMono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,794 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/malayalam/Manjari-Regular.otf', name='Manjari', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf', name='Liberation Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstOffice.ttf', name='KacstOffice', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstPen.ttf', name='KacstPen', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-gujr-extra/aakar-medium.ttf', name='aakar', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ttf-khmeros-core/KhmerOSsys.ttf', name='Khmer OS System', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Uroob.ttf', name='Uroob', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Laksaman-BoldItalic.ttf', name='Laksaman', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationMono-Regular.ttf', name='Liberation Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-RI.ttf', name='Ubuntu Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstNaskh.ttf', name='KacstNaskh', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Sahadeva/sahadeva.ttf', name='Sahadeva', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeMonoBold.ttf', name='FreeMono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,795 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-gujr-extra/padmaa-Bold.1.1.ttf', name='padmaa-Bold.1.1', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypist.ttf', name='Tlwg Typist', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-M.ttf', name='Ubuntu', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Waree.ttf', name='Waree', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSerif-Italic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Chilanka-Regular.ttf', name='Chilanka', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/abyssinica/AbyssinicaSIL-R.ttf', name='Abyssinica SIL', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypewriter.ttf', name='Tlwg Typewriter', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSans-BoldItalic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSansBoldOblique.ttf', name='FreeSans', style='oblique', variant='normal', weight=600, stretch='normal', size='scalable')) = 11.24
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/malayalam/Manjari-Bold.otf', name='Manjari', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Purisa.ttf', name='Purisa', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-assamese/Lohit-Assamese.ttf', name='Lohit Assamese', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Loma-Oblique.ttf', name='Loma', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari-Italic.ttf', name='Kinnari', style='italic', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Loma-Bold.ttf', name='Loma', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstArt.ttf', name='KacstArt', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-gujr-extra/padmaa.ttf', name='padmaa', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-deva-extra/kalimati.ttf', name='Kalimati', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypist-Oblique.ttf', name='Tlwg Typist', style='oblique', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-telu-extra/vemana2000.ttf', name='Vemana2000', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-BI.ttf', name='Ubuntu', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush-BoldOblique.ttf', name='Umpush', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/ani.ttf', name='Ani', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-tamil-classical/Lohit-Tamil-Classical.ttf', name='Lohit Tamil Classical', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,797 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSans.ttf', name='FreeSans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSerif.ttf', name='FreeSerif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ttf-khmeros-core/KhmerOS.ttf', name='Khmer OS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstDecorative.ttf', name='KacstDecorative', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-BoldItalic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-gujr-extra/padmaa-Medium-0.5.ttf', name='padmaa', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypist-Bold.ttf', name='Tlwg Typist', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Rachana-Bold.ttf', name='Rachana', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypewriter-Oblique.ttf', name='Tlwg Typewriter', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstLetter.ttf', name='KacstLetter', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Italic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-kannada/Lohit-Kannada.ttf', name='Lohit Kannada', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Sawasdee.ttf', name='Sawasdee', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-gujr-extra/Rekha.ttf', name='Rekha', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-tamil/Lohit-Tamil.ttf', name='Lohit Tamil', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSansBold.ttf', name='FreeSans', style='normal', variant='normal', weight=600, stretch='normal', size='scalable')) = 10.24
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush-Bold.ttf', name='Umpush', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari-Oblique.ttf', name='Kinnari', style='oblique', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Nakula/nakula.ttf', name='Nakula', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSerifBold.ttf', name='FreeSerif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-punjabi/Lohit-Gurmukhi.ttf', name='Lohit Gurmukhi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/noto/NotoSerifCJK-Bold.ttc', name='Noto Serif CJK JP', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Waree-BoldOblique.ttf', name='Waree', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypewriter-BoldOblique.ttf', name='Tlwg Typewriter', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/openoffice/opens___.ttf', name='OpenSymbol', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc', name='Noto Sans CJK JP', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-L.ttf', name='Ubuntu', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Garuda-BoldOblique.ttf', name='Garuda', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi-BoldItalic.ttf', name='Norasi', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi-BoldOblique.ttf', name='Norasi', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgTypo-Bold.ttf', name='Tlwg Typo', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Laksaman-Italic.ttf', name='Laksaman', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/RaghuMalayalamSans-Regular.ttf', name='RaghuMalayalam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgMono-Bold.ttf', name='Tlwg Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-guru-extra/Saab.ttf', name='Saab', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstTitleL.ttf', name='KacstTitleL', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Umpush-Oblique.ttf', name='Umpush', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Laksaman-Bold.ttf', name='Laksaman', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstTitle.ttf', name='KacstTitle', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-C.ttf', name='Ubuntu Condensed', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 10.25
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Laksaman.ttf', name='Laksaman', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/pagul/Pagul.ttf', name='Pagul', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-R.ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/JamrulNormal.ttf', name='Jamrul', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/samyak-fonts/Samyak-Tamil.ttf', name='Samyak Tamil', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/Sarai/Sarai.ttf', name='Sarai', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-beng-extra/MuktiNarrowBold.ttf', name='Mukti Narrow', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lohit-malayalam/Lohit-Malayalam.ttf', name='Lohit Malayalam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/lao/Phetsarath_OT.ttf', name='Phetsarath OT', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/TlwgMono-Oblique.ttf', name='Tlwg Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Purisa-Bold.ttf', name='Purisa', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Bold.ttf', name='Liberation Sans Narrow', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/padauk/Padauk-Bold.ttf', name='Padauk', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='condensed', size='scalable')) = 1.535
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/fonts-deva-extra/chandas1-2.ttf', name='Chandas', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation2/LiberationSans-Regular.ttf', name='Liberation Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari-BoldOblique.ttf', name='Kinnari', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-06-22 15:00:03,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-ExtraLight.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=200, stretch='normal', size='scalable')) = 0.24
2023-06-22 15:00:03,802 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Meera.ttf', name='Meera', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-06-22 15:00:03,802 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/kacst/KacstPoster.ttf', name='KacstPoster', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,802 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05
2023-06-22 15:00:03,802 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/freefont/FreeSerifItalic.ttf', name='FreeSerif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,802 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Norasi-Italic.ttf', name='Norasi', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-06-22 15:00:03,802 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Sawasdee-Bold.ttf', name='Sawasdee', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-06-22 15:00:03,802 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/malayalam/Suruma.ttf', name='Suruma', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,802 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/tlwg/Kinnari.ttf', name='Kinnari', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2023-06-22 15:00:03,802 - DEBUG - findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=16.0 to DejaVu Sans ('/mnt/disk1/hieunm/anaconda3/envs/duy/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.
2023-06-22 15:00:04,115 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client00//synthetic.png
2023-06-22 15:00:04,134 - INFO - c: 3.0 and total_data_in_this_class: 265
2023-06-22 15:00:04,134 - INFO - c: 4.0 and total_data_in_this_class: 270
2023-06-22 15:00:04,134 - INFO - c: 7.0 and total_data_in_this_class: 264
2023-06-22 15:00:04,134 - INFO - c: 3.0 and total_data_in_this_class: 68
2023-06-22 15:00:04,134 - INFO - c: 4.0 and total_data_in_this_class: 63
2023-06-22 15:00:04,134 - INFO - c: 7.0 and total_data_in_this_class: 69
2023-06-22 15:00:04,179 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00038433074951171875 sec
2023-06-22 15:00:04,179 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:00:04,181 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001987934112548828 sec
2023-06-22 15:00:04,182 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:04,204 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.02217578887939453 sec
2023-06-22 15:00:04,209 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00034546852111816406 sec
2023-06-22 15:00:04,209 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:00:04,211 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0017650127410888672 sec
2023-06-22 15:00:04,211 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:04,223 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011434316635131836 sec
2023-06-22 15:00:04,228 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00023174285888671875 sec
2023-06-22 15:00:04,230 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022363662719726562 sec
2023-06-22 15:00:04,231 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005114078521728516 sec
2023-06-22 15:00:04,233 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021505355834960938 sec
2023-06-22 15:00:04,234 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019431114196777344 sec
2023-06-22 15:00:04,235 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00049591064453125 sec
2023-06-22 15:00:04,236 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042128562927246094 sec
2023-06-22 15:00:04,237 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003075599670410156 sec
2023-06-22 15:00:04,238 - WARNING - Finished tracing + transforming fn for pjit in 0.0004696846008300781 sec
2023-06-22 15:00:04,240 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005860328674316406 sec
2023-06-22 15:00:04,241 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003635883331298828 sec
2023-06-22 15:00:04,243 - WARNING - Finished tracing + transforming fn for pjit in 0.0004150867462158203 sec
2023-06-22 15:00:04,244 - WARNING - Finished tracing + transforming fn for pjit in 0.00045943260192871094 sec
2023-06-22 15:00:04,245 - WARNING - Finished tracing + transforming fn for pjit in 0.00039315223693847656 sec
2023-06-22 15:00:04,246 - WARNING - Finished tracing + transforming fn for pjit in 0.0004565715789794922 sec
2023-06-22 15:00:04,248 - WARNING - Finished tracing + transforming fn for pjit in 0.0003559589385986328 sec
2023-06-22 15:00:04,252 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00025200843811035156 sec
2023-06-22 15:00:04,253 - WARNING - Finished tracing + transforming fn for pjit in 0.0003955364227294922 sec
2023-06-22 15:00:04,255 - WARNING - Finished tracing + transforming fn for pjit in 0.00043582916259765625 sec
2023-06-22 15:00:04,261 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005481243133544922 sec
2023-06-22 15:00:04,261 - WARNING - Finished tracing + transforming _mean for pjit in 0.0015976428985595703 sec
2023-06-22 15:00:04,263 - WARNING - Finished tracing + transforming fn for pjit in 0.00040984153747558594 sec
2023-06-22 15:00:04,264 - WARNING - Finished tracing + transforming fn for pjit in 0.0003933906555175781 sec
2023-06-22 15:00:04,266 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039005279541015625 sec
2023-06-22 15:00:04,267 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00048041343688964844 sec
2023-06-22 15:00:04,268 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003170967102050781 sec
2023-06-22 15:00:04,270 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004634857177734375 sec
2023-06-22 15:00:04,271 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004138946533203125 sec
2023-06-22 15:00:04,272 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004000663757324219 sec
2023-06-22 15:00:04,274 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006017684936523438 sec
2023-06-22 15:00:04,274 - WARNING - Finished tracing + transforming _where for pjit in 0.0016579627990722656 sec
2023-06-22 15:00:04,276 - WARNING - Finished tracing + transforming fn for pjit in 0.0004646778106689453 sec
2023-06-22 15:00:04,277 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004527568817138672 sec
2023-06-22 15:00:04,278 - WARNING - Finished tracing + transforming fn for pjit in 0.0003952980041503906 sec
2023-06-22 15:00:04,279 - WARNING - Finished tracing + transforming fn for pjit in 0.00040340423583984375 sec
2023-06-22 15:00:04,281 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039315223693847656 sec
2023-06-22 15:00:04,282 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004553794860839844 sec
2023-06-22 15:00:04,283 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004558563232421875 sec
2023-06-22 15:00:04,284 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004620552062988281 sec
2023-06-22 15:00:04,286 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004012584686279297 sec
2023-06-22 15:00:04,287 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004074573516845703 sec
2023-06-22 15:00:04,288 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004589557647705078 sec
2023-06-22 15:00:04,289 - WARNING - Finished tracing + transforming _where for pjit in 0.0014963150024414062 sec
2023-06-22 15:00:04,290 - WARNING - Finished tracing + transforming fn for pjit in 0.0004680156707763672 sec
2023-06-22 15:00:04,291 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000457763671875 sec
2023-06-22 15:00:04,293 - WARNING - Finished tracing + transforming fn for pjit in 0.00038623809814453125 sec
2023-06-22 15:00:04,300 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004496574401855469 sec
2023-06-22 15:00:04,302 - WARNING - Finished tracing + transforming fn for pjit in 0.0006144046783447266 sec
2023-06-22 15:00:04,303 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004911422729492188 sec
2023-06-22 15:00:04,304 - WARNING - Finished tracing + transforming fn for pjit in 0.0003972053527832031 sec
2023-06-22 15:00:04,311 - WARNING - Finished tracing + transforming fn for pjit in 0.0003428459167480469 sec
2023-06-22 15:00:04,314 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00028061866760253906 sec
2023-06-22 15:00:04,315 - WARNING - Finished tracing + transforming fn for pjit in 0.0005557537078857422 sec
2023-06-22 15:00:04,316 - WARNING - Finished tracing + transforming fn for pjit in 0.0004048347473144531 sec
2023-06-22 15:00:04,344 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.116790771484375 sec
2023-06-22 15:00:04,349 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00018906593322753906 sec
2023-06-22 15:00:04,350 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002052783966064453 sec
2023-06-22 15:00:04,351 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004858970642089844 sec
2023-06-22 15:00:04,354 - WARNING - Finished tracing + transforming fn for pjit in 0.0003368854522705078 sec
2023-06-22 15:00:04,356 - WARNING - Finished tracing + transforming fn for pjit in 0.0004513263702392578 sec
2023-06-22 15:00:04,358 - WARNING - Finished tracing + transforming fn for pjit in 0.00036597251892089844 sec
2023-06-22 15:00:04,368 - WARNING - Finished tracing + transforming fn for pjit in 0.0003693103790283203 sec
2023-06-22 15:00:04,369 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000377655029296875 sec
2023-06-22 15:00:04,371 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004508495330810547 sec
2023-06-22 15:00:04,372 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003132820129394531 sec
2023-06-22 15:00:04,373 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005860328674316406 sec
2023-06-22 15:00:04,375 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042891502380371094 sec
2023-06-22 15:00:04,376 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000396728515625 sec
2023-06-22 15:00:04,377 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004687309265136719 sec
2023-06-22 15:00:04,378 - WARNING - Finished tracing + transforming _where for pjit in 0.0014998912811279297 sec
2023-06-22 15:00:04,379 - WARNING - Finished tracing + transforming fn for pjit in 0.0004754066467285156 sec
2023-06-22 15:00:04,380 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004482269287109375 sec
2023-06-22 15:00:04,382 - WARNING - Finished tracing + transforming fn for pjit in 0.00039839744567871094 sec
2023-06-22 15:00:04,383 - WARNING - Finished tracing + transforming fn for pjit in 0.0005276203155517578 sec
2023-06-22 15:00:04,402 - WARNING - Finished tracing + transforming fn for pjit in 0.0003390312194824219 sec
2023-06-22 15:00:04,434 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08835768699645996 sec
2023-06-22 15:00:04,436 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001990795135498047 sec
2023-06-22 15:00:04,438 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00021910667419433594 sec
2023-06-22 15:00:04,438 - WARNING - Finished tracing + transforming _where for pjit in 0.0011019706726074219 sec
2023-06-22 15:00:04,439 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005235671997070312 sec
2023-06-22 15:00:04,440 - WARNING - Finished tracing + transforming trace for pjit in 0.00439763069152832 sec
2023-06-22 15:00:04,444 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00019168853759765625 sec
2023-06-22 15:00:04,445 - WARNING - Finished tracing + transforming tril for pjit in 0.0010859966278076172 sec
2023-06-22 15:00:04,446 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0029954910278320312 sec
2023-06-22 15:00:04,447 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001976490020751953 sec
2023-06-22 15:00:04,448 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019478797912597656 sec
2023-06-22 15:00:04,452 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0023756027221679688 sec
2023-06-22 15:00:04,458 - WARNING - Finished tracing + transforming _solve for pjit in 0.015557050704956055 sec
2023-06-22 15:00:04,459 - WARNING - Finished tracing + transforming dot for pjit in 0.0005619525909423828 sec
2023-06-22 15:00:04,463 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.23818731307983398 sec
2023-06-22 15:00:04,466 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:00:04,521 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.054181575775146484 sec
2023-06-22 15:00:04,521 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:04,680 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.158095121383667 sec
2023-06-22 15:00:04,719 - INFO - initial test loss: 0.02936778542109523
2023-06-22 15:00:04,719 - INFO - initial test acc: 0.5550000071525574
2023-06-22 15:00:04,733 - WARNING - Finished tracing + transforming dot for pjit in 0.0008494853973388672 sec
2023-06-22 15:00:04,734 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006992816925048828 sec
2023-06-22 15:00:04,737 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006871223449707031 sec
2023-06-22 15:00:04,738 - WARNING - Finished tracing + transforming _mean for pjit in 0.0021986961364746094 sec
2023-06-22 15:00:04,740 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00045180320739746094 sec
2023-06-22 15:00:04,742 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0005688667297363281 sec
2023-06-22 15:00:04,743 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005955696105957031 sec
2023-06-22 15:00:04,745 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008397102355957031 sec
2023-06-22 15:00:04,746 - WARNING - Finished tracing + transforming _mean for pjit in 0.0025131702423095703 sec
2023-06-22 15:00:04,748 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.023172855377197266 sec
2023-06-22 15:00:04,773 - WARNING - Finished tracing + transforming fn for pjit in 0.0006077289581298828 sec
2023-06-22 15:00:04,774 - WARNING - Finished tracing + transforming fn for pjit in 0.0007684230804443359 sec
2023-06-22 15:00:04,776 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005271434783935547 sec
2023-06-22 15:00:04,778 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006194114685058594 sec
2023-06-22 15:00:04,779 - WARNING - Finished tracing + transforming _where for pjit in 0.0020875930786132812 sec
2023-06-22 15:00:04,798 - WARNING - Finished tracing + transforming fn for pjit in 0.0006117820739746094 sec
2023-06-22 15:00:04,800 - WARNING - Finished tracing + transforming fn for pjit in 0.0006277561187744141 sec
2023-06-22 15:00:04,801 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005185604095458984 sec
2023-06-22 15:00:04,803 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006096363067626953 sec
2023-06-22 15:00:04,804 - WARNING - Finished tracing + transforming _where for pjit in 0.002282857894897461 sec
2023-06-22 15:00:04,862 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00033926963806152344 sec
2023-06-22 15:00:04,945 - WARNING - Finished tracing + transforming fn for pjit in 0.0004398822784423828 sec
2023-06-22 15:00:04,947 - WARNING - Finished tracing + transforming fn for pjit in 0.00043511390686035156 sec
2023-06-22 15:00:04,948 - WARNING - Finished tracing + transforming square for pjit in 0.0003108978271484375 sec
2023-06-22 15:00:04,951 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00038933753967285156 sec
2023-06-22 15:00:04,954 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042819976806640625 sec
2023-06-22 15:00:04,955 - WARNING - Finished tracing + transforming fn for pjit in 0.00048470497131347656 sec
2023-06-22 15:00:04,956 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003972053527832031 sec
2023-06-22 15:00:04,957 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004131793975830078 sec
2023-06-22 15:00:04,958 - WARNING - Finished tracing + transforming fn for pjit in 0.0004799365997314453 sec
2023-06-22 15:00:04,960 - WARNING - Finished tracing + transforming fn for pjit in 0.00042510032653808594 sec
2023-06-22 15:00:04,961 - WARNING - Finished tracing + transforming square for pjit in 0.0003116130828857422 sec
2023-06-22 15:00:04,964 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004067420959472656 sec
2023-06-22 15:00:04,967 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00030040740966796875 sec
2023-06-22 15:00:04,968 - WARNING - Finished tracing + transforming fn for pjit in 0.0004794597625732422 sec
2023-06-22 15:00:04,969 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00037932395935058594 sec
2023-06-22 15:00:04,970 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003895759582519531 sec
2023-06-22 15:00:04,972 - WARNING - Finished tracing + transforming update_fn for pjit in 0.24786090850830078 sec
2023-06-22 15:00:04,977 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[528,10]), ShapedArray(float32[528,10]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:00:05,078 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10014986991882324 sec
2023-06-22 15:00:05,078 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:00:05,450 - WARNING - Finished XLA compilation of jit(update_fn) in 0.37212610244750977 sec
2023-06-22 15:00:08,320 - INFO - Distilling data from client: Client01
2023-06-22 15:00:08,321 - INFO - train loss: 0.002613988946390462
2023-06-22 15:00:08,321 - INFO - train acc: 0.9943181872367859
2023-06-22 15:00:08,522 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.59      0.61        68
           4       0.53      0.65      0.59        63
           7       0.67      0.58      0.62        69

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.60      0.61       200

2023-06-22 15:00:08,522 - INFO - test loss 0.02580014574655744
2023-06-22 15:00:08,523 - INFO - test acc 0.6049999594688416
2023-06-22 15:00:12,603 - INFO - Distilling data from client: Client01
2023-06-22 15:00:12,604 - INFO - train loss: 0.0014928603760217458
2023-06-22 15:00:12,604 - INFO - train acc: 1.0
2023-06-22 15:00:12,662 - INFO - report:               precision    recall  f1-score   support

           3       0.64      0.60      0.62        68
           4       0.53      0.63      0.58        63
           7       0.61      0.54      0.57        69

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-06-22 15:00:12,663 - INFO - test loss 0.025944434476197095
2023-06-22 15:00:12,663 - INFO - test acc 0.5899999737739563
2023-06-22 15:00:15,641 - INFO - Distilling data from client: Client01
2023-06-22 15:00:15,641 - INFO - train loss: 0.0010346933390165624
2023-06-22 15:00:15,641 - INFO - train acc: 1.0
2023-06-22 15:00:15,690 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.57      0.60        68
           4       0.50      0.62      0.55        63
           7       0.61      0.52      0.56        69

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-06-22 15:00:15,691 - INFO - test loss 0.02680864379159585
2023-06-22 15:00:15,691 - INFO - test acc 0.5699999928474426
2023-06-22 15:00:18,538 - INFO - Distilling data from client: Client01
2023-06-22 15:00:18,539 - INFO - train loss: 0.0007527850900840049
2023-06-22 15:00:18,539 - INFO - train acc: 1.0
2023-06-22 15:00:18,597 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.56      0.59        68
           4       0.54      0.68      0.60        63
           7       0.67      0.58      0.62        69

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.60      0.61       200

2023-06-22 15:00:18,598 - INFO - test loss 0.02627580422330128
2023-06-22 15:00:18,598 - INFO - test acc 0.6049999594688416
2023-06-22 15:00:21,577 - INFO - Distilling data from client: Client01
2023-06-22 15:00:21,578 - INFO - train loss: 0.0007788852960696907
2023-06-22 15:00:21,578 - INFO - train acc: 1.0
2023-06-22 15:00:21,635 - INFO - report:               precision    recall  f1-score   support

           3       0.66      0.60      0.63        68
           4       0.54      0.63      0.58        63
           7       0.62      0.58      0.60        69

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.60      0.61       200

2023-06-22 15:00:21,636 - INFO - test loss 0.02626735027527299
2023-06-22 15:00:21,636 - INFO - test acc 0.6049999594688416
2023-06-22 15:00:24,455 - INFO - Distilling data from client: Client01
2023-06-22 15:00:24,456 - INFO - train loss: 0.0007619908056406066
2023-06-22 15:00:24,457 - INFO - train acc: 1.0
2023-06-22 15:00:24,500 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.62      0.62        68
           4       0.53      0.63      0.58        63
           7       0.65      0.54      0.59        69

    accuracy                           0.59       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.59      0.60       200

2023-06-22 15:00:24,508 - INFO - test loss 0.026780668449345534
2023-06-22 15:00:24,508 - INFO - test acc 0.5949999690055847
2023-06-22 15:00:27,458 - INFO - Distilling data from client: Client01
2023-06-22 15:00:27,459 - INFO - train loss: 0.000604599818109148
2023-06-22 15:00:27,459 - INFO - train acc: 1.0
2023-06-22 15:00:27,516 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.62      0.62        68
           4       0.54      0.63      0.58        63
           7       0.66      0.55      0.60        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:00:27,516 - INFO - test loss 0.02677355132027881
2023-06-22 15:00:27,517 - INFO - test acc 0.5999999642372131
2023-06-22 15:00:30,568 - INFO - Distilling data from client: Client01
2023-06-22 15:00:30,569 - INFO - train loss: 0.000456386800395759
2023-06-22 15:00:30,569 - INFO - train acc: 1.0
2023-06-22 15:00:30,631 - INFO - report:               precision    recall  f1-score   support

           3       0.58      0.53      0.55        68
           4       0.50      0.62      0.55        63
           7       0.62      0.54      0.57        69

    accuracy                           0.56       200
   macro avg       0.57      0.56      0.56       200
weighted avg       0.57      0.56      0.56       200

2023-06-22 15:00:30,632 - INFO - test loss 0.0272415973387348
2023-06-22 15:00:30,632 - INFO - test acc 0.5600000023841858
2023-06-22 15:00:33,632 - INFO - Distilling data from client: Client01
2023-06-22 15:00:33,633 - INFO - train loss: 0.0006246013991394074
2023-06-22 15:00:33,634 - INFO - train acc: 1.0
2023-06-22 15:00:33,679 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.57      0.60        68
           4       0.54      0.63      0.58        63
           7       0.62      0.58      0.60        69

    accuracy                           0.59       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.59      0.60       200

2023-06-22 15:00:33,686 - INFO - test loss 0.027530172385305973
2023-06-22 15:00:33,687 - INFO - test acc 0.5949999690055847
2023-06-22 15:00:36,660 - INFO - Distilling data from client: Client01
2023-06-22 15:00:36,661 - INFO - train loss: 0.0004090219389456734
2023-06-22 15:00:36,662 - INFO - train acc: 1.0
2023-06-22 15:00:36,723 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.59      0.60        68
           4       0.57      0.67      0.61        63
           7       0.64      0.57      0.60        69

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:00:36,731 - INFO - test loss 0.026629298848862917
2023-06-22 15:00:36,732 - INFO - test acc 0.6049999594688416
2023-06-22 15:00:39,586 - INFO - Distilling data from client: Client01
2023-06-22 15:00:39,587 - INFO - train loss: 0.0005335326680400805
2023-06-22 15:00:39,588 - INFO - train acc: 1.0
2023-06-22 15:00:39,790 - INFO - report:               precision    recall  f1-score   support

           3       0.64      0.62      0.63        68
           4       0.53      0.63      0.58        63
           7       0.68      0.58      0.63        69

    accuracy                           0.61       200
   macro avg       0.62      0.61      0.61       200
weighted avg       0.62      0.61      0.61       200

2023-06-22 15:00:39,790 - INFO - test loss 0.026455216281272714
2023-06-22 15:00:39,790 - INFO - test acc 0.6100000143051147
2023-06-22 15:00:42,680 - INFO - Distilling data from client: Client01
2023-06-22 15:00:42,681 - INFO - train loss: 0.00038719807738104407
2023-06-22 15:00:42,681 - INFO - train acc: 1.0
2023-06-22 15:00:42,739 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.56      0.59        68
           4       0.49      0.60      0.54        63
           7       0.62      0.55      0.58        69

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-06-22 15:00:42,745 - INFO - test loss 0.026829527788271745
2023-06-22 15:00:42,746 - INFO - test acc 0.5699999928474426
2023-06-22 15:00:45,576 - INFO - Distilling data from client: Client01
2023-06-22 15:00:45,577 - INFO - train loss: 0.0004468280574592187
2023-06-22 15:00:45,577 - INFO - train acc: 1.0
2023-06-22 15:00:45,635 - INFO - report:               precision    recall  f1-score   support

           3       0.65      0.62      0.63        68
           4       0.48      0.57      0.52        63
           7       0.63      0.55      0.59        69

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-06-22 15:00:45,636 - INFO - test loss 0.026905705153710123
2023-06-22 15:00:45,636 - INFO - test acc 0.5799999833106995
2023-06-22 15:00:48,690 - INFO - Distilling data from client: Client01
2023-06-22 15:00:48,690 - INFO - train loss: 0.0004249787888403253
2023-06-22 15:00:48,690 - INFO - train acc: 1.0
2023-06-22 15:00:48,753 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.59      0.60        68
           4       0.54      0.65      0.59        63
           7       0.66      0.57      0.61        69

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:00:48,757 - INFO - test loss 0.026723138435760817
2023-06-22 15:00:48,757 - INFO - test acc 0.5999999642372131
2023-06-22 15:00:51,877 - INFO - Distilling data from client: Client01
2023-06-22 15:00:51,877 - INFO - train loss: 0.0003313555703008345
2023-06-22 15:00:51,877 - INFO - train acc: 1.0
2023-06-22 15:00:51,942 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.59      0.61        68
           4       0.55      0.65      0.60        63
           7       0.61      0.55      0.58        69

    accuracy                           0.59       200
   macro avg       0.60      0.60      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-06-22 15:00:51,943 - INFO - test loss 0.027025802678914027
2023-06-22 15:00:51,944 - INFO - test acc 0.5949999690055847
2023-06-22 15:00:54,925 - INFO - Distilling data from client: Client01
2023-06-22 15:00:54,925 - INFO - train loss: 0.0003027163489014357
2023-06-22 15:00:54,925 - INFO - train acc: 1.0
2023-06-22 15:00:54,972 - INFO - report:               precision    recall  f1-score   support

           3       0.64      0.60      0.62        68
           4       0.53      0.65      0.59        63
           7       0.68      0.58      0.63        69

    accuracy                           0.61       200
   macro avg       0.62      0.61      0.61       200
weighted avg       0.62      0.61      0.61       200

2023-06-22 15:00:54,975 - INFO - test loss 0.02734082515167303
2023-06-22 15:00:54,976 - INFO - test acc 0.6100000143051147
2023-06-22 15:00:57,925 - INFO - Distilling data from client: Client01
2023-06-22 15:00:57,925 - INFO - train loss: 0.0003663262792786491
2023-06-22 15:00:57,925 - INFO - train acc: 1.0
2023-06-22 15:00:57,993 - INFO - report:               precision    recall  f1-score   support

           3       0.61      0.60      0.61        68
           4       0.55      0.68      0.61        63
           7       0.67      0.54      0.60        69

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:00:57,994 - INFO - test loss 0.02695907697235732
2023-06-22 15:00:57,994 - INFO - test acc 0.6049999594688416
2023-06-22 15:01:01,182 - INFO - Distilling data from client: Client01
2023-06-22 15:01:01,182 - INFO - train loss: 0.00036666879756345935
2023-06-22 15:01:01,182 - INFO - train acc: 1.0
2023-06-22 15:01:01,255 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.59      0.61        68
           4       0.50      0.62      0.55        63
           7       0.63      0.54      0.58        69

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-06-22 15:01:01,256 - INFO - test loss 0.026803052624445996
2023-06-22 15:01:01,256 - INFO - test acc 0.5799999833106995
2023-06-22 15:01:04,193 - INFO - Distilling data from client: Client01
2023-06-22 15:01:04,194 - INFO - train loss: 0.0003493118168715737
2023-06-22 15:01:04,194 - INFO - train acc: 1.0
2023-06-22 15:01:04,245 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.63      0.63        68
           4       0.53      0.62      0.57        63
           7       0.64      0.55      0.59        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:01:04,246 - INFO - test loss 0.02730764512821186
2023-06-22 15:01:04,247 - INFO - test acc 0.5999999642372131
2023-06-22 15:01:07,187 - INFO - Distilling data from client: Client01
2023-06-22 15:01:07,187 - INFO - train loss: 0.0003518465236289641
2023-06-22 15:01:07,188 - INFO - train acc: 1.0
2023-06-22 15:01:07,238 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.59      0.61        68
           4       0.54      0.65      0.59        63
           7       0.61      0.54      0.57        69

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-06-22 15:01:07,239 - INFO - test loss 0.02696213181424716
2023-06-22 15:01:07,239 - INFO - test acc 0.5899999737739563
2023-06-22 15:01:10,184 - INFO - Distilling data from client: Client01
2023-06-22 15:01:10,184 - INFO - train loss: 0.00027797408892162663
2023-06-22 15:01:10,184 - INFO - train acc: 1.0
2023-06-22 15:01:10,236 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.57      0.60        68
           4       0.50      0.60      0.55        63
           7       0.61      0.55      0.58        69

    accuracy                           0.57       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.57      0.58       200

2023-06-22 15:01:10,237 - INFO - test loss 0.026804029517802388
2023-06-22 15:01:10,243 - INFO - test acc 0.574999988079071
2023-06-22 15:01:13,266 - INFO - Distilling data from client: Client01
2023-06-22 15:01:13,266 - INFO - train loss: 0.00022281724575220064
2023-06-22 15:01:13,266 - INFO - train acc: 1.0
2023-06-22 15:01:13,326 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.59      0.61        68
           4       0.53      0.63      0.58        63
           7       0.62      0.55      0.58        69

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-06-22 15:01:13,330 - INFO - test loss 0.026762271745441737
2023-06-22 15:01:13,331 - INFO - test acc 0.5899999737739563
2023-06-22 15:01:16,304 - INFO - Distilling data from client: Client01
2023-06-22 15:01:16,304 - INFO - train loss: 0.000260028758234059
2023-06-22 15:01:16,304 - INFO - train acc: 1.0
2023-06-22 15:01:16,371 - INFO - report:               precision    recall  f1-score   support

           3       0.61      0.54      0.57        68
           4       0.51      0.63      0.57        63
           7       0.62      0.55      0.58        69

    accuracy                           0.57       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.57      0.58       200

2023-06-22 15:01:16,371 - INFO - test loss 0.0270269522065796
2023-06-22 15:01:16,372 - INFO - test acc 0.574999988079071
2023-06-22 15:01:19,337 - INFO - Distilling data from client: Client01
2023-06-22 15:01:19,338 - INFO - train loss: 0.00018310241158510772
2023-06-22 15:01:19,338 - INFO - train acc: 1.0
2023-06-22 15:01:19,387 - INFO - report:               precision    recall  f1-score   support

           3       0.61      0.57      0.59        68
           4       0.53      0.62      0.57        63
           7       0.63      0.58      0.61        69

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.59      0.59      0.59       200

2023-06-22 15:01:19,388 - INFO - test loss 0.026932119427820467
2023-06-22 15:01:19,388 - INFO - test acc 0.5899999737739563
2023-06-22 15:01:22,444 - INFO - Distilling data from client: Client01
2023-06-22 15:01:22,444 - INFO - train loss: 0.00023844624801875838
2023-06-22 15:01:22,445 - INFO - train acc: 1.0
2023-06-22 15:01:22,501 - INFO - report:               precision    recall  f1-score   support

           3       0.58      0.56      0.57        68
           4       0.51      0.62      0.56        63
           7       0.66      0.57      0.61        69

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-06-22 15:01:22,502 - INFO - test loss 0.02682592092897953
2023-06-22 15:01:22,503 - INFO - test acc 0.5799999833106995
2023-06-22 15:01:22,515 - WARNING - Finished tracing + transforming jit(gather) in 0.0024673938751220703 sec
2023-06-22 15:01:22,517 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[528,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:01:22,524 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.006772279739379883 sec
2023-06-22 15:01:22,526 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:01:22,550 - WARNING - Finished XLA compilation of jit(gather) in 0.023003578186035156 sec
2023-06-22 15:01:22,573 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:22,592 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:22,607 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:22,618 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:22,629 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:22,640 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:22,654 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:22,666 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:22,677 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:23,233 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client01//synthetic.png
2023-06-22 15:01:23,252 - INFO - c: 4.0 and total_data_in_this_class: 19
2023-06-22 15:01:23,252 - INFO - c: 5.0 and total_data_in_this_class: 248
2023-06-22 15:01:23,252 - INFO - c: 6.0 and total_data_in_this_class: 29
2023-06-22 15:01:23,252 - INFO - c: 7.0 and total_data_in_this_class: 237
2023-06-22 15:01:23,252 - INFO - c: 9.0 and total_data_in_this_class: 266
2023-06-22 15:01:23,252 - INFO - c: 4.0 and total_data_in_this_class: 6
2023-06-22 15:01:23,253 - INFO - c: 5.0 and total_data_in_this_class: 60
2023-06-22 15:01:23,253 - INFO - c: 6.0 and total_data_in_this_class: 6
2023-06-22 15:01:23,253 - INFO - c: 7.0 and total_data_in_this_class: 61
2023-06-22 15:01:23,253 - INFO - c: 9.0 and total_data_in_this_class: 67
2023-06-22 15:01:23,288 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.000362396240234375 sec
2023-06-22 15:01:23,289 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:01:23,291 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0018818378448486328 sec
2023-06-22 15:01:23,291 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:01:23,306 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.014917850494384766 sec
2023-06-22 15:01:23,308 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00030350685119628906 sec
2023-06-22 15:01:23,309 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:01:23,311 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0015819072723388672 sec
2023-06-22 15:01:23,311 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:01:23,322 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011072874069213867 sec
2023-06-22 15:01:23,327 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002319812774658203 sec
2023-06-22 15:01:23,329 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001995563507080078 sec
2023-06-22 15:01:23,330 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005891323089599609 sec
2023-06-22 15:01:23,333 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003428459167480469 sec
2023-06-22 15:01:23,333 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001952648162841797 sec
2023-06-22 15:01:23,334 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005195140838623047 sec
2023-06-22 15:01:23,336 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039887428283691406 sec
2023-06-22 15:01:23,336 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003101825714111328 sec
2023-06-22 15:01:23,337 - WARNING - Finished tracing + transforming fn for pjit in 0.0004961490631103516 sec
2023-06-22 15:01:23,339 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005953311920166016 sec
2023-06-22 15:01:23,340 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003619194030761719 sec
2023-06-22 15:01:23,342 - WARNING - Finished tracing + transforming fn for pjit in 0.00042748451232910156 sec
2023-06-22 15:01:23,343 - WARNING - Finished tracing + transforming fn for pjit in 0.00046825408935546875 sec
2023-06-22 15:01:23,344 - WARNING - Finished tracing + transforming fn for pjit in 0.0003960132598876953 sec
2023-06-22 15:01:23,345 - WARNING - Finished tracing + transforming fn for pjit in 0.0004532337188720703 sec
2023-06-22 15:01:23,348 - WARNING - Finished tracing + transforming fn for pjit in 0.0003974437713623047 sec
2023-06-22 15:01:23,351 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003170967102050781 sec
2023-06-22 15:01:23,352 - WARNING - Finished tracing + transforming fn for pjit in 0.0003743171691894531 sec
2023-06-22 15:01:23,353 - WARNING - Finished tracing + transforming fn for pjit in 0.0004088878631591797 sec
2023-06-22 15:01:23,359 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005786418914794922 sec
2023-06-22 15:01:23,360 - WARNING - Finished tracing + transforming _mean for pjit in 0.0015866756439208984 sec
2023-06-22 15:01:23,361 - WARNING - Finished tracing + transforming fn for pjit in 0.00040459632873535156 sec
2023-06-22 15:01:23,363 - WARNING - Finished tracing + transforming fn for pjit in 0.00039076805114746094 sec
2023-06-22 15:01:23,364 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005052089691162109 sec
2023-06-22 15:01:23,365 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004563331604003906 sec
2023-06-22 15:01:23,366 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003151893615722656 sec
2023-06-22 15:01:23,368 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00048613548278808594 sec
2023-06-22 15:01:23,369 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040793418884277344 sec
2023-06-22 15:01:23,370 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040411949157714844 sec
2023-06-22 15:01:23,372 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006155967712402344 sec
2023-06-22 15:01:23,373 - WARNING - Finished tracing + transforming _where for pjit in 0.001714944839477539 sec
2023-06-22 15:01:23,374 - WARNING - Finished tracing + transforming fn for pjit in 0.0004642009735107422 sec
2023-06-22 15:01:23,375 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004596710205078125 sec
2023-06-22 15:01:23,377 - WARNING - Finished tracing + transforming fn for pjit in 0.0003943443298339844 sec
2023-06-22 15:01:23,378 - WARNING - Finished tracing + transforming fn for pjit in 0.0003993511199951172 sec
2023-06-22 15:01:23,379 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003886222839355469 sec
2023-06-22 15:01:23,380 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045228004455566406 sec
2023-06-22 15:01:23,383 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0015001296997070312 sec
2023-06-22 15:01:23,384 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00047326087951660156 sec
2023-06-22 15:01:23,385 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003974437713623047 sec
2023-06-22 15:01:23,386 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003924369812011719 sec
2023-06-22 15:01:23,388 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00045800209045410156 sec
2023-06-22 15:01:23,388 - WARNING - Finished tracing + transforming _where for pjit in 0.0014982223510742188 sec
2023-06-22 15:01:23,390 - WARNING - Finished tracing + transforming fn for pjit in 0.0004622936248779297 sec
2023-06-22 15:01:23,391 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046372413635253906 sec
2023-06-22 15:01:23,393 - WARNING - Finished tracing + transforming fn for pjit in 0.00041556358337402344 sec
2023-06-22 15:01:23,400 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004565715789794922 sec
2023-06-22 15:01:23,401 - WARNING - Finished tracing + transforming fn for pjit in 0.0005824565887451172 sec
2023-06-22 15:01:23,402 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004799365997314453 sec
2023-06-22 15:01:23,404 - WARNING - Finished tracing + transforming fn for pjit in 0.00039768218994140625 sec
2023-06-22 15:01:23,410 - WARNING - Finished tracing + transforming fn for pjit in 0.00033783912658691406 sec
2023-06-22 15:01:23,413 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00027441978454589844 sec
2023-06-22 15:01:23,414 - WARNING - Finished tracing + transforming fn for pjit in 0.0005412101745605469 sec
2023-06-22 15:01:23,415 - WARNING - Finished tracing + transforming fn for pjit in 0.00041294097900390625 sec
2023-06-22 15:01:23,443 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11635327339172363 sec
2023-06-22 15:01:23,448 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020170211791992188 sec
2023-06-22 15:01:23,449 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001983642578125 sec
2023-06-22 15:01:23,450 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004894733428955078 sec
2023-06-22 15:01:23,453 - WARNING - Finished tracing + transforming fn for pjit in 0.00033855438232421875 sec
2023-06-22 15:01:23,454 - WARNING - Finished tracing + transforming fn for pjit in 0.00042366981506347656 sec
2023-06-22 15:01:23,457 - WARNING - Finished tracing + transforming fn for pjit in 0.0003781318664550781 sec
2023-06-22 15:01:23,466 - WARNING - Finished tracing + transforming fn for pjit in 0.00034737586975097656 sec
2023-06-22 15:01:23,468 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037932395935058594 sec
2023-06-22 15:01:23,469 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045108795166015625 sec
2023-06-22 15:01:23,470 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031876564025878906 sec
2023-06-22 15:01:23,472 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005621910095214844 sec
2023-06-22 15:01:23,473 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041031837463378906 sec
2023-06-22 15:01:23,474 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003948211669921875 sec
2023-06-22 15:01:23,476 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00045752525329589844 sec
2023-06-22 15:01:23,476 - WARNING - Finished tracing + transforming _where for pjit in 0.0015304088592529297 sec
2023-06-22 15:01:23,478 - WARNING - Finished tracing + transforming fn for pjit in 0.00046634674072265625 sec
2023-06-22 15:01:23,479 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004858970642089844 sec
2023-06-22 15:01:23,480 - WARNING - Finished tracing + transforming fn for pjit in 0.00039958953857421875 sec
2023-06-22 15:01:23,481 - WARNING - Finished tracing + transforming fn for pjit in 0.0005195140838623047 sec
2023-06-22 15:01:23,501 - WARNING - Finished tracing + transforming fn for pjit in 0.00033855438232421875 sec
2023-06-22 15:01:23,531 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0871422290802002 sec
2023-06-22 15:01:23,533 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021600723266601562 sec
2023-06-22 15:01:23,535 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002200603485107422 sec
2023-06-22 15:01:23,536 - WARNING - Finished tracing + transforming _where for pjit in 0.0010254383087158203 sec
2023-06-22 15:01:23,537 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005085468292236328 sec
2023-06-22 15:01:23,537 - WARNING - Finished tracing + transforming trace for pjit in 0.004376649856567383 sec
2023-06-22 15:01:23,541 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0002143383026123047 sec
2023-06-22 15:01:23,543 - WARNING - Finished tracing + transforming tril for pjit in 0.001115560531616211 sec
2023-06-22 15:01:23,544 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.003026247024536133 sec
2023-06-22 15:01:23,545 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.000202178955078125 sec
2023-06-22 15:01:23,546 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001938343048095703 sec
2023-06-22 15:01:23,549 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0023407936096191406 sec
2023-06-22 15:01:23,555 - WARNING - Finished tracing + transforming _solve for pjit in 0.015513181686401367 sec
2023-06-22 15:01:23,556 - WARNING - Finished tracing + transforming dot for pjit in 0.0005028247833251953 sec
2023-06-22 15:01:23,561 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.23635220527648926 sec
2023-06-22 15:01:23,564 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:01:23,619 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05454826354980469 sec
2023-06-22 15:01:23,619 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:01:23,760 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.14114832878112793 sec
2023-06-22 15:01:23,769 - INFO - initial test loss: 0.03583813926087848
2023-06-22 15:01:23,769 - INFO - initial test acc: 0.4949999749660492
2023-06-22 15:01:23,779 - WARNING - Finished tracing + transforming dot for pjit in 0.0007958412170410156 sec
2023-06-22 15:01:23,781 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006515979766845703 sec
2023-06-22 15:01:23,783 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006837844848632812 sec
2023-06-22 15:01:23,784 - WARNING - Finished tracing + transforming _mean for pjit in 0.0020906925201416016 sec
2023-06-22 15:01:23,786 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00043702125549316406 sec
2023-06-22 15:01:23,787 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0005578994750976562 sec
2023-06-22 15:01:23,789 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005466938018798828 sec
2023-06-22 15:01:23,791 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008265972137451172 sec
2023-06-22 15:01:23,792 - WARNING - Finished tracing + transforming _mean for pjit in 0.002418994903564453 sec
2023-06-22 15:01:23,793 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.02147984504699707 sec
2023-06-22 15:01:23,813 - WARNING - Finished tracing + transforming fn for pjit in 0.0005977153778076172 sec
2023-06-22 15:01:23,815 - WARNING - Finished tracing + transforming fn for pjit in 0.0007596015930175781 sec
2023-06-22 15:01:23,816 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005238056182861328 sec
2023-06-22 15:01:23,818 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005979537963867188 sec
2023-06-22 15:01:23,819 - WARNING - Finished tracing + transforming _where for pjit in 0.0019326210021972656 sec
2023-06-22 15:01:23,837 - WARNING - Finished tracing + transforming fn for pjit in 0.0006031990051269531 sec
2023-06-22 15:01:23,839 - WARNING - Finished tracing + transforming fn for pjit in 0.0006237030029296875 sec
2023-06-22 15:01:23,840 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005092620849609375 sec
2023-06-22 15:01:23,842 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005812644958496094 sec
2023-06-22 15:01:23,843 - WARNING - Finished tracing + transforming _where for pjit in 0.0019245147705078125 sec
2023-06-22 15:01:23,908 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00035572052001953125 sec
2023-06-22 15:01:23,989 - WARNING - Finished tracing + transforming fn for pjit in 0.0004189014434814453 sec
2023-06-22 15:01:23,991 - WARNING - Finished tracing + transforming fn for pjit in 0.0003879070281982422 sec
2023-06-22 15:01:23,992 - WARNING - Finished tracing + transforming square for pjit in 0.0002899169921875 sec
2023-06-22 15:01:23,995 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004096031188964844 sec
2023-06-22 15:01:23,998 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042939186096191406 sec
2023-06-22 15:01:23,999 - WARNING - Finished tracing + transforming fn for pjit in 0.0004546642303466797 sec
2023-06-22 15:01:24,000 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00039458274841308594 sec
2023-06-22 15:01:24,001 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037217140197753906 sec
2023-06-22 15:01:24,002 - WARNING - Finished tracing + transforming fn for pjit in 0.0004737377166748047 sec
2023-06-22 15:01:24,003 - WARNING - Finished tracing + transforming fn for pjit in 0.0004093647003173828 sec
2023-06-22 15:01:24,004 - WARNING - Finished tracing + transforming square for pjit in 0.000308990478515625 sec
2023-06-22 15:01:24,008 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003943443298339844 sec
2023-06-22 15:01:24,010 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002980232238769531 sec
2023-06-22 15:01:24,011 - WARNING - Finished tracing + transforming fn for pjit in 0.0004839897155761719 sec
2023-06-22 15:01:24,012 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003981590270996094 sec
2023-06-22 15:01:24,013 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003726482391357422 sec
2023-06-22 15:01:24,015 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2445063591003418 sec
2023-06-22 15:01:24,021 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,10]), ShapedArray(float32[76,10]), ShapedArray(float32[76,10]), ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,10]), ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:01:24,122 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10055136680603027 sec
2023-06-22 15:01:24,122 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:01:24,439 - WARNING - Finished XLA compilation of jit(update_fn) in 0.31610918045043945 sec
2023-06-22 15:01:24,629 - INFO - Distilling data from client: Client02
2023-06-22 15:01:24,629 - INFO - train loss: 0.017491416538974642
2023-06-22 15:01:24,629 - INFO - train acc: 0.8026315569877625
2023-06-22 15:01:24,656 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.61      0.72      0.66        60
           6       0.18      0.50      0.26         6
           7       0.64      0.49      0.56        61
           9       0.71      0.70      0.71        67

    accuracy                           0.61       200
   macro avg       0.43      0.48      0.44       200
weighted avg       0.62      0.61      0.61       200

2023-06-22 15:01:24,656 - INFO - test loss 0.02801425453635977
2023-06-22 15:01:24,656 - INFO - test acc 0.6150000095367432
2023-06-22 15:01:24,854 - INFO - Distilling data from client: Client02
2023-06-22 15:01:24,855 - INFO - train loss: 0.017919014817660152
2023-06-22 15:01:24,855 - INFO - train acc: 0.7763158082962036
2023-06-22 15:01:24,868 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.67      0.64        60
           6       0.19      0.50      0.27         6
           7       0.64      0.49      0.56        61
           9       0.67      0.72      0.69        67

    accuracy                           0.60       200
   macro avg       0.42      0.47      0.43       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:01:24,869 - INFO - test loss 0.02745818769708493
2023-06-22 15:01:24,870 - INFO - test acc 0.6049999594688416
2023-06-22 15:01:25,062 - INFO - Distilling data from client: Client02
2023-06-22 15:01:25,062 - INFO - train loss: 0.016875570250395978
2023-06-22 15:01:25,063 - INFO - train acc: 0.8289473652839661
2023-06-22 15:01:25,092 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.63      0.70      0.66        60
           6       0.20      0.33      0.25         6
           7       0.64      0.57      0.60        61
           9       0.68      0.69      0.68        67

    accuracy                           0.62       200
   macro avg       0.43      0.46      0.44       200
weighted avg       0.61      0.62      0.62       200

2023-06-22 15:01:25,092 - INFO - test loss 0.027348360778850087
2023-06-22 15:01:25,092 - INFO - test acc 0.625
2023-06-22 15:01:25,353 - INFO - Distilling data from client: Client02
2023-06-22 15:01:25,355 - INFO - train loss: 0.015657944046604436
2023-06-22 15:01:25,356 - INFO - train acc: 0.8552631735801697
2023-06-22 15:01:25,399 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.61      0.72      0.66        60
           6       0.15      0.33      0.21         6
           7       0.70      0.57      0.63        61
           9       0.73      0.73      0.73        67

    accuracy                           0.65       200
   macro avg       0.44      0.47      0.45       200
weighted avg       0.65      0.65      0.64       200

2023-06-22 15:01:25,399 - INFO - test loss 0.026493753972454168
2023-06-22 15:01:25,400 - INFO - test acc 0.6449999809265137
2023-06-22 15:01:25,579 - INFO - Distilling data from client: Client02
2023-06-22 15:01:25,579 - INFO - train loss: 0.016293740080041114
2023-06-22 15:01:25,579 - INFO - train acc: 0.7894737124443054
2023-06-22 15:01:25,591 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.59      0.75      0.66        60
           6       0.22      0.33      0.27         6
           7       0.70      0.54      0.61        61
           9       0.72      0.73      0.73        67

    accuracy                           0.65       200
   macro avg       0.45      0.47      0.45       200
weighted avg       0.64      0.65      0.64       200

2023-06-22 15:01:25,592 - INFO - test loss 0.027167368309123693
2023-06-22 15:01:25,592 - INFO - test acc 0.6449999809265137
2023-06-22 15:01:25,789 - INFO - Distilling data from client: Client02
2023-06-22 15:01:25,789 - INFO - train loss: 0.013396652852699578
2023-06-22 15:01:25,789 - INFO - train acc: 0.8815789818763733
2023-06-22 15:01:25,814 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.66      0.68      0.67        60
           6       0.23      0.50      0.32         6
           7       0.75      0.66      0.70        61
           9       0.69      0.75      0.72        67

    accuracy                           0.67       200
   macro avg       0.47      0.52      0.48       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:01:25,815 - INFO - test loss 0.02656053505697298
2023-06-22 15:01:25,815 - INFO - test acc 0.6699999570846558
2023-06-22 15:01:26,017 - INFO - Distilling data from client: Client02
2023-06-22 15:01:26,018 - INFO - train loss: 0.01564966539242203
2023-06-22 15:01:26,018 - INFO - train acc: 0.8684210777282715
2023-06-22 15:01:26,041 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.65      0.77      0.70        60
           6       0.30      0.50      0.37         6
           7       0.69      0.61      0.64        61
           9       0.77      0.75      0.76        67

    accuracy                           0.68       200
   macro avg       0.48      0.52      0.50       200
weighted avg       0.67      0.68      0.67       200

2023-06-22 15:01:26,041 - INFO - test loss 0.02607123833373192
2023-06-22 15:01:26,041 - INFO - test acc 0.6800000071525574
2023-06-22 15:01:26,239 - INFO - Distilling data from client: Client02
2023-06-22 15:01:26,239 - INFO - train loss: 0.014652456473232241
2023-06-22 15:01:26,239 - INFO - train acc: 0.8947368264198303
2023-06-22 15:01:26,252 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.59      0.72      0.65        60
           6       0.15      0.33      0.21         6
           7       0.63      0.52      0.57        61
           9       0.73      0.69      0.71        67

    accuracy                           0.61       200
   macro avg       0.42      0.45      0.43       200
weighted avg       0.62      0.61      0.61       200

2023-06-22 15:01:26,252 - INFO - test loss 0.02715712083077022
2023-06-22 15:01:26,252 - INFO - test acc 0.6150000095367432
2023-06-22 15:01:26,444 - INFO - Distilling data from client: Client02
2023-06-22 15:01:26,444 - INFO - train loss: 0.016171180588545803
2023-06-22 15:01:26,445 - INFO - train acc: 0.8157894611358643
2023-06-22 15:01:26,458 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.65      0.63        60
           6       0.17      0.33      0.22         6
           7       0.65      0.59      0.62        61
           9       0.70      0.73      0.72        67

    accuracy                           0.63       200
   macro avg       0.43      0.46      0.44       200
weighted avg       0.62      0.63      0.63       200

2023-06-22 15:01:26,459 - INFO - test loss 0.02694101799062276
2023-06-22 15:01:26,459 - INFO - test acc 0.6299999952316284
2023-06-22 15:01:26,646 - INFO - Distilling data from client: Client02
2023-06-22 15:01:26,646 - INFO - train loss: 0.014002661432575779
2023-06-22 15:01:26,646 - INFO - train acc: 0.8684210777282715
2023-06-22 15:01:26,658 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.59      0.68      0.64        60
           6       0.18      0.33      0.24         6
           7       0.61      0.51      0.55        61
           9       0.68      0.70      0.69        67

    accuracy                           0.60       200
   macro avg       0.41      0.45      0.42       200
weighted avg       0.60      0.60      0.60       200

2023-06-22 15:01:26,659 - INFO - test loss 0.027335255841757614
2023-06-22 15:01:26,659 - INFO - test acc 0.6049999594688416
2023-06-22 15:01:26,876 - INFO - Distilling data from client: Client02
2023-06-22 15:01:26,876 - INFO - train loss: 0.01466755209055484
2023-06-22 15:01:26,876 - INFO - train acc: 0.8421052694320679
2023-06-22 15:01:26,889 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.63      0.70      0.66        60
           6       0.29      0.33      0.31         6
           7       0.66      0.67      0.67        61
           9       0.75      0.72      0.73        67

    accuracy                           0.67       200
   macro avg       0.46      0.48      0.47       200
weighted avg       0.65      0.67      0.66       200

2023-06-22 15:01:26,889 - INFO - test loss 0.026949224654041475
2023-06-22 15:01:26,890 - INFO - test acc 0.6649999618530273
2023-06-22 15:01:27,226 - INFO - Distilling data from client: Client02
2023-06-22 15:01:27,226 - INFO - train loss: 0.012600835561715942
2023-06-22 15:01:27,226 - INFO - train acc: 0.8947368264198303
2023-06-22 15:01:27,238 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.64      0.78      0.70        60
           6       0.30      0.50      0.37         6
           7       0.70      0.54      0.61        61
           9       0.68      0.70      0.69        67

    accuracy                           0.65       200
   macro avg       0.46      0.51      0.48       200
weighted avg       0.64      0.65      0.64       200

2023-06-22 15:01:27,238 - INFO - test loss 0.02666185247231633
2023-06-22 15:01:27,238 - INFO - test acc 0.6499999761581421
2023-06-22 15:01:27,425 - INFO - Distilling data from client: Client02
2023-06-22 15:01:27,425 - INFO - train loss: 0.014101505206192442
2023-06-22 15:01:27,425 - INFO - train acc: 0.9078947305679321
2023-06-22 15:01:27,447 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.64      0.85      0.73        60
           6       0.25      0.33      0.29         6
           7       0.76      0.57      0.65        61
           9       0.74      0.73      0.74        67

    accuracy                           0.69       200
   macro avg       0.48      0.50      0.48       200
weighted avg       0.68      0.69      0.67       200

2023-06-22 15:01:27,447 - INFO - test loss 0.026375067042331495
2023-06-22 15:01:27,447 - INFO - test acc 0.6850000023841858
2023-06-22 15:01:27,668 - INFO - Distilling data from client: Client02
2023-06-22 15:01:27,669 - INFO - train loss: 0.01455448904328357
2023-06-22 15:01:27,669 - INFO - train acc: 0.8421052694320679
2023-06-22 15:01:27,680 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.65      0.65      0.65        60
           6       0.30      0.50      0.37         6
           7       0.62      0.66      0.63        61
           9       0.71      0.69      0.70        67

    accuracy                           0.64       200
   macro avg       0.45      0.50      0.47       200
weighted avg       0.63      0.64      0.63       200

2023-06-22 15:01:27,680 - INFO - test loss 0.027246301814335988
2023-06-22 15:01:27,680 - INFO - test acc 0.6399999856948853
2023-06-22 15:01:27,863 - INFO - Distilling data from client: Client02
2023-06-22 15:01:27,863 - INFO - train loss: 0.01325360871778017
2023-06-22 15:01:27,863 - INFO - train acc: 0.8947368264198303
2023-06-22 15:01:27,875 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.75      0.68        60
           6       0.11      0.17      0.13         6
           7       0.67      0.59      0.63        61
           9       0.72      0.69      0.70        67

    accuracy                           0.64       200
   macro avg       0.42      0.44      0.43       200
weighted avg       0.63      0.64      0.63       200

2023-06-22 15:01:27,876 - INFO - test loss 0.02696055611106581
2023-06-22 15:01:27,876 - INFO - test acc 0.6399999856948853
2023-06-22 15:01:28,066 - INFO - Distilling data from client: Client02
2023-06-22 15:01:28,067 - INFO - train loss: 0.01464423024873708
2023-06-22 15:01:28,067 - INFO - train acc: 0.8552631735801697
2023-06-22 15:01:28,079 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.72      0.67        60
           6       0.23      0.50      0.32         6
           7       0.64      0.52      0.58        61
           9       0.68      0.69      0.68        67

    accuracy                           0.62       200
   macro avg       0.43      0.49      0.45       200
weighted avg       0.62      0.62      0.61       200

2023-06-22 15:01:28,080 - INFO - test loss 0.026554569084255172
2023-06-22 15:01:28,080 - INFO - test acc 0.6200000047683716
2023-06-22 15:01:28,262 - INFO - Distilling data from client: Client02
2023-06-22 15:01:28,262 - INFO - train loss: 0.011984718044703805
2023-06-22 15:01:28,262 - INFO - train acc: 0.8947368264198303
2023-06-22 15:01:28,286 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.68      0.80      0.73        60
           6       0.33      0.50      0.40         6
           7       0.72      0.67      0.69        61
           9       0.76      0.72      0.74        67

    accuracy                           0.70       200
   macro avg       0.50      0.54      0.51       200
weighted avg       0.69      0.70      0.69       200

2023-06-22 15:01:28,287 - INFO - test loss 0.0265335175254891
2023-06-22 15:01:28,287 - INFO - test acc 0.699999988079071
2023-06-22 15:01:28,478 - INFO - Distilling data from client: Client02
2023-06-22 15:01:28,478 - INFO - train loss: 0.012384497528783626
2023-06-22 15:01:28,478 - INFO - train acc: 0.9473684430122375
2023-06-22 15:01:28,491 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.67      0.73      0.70        60
           6       0.33      0.50      0.40         6
           7       0.66      0.66      0.66        61
           9       0.73      0.70      0.72        67

    accuracy                           0.67       200
   macro avg       0.48      0.52      0.49       200
weighted avg       0.66      0.67      0.66       200

2023-06-22 15:01:28,491 - INFO - test loss 0.02570509044425602
2023-06-22 15:01:28,491 - INFO - test acc 0.6699999570846558
2023-06-22 15:01:28,675 - INFO - Distilling data from client: Client02
2023-06-22 15:01:28,675 - INFO - train loss: 0.01377457639123125
2023-06-22 15:01:28,675 - INFO - train acc: 0.8421052694320679
2023-06-22 15:01:28,687 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.70      0.66        60
           6       0.23      0.50      0.32         6
           7       0.70      0.57      0.63        61
           9       0.67      0.69      0.68        67

    accuracy                           0.63       200
   macro avg       0.44      0.49      0.46       200
weighted avg       0.63      0.63      0.63       200

2023-06-22 15:01:28,687 - INFO - test loss 0.026908266719822348
2023-06-22 15:01:28,687 - INFO - test acc 0.6299999952316284
2023-06-22 15:01:28,874 - INFO - Distilling data from client: Client02
2023-06-22 15:01:28,875 - INFO - train loss: 0.012697027999557818
2023-06-22 15:01:28,875 - INFO - train acc: 0.8947368264198303
2023-06-22 15:01:28,887 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.63      0.72      0.67        60
           6       0.33      0.50      0.40         6
           7       0.66      0.64      0.65        61
           9       0.72      0.69      0.70        67

    accuracy                           0.66       200
   macro avg       0.47      0.51      0.48       200
weighted avg       0.64      0.66      0.65       200

2023-06-22 15:01:28,887 - INFO - test loss 0.0260362468784674
2023-06-22 15:01:28,887 - INFO - test acc 0.6549999713897705
2023-06-22 15:01:29,073 - INFO - Distilling data from client: Client02
2023-06-22 15:01:29,073 - INFO - train loss: 0.010473922926261184
2023-06-22 15:01:29,073 - INFO - train acc: 0.9605263471603394
2023-06-22 15:01:29,086 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.61      0.73      0.67        60
           6       0.14      0.17      0.15         6
           7       0.64      0.59      0.62        61
           9       0.74      0.72      0.73        67

    accuracy                           0.65       200
   macro avg       0.43      0.44      0.43       200
weighted avg       0.63      0.65      0.64       200

2023-06-22 15:01:29,086 - INFO - test loss 0.026522781828539694
2023-06-22 15:01:29,086 - INFO - test acc 0.6449999809265137
2023-06-22 15:01:29,293 - INFO - Distilling data from client: Client02
2023-06-22 15:01:29,294 - INFO - train loss: 0.012999151119277548
2023-06-22 15:01:29,294 - INFO - train acc: 0.9210526347160339
2023-06-22 15:01:29,304 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.55      0.68      0.61        60
           6       0.30      0.50      0.37         6
           7       0.62      0.52      0.57        61
           9       0.70      0.67      0.69        67

    accuracy                           0.60       200
   macro avg       0.43      0.48      0.45       200
weighted avg       0.60      0.60      0.60       200

2023-06-22 15:01:29,305 - INFO - test loss 0.027322067727488467
2023-06-22 15:01:29,305 - INFO - test acc 0.6049999594688416
2023-06-22 15:01:29,489 - INFO - Distilling data from client: Client02
2023-06-22 15:01:29,489 - INFO - train loss: 0.012210771826200376
2023-06-22 15:01:29,489 - INFO - train acc: 0.9342105388641357
2023-06-22 15:01:29,500 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.66      0.75      0.70        60
           6       0.25      0.50      0.33         6
           7       0.67      0.59      0.63        61
           9       0.73      0.72      0.72        67

    accuracy                           0.66       200
   macro avg       0.46      0.51      0.48       200
weighted avg       0.65      0.66      0.65       200

2023-06-22 15:01:29,501 - INFO - test loss 0.026064803862909706
2023-06-22 15:01:29,501 - INFO - test acc 0.6599999666213989
2023-06-22 15:01:29,718 - INFO - Distilling data from client: Client02
2023-06-22 15:01:29,718 - INFO - train loss: 0.013538729212963823
2023-06-22 15:01:29,718 - INFO - train acc: 0.8815789818763733
2023-06-22 15:01:29,733 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.66      0.72      0.69        60
           6       0.38      0.50      0.43         6
           7       0.67      0.62      0.64        61
           9       0.74      0.78      0.76        67

    accuracy                           0.68       200
   macro avg       0.49      0.52      0.50       200
weighted avg       0.66      0.68      0.67       200

2023-06-22 15:01:29,734 - INFO - test loss 0.026200810711675574
2023-06-22 15:01:29,734 - INFO - test acc 0.6800000071525574
2023-06-22 15:01:29,928 - INFO - Distilling data from client: Client02
2023-06-22 15:01:29,928 - INFO - train loss: 0.011773387956437426
2023-06-22 15:01:29,929 - INFO - train acc: 0.9342105388641357
2023-06-22 15:01:29,940 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.72      0.67        60
           6       0.22      0.33      0.27         6
           7       0.66      0.61      0.63        61
           9       0.73      0.72      0.72        67

    accuracy                           0.65       200
   macro avg       0.45      0.47      0.46       200
weighted avg       0.64      0.65      0.64       200

2023-06-22 15:01:29,940 - INFO - test loss 0.026557448018707762
2023-06-22 15:01:29,940 - INFO - test acc 0.6499999761581421
2023-06-22 15:01:29,944 - WARNING - Finished tracing + transforming jit(gather) in 0.0004932880401611328 sec
2023-06-22 15:01:29,945 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[76,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:01:29,947 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0022497177124023438 sec
2023-06-22 15:01:29,948 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:01:29,963 - WARNING - Finished XLA compilation of jit(gather) in 0.014841556549072266 sec
2023-06-22 15:01:29,977 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:29,990 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:30,002 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:30,015 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:30,028 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:30,040 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:30,053 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:30,066 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:30,077 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:01:30,642 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client02//synthetic.png
2023-06-22 15:01:30,659 - INFO - c: 2.0 and total_data_in_this_class: 260
2023-06-22 15:01:30,659 - INFO - c: 4.0 and total_data_in_this_class: 267
2023-06-22 15:01:30,660 - INFO - c: 6.0 and total_data_in_this_class: 272
2023-06-22 15:01:30,660 - INFO - c: 2.0 and total_data_in_this_class: 73
2023-06-22 15:01:30,660 - INFO - c: 4.0 and total_data_in_this_class: 66
2023-06-22 15:01:30,660 - INFO - c: 6.0 and total_data_in_this_class: 61
2023-06-22 15:01:30,700 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00036406517028808594 sec
2023-06-22 15:01:30,700 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:01:30,703 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0018718242645263672 sec
2023-06-22 15:01:30,703 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:01:30,718 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.015170812606811523 sec
2023-06-22 15:01:30,722 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00030541419982910156 sec
2023-06-22 15:01:30,722 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:01:30,724 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0015976428985595703 sec
2023-06-22 15:01:30,724 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:01:30,736 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011239767074584961 sec
2023-06-22 15:01:30,740 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019550323486328125 sec
2023-06-22 15:01:30,742 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002048015594482422 sec
2023-06-22 15:01:30,743 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005552768707275391 sec
2023-06-22 15:01:30,746 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003447532653808594 sec
2023-06-22 15:01:30,746 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019288063049316406 sec
2023-06-22 15:01:30,747 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00048804283142089844 sec
2023-06-22 15:01:30,749 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003895759582519531 sec
2023-06-22 15:01:30,749 - WARNING - Finished tracing + transforming absolute for pjit in 0.000293731689453125 sec
2023-06-22 15:01:30,750 - WARNING - Finished tracing + transforming fn for pjit in 0.0005071163177490234 sec
2023-06-22 15:01:30,752 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.000537872314453125 sec
2023-06-22 15:01:30,753 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00036334991455078125 sec
2023-06-22 15:01:30,755 - WARNING - Finished tracing + transforming fn for pjit in 0.0004267692565917969 sec
2023-06-22 15:01:30,756 - WARNING - Finished tracing + transforming fn for pjit in 0.00046372413635253906 sec
2023-06-22 15:01:30,757 - WARNING - Finished tracing + transforming fn for pjit in 0.000396728515625 sec
2023-06-22 15:01:30,758 - WARNING - Finished tracing + transforming fn for pjit in 0.00045228004455566406 sec
2023-06-22 15:01:30,761 - WARNING - Finished tracing + transforming fn for pjit in 0.0003681182861328125 sec
2023-06-22 15:01:30,763 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002789497375488281 sec
2023-06-22 15:01:30,764 - WARNING - Finished tracing + transforming fn for pjit in 0.0003986358642578125 sec
2023-06-22 15:01:30,766 - WARNING - Finished tracing + transforming fn for pjit in 0.00040268898010253906 sec
2023-06-22 15:01:30,772 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008027553558349609 sec
2023-06-22 15:01:30,773 - WARNING - Finished tracing + transforming _mean for pjit in 0.0019469261169433594 sec
2023-06-22 15:01:30,774 - WARNING - Finished tracing + transforming fn for pjit in 0.00042438507080078125 sec
2023-06-22 15:01:30,776 - WARNING - Finished tracing + transforming fn for pjit in 0.0003972053527832031 sec
2023-06-22 15:01:30,777 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005285739898681641 sec
2023-06-22 15:01:30,778 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004513263702392578 sec
2023-06-22 15:01:30,779 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00036215782165527344 sec
2023-06-22 15:01:30,781 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045490264892578125 sec
2023-06-22 15:01:30,782 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003981590270996094 sec
2023-06-22 15:01:30,783 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003993511199951172 sec
2023-06-22 15:01:30,785 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006203651428222656 sec
2023-06-22 15:01:30,786 - WARNING - Finished tracing + transforming _where for pjit in 0.0016772747039794922 sec
2023-06-22 15:01:30,787 - WARNING - Finished tracing + transforming fn for pjit in 0.0004982948303222656 sec
2023-06-22 15:01:30,788 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004482269287109375 sec
2023-06-22 15:01:30,790 - WARNING - Finished tracing + transforming fn for pjit in 0.000392913818359375 sec
2023-06-22 15:01:30,790 - WARNING - Finished tracing + transforming fn for pjit in 0.0003917217254638672 sec
2023-06-22 15:01:30,792 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004031658172607422 sec
2023-06-22 15:01:30,793 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045108795166015625 sec
2023-06-22 15:01:30,794 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045490264892578125 sec
2023-06-22 15:01:30,796 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004630088806152344 sec
2023-06-22 15:01:30,797 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004017353057861328 sec
2023-06-22 15:01:30,798 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038933753967285156 sec
2023-06-22 15:01:30,800 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00045418739318847656 sec
2023-06-22 15:01:30,800 - WARNING - Finished tracing + transforming _where for pjit in 0.0015170574188232422 sec
2023-06-22 15:01:30,802 - WARNING - Finished tracing + transforming fn for pjit in 0.0004630088806152344 sec
2023-06-22 15:01:30,803 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042128562927246094 sec
2023-06-22 15:01:30,805 - WARNING - Finished tracing + transforming fn for pjit in 0.00039649009704589844 sec
2023-06-22 15:01:30,812 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004622936248779297 sec
2023-06-22 15:01:30,813 - WARNING - Finished tracing + transforming fn for pjit in 0.0006046295166015625 sec
2023-06-22 15:01:30,814 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000469207763671875 sec
2023-06-22 15:01:30,816 - WARNING - Finished tracing + transforming fn for pjit in 0.00039505958557128906 sec
2023-06-22 15:01:30,822 - WARNING - Finished tracing + transforming fn for pjit in 0.0003323554992675781 sec
2023-06-22 15:01:30,825 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002892017364501953 sec
2023-06-22 15:01:30,827 - WARNING - Finished tracing + transforming fn for pjit in 0.0017080307006835938 sec
2023-06-22 15:01:30,829 - WARNING - Finished tracing + transforming fn for pjit in 0.0003859996795654297 sec
2023-06-22 15:01:30,856 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11615633964538574 sec
2023-06-22 15:01:30,861 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020170211791992188 sec
2023-06-22 15:01:30,862 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019240379333496094 sec
2023-06-22 15:01:30,863 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005137920379638672 sec
2023-06-22 15:01:30,867 - WARNING - Finished tracing + transforming fn for pjit in 0.0003559589385986328 sec
2023-06-22 15:01:30,868 - WARNING - Finished tracing + transforming fn for pjit in 0.0004553794860839844 sec
2023-06-22 15:01:30,870 - WARNING - Finished tracing + transforming fn for pjit in 0.00036454200744628906 sec
2023-06-22 15:01:30,880 - WARNING - Finished tracing + transforming fn for pjit in 0.0003800392150878906 sec
2023-06-22 15:01:30,881 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037789344787597656 sec
2023-06-22 15:01:30,883 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044655799865722656 sec
2023-06-22 15:01:30,884 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003123283386230469 sec
2023-06-22 15:01:30,885 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005862712860107422 sec
2023-06-22 15:01:30,887 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039577484130859375 sec
2023-06-22 15:01:30,888 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038886070251464844 sec
2023-06-22 15:01:30,889 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00047087669372558594 sec
2023-06-22 15:01:30,890 - WARNING - Finished tracing + transforming _where for pjit in 0.0015091896057128906 sec
2023-06-22 15:01:30,891 - WARNING - Finished tracing + transforming fn for pjit in 0.0005075931549072266 sec
2023-06-22 15:01:30,892 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044536590576171875 sec
2023-06-22 15:01:30,894 - WARNING - Finished tracing + transforming fn for pjit in 0.0003924369812011719 sec
2023-06-22 15:01:30,895 - WARNING - Finished tracing + transforming fn for pjit in 0.0005383491516113281 sec
2023-06-22 15:01:30,914 - WARNING - Finished tracing + transforming fn for pjit in 0.000354766845703125 sec
2023-06-22 15:01:30,946 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08899235725402832 sec
2023-06-22 15:01:30,949 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001990795135498047 sec
2023-06-22 15:01:30,950 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00021576881408691406 sec
2023-06-22 15:01:30,951 - WARNING - Finished tracing + transforming _where for pjit in 0.0010993480682373047 sec
2023-06-22 15:01:30,952 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005125999450683594 sec
2023-06-22 15:01:30,953 - WARNING - Finished tracing + transforming trace for pjit in 0.004390239715576172 sec
2023-06-22 15:01:30,956 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0001823902130126953 sec
2023-06-22 15:01:30,958 - WARNING - Finished tracing + transforming tril for pjit in 0.0010819435119628906 sec
2023-06-22 15:01:30,959 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0029611587524414062 sec
2023-06-22 15:01:30,960 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00018930435180664062 sec
2023-06-22 15:01:30,961 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001881122589111328 sec
2023-06-22 15:01:30,964 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.002365589141845703 sec
2023-06-22 15:01:30,970 - WARNING - Finished tracing + transforming _solve for pjit in 0.015356063842773438 sec
2023-06-22 15:01:30,972 - WARNING - Finished tracing + transforming dot for pjit in 0.0005400180816650391 sec
2023-06-22 15:01:30,976 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.23798155784606934 sec
2023-06-22 15:01:30,979 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:01:31,035 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.055303335189819336 sec
2023-06-22 15:01:31,035 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:01:31,205 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.16965150833129883 sec
2023-06-22 15:01:31,242 - INFO - initial test loss: 0.03160755549663447
2023-06-22 15:01:31,243 - INFO - initial test acc: 0.5450000166893005
2023-06-22 15:01:31,258 - WARNING - Finished tracing + transforming dot for pjit in 0.0008084774017333984 sec
2023-06-22 15:01:31,260 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006742477416992188 sec
2023-06-22 15:01:31,262 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006761550903320312 sec
2023-06-22 15:01:31,263 - WARNING - Finished tracing + transforming _mean for pjit in 0.002103090286254883 sec
2023-06-22 15:01:31,265 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004341602325439453 sec
2023-06-22 15:01:31,267 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0005698204040527344 sec
2023-06-22 15:01:31,269 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006761550903320312 sec
2023-06-22 15:01:31,271 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008480548858642578 sec
2023-06-22 15:01:31,273 - WARNING - Finished tracing + transforming _mean for pjit in 0.0028688907623291016 sec
2023-06-22 15:01:31,275 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.02373814582824707 sec
2023-06-22 15:01:31,294 - WARNING - Finished tracing + transforming fn for pjit in 0.0006003379821777344 sec
2023-06-22 15:01:31,296 - WARNING - Finished tracing + transforming fn for pjit in 0.0007593631744384766 sec
2023-06-22 15:01:31,297 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005285739898681641 sec
2023-06-22 15:01:31,299 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006101131439208984 sec
2023-06-22 15:01:31,301 - WARNING - Finished tracing + transforming _where for pjit in 0.0022733211517333984 sec
2023-06-22 15:01:31,321 - WARNING - Finished tracing + transforming fn for pjit in 0.0006129741668701172 sec
2023-06-22 15:01:31,323 - WARNING - Finished tracing + transforming fn for pjit in 0.000640869140625 sec
2023-06-22 15:01:31,324 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005192756652832031 sec
2023-06-22 15:01:31,326 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005865097045898438 sec
2023-06-22 15:01:31,327 - WARNING - Finished tracing + transforming _where for pjit in 0.0021369457244873047 sec
2023-06-22 15:01:31,385 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003559589385986328 sec
2023-06-22 15:01:31,467 - WARNING - Finished tracing + transforming fn for pjit in 0.00046515464782714844 sec
2023-06-22 15:01:31,469 - WARNING - Finished tracing + transforming fn for pjit in 0.000423431396484375 sec
2023-06-22 15:01:31,470 - WARNING - Finished tracing + transforming square for pjit in 0.00030493736267089844 sec
2023-06-22 15:01:31,473 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00036406517028808594 sec
2023-06-22 15:01:31,476 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00043010711669921875 sec
2023-06-22 15:01:31,477 - WARNING - Finished tracing + transforming fn for pjit in 0.0004761219024658203 sec
2023-06-22 15:01:31,478 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004000663757324219 sec
2023-06-22 15:01:31,480 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004820823669433594 sec
2023-06-22 15:01:31,481 - WARNING - Finished tracing + transforming fn for pjit in 0.00048422813415527344 sec
2023-06-22 15:01:31,483 - WARNING - Finished tracing + transforming fn for pjit in 0.0004458427429199219 sec
2023-06-22 15:01:31,484 - WARNING - Finished tracing + transforming square for pjit in 0.0003159046173095703 sec
2023-06-22 15:01:31,487 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003979206085205078 sec
2023-06-22 15:01:31,490 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002987384796142578 sec
2023-06-22 15:01:31,491 - WARNING - Finished tracing + transforming fn for pjit in 0.0004937648773193359 sec
2023-06-22 15:01:31,492 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00039505958557128906 sec
2023-06-22 15:01:31,493 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041604042053222656 sec
2023-06-22 15:01:31,495 - WARNING - Finished tracing + transforming update_fn for pjit in 0.24507689476013184 sec
2023-06-22 15:01:31,501 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:01:31,606 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10464191436767578 sec
2023-06-22 15:01:31,606 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:01:32,042 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4354736804962158 sec
2023-06-22 15:01:34,929 - INFO - Distilling data from client: Client03
2023-06-22 15:01:34,931 - INFO - train loss: 0.003480170126795681
2023-06-22 15:01:34,933 - INFO - train acc: 0.9922928810119629
2023-06-22 15:01:36,429 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.56      0.57        73
           4       0.48      0.48      0.48        66
           6       0.60      0.62      0.61        61

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-06-22 15:01:36,430 - INFO - test loss 0.03164624088988132
2023-06-22 15:01:36,430 - INFO - test acc 0.5550000071525574
2023-06-22 15:01:39,341 - INFO - Distilling data from client: Client03
2023-06-22 15:01:39,341 - INFO - train loss: 0.0019129840777255811
2023-06-22 15:01:39,342 - INFO - train acc: 1.0
2023-06-22 15:01:39,717 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.59      0.61        73
           4       0.49      0.52      0.50        66
           6       0.59      0.59      0.59        61

    accuracy                           0.56       200
   macro avg       0.57      0.56      0.57       200
weighted avg       0.57      0.56      0.57       200

2023-06-22 15:01:39,718 - INFO - test loss 0.031404890339889836
2023-06-22 15:01:39,718 - INFO - test acc 0.5649999976158142
2023-06-22 15:01:42,574 - INFO - Distilling data from client: Client03
2023-06-22 15:01:42,575 - INFO - train loss: 0.001953643252566452
2023-06-22 15:01:42,575 - INFO - train acc: 1.0
2023-06-22 15:01:42,694 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.58      0.59        73
           4       0.48      0.48      0.48        66
           6       0.59      0.62      0.61        61

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-06-22 15:01:42,694 - INFO - test loss 0.03132165214684475
2023-06-22 15:01:42,695 - INFO - test acc 0.5600000023841858
2023-06-22 15:01:45,582 - INFO - Distilling data from client: Client03
2023-06-22 15:01:45,582 - INFO - train loss: 0.0014463495497252549
2023-06-22 15:01:45,582 - INFO - train acc: 0.9980732202529907
2023-06-22 15:01:46,664 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.63      0.63        73
           4       0.52      0.52      0.52        66
           6       0.60      0.61      0.60        61

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.59      0.58      0.59       200

2023-06-22 15:01:46,665 - INFO - test loss 0.032267736879602785
2023-06-22 15:01:46,665 - INFO - test acc 0.5849999785423279
2023-06-22 15:01:49,663 - INFO - Distilling data from client: Client03
2023-06-22 15:01:49,663 - INFO - train loss: 0.0010108486291283845
2023-06-22 15:01:49,666 - INFO - train acc: 1.0
2023-06-22 15:01:49,731 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.60      0.62        73
           4       0.47      0.50      0.49        66
           6       0.60      0.59      0.60        61

    accuracy                           0.56       200
   macro avg       0.57      0.56      0.57       200
weighted avg       0.57      0.56      0.57       200

2023-06-22 15:01:49,732 - INFO - test loss 0.0320867447062146
2023-06-22 15:01:49,732 - INFO - test acc 0.5649999976158142
2023-06-22 15:01:52,548 - INFO - Distilling data from client: Client03
2023-06-22 15:01:52,549 - INFO - train loss: 0.0010964282029716265
2023-06-22 15:01:52,550 - INFO - train acc: 1.0
2023-06-22 15:01:52,597 - INFO - report:               precision    recall  f1-score   support

           2       0.65      0.62      0.63        73
           4       0.48      0.50      0.49        66
           6       0.61      0.62      0.62        61

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-06-22 15:01:52,599 - INFO - test loss 0.03153769701160319
2023-06-22 15:01:52,599 - INFO - test acc 0.5799999833106995
2023-06-22 15:01:55,492 - INFO - Distilling data from client: Client03
2023-06-22 15:01:55,492 - INFO - train loss: 0.0012062409684901743
2023-06-22 15:01:55,493 - INFO - train acc: 1.0
2023-06-22 15:01:55,551 - INFO - report:               precision    recall  f1-score   support

           2       0.60      0.58      0.59        73
           4       0.47      0.48      0.48        66
           6       0.58      0.59      0.59        61

    accuracy                           0.55       200
   macro avg       0.55      0.55      0.55       200
weighted avg       0.55      0.55      0.55       200

2023-06-22 15:01:55,552 - INFO - test loss 0.0329797520886989
2023-06-22 15:01:55,552 - INFO - test acc 0.550000011920929
2023-06-22 15:01:58,405 - INFO - Distilling data from client: Client03
2023-06-22 15:01:58,406 - INFO - train loss: 0.0009280265200038327
2023-06-22 15:01:58,407 - INFO - train acc: 1.0
2023-06-22 15:01:58,459 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.56      0.58        73
           4       0.44      0.52      0.48        66
           6       0.59      0.52      0.56        61

    accuracy                           0.54       200
   macro avg       0.54      0.53      0.54       200
weighted avg       0.54      0.54      0.54       200

2023-06-22 15:01:58,463 - INFO - test loss 0.03288634137440719
2023-06-22 15:01:58,464 - INFO - test acc 0.5349999666213989
2023-06-22 15:02:01,469 - INFO - Distilling data from client: Client03
2023-06-22 15:02:01,470 - INFO - train loss: 0.0007933868435080951
2023-06-22 15:02:01,472 - INFO - train acc: 1.0
2023-06-22 15:02:01,524 - INFO - report:               precision    recall  f1-score   support

           2       0.60      0.58      0.59        73
           4       0.47      0.55      0.51        66
           6       0.59      0.52      0.56        61

    accuracy                           0.55       200
   macro avg       0.56      0.55      0.55       200
weighted avg       0.56      0.55      0.55       200

2023-06-22 15:02:01,531 - INFO - test loss 0.032679013642781536
2023-06-22 15:02:01,531 - INFO - test acc 0.550000011920929
2023-06-22 15:02:04,402 - INFO - Distilling data from client: Client03
2023-06-22 15:02:04,403 - INFO - train loss: 0.0009434715268779034
2023-06-22 15:02:04,403 - INFO - train acc: 1.0
2023-06-22 15:02:04,486 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.58      0.58        73
           4       0.46      0.45      0.46        66
           6       0.58      0.61      0.59        61

    accuracy                           0.55       200
   macro avg       0.54      0.55      0.54       200
weighted avg       0.54      0.55      0.54       200

2023-06-22 15:02:04,487 - INFO - test loss 0.033472344110340994
2023-06-22 15:02:04,488 - INFO - test acc 0.5450000166893005
2023-06-22 15:02:07,656 - INFO - Distilling data from client: Client03
2023-06-22 15:02:07,656 - INFO - train loss: 0.000935832528723055
2023-06-22 15:02:07,657 - INFO - train acc: 1.0
2023-06-22 15:02:07,726 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.58      0.58        73
           4       0.47      0.53      0.50        66
           6       0.58      0.51      0.54        61

    accuracy                           0.54       200
   macro avg       0.54      0.54      0.54       200
weighted avg       0.54      0.54      0.54       200

2023-06-22 15:02:07,728 - INFO - test loss 0.03330508873143488
2023-06-22 15:02:07,728 - INFO - test acc 0.5399999618530273
2023-06-22 15:02:10,700 - INFO - Distilling data from client: Client03
2023-06-22 15:02:10,701 - INFO - train loss: 0.0009500533430571514
2023-06-22 15:02:10,701 - INFO - train acc: 1.0
2023-06-22 15:02:10,751 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.64      0.64        73
           4       0.50      0.55      0.52        66
           6       0.63      0.56      0.59        61

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.59       200

2023-06-22 15:02:10,752 - INFO - test loss 0.03270184079410356
2023-06-22 15:02:10,753 - INFO - test acc 0.5849999785423279
2023-06-22 15:02:13,622 - INFO - Distilling data from client: Client03
2023-06-22 15:02:13,623 - INFO - train loss: 0.0007868023071622912
2023-06-22 15:02:13,623 - INFO - train acc: 1.0
2023-06-22 15:02:13,673 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.58      0.58        73
           4       0.48      0.47      0.47        66
           6       0.58      0.59      0.59        61

    accuracy                           0.55       200
   macro avg       0.54      0.55      0.54       200
weighted avg       0.54      0.55      0.54       200

2023-06-22 15:02:13,674 - INFO - test loss 0.03289752072402552
2023-06-22 15:02:13,674 - INFO - test acc 0.5450000166893005
2023-06-22 15:02:16,545 - INFO - Distilling data from client: Client03
2023-06-22 15:02:16,546 - INFO - train loss: 0.0007939668066764844
2023-06-22 15:02:16,547 - INFO - train acc: 1.0
2023-06-22 15:02:16,603 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.59      0.60        73
           4       0.51      0.50      0.50        66
           6       0.58      0.61      0.59        61

    accuracy                           0.56       200
   macro avg       0.56      0.57      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-06-22 15:02:16,607 - INFO - test loss 0.03245999244926561
2023-06-22 15:02:16,608 - INFO - test acc 0.5649999976158142
2023-06-22 15:02:19,401 - INFO - Distilling data from client: Client03
2023-06-22 15:02:19,401 - INFO - train loss: 0.0008972220263617037
2023-06-22 15:02:19,401 - INFO - train acc: 1.0
2023-06-22 15:02:19,459 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.58      0.60        73
           4       0.51      0.56      0.53        66
           6       0.61      0.59      0.60        61

    accuracy                           0.57       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.57      0.58       200

2023-06-22 15:02:19,459 - INFO - test loss 0.0326220008230628
2023-06-22 15:02:19,460 - INFO - test acc 0.574999988079071
2023-06-22 15:02:22,358 - INFO - Distilling data from client: Client03
2023-06-22 15:02:22,358 - INFO - train loss: 0.0005886958011811063
2023-06-22 15:02:22,358 - INFO - train acc: 1.0
2023-06-22 15:02:22,434 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.58      0.60        73
           4       0.45      0.52      0.48        66
           6       0.59      0.54      0.56        61

    accuracy                           0.55       200
   macro avg       0.55      0.54      0.55       200
weighted avg       0.55      0.55      0.55       200

2023-06-22 15:02:22,435 - INFO - test loss 0.03323133571119301
2023-06-22 15:02:22,435 - INFO - test acc 0.5450000166893005
2023-06-22 15:02:25,412 - INFO - Distilling data from client: Client03
2023-06-22 15:02:25,412 - INFO - train loss: 0.0007424983498421733
2023-06-22 15:02:25,412 - INFO - train acc: 1.0
2023-06-22 15:02:25,503 - INFO - report:               precision    recall  f1-score   support

           2       0.60      0.60      0.60        73
           4       0.49      0.52      0.50        66
           6       0.62      0.59      0.61        61

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-06-22 15:02:25,504 - INFO - test loss 0.032603641986131936
2023-06-22 15:02:25,504 - INFO - test acc 0.5699999928474426
2023-06-22 15:02:28,564 - INFO - Distilling data from client: Client03
2023-06-22 15:02:28,564 - INFO - train loss: 0.0006949136581261124
2023-06-22 15:02:28,564 - INFO - train acc: 1.0
2023-06-22 15:02:28,622 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.60      0.61        73
           4       0.46      0.50      0.48        66
           6       0.61      0.57      0.59        61

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-06-22 15:02:28,623 - INFO - test loss 0.033060025218334066
2023-06-22 15:02:28,624 - INFO - test acc 0.5600000023841858
2023-06-22 15:02:31,656 - INFO - Distilling data from client: Client03
2023-06-22 15:02:31,657 - INFO - train loss: 0.0006125325643475269
2023-06-22 15:02:31,657 - INFO - train acc: 1.0
2023-06-22 15:02:31,716 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.55      0.57        73
           4       0.48      0.53      0.50        66
           6       0.63      0.61      0.62        61

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-06-22 15:02:31,722 - INFO - test loss 0.0338034913990639
2023-06-22 15:02:31,723 - INFO - test acc 0.5600000023841858
2023-06-22 15:02:34,656 - INFO - Distilling data from client: Client03
2023-06-22 15:02:34,659 - INFO - train loss: 0.0005970864304283547
2023-06-22 15:02:34,659 - INFO - train acc: 1.0
2023-06-22 15:02:34,726 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.56      0.58        73
           4       0.50      0.56      0.53        66
           6       0.63      0.59      0.61        61

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-06-22 15:02:34,727 - INFO - test loss 0.03343627839397682
2023-06-22 15:02:34,727 - INFO - test acc 0.5699999928474426
2023-06-22 15:02:37,638 - INFO - Distilling data from client: Client03
2023-06-22 15:02:37,638 - INFO - train loss: 0.0006435944853499035
2023-06-22 15:02:37,638 - INFO - train acc: 1.0
2023-06-22 15:02:37,683 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.59      0.59        73
           4       0.50      0.55      0.52        66
           6       0.64      0.57      0.60        61

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-06-22 15:02:37,685 - INFO - test loss 0.03335647960463304
2023-06-22 15:02:37,685 - INFO - test acc 0.5699999928474426
2023-06-22 15:02:40,544 - INFO - Distilling data from client: Client03
2023-06-22 15:02:40,545 - INFO - train loss: 0.0004735830625607199
2023-06-22 15:02:40,545 - INFO - train acc: 1.0
2023-06-22 15:02:40,620 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.60      0.61        73
           4       0.45      0.45      0.45        66
           6       0.61      0.61      0.61        61

    accuracy                           0.56       200
   macro avg       0.56      0.55      0.55       200
weighted avg       0.56      0.56      0.56       200

2023-06-22 15:02:40,624 - INFO - test loss 0.03292425946452072
2023-06-22 15:02:40,626 - INFO - test acc 0.5550000071525574
2023-06-22 15:02:43,564 - INFO - Distilling data from client: Client03
2023-06-22 15:02:43,565 - INFO - train loss: 0.00048807411560864274
2023-06-22 15:02:43,565 - INFO - train acc: 1.0
2023-06-22 15:02:43,642 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.56      0.59        73
           4       0.47      0.56      0.51        66
           6       0.63      0.59      0.61        61

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-06-22 15:02:43,644 - INFO - test loss 0.032766778291446794
2023-06-22 15:02:43,644 - INFO - test acc 0.5699999928474426
2023-06-22 15:02:46,624 - INFO - Distilling data from client: Client03
2023-06-22 15:02:46,624 - INFO - train loss: 0.0005066449151957526
2023-06-22 15:02:46,624 - INFO - train acc: 1.0
2023-06-22 15:02:46,689 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.55      0.56        73
           4       0.45      0.52      0.48        66
           6       0.56      0.49      0.52        61

    accuracy                           0.52       200
   macro avg       0.52      0.52      0.52       200
weighted avg       0.53      0.52      0.52       200

2023-06-22 15:02:46,689 - INFO - test loss 0.03449583126745327
2023-06-22 15:02:46,689 - INFO - test acc 0.5199999809265137
2023-06-22 15:02:49,619 - INFO - Distilling data from client: Client03
2023-06-22 15:02:49,621 - INFO - train loss: 0.0006501517632093715
2023-06-22 15:02:49,621 - INFO - train acc: 1.0
2023-06-22 15:02:49,679 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.59      0.61        73
           4       0.48      0.56      0.52        66
           6       0.62      0.57      0.60        61

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.58       200
weighted avg       0.58      0.57      0.58       200

2023-06-22 15:02:49,679 - INFO - test loss 0.03319855417690067
2023-06-22 15:02:49,680 - INFO - test acc 0.574999988079071
2023-06-22 15:02:49,687 - WARNING - Finished tracing + transforming jit(gather) in 0.0007879734039306641 sec
2023-06-22 15:02:49,688 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:02:49,692 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.003376007080078125 sec
2023-06-22 15:02:49,693 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:02:49,715 - WARNING - Finished XLA compilation of jit(gather) in 0.02176690101623535 sec
2023-06-22 15:02:49,741 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:02:49,759 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:02:49,772 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:02:49,783 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:02:49,795 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:02:49,805 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:02:49,818 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:02:49,830 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:02:49,843 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:02:50,395 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client03//synthetic.png
2023-06-22 15:02:50,415 - INFO - c: 6.0 and total_data_in_this_class: 531
2023-06-22 15:02:50,415 - INFO - c: 7.0 and total_data_in_this_class: 268
2023-06-22 15:02:50,415 - INFO - c: 6.0 and total_data_in_this_class: 135
2023-06-22 15:02:50,415 - INFO - c: 7.0 and total_data_in_this_class: 65
2023-06-22 15:02:50,456 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00040078163146972656 sec
2023-06-22 15:02:50,456 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:02:50,458 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001924753189086914 sec
2023-06-22 15:02:50,458 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:02:50,474 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.014963388442993164 sec
2023-06-22 15:02:50,477 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00030231475830078125 sec
2023-06-22 15:02:50,477 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:02:50,479 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001651763916015625 sec
2023-06-22 15:02:50,479 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:02:50,491 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011124610900878906 sec
2023-06-22 15:02:50,496 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00023484230041503906 sec
2023-06-22 15:02:50,498 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001933574676513672 sec
2023-06-22 15:02:50,499 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005304813385009766 sec
2023-06-22 15:02:50,501 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020623207092285156 sec
2023-06-22 15:02:50,502 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003483295440673828 sec
2023-06-22 15:02:50,503 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004985332489013672 sec
2023-06-22 15:02:50,504 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041484832763671875 sec
2023-06-22 15:02:50,505 - WARNING - Finished tracing + transforming absolute for pjit in 0.00029921531677246094 sec
2023-06-22 15:02:50,506 - WARNING - Finished tracing + transforming fn for pjit in 0.0004494190216064453 sec
2023-06-22 15:02:50,508 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005774497985839844 sec
2023-06-22 15:02:50,509 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021529197692871094 sec
2023-06-22 15:02:50,511 - WARNING - Finished tracing + transforming fn for pjit in 0.00041031837463378906 sec
2023-06-22 15:02:50,512 - WARNING - Finished tracing + transforming fn for pjit in 0.0005967617034912109 sec
2023-06-22 15:02:50,513 - WARNING - Finished tracing + transforming fn for pjit in 0.0003921985626220703 sec
2023-06-22 15:02:50,514 - WARNING - Finished tracing + transforming fn for pjit in 0.0004477500915527344 sec
2023-06-22 15:02:50,516 - WARNING - Finished tracing + transforming fn for pjit in 0.00037860870361328125 sec
2023-06-22 15:02:50,519 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00043487548828125 sec
2023-06-22 15:02:50,520 - WARNING - Finished tracing + transforming fn for pjit in 0.0003895759582519531 sec
2023-06-22 15:02:50,522 - WARNING - Finished tracing + transforming fn for pjit in 0.0003993511199951172 sec
2023-06-22 15:02:50,528 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005931854248046875 sec
2023-06-22 15:02:50,528 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016367435455322266 sec
2023-06-22 15:02:50,530 - WARNING - Finished tracing + transforming fn for pjit in 0.0003962516784667969 sec
2023-06-22 15:02:50,531 - WARNING - Finished tracing + transforming fn for pjit in 0.00036334991455078125 sec
2023-06-22 15:02:50,533 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003662109375 sec
2023-06-22 15:02:50,534 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005640983581542969 sec
2023-06-22 15:02:50,535 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032973289489746094 sec
2023-06-22 15:02:50,536 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004565715789794922 sec
2023-06-22 15:02:50,538 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041747093200683594 sec
2023-06-22 15:02:50,539 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003972053527832031 sec
2023-06-22 15:02:50,540 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004894733428955078 sec
2023-06-22 15:02:50,541 - WARNING - Finished tracing + transforming _where for pjit in 0.0015408992767333984 sec
2023-06-22 15:02:50,542 - WARNING - Finished tracing + transforming fn for pjit in 0.00045800209045410156 sec
2023-06-22 15:02:50,543 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005917549133300781 sec
2023-06-22 15:02:50,545 - WARNING - Finished tracing + transforming fn for pjit in 0.0004208087921142578 sec
2023-06-22 15:02:50,546 - WARNING - Finished tracing + transforming fn for pjit in 0.00038933753967285156 sec
2023-06-22 15:02:50,547 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003845691680908203 sec
2023-06-22 15:02:50,549 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004458427429199219 sec
2023-06-22 15:02:50,549 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003070831298828125 sec
2023-06-22 15:02:50,551 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000453948974609375 sec
2023-06-22 15:02:50,552 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041365623474121094 sec
2023-06-22 15:02:50,553 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003859996795654297 sec
2023-06-22 15:02:50,555 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004696846008300781 sec
2023-06-22 15:02:50,555 - WARNING - Finished tracing + transforming _where for pjit in 0.0014796257019042969 sec
2023-06-22 15:02:50,557 - WARNING - Finished tracing + transforming fn for pjit in 0.0004570484161376953 sec
2023-06-22 15:02:50,558 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045037269592285156 sec
2023-06-22 15:02:50,560 - WARNING - Finished tracing + transforming fn for pjit in 0.00039458274841308594 sec
2023-06-22 15:02:50,567 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045180320739746094 sec
2023-06-22 15:02:50,568 - WARNING - Finished tracing + transforming fn for pjit in 0.0004725456237792969 sec
2023-06-22 15:02:50,569 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004673004150390625 sec
2023-06-22 15:02:50,571 - WARNING - Finished tracing + transforming fn for pjit in 0.0005204677581787109 sec
2023-06-22 15:02:50,577 - WARNING - Finished tracing + transforming fn for pjit in 0.0003502368927001953 sec
2023-06-22 15:02:50,580 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00030350685119628906 sec
2023-06-22 15:02:50,581 - WARNING - Finished tracing + transforming fn for pjit in 0.0005323886871337891 sec
2023-06-22 15:02:50,583 - WARNING - Finished tracing + transforming fn for pjit in 0.00039887428283691406 sec
2023-06-22 15:02:50,611 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11556077003479004 sec
2023-06-22 15:02:50,616 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002162456512451172 sec
2023-06-22 15:02:50,617 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020647048950195312 sec
2023-06-22 15:02:50,618 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000469207763671875 sec
2023-06-22 15:02:50,621 - WARNING - Finished tracing + transforming fn for pjit in 0.00034689903259277344 sec
2023-06-22 15:02:50,623 - WARNING - Finished tracing + transforming fn for pjit in 0.00043964385986328125 sec
2023-06-22 15:02:50,625 - WARNING - Finished tracing + transforming fn for pjit in 0.0003559589385986328 sec
2023-06-22 15:02:50,634 - WARNING - Finished tracing + transforming fn for pjit in 0.00037407875061035156 sec
2023-06-22 15:02:50,636 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003917217254638672 sec
2023-06-22 15:02:50,637 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004477500915527344 sec
2023-06-22 15:02:50,638 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031185150146484375 sec
2023-06-22 15:02:50,640 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005781650543212891 sec
2023-06-22 15:02:50,641 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038695335388183594 sec
2023-06-22 15:02:50,642 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003876686096191406 sec
2023-06-22 15:02:50,643 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00046133995056152344 sec
2023-06-22 15:02:50,644 - WARNING - Finished tracing + transforming _where for pjit in 0.0015103816986083984 sec
2023-06-22 15:02:50,645 - WARNING - Finished tracing + transforming fn for pjit in 0.00045609474182128906 sec
2023-06-22 15:02:50,646 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004420280456542969 sec
2023-06-22 15:02:50,648 - WARNING - Finished tracing + transforming fn for pjit in 0.00040030479431152344 sec
2023-06-22 15:02:50,649 - WARNING - Finished tracing + transforming fn for pjit in 0.0005016326904296875 sec
2023-06-22 15:02:50,668 - WARNING - Finished tracing + transforming fn for pjit in 0.0003459453582763672 sec
2023-06-22 15:02:50,698 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08534669876098633 sec
2023-06-22 15:02:50,700 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002186298370361328 sec
2023-06-22 15:02:50,702 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00023031234741210938 sec
2023-06-22 15:02:50,702 - WARNING - Finished tracing + transforming _where for pjit in 0.001100778579711914 sec
2023-06-22 15:02:50,703 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005121231079101562 sec
2023-06-22 15:02:50,704 - WARNING - Finished tracing + transforming trace for pjit in 0.004396200180053711 sec
2023-06-22 15:02:50,708 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00019240379333496094 sec
2023-06-22 15:02:50,709 - WARNING - Finished tracing + transforming tril for pjit in 0.0011019706726074219 sec
2023-06-22 15:02:50,710 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0034127235412597656 sec
2023-06-22 15:02:50,712 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001895427703857422 sec
2023-06-22 15:02:50,712 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019598007202148438 sec
2023-06-22 15:02:50,716 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.002382516860961914 sec
2023-06-22 15:02:50,722 - WARNING - Finished tracing + transforming _solve for pjit in 0.015732288360595703 sec
2023-06-22 15:02:50,723 - WARNING - Finished tracing + transforming dot for pjit in 0.0005474090576171875 sec
2023-06-22 15:02:50,727 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.23437952995300293 sec
2023-06-22 15:02:50,730 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:02:50,782 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05178213119506836 sec
2023-06-22 15:02:50,782 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:02:50,952 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.16974878311157227 sec
2023-06-22 15:02:50,983 - INFO - initial test loss: 0.01503723980372105
2023-06-22 15:02:50,984 - INFO - initial test acc: 0.8499999642372131
2023-06-22 15:02:50,995 - WARNING - Finished tracing + transforming dot for pjit in 0.0008208751678466797 sec
2023-06-22 15:02:50,997 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006830692291259766 sec
2023-06-22 15:02:51,000 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008227825164794922 sec
2023-06-22 15:02:51,001 - WARNING - Finished tracing + transforming _mean for pjit in 0.0022568702697753906 sec
2023-06-22 15:02:51,003 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004496574401855469 sec
2023-06-22 15:02:51,004 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0006215572357177734 sec
2023-06-22 15:02:51,006 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005850791931152344 sec
2023-06-22 15:02:51,009 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0012934207916259766 sec
2023-06-22 15:02:51,011 - WARNING - Finished tracing + transforming _mean for pjit in 0.0032987594604492188 sec
2023-06-22 15:02:51,013 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.024612903594970703 sec
2023-06-22 15:02:51,032 - WARNING - Finished tracing + transforming fn for pjit in 0.0006024837493896484 sec
2023-06-22 15:02:51,034 - WARNING - Finished tracing + transforming fn for pjit in 0.0006237030029296875 sec
2023-06-22 15:02:51,036 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005183219909667969 sec
2023-06-22 15:02:51,038 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006184577941894531 sec
2023-06-22 15:02:51,039 - WARNING - Finished tracing + transforming _where for pjit in 0.0024285316467285156 sec
2023-06-22 15:02:51,059 - WARNING - Finished tracing + transforming fn for pjit in 0.0005862712860107422 sec
2023-06-22 15:02:51,061 - WARNING - Finished tracing + transforming fn for pjit in 0.0008192062377929688 sec
2023-06-22 15:02:51,063 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0007054805755615234 sec
2023-06-22 15:02:51,065 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0007357597351074219 sec
2023-06-22 15:02:51,066 - WARNING - Finished tracing + transforming _where for pjit in 0.0024597644805908203 sec
2023-06-22 15:02:51,125 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003337860107421875 sec
2023-06-22 15:02:51,205 - WARNING - Finished tracing + transforming fn for pjit in 0.0004391670227050781 sec
2023-06-22 15:02:51,207 - WARNING - Finished tracing + transforming fn for pjit in 0.00040459632873535156 sec
2023-06-22 15:02:51,208 - WARNING - Finished tracing + transforming square for pjit in 0.00031113624572753906 sec
2023-06-22 15:02:51,211 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00039196014404296875 sec
2023-06-22 15:02:51,214 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041174888610839844 sec
2023-06-22 15:02:51,216 - WARNING - Finished tracing + transforming fn for pjit in 0.0004818439483642578 sec
2023-06-22 15:02:51,217 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004031658172607422 sec
2023-06-22 15:02:51,218 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041365623474121094 sec
2023-06-22 15:02:51,219 - WARNING - Finished tracing + transforming fn for pjit in 0.00048470497131347656 sec
2023-06-22 15:02:51,220 - WARNING - Finished tracing + transforming fn for pjit in 0.0004076957702636719 sec
2023-06-22 15:02:51,221 - WARNING - Finished tracing + transforming square for pjit in 0.0002999305725097656 sec
2023-06-22 15:02:51,225 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00037169456481933594 sec
2023-06-22 15:02:51,228 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031757354736328125 sec
2023-06-22 15:02:51,229 - WARNING - Finished tracing + transforming fn for pjit in 0.0004448890686035156 sec
2023-06-22 15:02:51,230 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003864765167236328 sec
2023-06-22 15:02:51,231 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004303455352783203 sec
2023-06-22 15:02:51,233 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2454237937927246 sec
2023-06-22 15:02:51,238 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:02:51,337 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.09861540794372559 sec
2023-06-22 15:02:51,337 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:02:51,770 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4322378635406494 sec
2023-06-22 15:02:53,876 - INFO - Distilling data from client: Client04
2023-06-22 15:02:53,877 - INFO - train loss: 0.0030304324551455103
2023-06-22 15:02:53,877 - INFO - train acc: 0.9832401871681213
2023-06-22 15:02:54,094 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.89      0.90       135
           7       0.78      0.82      0.80        65

    accuracy                           0.86       200
   macro avg       0.84      0.85      0.85       200
weighted avg       0.87      0.86      0.87       200

2023-06-22 15:02:54,094 - INFO - test loss 0.01224988218276644
2023-06-22 15:02:54,095 - INFO - test acc 0.8650000095367432
2023-06-22 15:02:56,295 - INFO - Distilling data from client: Client04
2023-06-22 15:02:56,295 - INFO - train loss: 0.002001664214464432
2023-06-22 15:02:56,296 - INFO - train acc: 0.9972066879272461
2023-06-22 15:02:56,724 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.79      0.83      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:02:56,725 - INFO - test loss 0.011810873647327524
2023-06-22 15:02:56,725 - INFO - test acc 0.875
2023-06-22 15:02:58,958 - INFO - Distilling data from client: Client04
2023-06-22 15:02:58,959 - INFO - train loss: 0.0019254022007604349
2023-06-22 15:02:58,959 - INFO - train acc: 0.9972066879272461
2023-06-22 15:02:59,019 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.89      0.90       135
           7       0.78      0.83      0.81        65

    accuracy                           0.87       200
   macro avg       0.85      0.86      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:02:59,020 - INFO - test loss 0.011786808479609775
2023-06-22 15:02:59,020 - INFO - test acc 0.8700000047683716
2023-06-22 15:03:01,370 - INFO - Distilling data from client: Client04
2023-06-22 15:03:01,370 - INFO - train loss: 0.001597850066508066
2023-06-22 15:03:01,370 - INFO - train acc: 0.9944133758544922
2023-06-22 15:03:01,430 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.91      0.91       135
           7       0.81      0.80      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.87      0.88      0.87       200

2023-06-22 15:03:01,431 - INFO - test loss 0.012237086934544646
2023-06-22 15:03:01,432 - INFO - test acc 0.875
2023-06-22 15:03:03,705 - INFO - Distilling data from client: Client04
2023-06-22 15:03:03,707 - INFO - train loss: 0.0016176884443855303
2023-06-22 15:03:03,708 - INFO - train acc: 0.9972066879272461
2023-06-22 15:03:03,768 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.90      0.90       135
           7       0.79      0.82      0.80        65

    accuracy                           0.87       200
   macro avg       0.85      0.86      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:03:03,771 - INFO - test loss 0.011632908822463645
2023-06-22 15:03:03,772 - INFO - test acc 0.8700000047683716
2023-06-22 15:03:06,030 - INFO - Distilling data from client: Client04
2023-06-22 15:03:06,031 - INFO - train loss: 0.0013919155341604876
2023-06-22 15:03:06,031 - INFO - train acc: 1.0
2023-06-22 15:03:06,726 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.80      0.85      0.82        65

    accuracy                           0.88       200
   macro avg       0.86      0.87      0.87       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:03:06,726 - INFO - test loss 0.012367828264906816
2023-06-22 15:03:06,727 - INFO - test acc 0.8799999952316284
2023-06-22 15:03:08,873 - INFO - Distilling data from client: Client04
2023-06-22 15:03:08,874 - INFO - train loss: 0.0013297051045798524
2023-06-22 15:03:08,874 - INFO - train acc: 1.0
2023-06-22 15:03:08,940 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.79      0.83      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:03:08,942 - INFO - test loss 0.01207827750578272
2023-06-22 15:03:08,943 - INFO - test acc 0.875
2023-06-22 15:03:11,244 - INFO - Distilling data from client: Client04
2023-06-22 15:03:11,245 - INFO - train loss: 0.0015773856173434593
2023-06-22 15:03:11,246 - INFO - train acc: 1.0
2023-06-22 15:03:11,288 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.79      0.83      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:03:11,291 - INFO - test loss 0.011844747111590767
2023-06-22 15:03:11,291 - INFO - test acc 0.875
2023-06-22 15:03:13,493 - INFO - Distilling data from client: Client04
2023-06-22 15:03:13,494 - INFO - train loss: 0.001126004406005587
2023-06-22 15:03:13,495 - INFO - train acc: 1.0
2023-06-22 15:03:13,538 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.87      0.89       135
           7       0.76      0.83      0.79        65

    accuracy                           0.86       200
   macro avg       0.84      0.85      0.84       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:03:13,541 - INFO - test loss 0.011962044319220901
2023-06-22 15:03:13,541 - INFO - test acc 0.85999995470047
2023-06-22 15:03:15,885 - INFO - Distilling data from client: Client04
2023-06-22 15:03:15,886 - INFO - train loss: 0.001177334545822923
2023-06-22 15:03:15,887 - INFO - train acc: 0.9972066879272461
2023-06-22 15:03:15,944 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.90      0.91       135
           7       0.80      0.82      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:03:15,945 - INFO - test loss 0.011622684441513976
2023-06-22 15:03:15,945 - INFO - test acc 0.875
2023-06-22 15:03:18,371 - INFO - Distilling data from client: Client04
2023-06-22 15:03:18,372 - INFO - train loss: 0.001123491416840548
2023-06-22 15:03:18,373 - INFO - train acc: 1.0
2023-06-22 15:03:18,417 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.91      0.90       135
           7       0.81      0.78      0.80        65

    accuracy                           0.87       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:03:18,418 - INFO - test loss 0.0117947747297423
2023-06-22 15:03:18,419 - INFO - test acc 0.8700000047683716
2023-06-22 15:03:20,755 - INFO - Distilling data from client: Client04
2023-06-22 15:03:20,755 - INFO - train loss: 0.0011601227484984262
2023-06-22 15:03:20,756 - INFO - train acc: 1.0
2023-06-22 15:03:20,842 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.90      0.91       135
           7       0.80      0.82      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:03:20,852 - INFO - test loss 0.011525417909349191
2023-06-22 15:03:20,852 - INFO - test acc 0.875
2023-06-22 15:03:23,157 - INFO - Distilling data from client: Client04
2023-06-22 15:03:23,157 - INFO - train loss: 0.0011965143354098356
2023-06-22 15:03:23,157 - INFO - train acc: 0.9972066879272461
2023-06-22 15:03:23,200 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.90      0.90       135
           7       0.80      0.78      0.79        65

    accuracy                           0.86       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:03:23,202 - INFO - test loss 0.012066657550344181
2023-06-22 15:03:23,202 - INFO - test acc 0.8650000095367432
2023-06-22 15:03:25,531 - INFO - Distilling data from client: Client04
2023-06-22 15:03:25,531 - INFO - train loss: 0.0009074464064582658
2023-06-22 15:03:25,532 - INFO - train acc: 1.0
2023-06-22 15:03:25,777 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.91      0.92       135
           7       0.82      0.85      0.83        65

    accuracy                           0.89       200
   macro avg       0.87      0.88      0.88       200
weighted avg       0.89      0.89      0.89       200

2023-06-22 15:03:25,777 - INFO - test loss 0.01117706824867707
2023-06-22 15:03:25,777 - INFO - test acc 0.8899999856948853
2023-06-22 15:03:27,963 - INFO - Distilling data from client: Client04
2023-06-22 15:03:27,964 - INFO - train loss: 0.0011434677881128398
2023-06-22 15:03:27,965 - INFO - train acc: 1.0
2023-06-22 15:03:28,014 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.91      0.91       135
           7       0.82      0.83      0.82        65

    accuracy                           0.89       200
   macro avg       0.87      0.87      0.87       200
weighted avg       0.89      0.89      0.89       200

2023-06-22 15:03:28,015 - INFO - test loss 0.01170846617592992
2023-06-22 15:03:28,016 - INFO - test acc 0.8849999904632568
2023-06-22 15:03:30,442 - INFO - Distilling data from client: Client04
2023-06-22 15:03:30,442 - INFO - train loss: 0.0011267675605512804
2023-06-22 15:03:30,443 - INFO - train acc: 1.0
2023-06-22 15:03:30,489 - INFO - report:               precision    recall  f1-score   support

           6       0.93      0.90      0.92       135
           7       0.81      0.86      0.84        65

    accuracy                           0.89       200
   macro avg       0.87      0.88      0.88       200
weighted avg       0.89      0.89      0.89       200

2023-06-22 15:03:30,490 - INFO - test loss 0.011521975286813606
2023-06-22 15:03:30,490 - INFO - test acc 0.8899999856948853
2023-06-22 15:03:32,740 - INFO - Distilling data from client: Client04
2023-06-22 15:03:32,740 - INFO - train loss: 0.0009712756244219286
2023-06-22 15:03:32,741 - INFO - train acc: 1.0
2023-06-22 15:03:32,781 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.91      0.92       135
           7       0.82      0.85      0.83        65

    accuracy                           0.89       200
   macro avg       0.87      0.88      0.88       200
weighted avg       0.89      0.89      0.89       200

2023-06-22 15:03:32,782 - INFO - test loss 0.01088111845019825
2023-06-22 15:03:32,782 - INFO - test acc 0.8899999856948853
2023-06-22 15:03:35,062 - INFO - Distilling data from client: Client04
2023-06-22 15:03:35,063 - INFO - train loss: 0.0010724603557985524
2023-06-22 15:03:35,063 - INFO - train acc: 1.0
2023-06-22 15:03:35,124 - INFO - report:               precision    recall  f1-score   support

           6       0.93      0.90      0.91       135
           7       0.80      0.86      0.83        65

    accuracy                           0.89       200
   macro avg       0.87      0.88      0.87       200
weighted avg       0.89      0.89      0.89       200

2023-06-22 15:03:35,126 - INFO - test loss 0.011329242702300256
2023-06-22 15:03:35,126 - INFO - test acc 0.8849999904632568
2023-06-22 15:03:37,479 - INFO - Distilling data from client: Client04
2023-06-22 15:03:37,480 - INFO - train loss: 0.0010843323084527585
2023-06-22 15:03:37,481 - INFO - train acc: 1.0
2023-06-22 15:03:37,607 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.88      0.89       135
           7       0.77      0.82      0.79        65

    accuracy                           0.86       200
   macro avg       0.84      0.85      0.84       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:03:37,610 - INFO - test loss 0.011728870263930219
2023-06-22 15:03:37,611 - INFO - test acc 0.85999995470047
2023-06-22 15:03:39,860 - INFO - Distilling data from client: Client04
2023-06-22 15:03:39,861 - INFO - train loss: 0.001044210375701615
2023-06-22 15:03:39,861 - INFO - train acc: 1.0
2023-06-22 15:03:39,919 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.81      0.83      0.82        65

    accuracy                           0.88       200
   macro avg       0.86      0.87      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:03:39,920 - INFO - test loss 0.012283711481868928
2023-06-22 15:03:39,920 - INFO - test acc 0.8799999952316284
2023-06-22 15:03:42,222 - INFO - Distilling data from client: Client04
2023-06-22 15:03:42,223 - INFO - train loss: 0.0009553674810152733
2023-06-22 15:03:42,223 - INFO - train acc: 1.0
2023-06-22 15:03:42,278 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.91      0.90       135
           7       0.81      0.78      0.80        65

    accuracy                           0.87       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:03:42,278 - INFO - test loss 0.012179785999212692
2023-06-22 15:03:42,278 - INFO - test acc 0.8700000047683716
2023-06-22 15:03:44,620 - INFO - Distilling data from client: Client04
2023-06-22 15:03:44,621 - INFO - train loss: 0.0011646635755291849
2023-06-22 15:03:44,621 - INFO - train acc: 0.9972066879272461
2023-06-22 15:03:44,683 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.81      0.83      0.82        65

    accuracy                           0.88       200
   macro avg       0.86      0.87      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:03:44,685 - INFO - test loss 0.011573725788448888
2023-06-22 15:03:44,685 - INFO - test acc 0.8799999952316284
2023-06-22 15:03:46,936 - INFO - Distilling data from client: Client04
2023-06-22 15:03:46,937 - INFO - train loss: 0.0009176921260044793
2023-06-22 15:03:46,937 - INFO - train acc: 1.0
2023-06-22 15:03:46,991 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.90      0.90       135
           7       0.80      0.78      0.79        65

    accuracy                           0.86       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:03:46,992 - INFO - test loss 0.0117322178532706
2023-06-22 15:03:46,992 - INFO - test acc 0.8650000095367432
2023-06-22 15:03:49,326 - INFO - Distilling data from client: Client04
2023-06-22 15:03:49,327 - INFO - train loss: 0.0008510398537404917
2023-06-22 15:03:49,328 - INFO - train acc: 1.0
2023-06-22 15:03:49,384 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.92      0.91       135
           7       0.82      0.78      0.80        65

    accuracy                           0.88       200
   macro avg       0.86      0.85      0.86       200
weighted avg       0.87      0.88      0.87       200

2023-06-22 15:03:49,385 - INFO - test loss 0.011739087555942242
2023-06-22 15:03:49,386 - INFO - test acc 0.875
2023-06-22 15:03:51,724 - INFO - Distilling data from client: Client04
2023-06-22 15:03:51,725 - INFO - train loss: 0.0008576477936468531
2023-06-22 15:03:51,725 - INFO - train acc: 1.0
2023-06-22 15:03:51,782 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.92      0.92       135
           7       0.83      0.82      0.82        65

    accuracy                           0.89       200
   macro avg       0.87      0.87      0.87       200
weighted avg       0.88      0.89      0.88       200

2023-06-22 15:03:51,784 - INFO - test loss 0.012194641346814292
2023-06-22 15:03:51,785 - INFO - test acc 0.8849999904632568
2023-06-22 15:03:51,793 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.0008282661437988281 sec
2023-06-22 15:03:51,796 - WARNING - Finished tracing + transforming fn for pjit in 0.0009045600891113281 sec
2023-06-22 15:03:51,799 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(int64[2]), ShapedArray(int64[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:03:51,804 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.0040776729583740234 sec
2023-06-22 15:03:51,806 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:51,822 - WARNING - Finished XLA compilation of jit(fn) in 0.015480279922485352 sec
2023-06-22 15:03:51,825 - WARNING - Finished tracing + transforming jit(add) in 0.0005931854248046875 sec
2023-06-22 15:03:51,826 - DEBUG - Compiling add for with global shapes and types [ShapedArray(int64[2]), ShapedArray(int64[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:03:51,830 - WARNING - Finished jaxpr to MLIR module conversion jit(add) in 0.003367900848388672 sec
2023-06-22 15:03:51,832 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:51,846 - WARNING - Finished XLA compilation of jit(add) in 0.01337122917175293 sec
2023-06-22 15:03:51,848 - WARNING - Finished tracing + transforming jit(select_n) in 0.00046563148498535156 sec
2023-06-22 15:03:51,849 - DEBUG - Compiling select_n for with global shapes and types [ShapedArray(bool[2]), ShapedArray(int64[2]), ShapedArray(int64[2])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:03:51,852 - WARNING - Finished jaxpr to MLIR module conversion jit(select_n) in 0.0024929046630859375 sec
2023-06-22 15:03:51,853 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:51,869 - WARNING - Finished XLA compilation of jit(select_n) in 0.014783382415771484 sec
2023-06-22 15:03:51,872 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003674030303955078 sec
2023-06-22 15:03:51,873 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.00025343894958496094 sec
2023-06-22 15:03:51,874 - DEBUG - Compiling convert_element_type for with global shapes and types [ShapedArray(int64[2])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:03:51,876 - WARNING - Finished jaxpr to MLIR module conversion jit(convert_element_type) in 0.001641988754272461 sec
2023-06-22 15:03:51,876 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:51,885 - WARNING - Finished XLA compilation of jit(convert_element_type) in 0.009114980697631836 sec
2023-06-22 15:03:51,886 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00032401084899902344 sec
2023-06-22 15:03:51,887 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(int32[2])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:03:51,889 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0015745162963867188 sec
2023-06-22 15:03:51,889 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:51,897 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.007532596588134766 sec
2023-06-22 15:03:51,898 - WARNING - Finished tracing + transforming jit(gather) in 0.00034165382385253906 sec
2023-06-22 15:03:51,898 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[358,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:03:51,900 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0017976760864257812 sec
2023-06-22 15:03:51,901 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:51,914 - WARNING - Finished XLA compilation of jit(gather) in 0.013262510299682617 sec
2023-06-22 15:03:51,916 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00034236907958984375 sec
2023-06-22 15:03:51,917 - WARNING - Finished tracing + transforming jit(copy) in 0.00019073486328125 sec
2023-06-22 15:03:51,917 - DEBUG - Compiling copy for with global shapes and types [ShapedArray(float32[2,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:03:51,919 - WARNING - Finished jaxpr to MLIR module conversion jit(copy) in 0.0014781951904296875 sec
2023-06-22 15:03:51,919 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:51,927 - WARNING - Finished XLA compilation of jit(copy) in 0.007489442825317383 sec
2023-06-22 15:03:51,941 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:03:51,954 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:03:51,966 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:03:51,976 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:03:51,979 - WARNING - Finished tracing + transforming _unstack for pjit in 0.0007851123809814453 sec
2023-06-22 15:03:51,979 - DEBUG - Compiling _unstack for with global shapes and types [ShapedArray(float32[2,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:03:51,982 - WARNING - Finished jaxpr to MLIR module conversion jit(_unstack) in 0.0021584033966064453 sec
2023-06-22 15:03:51,982 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:51,997 - WARNING - Finished XLA compilation of jit(_unstack) in 0.01532292366027832 sec
2023-06-22 15:03:52,010 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:03:52,023 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:03:52,453 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client04//synthetic.png
2023-06-22 15:03:52,473 - INFO - c: 1.0 and total_data_in_this_class: 262
2023-06-22 15:03:52,474 - INFO - c: 2.0 and total_data_in_this_class: 269
2023-06-22 15:03:52,474 - INFO - c: 8.0 and total_data_in_this_class: 268
2023-06-22 15:03:52,474 - INFO - c: 1.0 and total_data_in_this_class: 71
2023-06-22 15:03:52,474 - INFO - c: 2.0 and total_data_in_this_class: 64
2023-06-22 15:03:52,474 - INFO - c: 8.0 and total_data_in_this_class: 65
2023-06-22 15:03:52,515 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003867149353027344 sec
2023-06-22 15:03:52,515 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:03:52,517 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0018842220306396484 sec
2023-06-22 15:03:52,518 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:52,534 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.015674352645874023 sec
2023-06-22 15:03:52,537 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003190040588378906 sec
2023-06-22 15:03:52,538 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:03:52,540 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0016586780548095703 sec
2023-06-22 15:03:52,540 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:52,552 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01169586181640625 sec
2023-06-22 15:03:52,557 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019788742065429688 sec
2023-06-22 15:03:52,558 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019693374633789062 sec
2023-06-22 15:03:52,559 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005028247833251953 sec
2023-06-22 15:03:52,562 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003371238708496094 sec
2023-06-22 15:03:52,563 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020384788513183594 sec
2023-06-22 15:03:52,564 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000484466552734375 sec
2023-06-22 15:03:52,565 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004181861877441406 sec
2023-06-22 15:03:52,566 - WARNING - Finished tracing + transforming absolute for pjit in 0.00029921531677246094 sec
2023-06-22 15:03:52,567 - WARNING - Finished tracing + transforming fn for pjit in 0.0004634857177734375 sec
2023-06-22 15:03:52,568 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005750656127929688 sec
2023-06-22 15:03:52,569 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00035834312438964844 sec
2023-06-22 15:03:52,571 - WARNING - Finished tracing + transforming fn for pjit in 0.00038814544677734375 sec
2023-06-22 15:03:52,572 - WARNING - Finished tracing + transforming fn for pjit in 0.00045680999755859375 sec
2023-06-22 15:03:52,573 - WARNING - Finished tracing + transforming fn for pjit in 0.0003859996795654297 sec
2023-06-22 15:03:52,574 - WARNING - Finished tracing + transforming fn for pjit in 0.00045490264892578125 sec
2023-06-22 15:03:52,577 - WARNING - Finished tracing + transforming fn for pjit in 0.00036454200744628906 sec
2023-06-22 15:03:52,580 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002741813659667969 sec
2023-06-22 15:03:52,581 - WARNING - Finished tracing + transforming fn for pjit in 0.00039267539978027344 sec
2023-06-22 15:03:52,582 - WARNING - Finished tracing + transforming fn for pjit in 0.000396728515625 sec
2023-06-22 15:03:52,588 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005772113800048828 sec
2023-06-22 15:03:52,589 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016238689422607422 sec
2023-06-22 15:03:52,590 - WARNING - Finished tracing + transforming fn for pjit in 0.0004017353057861328 sec
2023-06-22 15:03:52,591 - WARNING - Finished tracing + transforming fn for pjit in 0.00038909912109375 sec
2023-06-22 15:03:52,593 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003809928894042969 sec
2023-06-22 15:03:52,594 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045561790466308594 sec
2023-06-22 15:03:52,595 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003070831298828125 sec
2023-06-22 15:03:52,597 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045800209045410156 sec
2023-06-22 15:03:52,598 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040078163146972656 sec
2023-06-22 15:03:52,599 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004076957702636719 sec
2023-06-22 15:03:52,601 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006020069122314453 sec
2023-06-22 15:03:52,601 - WARNING - Finished tracing + transforming _where for pjit in 0.0016529560089111328 sec
2023-06-22 15:03:52,603 - WARNING - Finished tracing + transforming fn for pjit in 0.0005075931549072266 sec
2023-06-22 15:03:52,604 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004470348358154297 sec
2023-06-22 15:03:52,605 - WARNING - Finished tracing + transforming fn for pjit in 0.00039076805114746094 sec
2023-06-22 15:03:52,606 - WARNING - Finished tracing + transforming fn for pjit in 0.0003857612609863281 sec
2023-06-22 15:03:52,608 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003809928894042969 sec
2023-06-22 15:03:52,609 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000457763671875 sec
2023-06-22 15:03:52,611 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0014443397521972656 sec
2023-06-22 15:03:52,612 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045800209045410156 sec
2023-06-22 15:03:52,614 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003879070281982422 sec
2023-06-22 15:03:52,615 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040149688720703125 sec
2023-06-22 15:03:52,616 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004534721374511719 sec
2023-06-22 15:03:52,617 - WARNING - Finished tracing + transforming _where for pjit in 0.001476287841796875 sec
2023-06-22 15:03:52,618 - WARNING - Finished tracing + transforming fn for pjit in 0.0004534721374511719 sec
2023-06-22 15:03:52,619 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004467964172363281 sec
2023-06-22 15:03:52,621 - WARNING - Finished tracing + transforming fn for pjit in 0.0005142688751220703 sec
2023-06-22 15:03:52,628 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004723072052001953 sec
2023-06-22 15:03:52,629 - WARNING - Finished tracing + transforming fn for pjit in 0.0005891323089599609 sec
2023-06-22 15:03:52,630 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046539306640625 sec
2023-06-22 15:03:52,632 - WARNING - Finished tracing + transforming fn for pjit in 0.00040078163146972656 sec
2023-06-22 15:03:52,638 - WARNING - Finished tracing + transforming fn for pjit in 0.0003516674041748047 sec
2023-06-22 15:03:52,641 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000286102294921875 sec
2023-06-22 15:03:52,642 - WARNING - Finished tracing + transforming fn for pjit in 0.0005440711975097656 sec
2023-06-22 15:03:52,644 - WARNING - Finished tracing + transforming fn for pjit in 0.00040435791015625 sec
2023-06-22 15:03:52,671 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11531305313110352 sec
2023-06-22 15:03:52,676 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001990795135498047 sec
2023-06-22 15:03:52,677 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019168853759765625 sec
2023-06-22 15:03:52,678 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004367828369140625 sec
2023-06-22 15:03:52,682 - WARNING - Finished tracing + transforming fn for pjit in 0.0003466606140136719 sec
2023-06-22 15:03:52,683 - WARNING - Finished tracing + transforming fn for pjit in 0.0004456043243408203 sec
2023-06-22 15:03:52,685 - WARNING - Finished tracing + transforming fn for pjit in 0.0003566741943359375 sec
2023-06-22 15:03:52,694 - WARNING - Finished tracing + transforming fn for pjit in 0.000339508056640625 sec
2023-06-22 15:03:52,696 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003826618194580078 sec
2023-06-22 15:03:52,697 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00043964385986328125 sec
2023-06-22 15:03:52,698 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000308990478515625 sec
2023-06-22 15:03:52,700 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005757808685302734 sec
2023-06-22 15:03:52,701 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039076805114746094 sec
2023-06-22 15:03:52,702 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003845691680908203 sec
2023-06-22 15:03:52,704 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004563331604003906 sec
2023-06-22 15:03:52,704 - WARNING - Finished tracing + transforming _where for pjit in 0.0014858245849609375 sec
2023-06-22 15:03:52,706 - WARNING - Finished tracing + transforming fn for pjit in 0.0004551410675048828 sec
2023-06-22 15:03:52,707 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004496574401855469 sec
2023-06-22 15:03:52,708 - WARNING - Finished tracing + transforming fn for pjit in 0.0003936290740966797 sec
2023-06-22 15:03:52,709 - WARNING - Finished tracing + transforming fn for pjit in 0.0005047321319580078 sec
2023-06-22 15:03:52,728 - WARNING - Finished tracing + transforming fn for pjit in 0.0003294944763183594 sec
2023-06-22 15:03:52,759 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08632969856262207 sec
2023-06-22 15:03:52,761 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019598007202148438 sec
2023-06-22 15:03:52,763 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00024175643920898438 sec
2023-06-22 15:03:52,763 - WARNING - Finished tracing + transforming _where for pjit in 0.00107574462890625 sec
2023-06-22 15:03:52,764 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005052089691162109 sec
2023-06-22 15:03:52,765 - WARNING - Finished tracing + transforming trace for pjit in 0.004326820373535156 sec
2023-06-22 15:03:52,769 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0001857280731201172 sec
2023-06-22 15:03:52,770 - WARNING - Finished tracing + transforming tril for pjit in 0.0010840892791748047 sec
2023-06-22 15:03:52,771 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.003093719482421875 sec
2023-06-22 15:03:52,773 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0002009868621826172 sec
2023-06-22 15:03:52,773 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001895427703857422 sec
2023-06-22 15:03:52,777 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0023102760314941406 sec
2023-06-22 15:03:52,783 - WARNING - Finished tracing + transforming _solve for pjit in 0.015255928039550781 sec
2023-06-22 15:03:52,784 - WARNING - Finished tracing + transforming dot for pjit in 0.0005362033843994141 sec
2023-06-22 15:03:52,788 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.2339000701904297 sec
2023-06-22 15:03:52,791 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:03:52,843 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05186319351196289 sec
2023-06-22 15:03:52,843 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:53,016 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17267394065856934 sec
2023-06-22 15:03:53,061 - INFO - initial test loss: 0.018041880918576333
2023-06-22 15:03:53,061 - INFO - initial test acc: 0.8100000023841858
2023-06-22 15:03:53,074 - WARNING - Finished tracing + transforming dot for pjit in 0.0008018016815185547 sec
2023-06-22 15:03:53,076 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00067901611328125 sec
2023-06-22 15:03:53,079 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008184909820556641 sec
2023-06-22 15:03:53,079 - WARNING - Finished tracing + transforming _mean for pjit in 0.0022406578063964844 sec
2023-06-22 15:03:53,082 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004341602325439453 sec
2023-06-22 15:03:53,083 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004334449768066406 sec
2023-06-22 15:03:53,084 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00057220458984375 sec
2023-06-22 15:03:53,086 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008482933044433594 sec
2023-06-22 15:03:53,088 - WARNING - Finished tracing + transforming _mean for pjit in 0.002510547637939453 sec
2023-06-22 15:03:53,089 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.0226590633392334 sec
2023-06-22 15:03:53,109 - WARNING - Finished tracing + transforming fn for pjit in 0.0006039142608642578 sec
2023-06-22 15:03:53,111 - WARNING - Finished tracing + transforming fn for pjit in 0.0006296634674072266 sec
2023-06-22 15:03:53,112 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00052642822265625 sec
2023-06-22 15:03:53,114 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006277561187744141 sec
2023-06-22 15:03:53,115 - WARNING - Finished tracing + transforming _where for pjit in 0.0020639896392822266 sec
2023-06-22 15:03:53,134 - WARNING - Finished tracing + transforming fn for pjit in 0.0006012916564941406 sec
2023-06-22 15:03:53,135 - WARNING - Finished tracing + transforming fn for pjit in 0.0006220340728759766 sec
2023-06-22 15:03:53,137 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005135536193847656 sec
2023-06-22 15:03:53,139 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0007104873657226562 sec
2023-06-22 15:03:53,140 - WARNING - Finished tracing + transforming _where for pjit in 0.0021584033966064453 sec
2023-06-22 15:03:53,198 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003619194030761719 sec
2023-06-22 15:03:53,279 - WARNING - Finished tracing + transforming fn for pjit in 0.0004761219024658203 sec
2023-06-22 15:03:53,281 - WARNING - Finished tracing + transforming fn for pjit in 0.00040793418884277344 sec
2023-06-22 15:03:53,282 - WARNING - Finished tracing + transforming square for pjit in 0.00030541419982910156 sec
2023-06-22 15:03:53,285 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004124641418457031 sec
2023-06-22 15:03:53,288 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004284381866455078 sec
2023-06-22 15:03:53,290 - WARNING - Finished tracing + transforming fn for pjit in 0.0006668567657470703 sec
2023-06-22 15:03:53,291 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00043845176696777344 sec
2023-06-22 15:03:53,292 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005183219909667969 sec
2023-06-22 15:03:53,293 - WARNING - Finished tracing + transforming fn for pjit in 0.0005772113800048828 sec
2023-06-22 15:03:53,295 - WARNING - Finished tracing + transforming fn for pjit in 0.0004730224609375 sec
2023-06-22 15:03:53,296 - WARNING - Finished tracing + transforming square for pjit in 0.0003867149353027344 sec
2023-06-22 15:03:53,299 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003986358642578125 sec
2023-06-22 15:03:53,302 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002911090850830078 sec
2023-06-22 15:03:53,303 - WARNING - Finished tracing + transforming fn for pjit in 0.00047898292541503906 sec
2023-06-22 15:03:53,304 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00039315223693847656 sec
2023-06-22 15:03:53,305 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004162788391113281 sec
2023-06-22 15:03:53,307 - WARNING - Finished tracing + transforming update_fn for pjit in 0.24126577377319336 sec
2023-06-22 15:03:53,313 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:03:53,416 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10255956649780273 sec
2023-06-22 15:03:53,416 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:03:53,859 - WARNING - Finished XLA compilation of jit(update_fn) in 0.44210267066955566 sec
2023-06-22 15:03:56,710 - INFO - Distilling data from client: Client05
2023-06-22 15:03:56,710 - INFO - train loss: 0.0019406408722113363
2023-06-22 15:03:56,710 - INFO - train acc: 0.9961904883384705
2023-06-22 15:03:58,509 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.68      0.74        71
           2       0.75      0.86      0.80        64
           8       0.72      0.77      0.75        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.76       200
weighted avg       0.77      0.77      0.76       200

2023-06-22 15:03:58,510 - INFO - test loss 0.01805364038859203
2023-06-22 15:03:58,510 - INFO - test acc 0.7649999856948853
2023-06-22 15:04:01,488 - INFO - Distilling data from client: Client05
2023-06-22 15:04:01,488 - INFO - train loss: 0.0009889922135156107
2023-06-22 15:04:01,488 - INFO - train acc: 1.0
2023-06-22 15:04:01,722 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.68      0.76        71
           2       0.78      0.84      0.81        64
           8       0.71      0.83      0.77        65

    accuracy                           0.78       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.79      0.78      0.78       200

2023-06-22 15:04:01,723 - INFO - test loss 0.01832993257916108
2023-06-22 15:04:01,723 - INFO - test acc 0.7799999713897705
2023-06-22 15:04:04,804 - INFO - Distilling data from client: Client05
2023-06-22 15:04:04,804 - INFO - train loss: 0.0006984868304665054
2023-06-22 15:04:04,805 - INFO - train acc: 1.0
2023-06-22 15:04:04,890 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.58      0.68        71
           2       0.74      0.83      0.78        64
           8       0.67      0.82      0.74        65

    accuracy                           0.73       200
   macro avg       0.75      0.74      0.73       200
weighted avg       0.75      0.73      0.73       200

2023-06-22 15:04:04,895 - INFO - test loss 0.01870231277047929
2023-06-22 15:04:04,897 - INFO - test acc 0.73499995470047
2023-06-22 15:04:07,979 - INFO - Distilling data from client: Client05
2023-06-22 15:04:07,981 - INFO - train loss: 0.0006353363140633127
2023-06-22 15:04:07,982 - INFO - train acc: 1.0
2023-06-22 15:04:08,040 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.62      0.72        71
           2       0.79      0.88      0.83        64
           8       0.68      0.82      0.74        65

    accuracy                           0.77       200
   macro avg       0.78      0.77      0.76       200
weighted avg       0.78      0.77      0.76       200

2023-06-22 15:04:08,040 - INFO - test loss 0.018825322519661538
2023-06-22 15:04:08,041 - INFO - test acc 0.7649999856948853
2023-06-22 15:04:11,083 - INFO - Distilling data from client: Client05
2023-06-22 15:04:11,084 - INFO - train loss: 0.0004543946150986697
2023-06-22 15:04:11,084 - INFO - train acc: 1.0
2023-06-22 15:04:11,134 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.66      0.73        71
           2       0.76      0.81      0.79        64
           8       0.68      0.78      0.73        65

    accuracy                           0.75       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-06-22 15:04:11,139 - INFO - test loss 0.019230134724125648
2023-06-22 15:04:11,141 - INFO - test acc 0.75
2023-06-22 15:04:14,186 - INFO - Distilling data from client: Client05
2023-06-22 15:04:14,187 - INFO - train loss: 0.0004502553297570195
2023-06-22 15:04:14,187 - INFO - train acc: 1.0
2023-06-22 15:04:14,247 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.65      0.72        71
           2       0.78      0.84      0.81        64
           8       0.69      0.80      0.74        65

    accuracy                           0.76       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:04:14,248 - INFO - test loss 0.018464714879166216
2023-06-22 15:04:14,248 - INFO - test acc 0.7599999904632568
2023-06-22 15:04:17,207 - INFO - Distilling data from client: Client05
2023-06-22 15:04:17,208 - INFO - train loss: 0.00044723403146491273
2023-06-22 15:04:17,208 - INFO - train acc: 1.0
2023-06-22 15:04:17,273 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.66      0.72        71
           2       0.76      0.88      0.81        64
           8       0.70      0.72      0.71        65

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-06-22 15:04:17,276 - INFO - test loss 0.018955302898244077
2023-06-22 15:04:17,276 - INFO - test acc 0.75
2023-06-22 15:04:20,215 - INFO - Distilling data from client: Client05
2023-06-22 15:04:20,216 - INFO - train loss: 0.00031576560535698467
2023-06-22 15:04:20,216 - INFO - train acc: 1.0
2023-06-22 15:04:20,272 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.63      0.73        71
           2       0.79      0.86      0.82        64
           8       0.69      0.82      0.75        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.76       200
weighted avg       0.78      0.77      0.76       200

2023-06-22 15:04:20,273 - INFO - test loss 0.019158379468473753
2023-06-22 15:04:20,273 - INFO - test acc 0.7649999856948853
2023-06-22 15:04:23,194 - INFO - Distilling data from client: Client05
2023-06-22 15:04:23,194 - INFO - train loss: 0.00034937750078342395
2023-06-22 15:04:23,195 - INFO - train acc: 1.0
2023-06-22 15:04:23,262 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.61      0.70        71
           2       0.78      0.88      0.82        64
           8       0.68      0.80      0.74        65

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:04:23,263 - INFO - test loss 0.01849036401186488
2023-06-22 15:04:23,263 - INFO - test acc 0.7549999952316284
2023-06-22 15:04:26,282 - INFO - Distilling data from client: Client05
2023-06-22 15:04:26,282 - INFO - train loss: 0.00033751470772364273
2023-06-22 15:04:26,282 - INFO - train acc: 1.0
2023-06-22 15:04:26,462 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.66      0.74        71
           2       0.81      0.86      0.83        64
           8       0.74      0.86      0.79        65

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.80      0.79      0.79       200

2023-06-22 15:04:26,462 - INFO - test loss 0.018664818969601985
2023-06-22 15:04:26,462 - INFO - test acc 0.7899999618530273
2023-06-22 15:04:29,355 - INFO - Distilling data from client: Client05
2023-06-22 15:04:29,356 - INFO - train loss: 0.0003762319857687779
2023-06-22 15:04:29,356 - INFO - train acc: 1.0
2023-06-22 15:04:29,451 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.65      0.72        71
           2       0.79      0.88      0.83        64
           8       0.71      0.80      0.75        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-06-22 15:04:29,454 - INFO - test loss 0.018773206171551246
2023-06-22 15:04:29,455 - INFO - test acc 0.7699999809265137
2023-06-22 15:04:32,460 - INFO - Distilling data from client: Client05
2023-06-22 15:04:32,460 - INFO - train loss: 0.00029785184462868965
2023-06-22 15:04:32,461 - INFO - train acc: 1.0
2023-06-22 15:04:32,531 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.65      0.73        71
           2       0.78      0.88      0.82        64
           8       0.68      0.77      0.72        65

    accuracy                           0.76       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:04:32,532 - INFO - test loss 0.01931367584494251
2023-06-22 15:04:32,532 - INFO - test acc 0.7599999904632568
2023-06-22 15:04:35,401 - INFO - Distilling data from client: Client05
2023-06-22 15:04:35,401 - INFO - train loss: 0.0002608369014849353
2023-06-22 15:04:35,402 - INFO - train acc: 1.0
2023-06-22 15:04:35,492 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.62      0.70        71
           2       0.76      0.83      0.79        64
           8       0.68      0.78      0.73        65

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:04:35,493 - INFO - test loss 0.018527658944952015
2023-06-22 15:04:35,493 - INFO - test acc 0.7400000095367432
2023-06-22 15:04:38,534 - INFO - Distilling data from client: Client05
2023-06-22 15:04:38,534 - INFO - train loss: 0.0002384066513299852
2023-06-22 15:04:38,534 - INFO - train acc: 1.0
2023-06-22 15:04:38,624 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.66      0.73        71
           2       0.76      0.86      0.81        64
           8       0.70      0.77      0.74        65

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:04:38,624 - INFO - test loss 0.018568184987112765
2023-06-22 15:04:38,624 - INFO - test acc 0.7599999904632568
2023-06-22 15:04:41,725 - INFO - Distilling data from client: Client05
2023-06-22 15:04:41,725 - INFO - train loss: 0.0002911087838295933
2023-06-22 15:04:41,725 - INFO - train acc: 1.0
2023-06-22 15:04:41,786 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.65      0.72        71
           2       0.82      0.86      0.84        64
           8       0.69      0.82      0.75        65

    accuracy                           0.77       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-06-22 15:04:41,788 - INFO - test loss 0.01904036512353064
2023-06-22 15:04:41,788 - INFO - test acc 0.7699999809265137
2023-06-22 15:04:44,782 - INFO - Distilling data from client: Client05
2023-06-22 15:04:44,782 - INFO - train loss: 0.00022132411657825747
2023-06-22 15:04:44,782 - INFO - train acc: 1.0
2023-06-22 15:04:44,843 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.65      0.73        71
           2       0.75      0.84      0.79        64
           8       0.67      0.75      0.71        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.76      0.74      0.74       200

2023-06-22 15:04:44,844 - INFO - test loss 0.01905585258753372
2023-06-22 15:04:44,844 - INFO - test acc 0.7450000047683716
2023-06-22 15:04:47,819 - INFO - Distilling data from client: Client05
2023-06-22 15:04:47,820 - INFO - train loss: 0.00020654849106031782
2023-06-22 15:04:47,821 - INFO - train acc: 1.0
2023-06-22 15:04:47,876 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.65      0.73        71
           2       0.79      0.88      0.83        64
           8       0.69      0.78      0.73        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.76       200
weighted avg       0.77      0.77      0.76       200

2023-06-22 15:04:47,880 - INFO - test loss 0.018968509000828377
2023-06-22 15:04:47,881 - INFO - test acc 0.7649999856948853
2023-06-22 15:04:50,966 - INFO - Distilling data from client: Client05
2023-06-22 15:04:50,966 - INFO - train loss: 0.00021927582313914775
2023-06-22 15:04:50,966 - INFO - train acc: 1.0
2023-06-22 15:04:51,015 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.65      0.73        71
           2       0.81      0.86      0.83        64
           8       0.69      0.82      0.75        65

    accuracy                           0.77       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-06-22 15:04:51,016 - INFO - test loss 0.01847777220085488
2023-06-22 15:04:51,016 - INFO - test acc 0.7699999809265137
2023-06-22 15:04:54,297 - INFO - Distilling data from client: Client05
2023-06-22 15:04:54,297 - INFO - train loss: 0.00016317001453162
2023-06-22 15:04:54,297 - INFO - train acc: 1.0
2023-06-22 15:04:54,373 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.66      0.75        71
           2       0.80      0.89      0.84        64
           8       0.70      0.80      0.75        65

    accuracy                           0.78       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.79      0.78      0.78       200

2023-06-22 15:04:54,374 - INFO - test loss 0.018830865747931354
2023-06-22 15:04:54,374 - INFO - test acc 0.7799999713897705
2023-06-22 15:04:57,501 - INFO - Distilling data from client: Client05
2023-06-22 15:04:57,502 - INFO - train loss: 0.0001817275638722965
2023-06-22 15:04:57,502 - INFO - train acc: 1.0
2023-06-22 15:04:57,563 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.68      0.75        71
           2       0.81      0.88      0.84        64
           8       0.72      0.82      0.76        65

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.78       200
weighted avg       0.79      0.79      0.78       200

2023-06-22 15:04:57,564 - INFO - test loss 0.01849650644037243
2023-06-22 15:04:57,564 - INFO - test acc 0.7849999666213989
2023-06-22 15:05:00,631 - INFO - Distilling data from client: Client05
2023-06-22 15:05:00,632 - INFO - train loss: 0.0002172304839008717
2023-06-22 15:05:00,632 - INFO - train acc: 1.0
2023-06-22 15:05:00,686 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.66      0.74        71
           2       0.79      0.88      0.83        64
           8       0.70      0.78      0.74        65

    accuracy                           0.77       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-06-22 15:05:00,686 - INFO - test loss 0.01851322443199721
2023-06-22 15:05:00,687 - INFO - test acc 0.7699999809265137
2023-06-22 15:05:03,598 - INFO - Distilling data from client: Client05
2023-06-22 15:05:03,599 - INFO - train loss: 0.0001672496702756678
2023-06-22 15:05:03,599 - INFO - train acc: 1.0
2023-06-22 15:05:03,657 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.69      0.75        71
           2       0.75      0.84      0.79        64
           8       0.70      0.74      0.72        65

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:05:03,657 - INFO - test loss 0.018769740588241844
2023-06-22 15:05:03,657 - INFO - test acc 0.7549999952316284
2023-06-22 15:05:06,629 - INFO - Distilling data from client: Client05
2023-06-22 15:05:06,629 - INFO - train loss: 0.0001591085900580096
2023-06-22 15:05:06,629 - INFO - train acc: 1.0
2023-06-22 15:05:06,678 - INFO - report:               precision    recall  f1-score   support

           1       0.81      0.68      0.74        71
           2       0.81      0.84      0.82        64
           8       0.66      0.75      0.71        65

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:05:06,678 - INFO - test loss 0.01870979732808047
2023-06-22 15:05:06,679 - INFO - test acc 0.7549999952316284
2023-06-22 15:05:09,652 - INFO - Distilling data from client: Client05
2023-06-22 15:05:09,653 - INFO - train loss: 0.00014810500529365166
2023-06-22 15:05:09,653 - INFO - train acc: 1.0
2023-06-22 15:05:09,712 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.68      0.76        71
           2       0.81      0.88      0.84        64
           8       0.71      0.82      0.76        65

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.78       200

2023-06-22 15:05:09,713 - INFO - test loss 0.019012839572469752
2023-06-22 15:05:09,713 - INFO - test acc 0.7849999666213989
2023-06-22 15:05:12,727 - INFO - Distilling data from client: Client05
2023-06-22 15:05:12,727 - INFO - train loss: 0.0001522550744173818
2023-06-22 15:05:12,728 - INFO - train acc: 1.0
2023-06-22 15:05:12,792 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.68      0.76        71
           2       0.81      0.84      0.82        64
           8       0.71      0.85      0.77        65

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.80      0.79      0.78       200

2023-06-22 15:05:12,792 - INFO - test loss 0.018573490969597255
2023-06-22 15:05:12,792 - INFO - test acc 0.7849999666213989
2023-06-22 15:05:12,800 - WARNING - Finished tracing + transforming jit(gather) in 0.0007762908935546875 sec
2023-06-22 15:05:12,801 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:05:12,805 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.003422260284423828 sec
2023-06-22 15:05:12,805 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:05:12,827 - WARNING - Finished XLA compilation of jit(gather) in 0.021120309829711914 sec
2023-06-22 15:05:12,851 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:05:12,868 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:05:12,885 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:05:12,898 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:05:12,910 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:05:12,922 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:05:12,935 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:05:12,948 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:05:12,960 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:05:13,526 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client05//synthetic.png
2023-06-22 15:05:13,545 - INFO - c: 4.0 and total_data_in_this_class: 272
2023-06-22 15:05:13,545 - INFO - c: 7.0 and total_data_in_this_class: 259
2023-06-22 15:05:13,545 - INFO - c: 8.0 and total_data_in_this_class: 268
2023-06-22 15:05:13,546 - INFO - c: 4.0 and total_data_in_this_class: 61
2023-06-22 15:05:13,546 - INFO - c: 7.0 and total_data_in_this_class: 74
2023-06-22 15:05:13,546 - INFO - c: 8.0 and total_data_in_this_class: 65
2023-06-22 15:05:13,668 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07719540596008301 sec
2023-06-22 15:05:13,744 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07482576370239258 sec
2023-06-22 15:05:13,753 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16349577903747559 sec
2023-06-22 15:05:13,756 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:05:13,811 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05521202087402344 sec
2023-06-22 15:05:13,811 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:05:13,984 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17209863662719727 sec
2023-06-22 15:05:14,027 - INFO - initial test loss: 0.021889062271158995
2023-06-22 15:05:14,028 - INFO - initial test acc: 0.7149999737739563
2023-06-22 15:05:14,045 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.01232147216796875 sec
2023-06-22 15:05:14,245 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2129209041595459 sec
2023-06-22 15:05:14,250 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:05:14,356 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10562944412231445 sec
2023-06-22 15:05:14,357 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:05:14,798 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4410431385040283 sec
2023-06-22 15:05:17,775 - INFO - Distilling data from client: Client06
2023-06-22 15:05:17,776 - INFO - train loss: 0.0024569279925232995
2023-06-22 15:05:17,776 - INFO - train acc: 0.9961464405059814
2023-06-22 15:05:19,954 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.72      0.67        61
           7       0.71      0.55      0.62        74
           8       0.77      0.85      0.81        65

    accuracy                           0.70       200
   macro avg       0.70      0.71      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:05:19,955 - INFO - test loss 0.02053227755505747
2023-06-22 15:05:19,955 - INFO - test acc 0.699999988079071
2023-06-22 15:05:22,991 - INFO - Distilling data from client: Client06
2023-06-22 15:05:22,991 - INFO - train loss: 0.0012059570895609313
2023-06-22 15:05:22,991 - INFO - train acc: 1.0
2023-06-22 15:05:23,053 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.70      0.64        61
           7       0.70      0.54      0.61        74
           8       0.77      0.82      0.79        65

    accuracy                           0.68       200
   macro avg       0.68      0.69      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:05:23,057 - INFO - test loss 0.02193742697529409
2023-06-22 15:05:23,057 - INFO - test acc 0.6800000071525574
2023-06-22 15:05:26,086 - INFO - Distilling data from client: Client06
2023-06-22 15:05:26,086 - INFO - train loss: 0.0010533486299628063
2023-06-22 15:05:26,086 - INFO - train acc: 1.0
2023-06-22 15:05:28,340 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.75      0.68        61
           7       0.75      0.57      0.65        74
           8       0.77      0.83      0.80        65

    accuracy                           0.71       200
   macro avg       0.71      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:05:28,340 - INFO - test loss 0.020938301018260972
2023-06-22 15:05:28,341 - INFO - test acc 0.7099999785423279
2023-06-22 15:05:31,181 - INFO - Distilling data from client: Client06
2023-06-22 15:05:32,650 - INFO - train loss: 0.0007528927718766017
2023-06-22 15:05:32,651 - INFO - train acc: 1.0
2023-06-22 15:05:32,700 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.72      0.65        61
           7       0.72      0.57      0.64        74
           8       0.78      0.80      0.79        65

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:05:32,701 - INFO - test loss 0.02134931184237977
2023-06-22 15:05:32,701 - INFO - test acc 0.6899999976158142
2023-06-22 15:05:35,488 - INFO - Distilling data from client: Client06
2023-06-22 15:05:35,488 - INFO - train loss: 0.0006340393448946093
2023-06-22 15:05:35,489 - INFO - train acc: 1.0
2023-06-22 15:05:35,549 - INFO - report:               precision    recall  f1-score   support

           4       0.54      0.70      0.61        61
           7       0.73      0.50      0.59        74
           8       0.75      0.80      0.78        65

    accuracy                           0.66       200
   macro avg       0.67      0.67      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-06-22 15:05:35,550 - INFO - test loss 0.02242291135199829
2023-06-22 15:05:35,550 - INFO - test acc 0.6599999666213989
2023-06-22 15:05:38,354 - INFO - Distilling data from client: Client06
2023-06-22 15:05:38,354 - INFO - train loss: 0.000625294489795841
2023-06-22 15:05:38,355 - INFO - train acc: 1.0
2023-06-22 15:05:38,422 - INFO - report:               precision    recall  f1-score   support

           4       0.57      0.69      0.62        61
           7       0.69      0.58      0.63        74
           8       0.80      0.78      0.79        65

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:05:38,423 - INFO - test loss 0.022059083004448237
2023-06-22 15:05:38,423 - INFO - test acc 0.6800000071525574
2023-06-22 15:05:41,419 - INFO - Distilling data from client: Client06
2023-06-22 15:05:41,420 - INFO - train loss: 0.0006940783811691236
2023-06-22 15:05:41,420 - INFO - train acc: 1.0
2023-06-22 15:05:41,479 - INFO - report:               precision    recall  f1-score   support

           4       0.57      0.70      0.63        61
           7       0.72      0.55      0.63        74
           8       0.78      0.82      0.80        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.68       200

2023-06-22 15:05:41,480 - INFO - test loss 0.02223518523471836
2023-06-22 15:05:41,480 - INFO - test acc 0.6850000023841858
2023-06-22 15:05:44,449 - INFO - Distilling data from client: Client06
2023-06-22 15:05:44,450 - INFO - train loss: 0.0007242408537603052
2023-06-22 15:05:44,450 - INFO - train acc: 1.0
2023-06-22 15:05:44,513 - INFO - report:               precision    recall  f1-score   support

           4       0.57      0.72      0.64        61
           7       0.70      0.53      0.60        74
           8       0.78      0.80      0.79        65

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.67       200

2023-06-22 15:05:44,514 - INFO - test loss 0.022485300464608924
2023-06-22 15:05:44,514 - INFO - test acc 0.675000011920929
2023-06-22 15:05:47,550 - INFO - Distilling data from client: Client06
2023-06-22 15:05:47,641 - INFO - train loss: 0.0004972464123396213
2023-06-22 15:05:47,641 - INFO - train acc: 1.0
2023-06-22 15:05:47,690 - INFO - report:               precision    recall  f1-score   support

           4       0.55      0.69      0.61        61
           7       0.70      0.57      0.63        74
           8       0.78      0.77      0.78        65

    accuracy                           0.67       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:05:47,690 - INFO - test loss 0.02230138701074673
2023-06-22 15:05:47,690 - INFO - test acc 0.6699999570846558
2023-06-22 15:05:50,496 - INFO - Distilling data from client: Client06
2023-06-22 15:05:50,496 - INFO - train loss: 0.0004523368874300929
2023-06-22 15:05:50,497 - INFO - train acc: 1.0
2023-06-22 15:05:50,554 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.74      0.65        61
           7       0.73      0.58      0.65        74
           8       0.81      0.80      0.81        65

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:05:50,555 - INFO - test loss 0.021807253932520743
2023-06-22 15:05:50,556 - INFO - test acc 0.699999988079071
2023-06-22 15:05:53,373 - INFO - Distilling data from client: Client06
2023-06-22 15:05:54,773 - INFO - train loss: 0.00045469964104931647
2023-06-22 15:05:54,774 - INFO - train acc: 1.0
2023-06-22 15:05:54,861 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.67      0.63        61
           7       0.70      0.58      0.64        74
           8       0.74      0.80      0.77        65

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:05:54,862 - INFO - test loss 0.022250415651700134
2023-06-22 15:05:54,862 - INFO - test acc 0.6800000071525574
2023-06-22 15:05:57,690 - INFO - Distilling data from client: Client06
2023-06-22 15:05:57,780 - INFO - train loss: 0.00040193738320418285
2023-06-22 15:05:57,780 - INFO - train acc: 1.0
2023-06-22 15:05:57,826 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.72      0.65        61
           7       0.68      0.54      0.60        74
           8       0.80      0.82      0.81        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.68       200

2023-06-22 15:05:57,826 - INFO - test loss 0.022271388154080175
2023-06-22 15:05:57,827 - INFO - test acc 0.6850000023841858
2023-06-22 15:06:00,657 - INFO - Distilling data from client: Client06
2023-06-22 15:06:00,657 - INFO - train loss: 0.0003673301909701394
2023-06-22 15:06:00,657 - INFO - train acc: 1.0
2023-06-22 15:06:00,710 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.69      0.64        61
           7       0.70      0.59      0.64        74
           8       0.79      0.80      0.79        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:06:00,710 - INFO - test loss 0.021866656669680247
2023-06-22 15:06:00,711 - INFO - test acc 0.6899999976158142
2023-06-22 15:06:03,563 - INFO - Distilling data from client: Client06
2023-06-22 15:06:03,571 - INFO - train loss: 0.00026909469194376996
2023-06-22 15:06:03,572 - INFO - train acc: 1.0
2023-06-22 15:06:03,631 - INFO - report:               precision    recall  f1-score   support

           4       0.57      0.70      0.63        61
           7       0.69      0.54      0.61        74
           8       0.80      0.82      0.81        65

    accuracy                           0.68       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:06:03,632 - INFO - test loss 0.022409089354833746
2023-06-22 15:06:03,632 - INFO - test acc 0.6800000071525574
2023-06-22 15:06:06,527 - INFO - Distilling data from client: Client06
2023-06-22 15:06:06,527 - INFO - train loss: 0.0003228108789565348
2023-06-22 15:06:06,527 - INFO - train acc: 1.0
2023-06-22 15:06:07,087 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.74      0.68        61
           7       0.73      0.59      0.66        74
           8       0.78      0.83      0.81        65

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:06:07,088 - INFO - test loss 0.021492611535606828
2023-06-22 15:06:07,088 - INFO - test acc 0.7149999737739563
2023-06-22 15:06:10,007 - INFO - Distilling data from client: Client06
2023-06-22 15:06:10,008 - INFO - train loss: 0.0003438104628541997
2023-06-22 15:06:10,008 - INFO - train acc: 1.0
2023-06-22 15:06:10,069 - INFO - report:               precision    recall  f1-score   support

           4       0.57      0.70      0.63        61
           7       0.69      0.54      0.61        74
           8       0.79      0.80      0.79        65

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.67       200

2023-06-22 15:06:10,071 - INFO - test loss 0.022206008273178528
2023-06-22 15:06:10,071 - INFO - test acc 0.675000011920929
2023-06-22 15:06:13,036 - INFO - Distilling data from client: Client06
2023-06-22 15:06:13,037 - INFO - train loss: 0.00032022358841064656
2023-06-22 15:06:13,037 - INFO - train acc: 1.0
2023-06-22 15:06:13,103 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.72      0.64        61
           7       0.71      0.55      0.62        74
           8       0.79      0.80      0.79        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.68       200

2023-06-22 15:06:13,105 - INFO - test loss 0.02237560067646803
2023-06-22 15:06:13,106 - INFO - test acc 0.6850000023841858
2023-06-22 15:06:15,987 - INFO - Distilling data from client: Client06
2023-06-22 15:06:15,987 - INFO - train loss: 0.00036105726919246717
2023-06-22 15:06:15,988 - INFO - train acc: 1.0
2023-06-22 15:06:16,043 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.70      0.64        61
           7       0.70      0.58      0.64        74
           8       0.77      0.78      0.78        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.68       200

2023-06-22 15:06:16,044 - INFO - test loss 0.02251527940864181
2023-06-22 15:06:16,050 - INFO - test acc 0.6850000023841858
2023-06-22 15:06:18,939 - INFO - Distilling data from client: Client06
2023-06-22 15:06:18,940 - INFO - train loss: 0.0002373317952505498
2023-06-22 15:06:18,940 - INFO - train acc: 1.0
2023-06-22 15:06:18,998 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.67      0.62        61
           7       0.66      0.55      0.60        74
           8       0.79      0.82      0.80        65

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.67       200

2023-06-22 15:06:18,998 - INFO - test loss 0.022255570232075486
2023-06-22 15:06:18,999 - INFO - test acc 0.675000011920929
2023-06-22 15:06:21,987 - INFO - Distilling data from client: Client06
2023-06-22 15:06:21,987 - INFO - train loss: 0.00032059530751857757
2023-06-22 15:06:21,987 - INFO - train acc: 1.0
2023-06-22 15:06:22,037 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.74      0.65        61
           7       0.74      0.54      0.62        74
           8       0.78      0.82      0.80        65

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:06:22,041 - INFO - test loss 0.02226910138412415
2023-06-22 15:06:22,042 - INFO - test acc 0.6899999976158142
2023-06-22 15:06:25,039 - INFO - Distilling data from client: Client06
2023-06-22 15:06:25,041 - INFO - train loss: 0.00023930539562983988
2023-06-22 15:06:25,042 - INFO - train acc: 1.0
2023-06-22 15:06:25,101 - INFO - report:               precision    recall  f1-score   support

           4       0.56      0.72      0.63        61
           7       0.72      0.51      0.60        74
           8       0.75      0.80      0.78        65

    accuracy                           0.67       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:06:25,102 - INFO - test loss 0.022152176140535936
2023-06-22 15:06:25,102 - INFO - test acc 0.6699999570846558
2023-06-22 15:06:28,059 - INFO - Distilling data from client: Client06
2023-06-22 15:06:28,059 - INFO - train loss: 0.0002477116670398119
2023-06-22 15:06:28,059 - INFO - train acc: 1.0
2023-06-22 15:06:28,114 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.69      0.63        61
           7       0.69      0.55      0.62        74
           8       0.75      0.80      0.78        65

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.68      0.68      0.67       200

2023-06-22 15:06:28,116 - INFO - test loss 0.02171520120989947
2023-06-22 15:06:28,116 - INFO - test acc 0.675000011920929
2023-06-22 15:06:31,176 - INFO - Distilling data from client: Client06
2023-06-22 15:06:31,176 - INFO - train loss: 0.0002531673625282584
2023-06-22 15:06:31,176 - INFO - train acc: 1.0
2023-06-22 15:06:31,312 - INFO - report:               precision    recall  f1-score   support

           4       0.56      0.69      0.62        61
           7       0.70      0.53      0.60        74
           8       0.77      0.82      0.79        65

    accuracy                           0.67       200
   macro avg       0.67      0.68      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:06:31,312 - INFO - test loss 0.021951642313852552
2023-06-22 15:06:31,312 - INFO - test acc 0.6699999570846558
2023-06-22 15:06:34,413 - INFO - Distilling data from client: Client06
2023-06-22 15:06:34,413 - INFO - train loss: 0.00021999554878126792
2023-06-22 15:06:34,413 - INFO - train acc: 1.0
2023-06-22 15:06:34,468 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.69      0.63        61
           7       0.69      0.57      0.62        74
           8       0.79      0.82      0.80        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.68       200

2023-06-22 15:06:34,469 - INFO - test loss 0.022016145803943725
2023-06-22 15:06:34,469 - INFO - test acc 0.6850000023841858
2023-06-22 15:06:37,520 - INFO - Distilling data from client: Client06
2023-06-22 15:06:37,521 - INFO - train loss: 0.0003409288201335003
2023-06-22 15:06:37,521 - INFO - train acc: 1.0
2023-06-22 15:06:37,574 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.72      0.65        61
           7       0.71      0.54      0.62        74
           8       0.77      0.82      0.79        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.69      0.68       200

2023-06-22 15:06:37,575 - INFO - test loss 0.02249297961488818
2023-06-22 15:06:37,575 - INFO - test acc 0.6850000023841858
2023-06-22 15:06:37,606 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:06:37,624 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:06:37,644 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:06:37,662 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:06:37,676 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:06:37,688 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:06:37,704 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:06:37,719 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:06:37,733 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:06:38,362 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client06//synthetic.png
2023-06-22 15:06:38,382 - INFO - c: 2.0 and total_data_in_this_class: 277
2023-06-22 15:06:38,383 - INFO - c: 5.0 and total_data_in_this_class: 266
2023-06-22 15:06:38,383 - INFO - c: 8.0 and total_data_in_this_class: 256
2023-06-22 15:06:38,383 - INFO - c: 2.0 and total_data_in_this_class: 56
2023-06-22 15:06:38,383 - INFO - c: 5.0 and total_data_in_this_class: 67
2023-06-22 15:06:38,383 - INFO - c: 8.0 and total_data_in_this_class: 77
2023-06-22 15:06:38,425 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0004925727844238281 sec
2023-06-22 15:06:38,425 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:06:38,428 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0022547245025634766 sec
2023-06-22 15:06:38,428 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:06:38,444 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01619243621826172 sec
2023-06-22 15:06:38,448 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003085136413574219 sec
2023-06-22 15:06:38,448 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:06:38,450 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001590728759765625 sec
2023-06-22 15:06:38,450 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:06:38,463 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.012005090713500977 sec
2023-06-22 15:06:38,468 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022292137145996094 sec
2023-06-22 15:06:38,470 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001995563507080078 sec
2023-06-22 15:06:38,471 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00054168701171875 sec
2023-06-22 15:06:38,473 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003409385681152344 sec
2023-06-22 15:06:38,474 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020551681518554688 sec
2023-06-22 15:06:38,475 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005004405975341797 sec
2023-06-22 15:06:38,476 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041794776916503906 sec
2023-06-22 15:06:38,477 - WARNING - Finished tracing + transforming absolute for pjit in 0.00029921531677246094 sec
2023-06-22 15:06:38,478 - WARNING - Finished tracing + transforming fn for pjit in 0.00046634674072265625 sec
2023-06-22 15:06:38,479 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005829334259033203 sec
2023-06-22 15:06:38,481 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003490447998046875 sec
2023-06-22 15:06:38,482 - WARNING - Finished tracing + transforming fn for pjit in 0.00036787986755371094 sec
2023-06-22 15:06:38,484 - WARNING - Finished tracing + transforming fn for pjit in 0.00045228004455566406 sec
2023-06-22 15:06:38,485 - WARNING - Finished tracing + transforming fn for pjit in 0.00036716461181640625 sec
2023-06-22 15:06:38,486 - WARNING - Finished tracing + transforming fn for pjit in 0.0004570484161376953 sec
2023-06-22 15:06:38,488 - WARNING - Finished tracing + transforming fn for pjit in 0.00039124488830566406 sec
2023-06-22 15:06:38,491 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00029206275939941406 sec
2023-06-22 15:06:38,492 - WARNING - Finished tracing + transforming fn for pjit in 0.0004024505615234375 sec
2023-06-22 15:06:38,493 - WARNING - Finished tracing + transforming fn for pjit in 0.0004012584686279297 sec
2023-06-22 15:06:38,499 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006015300750732422 sec
2023-06-22 15:06:38,500 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016417503356933594 sec
2023-06-22 15:06:38,502 - WARNING - Finished tracing + transforming fn for pjit in 0.0003826618194580078 sec
2023-06-22 15:06:38,503 - WARNING - Finished tracing + transforming fn for pjit in 0.00039768218994140625 sec
2023-06-22 15:06:38,504 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038123130798339844 sec
2023-06-22 15:06:38,506 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00047016143798828125 sec
2023-06-22 15:06:38,507 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003235340118408203 sec
2023-06-22 15:06:38,508 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004680156707763672 sec
2023-06-22 15:06:38,510 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004024505615234375 sec
2023-06-22 15:06:38,511 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003974437713623047 sec
2023-06-22 15:06:38,512 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.000629425048828125 sec
2023-06-22 15:06:38,513 - WARNING - Finished tracing + transforming _where for pjit in 0.0017042160034179688 sec
2023-06-22 15:06:38,514 - WARNING - Finished tracing + transforming fn for pjit in 0.0004258155822753906 sec
2023-06-22 15:06:38,515 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045371055603027344 sec
2023-06-22 15:06:38,517 - WARNING - Finished tracing + transforming fn for pjit in 0.00039696693420410156 sec
2023-06-22 15:06:38,518 - WARNING - Finished tracing + transforming fn for pjit in 0.0003979206085205078 sec
2023-06-22 15:06:38,519 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003879070281982422 sec
2023-06-22 15:06:38,520 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045752525329589844 sec
2023-06-22 15:06:38,522 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046515464782714844 sec
2023-06-22 15:06:38,523 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004601478576660156 sec
2023-06-22 15:06:38,524 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039768218994140625 sec
2023-06-22 15:06:38,525 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003943443298339844 sec
2023-06-22 15:06:38,527 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004734992980957031 sec
2023-06-22 15:06:38,528 - WARNING - Finished tracing + transforming _where for pjit in 0.001508951187133789 sec
2023-06-22 15:06:38,529 - WARNING - Finished tracing + transforming fn for pjit in 0.00045943260192871094 sec
2023-06-22 15:06:38,530 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004515647888183594 sec
2023-06-22 15:06:38,533 - WARNING - Finished tracing + transforming fn for pjit in 0.0003981590270996094 sec
2023-06-22 15:06:38,540 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004622936248779297 sec
2023-06-22 15:06:38,541 - WARNING - Finished tracing + transforming fn for pjit in 0.0005700588226318359 sec
2023-06-22 15:06:38,543 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046944618225097656 sec
2023-06-22 15:06:38,544 - WARNING - Finished tracing + transforming fn for pjit in 0.0003933906555175781 sec
2023-06-22 15:06:38,550 - WARNING - Finished tracing + transforming fn for pjit in 0.0003654956817626953 sec
2023-06-22 15:06:38,553 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002727508544921875 sec
2023-06-22 15:06:38,554 - WARNING - Finished tracing + transforming fn for pjit in 0.0005409717559814453 sec
2023-06-22 15:06:38,556 - WARNING - Finished tracing + transforming fn for pjit in 0.0004010200500488281 sec
2023-06-22 15:06:38,584 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11697936058044434 sec
2023-06-22 15:06:38,589 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002048015594482422 sec
2023-06-22 15:06:38,590 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020575523376464844 sec
2023-06-22 15:06:38,591 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00048279762268066406 sec
2023-06-22 15:06:38,595 - WARNING - Finished tracing + transforming fn for pjit in 0.000362396240234375 sec
2023-06-22 15:06:38,596 - WARNING - Finished tracing + transforming fn for pjit in 0.0004451274871826172 sec
2023-06-22 15:06:38,598 - WARNING - Finished tracing + transforming fn for pjit in 0.0003597736358642578 sec
2023-06-22 15:06:38,608 - WARNING - Finished tracing + transforming fn for pjit in 0.0003783702850341797 sec
2023-06-22 15:06:38,610 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00035834312438964844 sec
2023-06-22 15:06:38,611 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045371055603027344 sec
2023-06-22 15:06:38,612 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003066062927246094 sec
2023-06-22 15:06:38,613 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005772113800048828 sec
2023-06-22 15:06:38,615 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004029273986816406 sec
2023-06-22 15:06:38,616 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003910064697265625 sec
2023-06-22 15:06:38,617 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004925727844238281 sec
2023-06-22 15:06:38,618 - WARNING - Finished tracing + transforming _where for pjit in 0.0015387535095214844 sec
2023-06-22 15:06:38,619 - WARNING - Finished tracing + transforming fn for pjit in 0.0004725456237792969 sec
2023-06-22 15:06:38,620 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044226646423339844 sec
2023-06-22 15:06:38,622 - WARNING - Finished tracing + transforming fn for pjit in 0.00039505958557128906 sec
2023-06-22 15:06:38,623 - WARNING - Finished tracing + transforming fn for pjit in 0.0005166530609130859 sec
2023-06-22 15:06:38,643 - WARNING - Finished tracing + transforming fn for pjit in 0.00037789344787597656 sec
2023-06-22 15:06:38,677 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0911250114440918 sec
2023-06-22 15:06:38,679 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022268295288085938 sec
2023-06-22 15:06:38,681 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00023555755615234375 sec
2023-06-22 15:06:38,681 - WARNING - Finished tracing + transforming _where for pjit in 0.0010766983032226562 sec
2023-06-22 15:06:38,682 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005040168762207031 sec
2023-06-22 15:06:38,683 - WARNING - Finished tracing + transforming trace for pjit in 0.004380702972412109 sec
2023-06-22 15:06:38,687 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0001881122589111328 sec
2023-06-22 15:06:38,688 - WARNING - Finished tracing + transforming tril for pjit in 0.001096963882446289 sec
2023-06-22 15:06:38,689 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0031464099884033203 sec
2023-06-22 15:06:38,690 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001914501190185547 sec
2023-06-22 15:06:38,691 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001957416534423828 sec
2023-06-22 15:06:38,695 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0024099349975585938 sec
2023-06-22 15:06:38,701 - WARNING - Finished tracing + transforming _solve for pjit in 0.015845298767089844 sec
2023-06-22 15:06:38,702 - WARNING - Finished tracing + transforming dot for pjit in 0.0005424022674560547 sec
2023-06-22 15:06:38,706 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.24149870872497559 sec
2023-06-22 15:06:38,710 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:06:38,767 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05729794502258301 sec
2023-06-22 15:06:38,767 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:06:38,950 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1824185848236084 sec
2023-06-22 15:06:39,013 - INFO - initial test loss: 0.024133350626022797
2023-06-22 15:06:39,013 - INFO - initial test acc: 0.6800000071525574
2023-06-22 15:06:39,027 - WARNING - Finished tracing + transforming dot for pjit in 0.0008609294891357422 sec
2023-06-22 15:06:39,029 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0007445812225341797 sec
2023-06-22 15:06:39,032 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008544921875 sec
2023-06-22 15:06:39,033 - WARNING - Finished tracing + transforming _mean for pjit in 0.002829313278198242 sec
2023-06-22 15:06:39,035 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004885196685791016 sec
2023-06-22 15:06:39,037 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00044465065002441406 sec
2023-06-22 15:06:39,038 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006203651428222656 sec
2023-06-22 15:06:39,041 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008797645568847656 sec
2023-06-22 15:06:39,042 - WARNING - Finished tracing + transforming _mean for pjit in 0.002610445022583008 sec
2023-06-22 15:06:39,044 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.024616718292236328 sec
2023-06-22 15:06:39,065 - WARNING - Finished tracing + transforming fn for pjit in 0.0006339550018310547 sec
2023-06-22 15:06:39,067 - WARNING - Finished tracing + transforming fn for pjit in 0.0006351470947265625 sec
2023-06-22 15:06:39,070 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0007352828979492188 sec
2023-06-22 15:06:39,072 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00067901611328125 sec
2023-06-22 15:06:39,074 - WARNING - Finished tracing + transforming _where for pjit in 0.002881765365600586 sec
2023-06-22 15:06:39,095 - WARNING - Finished tracing + transforming fn for pjit in 0.0006363391876220703 sec
2023-06-22 15:06:39,097 - WARNING - Finished tracing + transforming fn for pjit in 0.0008618831634521484 sec
2023-06-22 15:06:39,099 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005652904510498047 sec
2023-06-22 15:06:39,102 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.000728607177734375 sec
2023-06-22 15:06:39,103 - WARNING - Finished tracing + transforming _where for pjit in 0.0028705596923828125 sec
2023-06-22 15:06:39,161 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037026405334472656 sec
2023-06-22 15:06:39,251 - WARNING - Finished tracing + transforming fn for pjit in 0.0005214214324951172 sec
2023-06-22 15:06:39,253 - WARNING - Finished tracing + transforming fn for pjit in 0.00042247772216796875 sec
2023-06-22 15:06:39,254 - WARNING - Finished tracing + transforming square for pjit in 0.0003190040588378906 sec
2023-06-22 15:06:39,257 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004036426544189453 sec
2023-06-22 15:06:39,260 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004565715789794922 sec
2023-06-22 15:06:39,261 - WARNING - Finished tracing + transforming fn for pjit in 0.0004875659942626953 sec
2023-06-22 15:06:39,263 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004112720489501953 sec
2023-06-22 15:06:39,263 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004200935363769531 sec
2023-06-22 15:06:39,265 - WARNING - Finished tracing + transforming fn for pjit in 0.0004773139953613281 sec
2023-06-22 15:06:39,266 - WARNING - Finished tracing + transforming fn for pjit in 0.0004086494445800781 sec
2023-06-22 15:06:39,267 - WARNING - Finished tracing + transforming square for pjit in 0.0003230571746826172 sec
2023-06-22 15:06:39,270 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003886222839355469 sec
2023-06-22 15:06:39,273 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031685829162597656 sec
2023-06-22 15:06:39,274 - WARNING - Finished tracing + transforming fn for pjit in 0.000476837158203125 sec
2023-06-22 15:06:39,275 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000396728515625 sec
2023-06-22 15:06:39,276 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004150867462158203 sec
2023-06-22 15:06:39,278 - WARNING - Finished tracing + transforming update_fn for pjit in 0.25999021530151367 sec
2023-06-22 15:06:39,284 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:06:39,394 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10998940467834473 sec
2023-06-22 15:06:39,395 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:06:39,868 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4725468158721924 sec
2023-06-22 15:06:42,732 - INFO - Distilling data from client: Client07
2023-06-22 15:06:42,732 - INFO - train loss: 0.0031251376653622285
2023-06-22 15:06:42,733 - INFO - train acc: 0.9902534484863281
2023-06-22 15:06:43,013 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.66      0.60        56
           5       0.68      0.60      0.63        67
           8       0.81      0.77      0.79        77

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:06:43,013 - INFO - test loss 0.023073886497011628
2023-06-22 15:06:43,014 - INFO - test acc 0.6800000071525574
2023-06-22 15:06:45,985 - INFO - Distilling data from client: Client07
2023-06-22 15:06:45,986 - INFO - train loss: 0.0016466450895520261
2023-06-22 15:06:45,986 - INFO - train acc: 0.9961013793945312
2023-06-22 15:06:46,293 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.62      0.57        56
           5       0.70      0.60      0.65        67
           8       0.82      0.81      0.81        77

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:06:46,294 - INFO - test loss 0.022133899742013477
2023-06-22 15:06:46,294 - INFO - test acc 0.6850000023841858
2023-06-22 15:06:49,158 - INFO - Distilling data from client: Client07
2023-06-22 15:06:49,158 - INFO - train loss: 0.0012577700417795992
2023-06-22 15:06:49,158 - INFO - train acc: 1.0
2023-06-22 15:06:49,243 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.62      0.57        56
           5       0.67      0.57      0.61        67
           8       0.79      0.78      0.78        77

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:06:49,244 - INFO - test loss 0.023226944639340576
2023-06-22 15:06:49,245 - INFO - test acc 0.6649999618530273
2023-06-22 15:06:52,162 - INFO - Distilling data from client: Client07
2023-06-22 15:06:52,163 - INFO - train loss: 0.0009552148213717584
2023-06-22 15:06:52,163 - INFO - train acc: 1.0
2023-06-22 15:06:52,225 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.61      0.55        56
           5       0.67      0.60      0.63        67
           8       0.81      0.77      0.79        77

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:06:52,226 - INFO - test loss 0.02342905776215396
2023-06-22 15:06:52,226 - INFO - test acc 0.6649999618530273
2023-06-22 15:06:55,123 - INFO - Distilling data from client: Client07
2023-06-22 15:06:55,123 - INFO - train loss: 0.0008461273914770912
2023-06-22 15:06:55,123 - INFO - train acc: 1.0
2023-06-22 15:06:55,186 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.68      0.60        56
           5       0.68      0.60      0.63        67
           8       0.83      0.75      0.79        77

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.70      0.68      0.68       200

2023-06-22 15:06:55,187 - INFO - test loss 0.023920677446792857
2023-06-22 15:06:55,187 - INFO - test acc 0.6800000071525574
2023-06-22 15:06:58,180 - INFO - Distilling data from client: Client07
2023-06-22 15:06:58,181 - INFO - train loss: 0.0006546909150471652
2023-06-22 15:06:58,181 - INFO - train acc: 1.0
2023-06-22 15:06:58,242 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.68      0.58        56
           5       0.69      0.55      0.61        67
           8       0.83      0.77      0.80        77

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.66       200
weighted avg       0.69      0.67      0.67       200

2023-06-22 15:06:58,247 - INFO - test loss 0.024688959540708624
2023-06-22 15:06:58,248 - INFO - test acc 0.6699999570846558
2023-06-22 15:07:01,171 - INFO - Distilling data from client: Client07
2023-06-22 15:07:01,172 - INFO - train loss: 0.0008743546498756667
2023-06-22 15:07:01,172 - INFO - train acc: 1.0
2023-06-22 15:07:01,232 - INFO - report:               precision    recall  f1-score   support

           2       0.55      0.66      0.60        56
           5       0.64      0.58      0.61        67
           8       0.82      0.77      0.79        77

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:07:01,233 - INFO - test loss 0.024200930595057863
2023-06-22 15:07:01,233 - INFO - test acc 0.675000011920929
2023-06-22 15:07:04,125 - INFO - Distilling data from client: Client07
2023-06-22 15:07:04,126 - INFO - train loss: 0.0007156299439788705
2023-06-22 15:07:04,127 - INFO - train acc: 1.0
2023-06-22 15:07:04,193 - INFO - report:               precision    recall  f1-score   support

           2       0.49      0.59      0.54        56
           5       0.67      0.60      0.63        67
           8       0.81      0.77      0.79        77

    accuracy                           0.66       200
   macro avg       0.66      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:07:04,193 - INFO - test loss 0.024571266623915813
2023-06-22 15:07:04,194 - INFO - test acc 0.6599999666213989
2023-06-22 15:07:07,167 - INFO - Distilling data from client: Client07
2023-06-22 15:07:07,168 - INFO - train loss: 0.0005676647221171174
2023-06-22 15:07:07,170 - INFO - train acc: 1.0
2023-06-22 15:07:07,236 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.62      0.57        56
           5       0.71      0.61      0.66        67
           8       0.80      0.78      0.79        77

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:07:07,236 - INFO - test loss 0.023864791911882677
2023-06-22 15:07:07,237 - INFO - test acc 0.6800000071525574
2023-06-22 15:07:10,109 - INFO - Distilling data from client: Client07
2023-06-22 15:07:10,109 - INFO - train loss: 0.0004933935414539755
2023-06-22 15:07:10,110 - INFO - train acc: 1.0
2023-06-22 15:07:10,170 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.68      0.60        56
           5       0.68      0.58      0.63        67
           8       0.81      0.75      0.78        77

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:07:10,172 - INFO - test loss 0.024041940246572725
2023-06-22 15:07:10,173 - INFO - test acc 0.675000011920929
2023-06-22 15:07:13,164 - INFO - Distilling data from client: Client07
2023-06-22 15:07:13,165 - INFO - train loss: 0.0006069319626258767
2023-06-22 15:07:13,165 - INFO - train acc: 1.0
2023-06-22 15:07:13,217 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.62      0.56        56
           5       0.62      0.57      0.59        67
           8       0.81      0.74      0.78        77

    accuracy                           0.65       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.66      0.65      0.65       200

2023-06-22 15:07:13,218 - INFO - test loss 0.024650609610935972
2023-06-22 15:07:13,218 - INFO - test acc 0.6499999761581421
2023-06-22 15:07:16,155 - INFO - Distilling data from client: Client07
2023-06-22 15:07:16,155 - INFO - train loss: 0.0006148425590345733
2023-06-22 15:07:16,155 - INFO - train acc: 1.0
2023-06-22 15:07:16,207 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.61      0.56        56
           5       0.67      0.61      0.64        67
           8       0.81      0.77      0.79        77

    accuracy                           0.67       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:07:16,214 - INFO - test loss 0.024063289684981257
2023-06-22 15:07:16,214 - INFO - test acc 0.6699999570846558
2023-06-22 15:07:19,263 - INFO - Distilling data from client: Client07
2023-06-22 15:07:19,264 - INFO - train loss: 0.00047062255679618806
2023-06-22 15:07:19,265 - INFO - train acc: 1.0
2023-06-22 15:07:19,324 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.66      0.57        56
           5       0.64      0.54      0.59        67
           8       0.80      0.74      0.77        77

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.64       200
weighted avg       0.67      0.65      0.65       200

2023-06-22 15:07:19,325 - INFO - test loss 0.024769372471074483
2023-06-22 15:07:19,326 - INFO - test acc 0.6499999761581421
2023-06-22 15:07:22,367 - INFO - Distilling data from client: Client07
2023-06-22 15:07:22,368 - INFO - train loss: 0.00046799145722727027
2023-06-22 15:07:22,368 - INFO - train acc: 1.0
2023-06-22 15:07:22,428 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.66      0.58        56
           5       0.64      0.52      0.57        67
           8       0.80      0.77      0.78        77

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:07:22,430 - INFO - test loss 0.024203682763574527
2023-06-22 15:07:22,430 - INFO - test acc 0.6549999713897705
2023-06-22 15:07:25,473 - INFO - Distilling data from client: Client07
2023-06-22 15:07:25,474 - INFO - train loss: 0.00044668138972834404
2023-06-22 15:07:25,474 - INFO - train acc: 1.0
2023-06-22 15:07:25,536 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.64      0.57        56
           5       0.65      0.55      0.60        67
           8       0.81      0.77      0.79        77

    accuracy                           0.66       200
   macro avg       0.66      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:07:25,537 - INFO - test loss 0.024608705479892766
2023-06-22 15:07:25,537 - INFO - test acc 0.6599999666213989
2023-06-22 15:07:28,410 - INFO - Distilling data from client: Client07
2023-06-22 15:07:28,410 - INFO - train loss: 0.0004567342625368628
2023-06-22 15:07:28,410 - INFO - train acc: 1.0
2023-06-22 15:07:28,459 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.66      0.57        56
           5       0.65      0.54      0.59        67
           8       0.82      0.77      0.79        77

    accuracy                           0.66       200
   macro avg       0.66      0.65      0.65       200
weighted avg       0.68      0.66      0.66       200

2023-06-22 15:07:28,459 - INFO - test loss 0.02402646812768429
2023-06-22 15:07:28,459 - INFO - test acc 0.6599999666213989
2023-06-22 15:07:31,401 - INFO - Distilling data from client: Client07
2023-06-22 15:07:31,401 - INFO - train loss: 0.0004979817141479301
2023-06-22 15:07:31,402 - INFO - train acc: 1.0
2023-06-22 15:07:31,452 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.64      0.57        56
           5       0.64      0.55      0.59        67
           8       0.82      0.75      0.78        77

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:07:31,453 - INFO - test loss 0.023956375600368267
2023-06-22 15:07:31,453 - INFO - test acc 0.6549999713897705
2023-06-22 15:07:34,237 - INFO - Distilling data from client: Client07
2023-06-22 15:07:34,238 - INFO - train loss: 0.0004914769433678057
2023-06-22 15:07:34,238 - INFO - train acc: 1.0
2023-06-22 15:07:34,293 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.66      0.57        56
           5       0.67      0.55      0.61        67
           8       0.81      0.75      0.78        77

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.65       200
weighted avg       0.68      0.66      0.66       200

2023-06-22 15:07:34,293 - INFO - test loss 0.024944235398520503
2023-06-22 15:07:34,294 - INFO - test acc 0.6599999666213989
2023-06-22 15:07:37,165 - INFO - Distilling data from client: Client07
2023-06-22 15:07:37,166 - INFO - train loss: 0.0003899600985906108
2023-06-22 15:07:37,167 - INFO - train acc: 1.0
2023-06-22 15:07:37,223 - INFO - report:               precision    recall  f1-score   support

           2       0.50      0.57      0.53        56
           5       0.61      0.58      0.60        67
           8       0.81      0.75      0.78        77

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:07:37,224 - INFO - test loss 0.02454482619859705
2023-06-22 15:07:37,225 - INFO - test acc 0.6449999809265137
2023-06-22 15:07:40,098 - INFO - Distilling data from client: Client07
2023-06-22 15:07:40,104 - INFO - train loss: 0.00040317062157168665
2023-06-22 15:07:40,105 - INFO - train acc: 1.0
2023-06-22 15:07:40,161 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.66      0.59        56
           5       0.63      0.57      0.60        67
           8       0.81      0.74      0.78        77

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:07:40,163 - INFO - test loss 0.024801658933948492
2023-06-22 15:07:40,163 - INFO - test acc 0.6599999666213989
2023-06-22 15:07:42,961 - INFO - Distilling data from client: Client07
2023-06-22 15:07:42,961 - INFO - train loss: 0.0003586486225990771
2023-06-22 15:07:42,961 - INFO - train acc: 1.0
2023-06-22 15:07:43,020 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.62      0.56        56
           5       0.65      0.58      0.61        67
           8       0.82      0.77      0.79        77

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:07:43,021 - INFO - test loss 0.023961432319101698
2023-06-22 15:07:43,021 - INFO - test acc 0.6649999618530273
2023-06-22 15:07:45,865 - INFO - Distilling data from client: Client07
2023-06-22 15:07:45,866 - INFO - train loss: 0.0002807876085436567
2023-06-22 15:07:45,866 - INFO - train acc: 1.0
2023-06-22 15:07:45,922 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.64      0.57        56
           5       0.64      0.57      0.60        67
           8       0.82      0.75      0.78        77

    accuracy                           0.66       200
   macro avg       0.66      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:07:45,923 - INFO - test loss 0.024855195737932774
2023-06-22 15:07:45,925 - INFO - test acc 0.6599999666213989
2023-06-22 15:07:48,774 - INFO - Distilling data from client: Client07
2023-06-22 15:07:48,774 - INFO - train loss: 0.00038833843611795095
2023-06-22 15:07:48,774 - INFO - train acc: 1.0
2023-06-22 15:07:48,832 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.64      0.57        56
           5       0.66      0.58      0.62        67
           8       0.81      0.74      0.78        77

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.65       200
weighted avg       0.68      0.66      0.66       200

2023-06-22 15:07:48,833 - INFO - test loss 0.024158526461701548
2023-06-22 15:07:48,833 - INFO - test acc 0.6599999666213989
2023-06-22 15:07:51,628 - INFO - Distilling data from client: Client07
2023-06-22 15:07:51,629 - INFO - train loss: 0.0004315888214339617
2023-06-22 15:07:51,629 - INFO - train acc: 1.0
2023-06-22 15:07:51,694 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.66      0.59        56
           5       0.68      0.61      0.65        67
           8       0.83      0.77      0.80        77

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:07:51,707 - INFO - test loss 0.024432785964164407
2023-06-22 15:07:51,707 - INFO - test acc 0.6850000023841858
2023-06-22 15:07:54,525 - INFO - Distilling data from client: Client07
2023-06-22 15:07:54,525 - INFO - train loss: 0.00032449120052904237
2023-06-22 15:07:54,525 - INFO - train acc: 1.0
2023-06-22 15:07:54,584 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.68      0.60        56
           5       0.66      0.58      0.62        67
           8       0.83      0.75      0.79        77

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:07:54,586 - INFO - test loss 0.024633606068940744
2023-06-22 15:07:54,586 - INFO - test acc 0.675000011920929
2023-06-22 15:07:54,620 - WARNING - Finished tracing + transforming jit(gather) in 0.0017447471618652344 sec
2023-06-22 15:07:54,621 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:07:54,627 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0050504207611083984 sec
2023-06-22 15:07:54,628 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:07:54,652 - WARNING - Finished XLA compilation of jit(gather) in 0.0228726863861084 sec
2023-06-22 15:07:54,692 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:07:54,703 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:07:54,714 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:07:54,726 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:07:54,737 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:07:54,748 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:07:54,761 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:07:54,773 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:07:54,785 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:07:55,359 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client07//synthetic.png
2023-06-22 15:07:55,379 - INFO - c: 0.0 and total_data_in_this_class: 260
2023-06-22 15:07:55,379 - INFO - c: 1.0 and total_data_in_this_class: 269
2023-06-22 15:07:55,379 - INFO - c: 5.0 and total_data_in_this_class: 270
2023-06-22 15:07:55,380 - INFO - c: 0.0 and total_data_in_this_class: 73
2023-06-22 15:07:55,380 - INFO - c: 1.0 and total_data_in_this_class: 64
2023-06-22 15:07:55,380 - INFO - c: 5.0 and total_data_in_this_class: 63
2023-06-22 15:07:55,502 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07566428184509277 sec
2023-06-22 15:07:55,579 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07558608055114746 sec
2023-06-22 15:07:55,587 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16288995742797852 sec
2023-06-22 15:07:55,590 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:07:55,645 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05479884147644043 sec
2023-06-22 15:07:55,646 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:07:55,824 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17801880836486816 sec
2023-06-22 15:07:55,894 - INFO - initial test loss: 0.02292455103742306
2023-06-22 15:07:55,894 - INFO - initial test acc: 0.75
2023-06-22 15:07:55,913 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012933015823364258 sec
2023-06-22 15:07:56,111 - WARNING - Finished tracing + transforming update_fn for pjit in 0.21193885803222656 sec
2023-06-22 15:07:56,116 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:07:56,220 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10320448875427246 sec
2023-06-22 15:07:56,220 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:07:56,663 - WARNING - Finished XLA compilation of jit(update_fn) in 0.44234442710876465 sec
2023-06-22 15:07:59,617 - INFO - Distilling data from client: Client08
2023-06-22 15:07:59,618 - INFO - train loss: 0.0017070384125580478
2023-06-22 15:07:59,618 - INFO - train acc: 0.9980732202529907
2023-06-22 15:07:59,788 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.78      0.78        73
           1       0.74      0.72      0.73        64
           5       0.80      0.81      0.80        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:07:59,789 - INFO - test loss 0.019584990207188818
2023-06-22 15:07:59,789 - INFO - test acc 0.7699999809265137
2023-06-22 15:08:02,564 - INFO - Distilling data from client: Client08
2023-06-22 15:08:02,564 - INFO - train loss: 0.0008966371266537784
2023-06-22 15:08:02,564 - INFO - train acc: 1.0
2023-06-22 15:08:02,611 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.74      0.74        73
           1       0.74      0.70      0.72        64
           5       0.77      0.81      0.79        63

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-06-22 15:08:02,612 - INFO - test loss 0.019344997347730687
2023-06-22 15:08:02,613 - INFO - test acc 0.75
2023-06-22 15:08:05,443 - INFO - Distilling data from client: Client08
2023-06-22 15:08:05,443 - INFO - train loss: 0.0007151586430898691
2023-06-22 15:08:05,443 - INFO - train acc: 1.0
2023-06-22 15:08:05,508 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.79      0.77        73
           1       0.75      0.70      0.73        64
           5       0.79      0.78      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:08:05,509 - INFO - test loss 0.019867558052185926
2023-06-22 15:08:05,509 - INFO - test acc 0.7599999904632568
2023-06-22 15:08:08,427 - INFO - Distilling data from client: Client08
2023-06-22 15:08:08,427 - INFO - train loss: 0.0005283992807713053
2023-06-22 15:08:08,428 - INFO - train acc: 1.0
2023-06-22 15:08:08,486 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.74      0.75        73
           1       0.75      0.72      0.74        64
           5       0.75      0.81      0.78        63

    accuracy                           0.76       200
   macro avg       0.75      0.76      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:08:08,487 - INFO - test loss 0.019689504479955794
2023-06-22 15:08:08,487 - INFO - test acc 0.7549999952316284
2023-06-22 15:08:11,357 - INFO - Distilling data from client: Client08
2023-06-22 15:08:11,357 - INFO - train loss: 0.00043929950919022756
2023-06-22 15:08:11,357 - INFO - train acc: 1.0
2023-06-22 15:08:11,406 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.78      0.76        73
           1       0.78      0.73      0.76        64
           5       0.79      0.79      0.79        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:08:11,406 - INFO - test loss 0.019210309446953594
2023-06-22 15:08:11,407 - INFO - test acc 0.7699999809265137
2023-06-22 15:08:14,490 - INFO - Distilling data from client: Client08
2023-06-22 15:08:14,491 - INFO - train loss: 0.00046128506087338763
2023-06-22 15:08:14,491 - INFO - train acc: 1.0
2023-06-22 15:08:14,565 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.75      0.75        73
           1       0.74      0.67      0.70        64
           5       0.74      0.79      0.76        63

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:08:14,565 - INFO - test loss 0.01990755519625133
2023-06-22 15:08:14,566 - INFO - test acc 0.7400000095367432
2023-06-22 15:08:17,845 - INFO - Distilling data from client: Client08
2023-06-22 15:08:17,845 - INFO - train loss: 0.0003620566284697813
2023-06-22 15:08:17,845 - INFO - train acc: 1.0
2023-06-22 15:08:17,905 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.77      0.76        73
           1       0.77      0.72      0.74        64
           5       0.75      0.78      0.77        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:08:17,905 - INFO - test loss 0.01965971645335072
2023-06-22 15:08:17,906 - INFO - test acc 0.7549999952316284
2023-06-22 15:08:20,802 - INFO - Distilling data from client: Client08
2023-06-22 15:08:20,803 - INFO - train loss: 0.00028883082095995465
2023-06-22 15:08:20,804 - INFO - train acc: 1.0
2023-06-22 15:08:20,967 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.78      0.78        73
           1       0.77      0.73      0.75        64
           5       0.79      0.83      0.81        63

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:08:20,967 - INFO - test loss 0.019783724727203807
2023-06-22 15:08:20,967 - INFO - test acc 0.7799999713897705
2023-06-22 15:08:23,901 - INFO - Distilling data from client: Client08
2023-06-22 15:08:23,901 - INFO - train loss: 0.00032509544032587165
2023-06-22 15:08:23,901 - INFO - train acc: 1.0
2023-06-22 15:08:24,049 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.78      0.78        73
           1       0.78      0.72      0.75        64
           5       0.81      0.86      0.83        63

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.78       200
weighted avg       0.78      0.79      0.78       200

2023-06-22 15:08:24,049 - INFO - test loss 0.019742561643265794
2023-06-22 15:08:24,049 - INFO - test acc 0.7849999666213989
2023-06-22 15:08:26,974 - INFO - Distilling data from client: Client08
2023-06-22 15:08:26,974 - INFO - train loss: 0.00036257251273251007
2023-06-22 15:08:26,975 - INFO - train acc: 1.0
2023-06-22 15:08:27,032 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.77      0.75        73
           1       0.77      0.72      0.74        64
           5       0.78      0.78      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.76       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:08:27,033 - INFO - test loss 0.020298933761682705
2023-06-22 15:08:27,033 - INFO - test acc 0.7549999952316284
2023-06-22 15:08:30,013 - INFO - Distilling data from client: Client08
2023-06-22 15:08:30,014 - INFO - train loss: 0.0003288566346878748
2023-06-22 15:08:30,014 - INFO - train acc: 1.0
2023-06-22 15:08:30,074 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.77      0.75        73
           1       0.75      0.72      0.74        64
           5       0.78      0.78      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.76       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:08:30,077 - INFO - test loss 0.019805339945013146
2023-06-22 15:08:30,078 - INFO - test acc 0.7549999952316284
2023-06-22 15:08:33,272 - INFO - Distilling data from client: Client08
2023-06-22 15:08:33,272 - INFO - train loss: 0.0002311945656751455
2023-06-22 15:08:33,273 - INFO - train acc: 1.0
2023-06-22 15:08:33,334 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.78      0.75        73
           1       0.78      0.72      0.75        64
           5       0.78      0.78      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:08:33,338 - INFO - test loss 0.019966635707434684
2023-06-22 15:08:33,338 - INFO - test acc 0.7599999904632568
2023-06-22 15:08:36,375 - INFO - Distilling data from client: Client08
2023-06-22 15:08:36,375 - INFO - train loss: 0.0002350986263092867
2023-06-22 15:08:36,375 - INFO - train acc: 1.0
2023-06-22 15:08:36,437 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.75      0.75        73
           1       0.77      0.73      0.75        64
           5       0.79      0.83      0.81        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:08:36,443 - INFO - test loss 0.019938913793178047
2023-06-22 15:08:36,443 - INFO - test acc 0.7699999809265137
2023-06-22 15:08:39,581 - INFO - Distilling data from client: Client08
2023-06-22 15:08:39,581 - INFO - train loss: 0.00020938977546944464
2023-06-22 15:08:39,581 - INFO - train acc: 1.0
2023-06-22 15:08:39,648 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.74      0.75        73
           1       0.75      0.73      0.74        64
           5       0.77      0.81      0.79        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:08:39,648 - INFO - test loss 0.019615354959362067
2023-06-22 15:08:39,649 - INFO - test acc 0.7599999904632568
2023-06-22 15:08:42,559 - INFO - Distilling data from client: Client08
2023-06-22 15:08:42,560 - INFO - train loss: 0.0002224879134308758
2023-06-22 15:08:42,560 - INFO - train acc: 1.0
2023-06-22 15:08:42,637 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.78      0.77        73
           1       0.77      0.69      0.73        64
           5       0.76      0.81      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:08:42,638 - INFO - test loss 0.01959071145626274
2023-06-22 15:08:42,639 - INFO - test acc 0.7599999904632568
2023-06-22 15:08:45,679 - INFO - Distilling data from client: Client08
2023-06-22 15:08:45,679 - INFO - train loss: 0.00022852583359574943
2023-06-22 15:08:45,680 - INFO - train acc: 1.0
2023-06-22 15:08:45,742 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.78      0.77        73
           1       0.77      0.73      0.75        64
           5       0.80      0.81      0.80        63

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.78       200
weighted avg       0.77      0.78      0.77       200

2023-06-22 15:08:45,743 - INFO - test loss 0.019784573962021452
2023-06-22 15:08:45,743 - INFO - test acc 0.7749999761581421
2023-06-22 15:08:48,647 - INFO - Distilling data from client: Client08
2023-06-22 15:08:48,647 - INFO - train loss: 0.00021676256046169337
2023-06-22 15:08:48,647 - INFO - train acc: 1.0
2023-06-22 15:08:48,714 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.75      0.74        73
           1       0.77      0.72      0.74        64
           5       0.75      0.78      0.77        63

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-06-22 15:08:48,715 - INFO - test loss 0.01973759707823102
2023-06-22 15:08:48,716 - INFO - test acc 0.75
2023-06-22 15:08:51,668 - INFO - Distilling data from client: Client08
2023-06-22 15:08:51,669 - INFO - train loss: 0.00019723241155090822
2023-06-22 15:08:51,669 - INFO - train acc: 1.0
2023-06-22 15:08:51,729 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.77      0.76        73
           1       0.77      0.72      0.74        64
           5       0.78      0.81      0.80        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.76       200

2023-06-22 15:08:51,730 - INFO - test loss 0.02003110440576124
2023-06-22 15:08:51,730 - INFO - test acc 0.7649999856948853
2023-06-22 15:08:54,765 - INFO - Distilling data from client: Client08
2023-06-22 15:08:54,766 - INFO - train loss: 0.00019839740410466426
2023-06-22 15:08:54,766 - INFO - train acc: 1.0
2023-06-22 15:08:54,833 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.78      0.77        73
           1       0.78      0.73      0.76        64
           5       0.78      0.79      0.79        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:08:54,833 - INFO - test loss 0.01913865155250849
2023-06-22 15:08:54,834 - INFO - test acc 0.7699999809265137
2023-06-22 15:08:57,972 - INFO - Distilling data from client: Client08
2023-06-22 15:08:57,972 - INFO - train loss: 0.0002168210481095217
2023-06-22 15:08:57,972 - INFO - train acc: 1.0
2023-06-22 15:08:58,032 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.73        73
           1       0.77      0.73      0.75        64
           5       0.76      0.81      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:08:58,032 - INFO - test loss 0.019530627076944987
2023-06-22 15:08:58,032 - INFO - test acc 0.7549999952316284
2023-06-22 15:09:00,993 - INFO - Distilling data from client: Client08
2023-06-22 15:09:00,993 - INFO - train loss: 0.00017861587459964962
2023-06-22 15:09:00,993 - INFO - train acc: 1.0
2023-06-22 15:09:01,057 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.78      0.76        73
           1       0.77      0.75      0.76        64
           5       0.80      0.78      0.79        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:09:01,057 - INFO - test loss 0.019650762641772904
2023-06-22 15:09:01,058 - INFO - test acc 0.7699999809265137
2023-06-22 15:09:04,085 - INFO - Distilling data from client: Client08
2023-06-22 15:09:04,085 - INFO - train loss: 0.00017937045769214372
2023-06-22 15:09:04,085 - INFO - train acc: 1.0
2023-06-22 15:09:04,139 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.78      0.76        73
           1       0.80      0.73      0.76        64
           5       0.78      0.79      0.79        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:09:04,140 - INFO - test loss 0.01978526954430271
2023-06-22 15:09:04,140 - INFO - test acc 0.7699999809265137
2023-06-22 15:09:07,030 - INFO - Distilling data from client: Client08
2023-06-22 15:09:07,030 - INFO - train loss: 0.0001550145083919595
2023-06-22 15:09:07,031 - INFO - train acc: 1.0
2023-06-22 15:09:07,086 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.78      0.77        73
           1       0.76      0.73      0.75        64
           5       0.81      0.79      0.80        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:09:07,087 - INFO - test loss 0.019951345893843033
2023-06-22 15:09:07,088 - INFO - test acc 0.7699999809265137
2023-06-22 15:09:10,024 - INFO - Distilling data from client: Client08
2023-06-22 15:09:10,024 - INFO - train loss: 0.00016378737741561496
2023-06-22 15:09:10,024 - INFO - train acc: 1.0
2023-06-22 15:09:10,080 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.79      0.78        73
           1       0.76      0.70      0.73        64
           5       0.77      0.79      0.78        63

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.76      0.77      0.76       200

2023-06-22 15:09:10,080 - INFO - test loss 0.01972863557390044
2023-06-22 15:09:10,080 - INFO - test acc 0.7649999856948853
2023-06-22 15:09:12,981 - INFO - Distilling data from client: Client08
2023-06-22 15:09:12,981 - INFO - train loss: 0.00015225872788432953
2023-06-22 15:09:12,982 - INFO - train acc: 1.0
2023-06-22 15:09:13,050 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.75      0.73        73
           1       0.75      0.72      0.74        64
           5       0.77      0.76      0.77        63

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.75       200
weighted avg       0.75      0.74      0.75       200

2023-06-22 15:09:13,050 - INFO - test loss 0.020071261157110942
2023-06-22 15:09:13,050 - INFO - test acc 0.7450000047683716
2023-06-22 15:09:13,081 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:09:13,099 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:09:13,118 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:09:13,136 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:09:13,148 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:09:13,160 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:09:13,173 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:09:13,187 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:09:13,200 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:09:13,774 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client08//synthetic.png
2023-06-22 15:09:13,798 - INFO - c: 2.0 and total_data_in_this_class: 262
2023-06-22 15:09:13,798 - INFO - c: 5.0 and total_data_in_this_class: 272
2023-06-22 15:09:13,798 - INFO - c: 6.0 and total_data_in_this_class: 265
2023-06-22 15:09:13,798 - INFO - c: 2.0 and total_data_in_this_class: 71
2023-06-22 15:09:13,798 - INFO - c: 5.0 and total_data_in_this_class: 61
2023-06-22 15:09:13,798 - INFO - c: 6.0 and total_data_in_this_class: 68
2023-06-22 15:09:13,931 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08127760887145996 sec
2023-06-22 15:09:14,008 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07564139366149902 sec
2023-06-22 15:09:14,017 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16939520835876465 sec
2023-06-22 15:09:14,020 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:09:14,078 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05786395072937012 sec
2023-06-22 15:09:14,079 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:09:14,264 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1846923828125 sec
2023-06-22 15:09:14,314 - INFO - initial test loss: 0.030013232514812062
2023-06-22 15:09:14,314 - INFO - initial test acc: 0.6150000095367432
2023-06-22 15:09:14,335 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.014292716979980469 sec
2023-06-22 15:09:14,539 - WARNING - Finished tracing + transforming update_fn for pjit in 0.22086024284362793 sec
2023-06-22 15:09:14,545 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:09:14,660 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.11475539207458496 sec
2023-06-22 15:09:14,661 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:09:15,124 - WARNING - Finished XLA compilation of jit(update_fn) in 0.46336889266967773 sec
2023-06-22 15:09:18,038 - INFO - Distilling data from client: Client09
2023-06-22 15:09:18,038 - INFO - train loss: 0.003358547089643528
2023-06-22 15:09:18,039 - INFO - train acc: 0.9961904883384705
2023-06-22 15:09:18,243 - INFO - report:               precision    recall  f1-score   support

           2       0.55      0.48      0.51        71
           5       0.53      0.51      0.52        61
           6       0.53      0.62      0.57        68

    accuracy                           0.54       200
   macro avg       0.54      0.53      0.53       200
weighted avg       0.54      0.54      0.53       200

2023-06-22 15:09:18,244 - INFO - test loss 0.02955113486587621
2023-06-22 15:09:18,244 - INFO - test acc 0.5349999666213989
2023-06-22 15:09:21,282 - INFO - Distilling data from client: Client09
2023-06-22 15:09:21,282 - INFO - train loss: 0.0017761163865900275
2023-06-22 15:09:21,282 - INFO - train acc: 1.0
2023-06-22 15:09:21,449 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.51      0.55        71
           5       0.59      0.61      0.60        61
           6       0.61      0.68      0.64        68

    accuracy                           0.59       200
   macro avg       0.59      0.60      0.59       200
weighted avg       0.59      0.59      0.59       200

2023-06-22 15:09:21,449 - INFO - test loss 0.02909075802302183
2023-06-22 15:09:21,449 - INFO - test acc 0.5949999690055847
2023-06-22 15:09:24,482 - INFO - Distilling data from client: Client09
2023-06-22 15:09:24,483 - INFO - train loss: 0.0012751001753364493
2023-06-22 15:09:24,484 - INFO - train acc: 1.0
2023-06-22 15:09:24,559 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.54      0.56        71
           5       0.58      0.57      0.58        61
           6       0.59      0.65      0.62        68

    accuracy                           0.58       200
   macro avg       0.58      0.59      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-06-22 15:09:24,559 - INFO - test loss 0.02962779114463747
2023-06-22 15:09:24,559 - INFO - test acc 0.5849999785423279
2023-06-22 15:09:27,596 - INFO - Distilling data from client: Client09
2023-06-22 15:09:27,596 - INFO - train loss: 0.0011015006686853428
2023-06-22 15:09:27,596 - INFO - train acc: 1.0
2023-06-22 15:09:27,658 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.55      0.56        71
           5       0.58      0.62      0.60        61
           6       0.59      0.57      0.58        68

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-06-22 15:09:27,658 - INFO - test loss 0.029784772614616768
2023-06-22 15:09:27,658 - INFO - test acc 0.5799999833106995
2023-06-22 15:09:30,702 - INFO - Distilling data from client: Client09
2023-06-22 15:09:30,703 - INFO - train loss: 0.0011832694907292487
2023-06-22 15:09:30,703 - INFO - train acc: 1.0
2023-06-22 15:09:30,774 - INFO - report:               precision    recall  f1-score   support

           2       0.55      0.51      0.53        71
           5       0.56      0.54      0.55        61
           6       0.57      0.63      0.60        68

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-06-22 15:09:30,779 - INFO - test loss 0.03066776437222982
2023-06-22 15:09:30,780 - INFO - test acc 0.5600000023841858
2023-06-22 15:09:33,764 - INFO - Distilling data from client: Client09
2023-06-22 15:09:33,764 - INFO - train loss: 0.0008708597641230714
2023-06-22 15:09:33,764 - INFO - train acc: 1.0
2023-06-22 15:09:33,860 - INFO - report:               precision    recall  f1-score   support

           2       0.55      0.54      0.54        71
           5       0.58      0.56      0.57        61
           6       0.60      0.63      0.61        68

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-06-22 15:09:33,860 - INFO - test loss 0.030585774156019895
2023-06-22 15:09:33,860 - INFO - test acc 0.574999988079071
2023-06-22 15:09:36,953 - INFO - Distilling data from client: Client09
2023-06-22 15:09:36,954 - INFO - train loss: 0.0009169591480534524
2023-06-22 15:09:36,955 - INFO - train acc: 1.0
2023-06-22 15:09:37,019 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.56      0.57        71
           5       0.59      0.57      0.58        61
           6       0.58      0.60      0.59        68

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-06-22 15:09:37,020 - INFO - test loss 0.030132625419928723
2023-06-22 15:09:37,021 - INFO - test acc 0.5799999833106995
2023-06-22 15:09:40,193 - INFO - Distilling data from client: Client09
2023-06-22 15:09:40,193 - INFO - train loss: 0.00068794321669334
2023-06-22 15:09:40,193 - INFO - train acc: 1.0
2023-06-22 15:09:40,275 - INFO - report:               precision    recall  f1-score   support

           2       0.56      0.49      0.53        71
           5       0.55      0.54      0.55        61
           6       0.58      0.66      0.62        68

    accuracy                           0.56       200
   macro avg       0.56      0.57      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-06-22 15:09:40,276 - INFO - test loss 0.0310081227962117
2023-06-22 15:09:40,276 - INFO - test acc 0.5649999976158142
2023-06-22 15:09:43,307 - INFO - Distilling data from client: Client09
2023-06-22 15:09:43,307 - INFO - train loss: 0.0007261838237681519
2023-06-22 15:09:43,307 - INFO - train acc: 1.0
2023-06-22 15:09:43,381 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.54      0.55        71
           5       0.56      0.57      0.56        61
           6       0.59      0.60      0.59        68

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-06-22 15:09:43,382 - INFO - test loss 0.030196454141203335
2023-06-22 15:09:43,382 - INFO - test acc 0.5699999928474426
2023-06-22 15:09:46,535 - INFO - Distilling data from client: Client09
2023-06-22 15:09:46,535 - INFO - train loss: 0.0006216932736783263
2023-06-22 15:09:46,535 - INFO - train acc: 1.0
2023-06-22 15:09:46,599 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.52      0.55        71
           5       0.52      0.52      0.52        61
           6       0.56      0.62      0.59        68

    accuracy                           0.56       200
   macro avg       0.55      0.55      0.55       200
weighted avg       0.56      0.56      0.55       200

2023-06-22 15:09:46,599 - INFO - test loss 0.02988049016024116
2023-06-22 15:09:46,599 - INFO - test acc 0.5550000071525574
2023-06-22 15:09:49,684 - INFO - Distilling data from client: Client09
2023-06-22 15:09:49,684 - INFO - train loss: 0.0005468136177714439
2023-06-22 15:09:49,684 - INFO - train acc: 1.0
2023-06-22 15:09:49,861 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.59      0.58        71
           5       0.64      0.57      0.60        61
           6       0.61      0.65      0.63        68

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.61       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:09:49,861 - INFO - test loss 0.030432906330354535
2023-06-22 15:09:49,862 - INFO - test acc 0.6049999594688416
2023-06-22 15:09:53,101 - INFO - Distilling data from client: Client09
2023-06-22 15:09:53,102 - INFO - train loss: 0.0005461864344875891
2023-06-22 15:09:53,102 - INFO - train acc: 1.0
2023-06-22 15:09:53,189 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.55      0.57        71
           5       0.57      0.56      0.56        61
           6       0.58      0.62      0.60        68

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-06-22 15:09:53,190 - INFO - test loss 0.030720049850897123
2023-06-22 15:09:53,190 - INFO - test acc 0.574999988079071
2023-06-22 15:09:56,459 - INFO - Distilling data from client: Client09
2023-06-22 15:09:56,460 - INFO - train loss: 0.00046834659182237395
2023-06-22 15:09:56,460 - INFO - train acc: 1.0
2023-06-22 15:09:56,618 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.61      0.62        71
           5       0.59      0.57      0.58        61
           6       0.60      0.65      0.62        68

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-06-22 15:09:56,618 - INFO - test loss 0.030149292722138128
2023-06-22 15:09:56,619 - INFO - test acc 0.6100000143051147
2023-06-22 15:09:59,515 - INFO - Distilling data from client: Client09
2023-06-22 15:09:59,515 - INFO - train loss: 0.0005635947908769472
2023-06-22 15:09:59,515 - INFO - train acc: 1.0
2023-06-22 15:09:59,625 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.52      0.54        71
           5       0.54      0.59      0.56        61
           6       0.60      0.60      0.60        68

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-06-22 15:09:59,626 - INFO - test loss 0.03092916709702171
2023-06-22 15:09:59,627 - INFO - test acc 0.5699999928474426
2023-06-22 15:10:02,961 - INFO - Distilling data from client: Client09
2023-06-22 15:10:02,962 - INFO - train loss: 0.0004317480087890347
2023-06-22 15:10:02,962 - INFO - train acc: 1.0
2023-06-22 15:10:03,119 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.56      0.58        71
           5       0.56      0.57      0.56        61
           6       0.58      0.59      0.58        68

    accuracy                           0.57       200
   macro avg       0.57      0.58      0.57       200
weighted avg       0.58      0.57      0.58       200

2023-06-22 15:10:03,120 - INFO - test loss 0.030971002248948706
2023-06-22 15:10:03,120 - INFO - test acc 0.574999988079071
2023-06-22 15:10:06,822 - INFO - Distilling data from client: Client09
2023-06-22 15:10:06,823 - INFO - train loss: 0.00036476659448877015
2023-06-22 15:10:06,823 - INFO - train acc: 1.0
2023-06-22 15:10:06,880 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.52      0.54        71
           5       0.58      0.57      0.58        61
           6       0.57      0.63      0.60        68

    accuracy                           0.57       200
   macro avg       0.58      0.58      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-06-22 15:10:06,881 - INFO - test loss 0.03056930534681884
2023-06-22 15:10:06,881 - INFO - test acc 0.574999988079071
2023-06-22 15:10:09,752 - INFO - Distilling data from client: Client09
2023-06-22 15:10:09,752 - INFO - train loss: 0.00036274254022454906
2023-06-22 15:10:09,753 - INFO - train acc: 1.0
2023-06-22 15:10:09,805 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.54      0.57        71
           5       0.59      0.57      0.58        61
           6       0.58      0.68      0.63        68

    accuracy                           0.59       200
   macro avg       0.60      0.60      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-06-22 15:10:09,806 - INFO - test loss 0.0313317834819103
2023-06-22 15:10:09,806 - INFO - test acc 0.5949999690055847
2023-06-22 15:10:12,994 - INFO - Distilling data from client: Client09
2023-06-22 15:10:12,998 - INFO - train loss: 0.00043050499331464013
2023-06-22 15:10:12,999 - INFO - train acc: 1.0
2023-06-22 15:10:13,066 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.49      0.51        71
           5       0.56      0.57      0.56        61
           6       0.55      0.57      0.56        68

    accuracy                           0.55       200
   macro avg       0.55      0.55      0.55       200
weighted avg       0.54      0.55      0.54       200

2023-06-22 15:10:13,067 - INFO - test loss 0.031363986150685814
2023-06-22 15:10:13,067 - INFO - test acc 0.5450000166893005
2023-06-22 15:10:16,173 - INFO - Distilling data from client: Client09
2023-06-22 15:10:16,174 - INFO - train loss: 0.0004873172089916859
2023-06-22 15:10:16,174 - INFO - train acc: 1.0
2023-06-22 15:10:16,231 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.56      0.57        71
           5       0.55      0.51      0.53        61
           6       0.56      0.62      0.59        68

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.57      0.56      0.56       200

2023-06-22 15:10:16,231 - INFO - test loss 0.030352702346845884
2023-06-22 15:10:16,231 - INFO - test acc 0.5649999976158142
2023-06-22 15:10:19,274 - INFO - Distilling data from client: Client09
2023-06-22 15:10:19,275 - INFO - train loss: 0.00041377851214709536
2023-06-22 15:10:19,276 - INFO - train acc: 1.0
2023-06-22 15:10:19,324 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.52      0.52        71
           5       0.58      0.52      0.55        61
           6       0.56      0.62      0.59        68

    accuracy                           0.56       200
   macro avg       0.56      0.55      0.55       200
weighted avg       0.56      0.56      0.55       200

2023-06-22 15:10:19,325 - INFO - test loss 0.03091360880621423
2023-06-22 15:10:19,325 - INFO - test acc 0.5550000071525574
2023-06-22 15:10:22,327 - INFO - Distilling data from client: Client09
2023-06-22 15:10:22,327 - INFO - train loss: 0.0003704729813983368
2023-06-22 15:10:22,328 - INFO - train acc: 1.0
2023-06-22 15:10:22,389 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.54      0.55        71
           5       0.58      0.56      0.57        61
           6       0.55      0.60      0.58        68

    accuracy                           0.56       200
   macro avg       0.57      0.57      0.56       200
weighted avg       0.57      0.56      0.56       200

2023-06-22 15:10:22,389 - INFO - test loss 0.030561795241379437
2023-06-22 15:10:22,390 - INFO - test acc 0.5649999976158142
2023-06-22 15:10:25,618 - INFO - Distilling data from client: Client09
2023-06-22 15:10:25,619 - INFO - train loss: 0.00034155026711894587
2023-06-22 15:10:25,619 - INFO - train acc: 1.0
2023-06-22 15:10:25,679 - INFO - report:               precision    recall  f1-score   support

           2       0.60      0.62      0.61        71
           5       0.58      0.52      0.55        61
           6       0.60      0.63      0.61        68

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.59      0.59      0.59       200

2023-06-22 15:10:25,681 - INFO - test loss 0.03081371127462528
2023-06-22 15:10:25,681 - INFO - test acc 0.5949999690055847
2023-06-22 15:10:28,690 - INFO - Distilling data from client: Client09
2023-06-22 15:10:28,691 - INFO - train loss: 0.00047730044713347586
2023-06-22 15:10:28,691 - INFO - train acc: 1.0
2023-06-22 15:10:28,745 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.54      0.56        71
           5       0.57      0.59      0.58        61
           6       0.59      0.63      0.61        68

    accuracy                           0.58       200
   macro avg       0.58      0.59      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-06-22 15:10:28,746 - INFO - test loss 0.031004155903263577
2023-06-22 15:10:28,747 - INFO - test acc 0.5849999785423279
2023-06-22 15:10:31,764 - INFO - Distilling data from client: Client09
2023-06-22 15:10:31,764 - INFO - train loss: 0.0004396971743887976
2023-06-22 15:10:31,764 - INFO - train acc: 1.0
2023-06-22 15:10:31,826 - INFO - report:               precision    recall  f1-score   support

           2       0.56      0.56      0.56        71
           5       0.58      0.57      0.58        61
           6       0.59      0.60      0.60        68

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-06-22 15:10:31,826 - INFO - test loss 0.0313919470165003
2023-06-22 15:10:31,827 - INFO - test acc 0.5799999833106995
2023-06-22 15:10:34,793 - INFO - Distilling data from client: Client09
2023-06-22 15:10:34,793 - INFO - train loss: 0.0003517604541293656
2023-06-22 15:10:34,794 - INFO - train acc: 1.0
2023-06-22 15:10:34,855 - INFO - report:               precision    recall  f1-score   support

           2       0.56      0.52      0.54        71
           5       0.54      0.54      0.54        61
           6       0.56      0.60      0.58        68

    accuracy                           0.56       200
   macro avg       0.55      0.56      0.55       200
weighted avg       0.55      0.56      0.55       200

2023-06-22 15:10:34,856 - INFO - test loss 0.030982289631812245
2023-06-22 15:10:34,856 - INFO - test acc 0.5550000071525574
2023-06-22 15:10:34,889 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:10:34,910 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:10:34,931 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:10:34,950 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:10:34,962 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:10:34,974 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:10:34,988 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:10:35,000 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:10:35,012 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:10:35,589 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client09//synthetic.png
2023-06-22 15:10:35,611 - INFO - c: 1.0 and total_data_in_this_class: 265
2023-06-22 15:10:35,611 - INFO - c: 6.0 and total_data_in_this_class: 266
2023-06-22 15:10:35,611 - INFO - c: 8.0 and total_data_in_this_class: 268
2023-06-22 15:10:35,611 - INFO - c: 1.0 and total_data_in_this_class: 68
2023-06-22 15:10:35,612 - INFO - c: 6.0 and total_data_in_this_class: 67
2023-06-22 15:10:35,612 - INFO - c: 8.0 and total_data_in_this_class: 65
2023-06-22 15:10:35,664 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005719661712646484 sec
2023-06-22 15:10:35,665 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:10:35,668 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.002657651901245117 sec
2023-06-22 15:10:35,668 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:10:35,685 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.016342878341674805 sec
2023-06-22 15:10:35,689 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003457069396972656 sec
2023-06-22 15:10:35,690 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:10:35,692 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0019125938415527344 sec
2023-06-22 15:10:35,692 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:10:35,705 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.012470006942749023 sec
2023-06-22 15:10:35,710 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00023412704467773438 sec
2023-06-22 15:10:35,712 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002219676971435547 sec
2023-06-22 15:10:35,713 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005681514739990234 sec
2023-06-22 15:10:35,716 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003628730773925781 sec
2023-06-22 15:10:35,717 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002014636993408203 sec
2023-06-22 15:10:35,718 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004994869232177734 sec
2023-06-22 15:10:35,719 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004291534423828125 sec
2023-06-22 15:10:35,720 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003218650817871094 sec
2023-06-22 15:10:35,721 - WARNING - Finished tracing + transforming fn for pjit in 0.0004935264587402344 sec
2023-06-22 15:10:35,722 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0006039142608642578 sec
2023-06-22 15:10:35,724 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003695487976074219 sec
2023-06-22 15:10:35,726 - WARNING - Finished tracing + transforming fn for pjit in 0.000370025634765625 sec
2023-06-22 15:10:35,727 - WARNING - Finished tracing + transforming fn for pjit in 0.0004639625549316406 sec
2023-06-22 15:10:35,728 - WARNING - Finished tracing + transforming fn for pjit in 0.000396728515625 sec
2023-06-22 15:10:35,729 - WARNING - Finished tracing + transforming fn for pjit in 0.0004680156707763672 sec
2023-06-22 15:10:35,731 - WARNING - Finished tracing + transforming fn for pjit in 0.0003993511199951172 sec
2023-06-22 15:10:35,735 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00029730796813964844 sec
2023-06-22 15:10:35,736 - WARNING - Finished tracing + transforming fn for pjit in 0.0004000663757324219 sec
2023-06-22 15:10:35,737 - WARNING - Finished tracing + transforming fn for pjit in 0.0003998279571533203 sec
2023-06-22 15:10:35,743 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006287097930908203 sec
2023-06-22 15:10:35,744 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016825199127197266 sec
2023-06-22 15:10:35,745 - WARNING - Finished tracing + transforming fn for pjit in 0.0003800392150878906 sec
2023-06-22 15:10:35,747 - WARNING - Finished tracing + transforming fn for pjit in 0.0003902912139892578 sec
2023-06-22 15:10:35,748 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003876686096191406 sec
2023-06-22 15:10:35,750 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004677772521972656 sec
2023-06-22 15:10:35,751 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003161430358886719 sec
2023-06-22 15:10:35,752 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046753883361816406 sec
2023-06-22 15:10:35,753 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040435791015625 sec
2023-06-22 15:10:35,754 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040078163146972656 sec
2023-06-22 15:10:35,757 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.001996278762817383 sec
2023-06-22 15:10:35,758 - WARNING - Finished tracing + transforming _where for pjit in 0.0030837059020996094 sec
2023-06-22 15:10:35,759 - WARNING - Finished tracing + transforming fn for pjit in 0.00046563148498535156 sec
2023-06-22 15:10:35,761 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004508495330810547 sec
2023-06-22 15:10:35,762 - WARNING - Finished tracing + transforming fn for pjit in 0.0003981590270996094 sec
2023-06-22 15:10:35,763 - WARNING - Finished tracing + transforming fn for pjit in 0.0004150867462158203 sec
2023-06-22 15:10:35,765 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003895759582519531 sec
2023-06-22 15:10:35,766 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045013427734375 sec
2023-06-22 15:10:35,767 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004696846008300781 sec
2023-06-22 15:10:35,768 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045990943908691406 sec
2023-06-22 15:10:35,770 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004050731658935547 sec
2023-06-22 15:10:35,771 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004088878631591797 sec
2023-06-22 15:10:35,772 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.000492095947265625 sec
2023-06-22 15:10:35,773 - WARNING - Finished tracing + transforming _where for pjit in 0.0015909671783447266 sec
2023-06-22 15:10:35,774 - WARNING - Finished tracing + transforming fn for pjit in 0.0004677772521972656 sec
2023-06-22 15:10:35,776 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004630088806152344 sec
2023-06-22 15:10:35,778 - WARNING - Finished tracing + transforming fn for pjit in 0.0003814697265625 sec
2023-06-22 15:10:35,785 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000453948974609375 sec
2023-06-22 15:10:35,786 - WARNING - Finished tracing + transforming fn for pjit in 0.0006358623504638672 sec
2023-06-22 15:10:35,787 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004894733428955078 sec
2023-06-22 15:10:35,789 - WARNING - Finished tracing + transforming fn for pjit in 0.00040340423583984375 sec
2023-06-22 15:10:35,796 - WARNING - Finished tracing + transforming fn for pjit in 0.00041747093200683594 sec
2023-06-22 15:10:35,799 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000339508056640625 sec
2023-06-22 15:10:35,800 - WARNING - Finished tracing + transforming fn for pjit in 0.0005810260772705078 sec
2023-06-22 15:10:35,802 - WARNING - Finished tracing + transforming fn for pjit in 0.00042748451232910156 sec
2023-06-22 15:10:35,831 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.12136435508728027 sec
2023-06-22 15:10:35,837 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021982192993164062 sec
2023-06-22 15:10:35,837 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020694732666015625 sec
2023-06-22 15:10:35,838 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00048232078552246094 sec
2023-06-22 15:10:35,842 - WARNING - Finished tracing + transforming fn for pjit in 0.00037097930908203125 sec
2023-06-22 15:10:35,843 - WARNING - Finished tracing + transforming fn for pjit in 0.0004558563232421875 sec
2023-06-22 15:10:35,846 - WARNING - Finished tracing + transforming fn for pjit in 0.00036525726318359375 sec
2023-06-22 15:10:35,856 - WARNING - Finished tracing + transforming fn for pjit in 0.0003821849822998047 sec
2023-06-22 15:10:35,858 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003693103790283203 sec
2023-06-22 15:10:35,859 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004627704620361328 sec
2023-06-22 15:10:35,860 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031447410583496094 sec
2023-06-22 15:10:35,861 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005884170532226562 sec
2023-06-22 15:10:35,863 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004112720489501953 sec
2023-06-22 15:10:35,864 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003948211669921875 sec
2023-06-22 15:10:35,865 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005156993865966797 sec
2023-06-22 15:10:35,866 - WARNING - Finished tracing + transforming _where for pjit in 0.0015804767608642578 sec
2023-06-22 15:10:35,867 - WARNING - Finished tracing + transforming fn for pjit in 0.0004634857177734375 sec
2023-06-22 15:10:35,869 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004470348358154297 sec
2023-06-22 15:10:35,870 - WARNING - Finished tracing + transforming fn for pjit in 0.0003864765167236328 sec
2023-06-22 15:10:35,872 - WARNING - Finished tracing + transforming fn for pjit in 0.0012524127960205078 sec
2023-06-22 15:10:35,892 - WARNING - Finished tracing + transforming fn for pjit in 0.0003795623779296875 sec
2023-06-22 15:10:35,924 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.09112143516540527 sec
2023-06-22 15:10:35,926 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002040863037109375 sec
2023-06-22 15:10:35,928 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00022077560424804688 sec
2023-06-22 15:10:35,928 - WARNING - Finished tracing + transforming _where for pjit in 0.0011014938354492188 sec
2023-06-22 15:10:35,930 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005228519439697266 sec
2023-06-22 15:10:35,930 - WARNING - Finished tracing + transforming trace for pjit in 0.004491090774536133 sec
2023-06-22 15:10:35,934 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00017881393432617188 sec
2023-06-22 15:10:35,936 - WARNING - Finished tracing + transforming tril for pjit in 0.0011086463928222656 sec
2023-06-22 15:10:35,936 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.003178119659423828 sec
2023-06-22 15:10:35,938 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019288063049316406 sec
2023-06-22 15:10:35,939 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019216537475585938 sec
2023-06-22 15:10:35,942 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.002477884292602539 sec
2023-06-22 15:10:35,949 - WARNING - Finished tracing + transforming _solve for pjit in 0.0160675048828125 sec
2023-06-22 15:10:35,950 - WARNING - Finished tracing + transforming dot for pjit in 0.000545501708984375 sec
2023-06-22 15:10:35,954 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.24681591987609863 sec
2023-06-22 15:10:35,957 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:10:36,014 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05607724189758301 sec
2023-06-22 15:10:36,014 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:10:36,191 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17618846893310547 sec
2023-06-22 15:10:36,248 - INFO - initial test loss: 0.019266028931610883
2023-06-22 15:10:36,249 - INFO - initial test acc: 0.7849999666213989
2023-06-22 15:10:36,261 - WARNING - Finished tracing + transforming dot for pjit in 0.0008180141448974609 sec
2023-06-22 15:10:36,263 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000705718994140625 sec
2023-06-22 15:10:36,266 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008306503295898438 sec
2023-06-22 15:10:36,267 - WARNING - Finished tracing + transforming _mean for pjit in 0.0023620128631591797 sec
2023-06-22 15:10:36,269 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004525184631347656 sec
2023-06-22 15:10:36,270 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00042319297790527344 sec
2023-06-22 15:10:36,272 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005800724029541016 sec
2023-06-22 15:10:36,274 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008742809295654297 sec
2023-06-22 15:10:36,275 - WARNING - Finished tracing + transforming _mean for pjit in 0.0025453567504882812 sec
2023-06-22 15:10:36,277 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.022875308990478516 sec
2023-06-22 15:10:36,297 - WARNING - Finished tracing + transforming fn for pjit in 0.0006093978881835938 sec
2023-06-22 15:10:36,299 - WARNING - Finished tracing + transforming fn for pjit in 0.0006196498870849609 sec
2023-06-22 15:10:36,300 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005095005035400391 sec
2023-06-22 15:10:36,302 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006120204925537109 sec
2023-06-22 15:10:36,303 - WARNING - Finished tracing + transforming _where for pjit in 0.002073049545288086 sec
2023-06-22 15:10:36,322 - WARNING - Finished tracing + transforming fn for pjit in 0.0005965232849121094 sec
2023-06-22 15:10:36,324 - WARNING - Finished tracing + transforming fn for pjit in 0.0006299018859863281 sec
2023-06-22 15:10:36,325 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005290508270263672 sec
2023-06-22 15:10:36,327 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0007231235504150391 sec
2023-06-22 15:10:36,328 - WARNING - Finished tracing + transforming _where for pjit in 0.0021741390228271484 sec
2023-06-22 15:10:36,388 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037407875061035156 sec
2023-06-22 15:10:36,471 - WARNING - Finished tracing + transforming fn for pjit in 0.0005028247833251953 sec
2023-06-22 15:10:36,473 - WARNING - Finished tracing + transforming fn for pjit in 0.0004177093505859375 sec
2023-06-22 15:10:36,474 - WARNING - Finished tracing + transforming square for pjit in 0.0003120899200439453 sec
2023-06-22 15:10:36,477 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003826618194580078 sec
2023-06-22 15:10:36,480 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00048828125 sec
2023-06-22 15:10:36,482 - WARNING - Finished tracing + transforming fn for pjit in 0.00047898292541503906 sec
2023-06-22 15:10:36,483 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00036787986755371094 sec
2023-06-22 15:10:36,484 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004131793975830078 sec
2023-06-22 15:10:36,485 - WARNING - Finished tracing + transforming fn for pjit in 0.0004477500915527344 sec
2023-06-22 15:10:36,486 - WARNING - Finished tracing + transforming fn for pjit in 0.0003933906555175781 sec
2023-06-22 15:10:36,487 - WARNING - Finished tracing + transforming square for pjit in 0.00032329559326171875 sec
2023-06-22 15:10:36,490 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003829002380371094 sec
2023-06-22 15:10:36,493 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032067298889160156 sec
2023-06-22 15:10:36,494 - WARNING - Finished tracing + transforming fn for pjit in 0.00048160552978515625 sec
2023-06-22 15:10:36,495 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004215240478515625 sec
2023-06-22 15:10:36,496 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004277229309082031 sec
2023-06-22 15:10:36,498 - WARNING - Finished tracing + transforming update_fn for pjit in 0.24544024467468262 sec
2023-06-22 15:10:36,504 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[531,10]), ShapedArray(float32[531,10]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:10:36,610 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10590243339538574 sec
2023-06-22 15:10:36,611 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:10:37,065 - WARNING - Finished XLA compilation of jit(update_fn) in 0.45395755767822266 sec
2023-06-22 15:10:40,293 - INFO - Distilling data from client: Client10
2023-06-22 15:10:40,294 - INFO - train loss: 0.001560097512894129
2023-06-22 15:10:40,294 - INFO - train acc: 0.9981167316436768
2023-06-22 15:10:40,446 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.66      0.70        68
           6       0.70      0.78      0.74        67
           8       0.79      0.80      0.79        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:10:40,446 - INFO - test loss 0.019705266782807592
2023-06-22 15:10:40,446 - INFO - test acc 0.7450000047683716
2023-06-22 15:10:43,373 - INFO - Distilling data from client: Client10
2023-06-22 15:10:43,374 - INFO - train loss: 0.0007477777857965215
2023-06-22 15:10:43,374 - INFO - train acc: 1.0
2023-06-22 15:10:43,552 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.65      0.70        68
           6       0.72      0.82      0.77        67
           8       0.78      0.80      0.79        65

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:10:43,552 - INFO - test loss 0.019451448492801435
2023-06-22 15:10:43,552 - INFO - test acc 0.7549999952316284
2023-06-22 15:10:46,533 - INFO - Distilling data from client: Client10
2023-06-22 15:10:46,534 - INFO - train loss: 0.0004565187508625848
2023-06-22 15:10:46,534 - INFO - train acc: 1.0
2023-06-22 15:10:46,589 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.63      0.70        68
           6       0.72      0.81      0.76        67
           8       0.76      0.82      0.79        65

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-06-22 15:10:46,589 - INFO - test loss 0.020131799047999898
2023-06-22 15:10:46,589 - INFO - test acc 0.75
2023-06-22 15:10:49,551 - INFO - Distilling data from client: Client10
2023-06-22 15:10:49,551 - INFO - train loss: 0.0004199289039062359
2023-06-22 15:10:49,551 - INFO - train acc: 1.0
2023-06-22 15:10:49,646 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.62      0.69        68
           6       0.71      0.81      0.76        67
           8       0.74      0.80      0.77        65

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:10:49,647 - INFO - test loss 0.020481482656911968
2023-06-22 15:10:49,647 - INFO - test acc 0.7400000095367432
2023-06-22 15:10:52,515 - INFO - Distilling data from client: Client10
2023-06-22 15:10:52,515 - INFO - train loss: 0.0004475052360523808
2023-06-22 15:10:52,515 - INFO - train acc: 1.0
2023-06-22 15:10:52,582 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.60      0.68        68
           6       0.69      0.81      0.74        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:10:52,582 - INFO - test loss 0.020089077013033904
2023-06-22 15:10:52,583 - INFO - test acc 0.7299999594688416
2023-06-22 15:10:55,443 - INFO - Distilling data from client: Client10
2023-06-22 15:10:55,443 - INFO - train loss: 0.0003282220386317777
2023-06-22 15:10:55,444 - INFO - train acc: 1.0
2023-06-22 15:10:55,499 - INFO - report:               precision    recall  f1-score   support

           1       0.74      0.62      0.67        68
           6       0.72      0.76      0.74        67
           8       0.74      0.82      0.77        65

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-06-22 15:10:55,500 - INFO - test loss 0.020416607556493695
2023-06-22 15:10:55,500 - INFO - test acc 0.7299999594688416
2023-06-22 15:10:58,586 - INFO - Distilling data from client: Client10
2023-06-22 15:10:58,587 - INFO - train loss: 0.0002661988390179778
2023-06-22 15:10:58,587 - INFO - train acc: 1.0
2023-06-22 15:10:58,631 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.62      0.69        68
           6       0.72      0.79      0.75        67
           8       0.72      0.80      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:10:58,632 - INFO - test loss 0.020750211762838484
2023-06-22 15:10:58,632 - INFO - test acc 0.73499995470047
2023-06-22 15:11:01,500 - INFO - Distilling data from client: Client10
2023-06-22 15:11:01,501 - INFO - train loss: 0.00024333404741620426
2023-06-22 15:11:01,501 - INFO - train acc: 1.0
2023-06-22 15:11:01,551 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.62      0.69        68
           6       0.71      0.79      0.75        67
           8       0.72      0.80      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:11:01,551 - INFO - test loss 0.02048993569543708
2023-06-22 15:11:01,551 - INFO - test acc 0.73499995470047
2023-06-22 15:11:04,385 - INFO - Distilling data from client: Client10
2023-06-22 15:11:04,385 - INFO - train loss: 0.00024015439715923886
2023-06-22 15:11:04,386 - INFO - train acc: 1.0
2023-06-22 15:11:04,440 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.59      0.68        68
           6       0.71      0.79      0.75        67
           8       0.73      0.85      0.79        65

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:11:04,441 - INFO - test loss 0.020496139336834498
2023-06-22 15:11:04,442 - INFO - test acc 0.7400000095367432
2023-06-22 15:11:07,414 - INFO - Distilling data from client: Client10
2023-06-22 15:11:07,414 - INFO - train loss: 0.00019327199724814123
2023-06-22 15:11:07,415 - INFO - train acc: 1.0
2023-06-22 15:11:07,477 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.63      0.70        68
           6       0.71      0.79      0.75        67
           8       0.72      0.78      0.75        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:11:07,477 - INFO - test loss 0.020283142517697094
2023-06-22 15:11:07,477 - INFO - test acc 0.73499995470047
2023-06-22 15:11:10,313 - INFO - Distilling data from client: Client10
2023-06-22 15:11:10,313 - INFO - train loss: 0.00018481578943435552
2023-06-22 15:11:10,314 - INFO - train acc: 1.0
2023-06-22 15:11:10,364 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.63      0.70        68
           6       0.71      0.79      0.75        67
           8       0.75      0.82      0.78        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:11:10,365 - INFO - test loss 0.020361079335811206
2023-06-22 15:11:10,365 - INFO - test acc 0.7450000047683716
2023-06-22 15:11:13,232 - INFO - Distilling data from client: Client10
2023-06-22 15:11:13,232 - INFO - train loss: 0.00015003644253209356
2023-06-22 15:11:13,233 - INFO - train acc: 1.0
2023-06-22 15:11:13,285 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.63      0.70        68
           6       0.72      0.79      0.75        67
           8       0.75      0.83      0.79        65

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-06-22 15:11:13,286 - INFO - test loss 0.02009414993740996
2023-06-22 15:11:13,286 - INFO - test acc 0.75
2023-06-22 15:11:16,184 - INFO - Distilling data from client: Client10
2023-06-22 15:11:16,184 - INFO - train loss: 0.00013469742470837597
2023-06-22 15:11:16,184 - INFO - train acc: 1.0
2023-06-22 15:11:16,241 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.60      0.68        68
           6       0.69      0.81      0.74        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:11:16,242 - INFO - test loss 0.0205926205792798
2023-06-22 15:11:16,242 - INFO - test acc 0.7299999594688416
2023-06-22 15:11:19,087 - INFO - Distilling data from client: Client10
2023-06-22 15:11:19,087 - INFO - train loss: 0.00015713856116737055
2023-06-22 15:11:19,087 - INFO - train acc: 1.0
2023-06-22 15:11:19,140 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.63      0.70        68
           6       0.72      0.79      0.75        67
           8       0.75      0.82      0.78        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:11:19,141 - INFO - test loss 0.020783016366631116
2023-06-22 15:11:19,141 - INFO - test acc 0.7450000047683716
2023-06-22 15:11:22,043 - INFO - Distilling data from client: Client10
2023-06-22 15:11:22,043 - INFO - train loss: 0.0001304821158325457
2023-06-22 15:11:22,043 - INFO - train acc: 1.0
2023-06-22 15:11:22,096 - INFO - report:               precision    recall  f1-score   support

           1       0.76      0.65      0.70        68
           6       0.74      0.79      0.76        67
           8       0.74      0.80      0.77        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:11:22,097 - INFO - test loss 0.02021754434496149
2023-06-22 15:11:22,098 - INFO - test acc 0.7450000047683716
2023-06-22 15:11:25,150 - INFO - Distilling data from client: Client10
2023-06-22 15:11:25,150 - INFO - train loss: 0.00016835062064874052
2023-06-22 15:11:25,150 - INFO - train acc: 1.0
2023-06-22 15:11:25,211 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.63      0.70        68
           6       0.71      0.79      0.75        67
           8       0.73      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:11:25,211 - INFO - test loss 0.020183096852025243
2023-06-22 15:11:25,212 - INFO - test acc 0.73499995470047
2023-06-22 15:11:28,309 - INFO - Distilling data from client: Client10
2023-06-22 15:11:28,309 - INFO - train loss: 0.00014981458526820178
2023-06-22 15:11:28,309 - INFO - train acc: 1.0
2023-06-22 15:11:28,365 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.65      0.71        68
           6       0.70      0.79      0.74        67
           8       0.76      0.80      0.78        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:11:28,365 - INFO - test loss 0.020089196957807757
2023-06-22 15:11:28,365 - INFO - test acc 0.7450000047683716
2023-06-22 15:11:31,313 - INFO - Distilling data from client: Client10
2023-06-22 15:11:31,314 - INFO - train loss: 0.00011042562357523583
2023-06-22 15:11:31,314 - INFO - train acc: 1.0
2023-06-22 15:11:31,371 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.63      0.70        68
           6       0.68      0.78      0.73        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:11:31,371 - INFO - test loss 0.02029052547747193
2023-06-22 15:11:31,371 - INFO - test acc 0.7299999594688416
2023-06-22 15:11:34,214 - INFO - Distilling data from client: Client10
2023-06-22 15:11:34,215 - INFO - train loss: 0.00017596810699794756
2023-06-22 15:11:34,215 - INFO - train acc: 1.0
2023-06-22 15:11:34,271 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.62      0.69        68
           6       0.68      0.79      0.73        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:11:34,271 - INFO - test loss 0.020256942754074542
2023-06-22 15:11:34,271 - INFO - test acc 0.7299999594688416
2023-06-22 15:11:37,105 - INFO - Distilling data from client: Client10
2023-06-22 15:11:37,105 - INFO - train loss: 0.00014062379281569687
2023-06-22 15:11:37,106 - INFO - train acc: 1.0
2023-06-22 15:11:37,166 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.63      0.70        68
           6       0.70      0.78      0.74        67
           8       0.75      0.82      0.78        65

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:11:37,167 - INFO - test loss 0.020402449387317986
2023-06-22 15:11:37,167 - INFO - test acc 0.7400000095367432
2023-06-22 15:11:40,017 - INFO - Distilling data from client: Client10
2023-06-22 15:11:40,019 - INFO - train loss: 0.0001406981634418756
2023-06-22 15:11:40,020 - INFO - train acc: 1.0
2023-06-22 15:11:40,083 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.62      0.69        68
           6       0.73      0.79      0.76        67
           8       0.73      0.82      0.77        65

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:11:40,084 - INFO - test loss 0.020313011208966444
2023-06-22 15:11:40,084 - INFO - test acc 0.7400000095367432
2023-06-22 15:11:42,930 - INFO - Distilling data from client: Client10
2023-06-22 15:11:42,931 - INFO - train loss: 0.0001297072584668796
2023-06-22 15:11:42,931 - INFO - train acc: 1.0
2023-06-22 15:11:42,995 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.62      0.69        68
           6       0.73      0.79      0.76        67
           8       0.73      0.83      0.78        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:11:42,998 - INFO - test loss 0.02049479494337713
2023-06-22 15:11:43,000 - INFO - test acc 0.7450000047683716
2023-06-22 15:11:45,866 - INFO - Distilling data from client: Client10
2023-06-22 15:11:45,866 - INFO - train loss: 0.00012281940551362778
2023-06-22 15:11:45,866 - INFO - train acc: 1.0
2023-06-22 15:11:45,933 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.62      0.68        68
           6       0.71      0.79      0.75        67
           8       0.72      0.77      0.75        65

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:11:45,934 - INFO - test loss 0.020355316935084893
2023-06-22 15:11:45,935 - INFO - test acc 0.7249999642372131
2023-06-22 15:11:48,841 - INFO - Distilling data from client: Client10
2023-06-22 15:11:48,842 - INFO - train loss: 0.00016071363991574452
2023-06-22 15:11:48,842 - INFO - train acc: 1.0
2023-06-22 15:11:48,894 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.63      0.69        68
           6       0.72      0.79      0.75        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:11:48,895 - INFO - test loss 0.020334510343716854
2023-06-22 15:11:48,896 - INFO - test acc 0.73499995470047
2023-06-22 15:11:51,929 - INFO - Distilling data from client: Client10
2023-06-22 15:11:51,930 - INFO - train loss: 0.00013723534534009643
2023-06-22 15:11:51,931 - INFO - train acc: 1.0
2023-06-22 15:11:51,995 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.63      0.69        68
           6       0.72      0.79      0.75        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:11:51,999 - INFO - test loss 0.020449765723363777
2023-06-22 15:11:52,000 - INFO - test acc 0.73499995470047
2023-06-22 15:11:52,010 - WARNING - Finished tracing + transforming jit(gather) in 0.0011475086212158203 sec
2023-06-22 15:11:52,011 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[531,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:11:52,017 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0043566226959228516 sec
2023-06-22 15:11:52,018 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:11:52,044 - WARNING - Finished XLA compilation of jit(gather) in 0.02478194236755371 sec
2023-06-22 15:11:52,080 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:11:52,097 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:11:52,108 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:11:52,121 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:11:52,132 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:11:52,143 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:11:52,156 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:11:52,169 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:11:52,180 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:11:52,773 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client10//synthetic.png
2023-06-22 15:11:52,795 - INFO - c: 0.0 and total_data_in_this_class: 268
2023-06-22 15:11:52,795 - INFO - c: 5.0 and total_data_in_this_class: 268
2023-06-22 15:11:52,796 - INFO - c: 7.0 and total_data_in_this_class: 263
2023-06-22 15:11:52,796 - INFO - c: 0.0 and total_data_in_this_class: 65
2023-06-22 15:11:52,796 - INFO - c: 5.0 and total_data_in_this_class: 65
2023-06-22 15:11:52,796 - INFO - c: 7.0 and total_data_in_this_class: 70
2023-06-22 15:11:52,924 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07582521438598633 sec
2023-06-22 15:11:52,998 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07223653793334961 sec
2023-06-22 15:11:53,006 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16007328033447266 sec
2023-06-22 15:11:53,009 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:11:53,064 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05390429496765137 sec
2023-06-22 15:11:53,064 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:11:53,235 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17120575904846191 sec
2023-06-22 15:11:53,288 - INFO - initial test loss: 0.022151852191614977
2023-06-22 15:11:53,288 - INFO - initial test acc: 0.7099999785423279
2023-06-22 15:11:53,304 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009545326232910156 sec
2023-06-22 15:11:53,501 - WARNING - Finished tracing + transforming update_fn for pjit in 0.20725083351135254 sec
2023-06-22 15:11:53,506 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:11:53,612 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10511946678161621 sec
2023-06-22 15:11:53,612 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:11:54,060 - WARNING - Finished XLA compilation of jit(update_fn) in 0.44675207138061523 sec
2023-06-22 15:11:56,806 - INFO - Distilling data from client: Client11
2023-06-22 15:11:56,807 - INFO - train loss: 0.002086484655091479
2023-06-22 15:11:56,807 - INFO - train acc: 1.0
2023-06-22 15:11:57,047 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.86      0.79        65
           5       0.71      0.68      0.69        65
           7       0.67      0.59      0.63        70

    accuracy                           0.70       200
   macro avg       0.70      0.71      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:11:57,048 - INFO - test loss 0.021850958955662924
2023-06-22 15:11:57,048 - INFO - test acc 0.7049999833106995
2023-06-22 15:11:59,860 - INFO - Distilling data from client: Client11
2023-06-22 15:11:59,861 - INFO - train loss: 0.0010362348171352766
2023-06-22 15:11:59,861 - INFO - train acc: 0.9980952143669128
2023-06-22 15:12:00,070 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.88      0.80        65
           5       0.73      0.68      0.70        65
           7       0.71      0.63      0.67        70

    accuracy                           0.73       200
   macro avg       0.72      0.73      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:12:00,070 - INFO - test loss 0.02108649409885581
2023-06-22 15:12:00,070 - INFO - test acc 0.7249999642372131
2023-06-22 15:12:02,929 - INFO - Distilling data from client: Client11
2023-06-22 15:12:02,930 - INFO - train loss: 0.0007368500609379834
2023-06-22 15:12:02,930 - INFO - train acc: 1.0
2023-06-22 15:12:02,983 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.88      0.80        65
           5       0.72      0.66      0.69        65
           7       0.71      0.63      0.67        70

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:12:02,984 - INFO - test loss 0.021219716912058754
2023-06-22 15:12:02,984 - INFO - test acc 0.7199999690055847
2023-06-22 15:12:05,781 - INFO - Distilling data from client: Client11
2023-06-22 15:12:05,782 - INFO - train loss: 0.0005507830668876946
2023-06-22 15:12:05,783 - INFO - train acc: 1.0
2023-06-22 15:12:05,843 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.88      0.78        65
           5       0.73      0.68      0.70        65
           7       0.69      0.59      0.64        70

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:12:05,846 - INFO - test loss 0.021536138573415308
2023-06-22 15:12:05,847 - INFO - test acc 0.7099999785423279
2023-06-22 15:12:08,708 - INFO - Distilling data from client: Client11
2023-06-22 15:12:08,708 - INFO - train loss: 0.0005484663455976462
2023-06-22 15:12:08,708 - INFO - train acc: 1.0
2023-06-22 15:12:08,753 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.86      0.80        65
           5       0.75      0.66      0.70        65
           7       0.68      0.66      0.67        70

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:12:08,754 - INFO - test loss 0.02187964694046797
2023-06-22 15:12:08,754 - INFO - test acc 0.7249999642372131
2023-06-22 15:12:11,709 - INFO - Distilling data from client: Client11
2023-06-22 15:12:11,710 - INFO - train loss: 0.00047641095623266746
2023-06-22 15:12:11,710 - INFO - train acc: 1.0
2023-06-22 15:12:11,772 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.85      0.78        65
           5       0.68      0.66      0.67        65
           7       0.69      0.60      0.64        70

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:12:11,773 - INFO - test loss 0.02211793022746027
2023-06-22 15:12:11,773 - INFO - test acc 0.699999988079071
2023-06-22 15:12:14,828 - INFO - Distilling data from client: Client11
2023-06-22 15:12:14,829 - INFO - train loss: 0.00038508560588435524
2023-06-22 15:12:14,829 - INFO - train acc: 1.0
2023-06-22 15:12:14,894 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.85      0.78        65
           5       0.75      0.71      0.73        65
           7       0.68      0.61      0.65        70

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:12:14,897 - INFO - test loss 0.02170589616609679
2023-06-22 15:12:14,897 - INFO - test acc 0.7199999690055847
2023-06-22 15:12:17,897 - INFO - Distilling data from client: Client11
2023-06-22 15:12:17,898 - INFO - train loss: 0.0003914084236763506
2023-06-22 15:12:17,899 - INFO - train acc: 1.0
2023-06-22 15:12:17,964 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.85      0.76        65
           5       0.72      0.66      0.69        65
           7       0.67      0.59      0.63        70

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:12:17,965 - INFO - test loss 0.022046926520951752
2023-06-22 15:12:17,966 - INFO - test acc 0.6949999928474426
2023-06-22 15:12:20,868 - INFO - Distilling data from client: Client11
2023-06-22 15:12:20,869 - INFO - train loss: 0.0003060570168105516
2023-06-22 15:12:20,869 - INFO - train acc: 1.0
2023-06-22 15:12:20,935 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.86      0.79        65
           5       0.74      0.69      0.71        65
           7       0.71      0.63      0.67        70

    accuracy                           0.73       200
   macro avg       0.72      0.73      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:12:20,937 - INFO - test loss 0.02165076180081197
2023-06-22 15:12:20,938 - INFO - test acc 0.7249999642372131
2023-06-22 15:12:23,874 - INFO - Distilling data from client: Client11
2023-06-22 15:12:23,874 - INFO - train loss: 0.00035749176805456427
2023-06-22 15:12:23,874 - INFO - train acc: 1.0
2023-06-22 15:12:23,933 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.88      0.78        65
           5       0.73      0.62      0.67        65
           7       0.69      0.63      0.66        70

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:12:23,934 - INFO - test loss 0.022077901104325545
2023-06-22 15:12:23,934 - INFO - test acc 0.7049999833106995
2023-06-22 15:12:26,671 - INFO - Distilling data from client: Client11
2023-06-22 15:12:26,671 - INFO - train loss: 0.00026715656945880157
2023-06-22 15:12:26,671 - INFO - train acc: 1.0
2023-06-22 15:12:26,732 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.88      0.79        65
           5       0.73      0.66      0.69        65
           7       0.70      0.61      0.66        70

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:12:26,734 - INFO - test loss 0.02110229214230335
2023-06-22 15:12:26,734 - INFO - test acc 0.7149999737739563
2023-06-22 15:12:29,510 - INFO - Distilling data from client: Client11
2023-06-22 15:12:29,511 - INFO - train loss: 0.00021099589577274207
2023-06-22 15:12:29,512 - INFO - train acc: 1.0
2023-06-22 15:12:29,606 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.78      0.74        65
           5       0.73      0.69      0.71        65
           7       0.68      0.63      0.65        70

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:12:29,611 - INFO - test loss 0.021508888118302264
2023-06-22 15:12:29,612 - INFO - test acc 0.699999988079071
2023-06-22 15:12:32,456 - INFO - Distilling data from client: Client11
2023-06-22 15:12:32,457 - INFO - train loss: 0.00026466828967319664
2023-06-22 15:12:32,457 - INFO - train acc: 1.0
2023-06-22 15:12:32,506 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.86      0.79        65
           5       0.74      0.65      0.69        65
           7       0.70      0.66      0.68        70

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:12:32,509 - INFO - test loss 0.021469161349826137
2023-06-22 15:12:32,510 - INFO - test acc 0.7199999690055847
2023-06-22 15:12:35,370 - INFO - Distilling data from client: Client11
2023-06-22 15:12:35,371 - INFO - train loss: 0.00024959670566911146
2023-06-22 15:12:35,371 - INFO - train acc: 1.0
2023-06-22 15:12:35,416 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.86      0.77        65
           5       0.75      0.66      0.70        65
           7       0.70      0.63      0.66        70

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:12:35,417 - INFO - test loss 0.0219812906109756
2023-06-22 15:12:35,417 - INFO - test acc 0.7149999737739563
2023-06-22 15:12:38,195 - INFO - Distilling data from client: Client11
2023-06-22 15:12:38,196 - INFO - train loss: 0.0002169999109231157
2023-06-22 15:12:38,197 - INFO - train acc: 1.0
2023-06-22 15:12:38,252 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.86      0.77        65
           5       0.72      0.65      0.68        65
           7       0.69      0.61      0.65        70

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:12:38,255 - INFO - test loss 0.021703798160102896
2023-06-22 15:12:38,256 - INFO - test acc 0.7049999833106995
2023-06-22 15:12:41,198 - INFO - Distilling data from client: Client11
2023-06-22 15:12:41,198 - INFO - train loss: 0.00019552616585178162
2023-06-22 15:12:41,198 - INFO - train acc: 1.0
2023-06-22 15:12:41,255 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.85      0.77        65
           5       0.73      0.66      0.69        65
           7       0.73      0.67      0.70        70

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:12:41,255 - INFO - test loss 0.021427287416093368
2023-06-22 15:12:41,255 - INFO - test acc 0.7249999642372131
2023-06-22 15:12:44,169 - INFO - Distilling data from client: Client11
2023-06-22 15:12:44,169 - INFO - train loss: 0.0001793569117631157
2023-06-22 15:12:44,170 - INFO - train acc: 1.0
2023-06-22 15:12:44,220 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.86      0.78        65
           5       0.72      0.68      0.70        65
           7       0.70      0.61      0.66        70

    accuracy                           0.71       200
   macro avg       0.71      0.72      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:12:44,221 - INFO - test loss 0.021349658533349452
2023-06-22 15:12:44,221 - INFO - test acc 0.7149999737739563
2023-06-22 15:12:47,099 - INFO - Distilling data from client: Client11
2023-06-22 15:12:47,099 - INFO - train loss: 0.00019039583685579863
2023-06-22 15:12:47,099 - INFO - train acc: 1.0
2023-06-22 15:12:47,151 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.85      0.77        65
           5       0.73      0.68      0.70        65
           7       0.70      0.63      0.66        70

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:12:47,153 - INFO - test loss 0.021594047689414538
2023-06-22 15:12:47,153 - INFO - test acc 0.7149999737739563
2023-06-22 15:12:49,973 - INFO - Distilling data from client: Client11
2023-06-22 15:12:49,974 - INFO - train loss: 0.00016960900532183684
2023-06-22 15:12:49,975 - INFO - train acc: 1.0
2023-06-22 15:12:50,038 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.83      0.77        65
           5       0.70      0.66      0.68        65
           7       0.69      0.63      0.66        70

    accuracy                           0.70       200
   macro avg       0.70      0.71      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:12:50,040 - INFO - test loss 0.02179513868433161
2023-06-22 15:12:50,040 - INFO - test acc 0.7049999833106995
2023-06-22 15:12:52,815 - INFO - Distilling data from client: Client11
2023-06-22 15:12:52,816 - INFO - train loss: 0.00019495674121263652
2023-06-22 15:12:52,816 - INFO - train acc: 1.0
2023-06-22 15:12:52,981 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.91      0.80        65
           5       0.75      0.66      0.70        65
           7       0.74      0.64      0.69        70

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:12:52,981 - INFO - test loss 0.021608680440538267
2023-06-22 15:12:52,981 - INFO - test acc 0.73499995470047
2023-06-22 15:12:55,754 - INFO - Distilling data from client: Client11
2023-06-22 15:12:55,755 - INFO - train loss: 0.00017367933453508848
2023-06-22 15:12:55,755 - INFO - train acc: 1.0
2023-06-22 15:12:55,914 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.89      0.79        65
           5       0.76      0.68      0.72        65
           7       0.75      0.66      0.70        70

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:12:55,914 - INFO - test loss 0.021369523779819866
2023-06-22 15:12:55,914 - INFO - test acc 0.7400000095367432
2023-06-22 15:12:58,646 - INFO - Distilling data from client: Client11
2023-06-22 15:12:58,647 - INFO - train loss: 0.0001793150049861306
2023-06-22 15:12:58,647 - INFO - train acc: 1.0
2023-06-22 15:12:58,699 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.85      0.79        65
           5       0.75      0.69      0.72        65
           7       0.71      0.66      0.68        70

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-06-22 15:12:58,700 - INFO - test loss 0.02200117637475312
2023-06-22 15:12:58,701 - INFO - test acc 0.7299999594688416
2023-06-22 15:13:01,621 - INFO - Distilling data from client: Client11
2023-06-22 15:13:01,622 - INFO - train loss: 0.0001502874500871219
2023-06-22 15:13:01,622 - INFO - train acc: 1.0
2023-06-22 15:13:01,686 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.83      0.77        65
           5       0.72      0.68      0.70        65
           7       0.69      0.63      0.66        70

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:13:01,689 - INFO - test loss 0.021740429289868975
2023-06-22 15:13:01,690 - INFO - test acc 0.7099999785423279
2023-06-22 15:13:04,671 - INFO - Distilling data from client: Client11
2023-06-22 15:13:04,677 - INFO - train loss: 0.00015319112635225245
2023-06-22 15:13:04,677 - INFO - train acc: 1.0
2023-06-22 15:13:04,736 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.85      0.78        65
           5       0.73      0.69      0.71        65
           7       0.69      0.61      0.65        70

    accuracy                           0.71       200
   macro avg       0.71      0.72      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:13:04,737 - INFO - test loss 0.021681456843093888
2023-06-22 15:13:04,737 - INFO - test acc 0.7149999737739563
2023-06-22 15:13:07,745 - INFO - Distilling data from client: Client11
2023-06-22 15:13:07,745 - INFO - train loss: 0.00016937625291594903
2023-06-22 15:13:07,746 - INFO - train acc: 1.0
2023-06-22 15:13:07,808 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.85      0.77        65
           5       0.75      0.68      0.71        65
           7       0.70      0.63      0.66        70

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:13:07,808 - INFO - test loss 0.02119621201584237
2023-06-22 15:13:07,809 - INFO - test acc 0.7149999737739563
2023-06-22 15:13:07,882 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:13:07,901 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:13:07,914 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:13:07,925 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:13:07,936 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:13:07,946 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:13:07,959 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:13:07,971 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:13:07,982 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:13:08,528 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client11//synthetic.png
2023-06-22 15:13:08,553 - INFO - c: 1.0 and total_data_in_this_class: 272
2023-06-22 15:13:08,553 - INFO - c: 6.0 and total_data_in_this_class: 267
2023-06-22 15:13:08,553 - INFO - c: 7.0 and total_data_in_this_class: 260
2023-06-22 15:13:08,553 - INFO - c: 1.0 and total_data_in_this_class: 61
2023-06-22 15:13:08,553 - INFO - c: 6.0 and total_data_in_this_class: 66
2023-06-22 15:13:08,553 - INFO - c: 7.0 and total_data_in_this_class: 73
2023-06-22 15:13:08,681 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07692241668701172 sec
2023-06-22 15:13:08,752 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06944823265075684 sec
2023-06-22 15:13:08,760 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.15804076194763184 sec
2023-06-22 15:13:08,763 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:13:08,818 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.054903268814086914 sec
2023-06-22 15:13:08,818 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:13:08,989 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.16991782188415527 sec
2023-06-22 15:13:09,039 - INFO - initial test loss: 0.0233406838998526
2023-06-22 15:13:09,039 - INFO - initial test acc: 0.7149999737739563
2023-06-22 15:13:09,059 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012874126434326172 sec
2023-06-22 15:13:09,249 - WARNING - Finished tracing + transforming update_fn for pjit in 0.20471858978271484 sec
2023-06-22 15:13:09,254 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:13:09,356 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10137939453125 sec
2023-06-22 15:13:09,356 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:13:09,779 - WARNING - Finished XLA compilation of jit(update_fn) in 0.42303991317749023 sec
2023-06-22 15:13:12,581 - INFO - Distilling data from client: Client12
2023-06-22 15:13:12,582 - INFO - train loss: 0.0018933360819733454
2023-06-22 15:13:12,583 - INFO - train acc: 1.0
2023-06-22 15:13:12,753 - INFO - report:               precision    recall  f1-score   support

           1       0.65      0.74      0.69        61
           6       0.67      0.74      0.71        66
           7       0.76      0.60      0.67        73

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:13:12,753 - INFO - test loss 0.022449521234692547
2023-06-22 15:13:12,753 - INFO - test acc 0.6899999976158142
2023-06-22 15:13:15,512 - INFO - Distilling data from client: Client12
2023-06-22 15:13:15,513 - INFO - train loss: 0.0009339936233867512
2023-06-22 15:13:15,514 - INFO - train acc: 1.0
2023-06-22 15:13:15,701 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.80      0.74        61
           6       0.67      0.74      0.71        66
           7       0.78      0.59      0.67        73

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:13:15,701 - INFO - test loss 0.023188995284337204
2023-06-22 15:13:15,701 - INFO - test acc 0.7049999833106995
2023-06-22 15:13:18,404 - INFO - Distilling data from client: Client12
2023-06-22 15:13:18,405 - INFO - train loss: 0.0007391885245217466
2023-06-22 15:13:18,405 - INFO - train acc: 1.0
2023-06-22 15:13:18,566 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.80      0.74        61
           6       0.74      0.77      0.76        66
           7       0.76      0.62      0.68        73

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:13:18,566 - INFO - test loss 0.0231912602952315
2023-06-22 15:13:18,566 - INFO - test acc 0.7249999642372131
2023-06-22 15:13:21,261 - INFO - Distilling data from client: Client12
2023-06-22 15:13:21,262 - INFO - train loss: 0.0005980966719978491
2023-06-22 15:13:21,262 - INFO - train acc: 1.0
2023-06-22 15:13:21,335 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.82      0.73        61
           6       0.65      0.73      0.69        66
           7       0.78      0.53      0.63        73

    accuracy                           0.69       200
   macro avg       0.70      0.69      0.68       200
weighted avg       0.70      0.69      0.68       200

2023-06-22 15:13:21,337 - INFO - test loss 0.023800615036080146
2023-06-22 15:13:21,337 - INFO - test acc 0.6850000023841858
2023-06-22 15:13:24,166 - INFO - Distilling data from client: Client12
2023-06-22 15:13:24,166 - INFO - train loss: 0.0006729940050977437
2023-06-22 15:13:24,166 - INFO - train acc: 1.0
2023-06-22 15:13:24,223 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.82      0.73        61
           6       0.68      0.71      0.70        66
           7       0.78      0.59      0.67        73

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:13:24,225 - INFO - test loss 0.023124243694568277
2023-06-22 15:13:24,225 - INFO - test acc 0.699999988079071
2023-06-22 15:13:27,030 - INFO - Distilling data from client: Client12
2023-06-22 15:13:27,031 - INFO - train loss: 0.0005163525732062464
2023-06-22 15:13:27,031 - INFO - train acc: 1.0
2023-06-22 15:13:27,108 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.75      0.72        61
           6       0.63      0.76      0.69        66
           7       0.76      0.56      0.65        73

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.70      0.69      0.68       200

2023-06-22 15:13:27,111 - INFO - test loss 0.023702433478739536
2023-06-22 15:13:27,112 - INFO - test acc 0.6850000023841858
2023-06-22 15:13:30,137 - INFO - Distilling data from client: Client12
2023-06-22 15:13:30,139 - INFO - train loss: 0.00045738501147411597
2023-06-22 15:13:30,140 - INFO - train acc: 1.0
2023-06-22 15:13:30,232 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.77      0.73        61
           6       0.67      0.76      0.71        66
           7       0.75      0.59      0.66        73

    accuracy                           0.70       200
   macro avg       0.70      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:13:30,233 - INFO - test loss 0.02404870227232113
2023-06-22 15:13:30,234 - INFO - test acc 0.699999988079071
2023-06-22 15:13:33,050 - INFO - Distilling data from client: Client12
2023-06-22 15:13:33,052 - INFO - train loss: 0.0003317386690387873
2023-06-22 15:13:33,052 - INFO - train acc: 1.0
2023-06-22 15:13:33,097 - INFO - report:               precision    recall  f1-score   support

           1       0.64      0.77      0.70        61
           6       0.63      0.67      0.65        66
           7       0.75      0.58      0.65        73

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.66       200
weighted avg       0.67      0.67      0.66       200

2023-06-22 15:13:33,098 - INFO - test loss 0.024250673549491744
2023-06-22 15:13:33,098 - INFO - test acc 0.6649999618530273
2023-06-22 15:13:35,917 - INFO - Distilling data from client: Client12
2023-06-22 15:13:35,917 - INFO - train loss: 0.00035520052957495454
2023-06-22 15:13:35,918 - INFO - train acc: 1.0
2023-06-22 15:13:35,992 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.80      0.74        61
           6       0.63      0.70      0.66        66
           7       0.76      0.58      0.66        73

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.69      0.68       200

2023-06-22 15:13:35,993 - INFO - test loss 0.023608786962138938
2023-06-22 15:13:35,995 - INFO - test acc 0.6850000023841858
2023-06-22 15:13:38,798 - INFO - Distilling data from client: Client12
2023-06-22 15:13:38,798 - INFO - train loss: 0.00028422606239530664
2023-06-22 15:13:38,799 - INFO - train acc: 1.0
2023-06-22 15:13:38,847 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.75      0.71        61
           6       0.64      0.74      0.69        66
           7       0.75      0.56      0.64        73

    accuracy                           0.68       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:13:38,853 - INFO - test loss 0.023570813921715925
2023-06-22 15:13:38,853 - INFO - test acc 0.6800000071525574
2023-06-22 15:13:41,633 - INFO - Distilling data from client: Client12
2023-06-22 15:13:41,634 - INFO - train loss: 0.00033036333709778596
2023-06-22 15:13:41,634 - INFO - train acc: 1.0
2023-06-22 15:13:41,681 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.80      0.73        61
           6       0.63      0.71      0.67        66
           7       0.80      0.56      0.66        73

    accuracy                           0.69       200
   macro avg       0.70      0.69      0.68       200
weighted avg       0.70      0.69      0.68       200

2023-06-22 15:13:41,689 - INFO - test loss 0.02357998087940604
2023-06-22 15:13:41,690 - INFO - test acc 0.6850000023841858
2023-06-22 15:13:44,552 - INFO - Distilling data from client: Client12
2023-06-22 15:13:44,553 - INFO - train loss: 0.00026720313591215554
2023-06-22 15:13:44,553 - INFO - train acc: 1.0
2023-06-22 15:13:44,601 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.79      0.72        61
           6       0.62      0.71      0.66        66
           7       0.80      0.56      0.66        73

    accuracy                           0.68       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.70      0.68      0.68       200

2023-06-22 15:13:44,604 - INFO - test loss 0.023844374763690432
2023-06-22 15:13:44,605 - INFO - test acc 0.6800000071525574
2023-06-22 15:13:47,500 - INFO - Distilling data from client: Client12
2023-06-22 15:13:47,500 - INFO - train loss: 0.00026324202192272173
2023-06-22 15:13:47,500 - INFO - train acc: 1.0
2023-06-22 15:13:47,554 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.80      0.74        61
           6       0.66      0.71      0.69        66
           7       0.79      0.63      0.70        73

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:13:47,555 - INFO - test loss 0.023394919357932743
2023-06-22 15:13:47,556 - INFO - test acc 0.7099999785423279
2023-06-22 15:13:50,599 - INFO - Distilling data from client: Client12
2023-06-22 15:13:50,600 - INFO - train loss: 0.00021737514316700338
2023-06-22 15:13:50,600 - INFO - train acc: 1.0
2023-06-22 15:13:50,652 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.79      0.73        61
           6       0.65      0.71      0.68        66
           7       0.77      0.60      0.68        73

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:13:50,653 - INFO - test loss 0.023543443968940567
2023-06-22 15:13:50,653 - INFO - test acc 0.6949999928474426
2023-06-22 15:13:53,729 - INFO - Distilling data from client: Client12
2023-06-22 15:13:53,729 - INFO - train loss: 0.0002935291486344926
2023-06-22 15:13:53,729 - INFO - train acc: 1.0
2023-06-22 15:13:53,780 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.79      0.73        61
           6       0.63      0.73      0.68        66
           7       0.80      0.59      0.68        73

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.69      0.69       200

2023-06-22 15:13:53,780 - INFO - test loss 0.02350106070836944
2023-06-22 15:13:53,780 - INFO - test acc 0.6949999928474426
2023-06-22 15:13:56,839 - INFO - Distilling data from client: Client12
2023-06-22 15:13:56,840 - INFO - train loss: 0.0002740718359935697
2023-06-22 15:13:56,840 - INFO - train acc: 1.0
2023-06-22 15:13:56,893 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.79      0.73        61
           6       0.65      0.73      0.69        66
           7       0.78      0.59      0.67        73

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.71      0.69      0.69       200

2023-06-22 15:13:56,894 - INFO - test loss 0.02364701263981567
2023-06-22 15:13:56,894 - INFO - test acc 0.6949999928474426
2023-06-22 15:13:59,814 - INFO - Distilling data from client: Client12
2023-06-22 15:13:59,814 - INFO - train loss: 0.00028973010891793677
2023-06-22 15:13:59,814 - INFO - train acc: 1.0
2023-06-22 15:13:59,877 - INFO - report:               precision    recall  f1-score   support

           1       0.65      0.74      0.69        61
           6       0.64      0.74      0.69        66
           7       0.76      0.58      0.66        73

    accuracy                           0.68       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:13:59,879 - INFO - test loss 0.023505176637015176
2023-06-22 15:13:59,879 - INFO - test acc 0.6800000071525574
2023-06-22 15:14:02,725 - INFO - Distilling data from client: Client12
2023-06-22 15:14:02,726 - INFO - train loss: 0.0002128884662002477
2023-06-22 15:14:02,726 - INFO - train acc: 1.0
2023-06-22 15:14:02,790 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.77      0.71        61
           6       0.62      0.70      0.66        66
           7       0.75      0.56      0.64        73

    accuracy                           0.67       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:14:02,791 - INFO - test loss 0.024003640819479807
2023-06-22 15:14:02,792 - INFO - test acc 0.6699999570846558
2023-06-22 15:14:05,646 - INFO - Distilling data from client: Client12
2023-06-22 15:14:05,646 - INFO - train loss: 0.00019595361105365024
2023-06-22 15:14:05,646 - INFO - train acc: 1.0
2023-06-22 15:14:05,698 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.79      0.73        61
           6       0.65      0.70      0.67        66
           7       0.78      0.63      0.70        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:14:05,699 - INFO - test loss 0.02389378400889143
2023-06-22 15:14:05,699 - INFO - test acc 0.699999988079071
2023-06-22 15:14:08,566 - INFO - Distilling data from client: Client12
2023-06-22 15:14:08,567 - INFO - train loss: 0.00019833876699073852
2023-06-22 15:14:08,567 - INFO - train acc: 1.0
2023-06-22 15:14:08,623 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.74      0.70        61
           6       0.63      0.73      0.68        66
           7       0.77      0.59      0.67        73

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:14:08,624 - INFO - test loss 0.024568377981750784
2023-06-22 15:14:08,624 - INFO - test acc 0.6800000071525574
2023-06-22 15:14:11,532 - INFO - Distilling data from client: Client12
2023-06-22 15:14:11,532 - INFO - train loss: 0.00021977063766750825
2023-06-22 15:14:11,532 - INFO - train acc: 1.0
2023-06-22 15:14:11,585 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.79      0.73        61
           6       0.68      0.76      0.72        66
           7       0.79      0.62      0.69        73

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:14:11,586 - INFO - test loss 0.023190171095598865
2023-06-22 15:14:11,586 - INFO - test acc 0.7149999737739563
2023-06-22 15:14:14,586 - INFO - Distilling data from client: Client12
2023-06-22 15:14:14,586 - INFO - train loss: 0.00025657363354567183
2023-06-22 15:14:14,586 - INFO - train acc: 1.0
2023-06-22 15:14:14,636 - INFO - report:               precision    recall  f1-score   support

           1       0.65      0.77      0.71        61
           6       0.63      0.73      0.68        66
           7       0.81      0.58      0.67        73

    accuracy                           0.69       200
   macro avg       0.70      0.69      0.68       200
weighted avg       0.70      0.69      0.68       200

2023-06-22 15:14:14,637 - INFO - test loss 0.023606283654385992
2023-06-22 15:14:14,638 - INFO - test acc 0.6850000023841858
2023-06-22 15:14:17,602 - INFO - Distilling data from client: Client12
2023-06-22 15:14:17,603 - INFO - train loss: 0.00023722382704367064
2023-06-22 15:14:17,603 - INFO - train acc: 1.0
2023-06-22 15:14:17,657 - INFO - report:               precision    recall  f1-score   support

           1       0.65      0.77      0.71        61
           6       0.66      0.73      0.69        66
           7       0.82      0.62      0.70        73

    accuracy                           0.70       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:14:17,658 - INFO - test loss 0.023693022501175113
2023-06-22 15:14:17,658 - INFO - test acc 0.699999988079071
2023-06-22 15:14:20,564 - INFO - Distilling data from client: Client12
2023-06-22 15:14:20,564 - INFO - train loss: 0.00019491412697566197
2023-06-22 15:14:20,567 - INFO - train acc: 1.0
2023-06-22 15:14:20,618 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.80      0.74        61
           6       0.66      0.73      0.69        66
           7       0.78      0.59      0.67        73

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:14:20,619 - INFO - test loss 0.023492503843825178
2023-06-22 15:14:20,624 - INFO - test acc 0.699999988079071
2023-06-22 15:14:23,522 - INFO - Distilling data from client: Client12
2023-06-22 15:14:23,522 - INFO - train loss: 0.00023806710511878256
2023-06-22 15:14:23,523 - INFO - train acc: 1.0
2023-06-22 15:14:23,576 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.77      0.72        61
           6       0.63      0.73      0.68        66
           7       0.75      0.56      0.64        73

    accuracy                           0.68       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:14:23,578 - INFO - test loss 0.024066464289881274
2023-06-22 15:14:23,578 - INFO - test acc 0.6800000071525574
2023-06-22 15:14:23,609 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:14:23,629 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:14:23,647 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:14:23,664 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:14:23,678 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:14:23,690 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:14:23,702 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:14:23,713 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:14:23,724 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:14:24,278 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client12//synthetic.png
2023-06-22 15:14:24,297 - INFO - c: 0.0 and total_data_in_this_class: 270
2023-06-22 15:14:24,297 - INFO - c: 4.0 and total_data_in_this_class: 256
2023-06-22 15:14:24,298 - INFO - c: 7.0 and total_data_in_this_class: 273
2023-06-22 15:14:24,298 - INFO - c: 0.0 and total_data_in_this_class: 63
2023-06-22 15:14:24,298 - INFO - c: 4.0 and total_data_in_this_class: 77
2023-06-22 15:14:24,298 - INFO - c: 7.0 and total_data_in_this_class: 60
2023-06-22 15:14:24,420 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0761556625366211 sec
2023-06-22 15:14:24,496 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07516145706176758 sec
2023-06-22 15:14:24,505 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.1629934310913086 sec
2023-06-22 15:14:24,508 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:14:24,563 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05455279350280762 sec
2023-06-22 15:14:24,563 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:14:24,736 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17227792739868164 sec
2023-06-22 15:14:24,774 - INFO - initial test loss: 0.02453060214251138
2023-06-22 15:14:24,775 - INFO - initial test acc: 0.6599999666213989
2023-06-22 15:14:24,792 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012540578842163086 sec
2023-06-22 15:14:24,985 - WARNING - Finished tracing + transforming update_fn for pjit in 0.20627832412719727 sec
2023-06-22 15:14:24,990 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:14:25,090 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10015463829040527 sec
2023-06-22 15:14:25,091 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:14:25,518 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4269990921020508 sec
2023-06-22 15:14:28,258 - INFO - Distilling data from client: Client13
2023-06-22 15:14:28,259 - INFO - train loss: 0.002492028173277688
2023-06-22 15:14:28,259 - INFO - train acc: 0.9941520690917969
2023-06-22 15:14:28,429 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.75      0.76        63
           4       0.67      0.69      0.68        77
           7       0.58      0.58      0.58        60

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:14:28,429 - INFO - test loss 0.023836718867039145
2023-06-22 15:14:28,429 - INFO - test acc 0.675000011920929
2023-06-22 15:14:31,318 - INFO - Distilling data from client: Client13
2023-06-22 15:14:31,318 - INFO - train loss: 0.0013839283850304305
2023-06-22 15:14:31,318 - INFO - train acc: 0.9980506896972656
2023-06-22 15:14:31,367 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        63
           4       0.68      0.68      0.68        77
           7       0.60      0.62      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:14:31,367 - INFO - test loss 0.023495422078648753
2023-06-22 15:14:31,367 - INFO - test acc 0.675000011920929
2023-06-22 15:14:34,320 - INFO - Distilling data from client: Client13
2023-06-22 15:14:34,320 - INFO - train loss: 0.0011411714190800253
2023-06-22 15:14:34,320 - INFO - train acc: 1.0
2023-06-22 15:14:34,375 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.70      0.73        63
           4       0.62      0.65      0.63        77
           7       0.55      0.57      0.56        60

    accuracy                           0.64       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.64      0.64       200

2023-06-22 15:14:34,376 - INFO - test loss 0.02384639741305953
2023-06-22 15:14:34,376 - INFO - test acc 0.6399999856948853
2023-06-22 15:14:37,277 - INFO - Distilling data from client: Client13
2023-06-22 15:14:37,277 - INFO - train loss: 0.0009659095411245067
2023-06-22 15:14:37,278 - INFO - train acc: 1.0
2023-06-22 15:14:37,353 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.73      0.75        63
           4       0.64      0.69      0.66        77
           7       0.60      0.58      0.59        60

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:14:37,353 - INFO - test loss 0.02381829466233552
2023-06-22 15:14:37,353 - INFO - test acc 0.6699999570846558
2023-06-22 15:14:40,233 - INFO - Distilling data from client: Client13
2023-06-22 15:14:40,233 - INFO - train loss: 0.0008561507351085277
2023-06-22 15:14:40,233 - INFO - train acc: 1.0
2023-06-22 15:14:40,432 - INFO - report:               precision    recall  f1-score   support

           0       0.81      0.75      0.78        63
           4       0.70      0.68      0.69        77
           7       0.60      0.68      0.64        60

    accuracy                           0.70       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:14:40,433 - INFO - test loss 0.02405602477591879
2023-06-22 15:14:40,433 - INFO - test acc 0.699999988079071
2023-06-22 15:14:43,350 - INFO - Distilling data from client: Client13
2023-06-22 15:14:43,350 - INFO - train loss: 0.0006609201601307378
2023-06-22 15:14:43,351 - INFO - train acc: 0.9980506896972656
2023-06-22 15:14:43,412 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.71      0.73        63
           4       0.68      0.70      0.69        77
           7       0.60      0.60      0.60        60

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:14:43,414 - INFO - test loss 0.024370646135197148
2023-06-22 15:14:43,414 - INFO - test acc 0.675000011920929
2023-06-22 15:14:46,515 - INFO - Distilling data from client: Client13
2023-06-22 15:14:46,515 - INFO - train loss: 0.0008240391718545957
2023-06-22 15:14:46,515 - INFO - train acc: 1.0
2023-06-22 15:14:46,573 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.79      0.78        63
           4       0.69      0.66      0.68        77
           7       0.61      0.62      0.61        60

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:14:46,573 - INFO - test loss 0.02380000236415084
2023-06-22 15:14:46,574 - INFO - test acc 0.6899999976158142
2023-06-22 15:14:49,432 - INFO - Distilling data from client: Client13
2023-06-22 15:14:49,433 - INFO - train loss: 0.0007472501792183362
2023-06-22 15:14:49,433 - INFO - train acc: 1.0
2023-06-22 15:14:49,496 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.78      0.77        63
           4       0.69      0.69      0.69        77
           7       0.62      0.60      0.61        60

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:14:49,498 - INFO - test loss 0.02377597937242283
2023-06-22 15:14:49,499 - INFO - test acc 0.6899999976158142
2023-06-22 15:14:52,383 - INFO - Distilling data from client: Client13
2023-06-22 15:14:52,384 - INFO - train loss: 0.0006456969009335933
2023-06-22 15:14:52,384 - INFO - train acc: 1.0
2023-06-22 15:14:52,433 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.76      0.75        63
           4       0.67      0.69      0.68        77
           7       0.61      0.57      0.59        60

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.67      0.68      0.67       200

2023-06-22 15:14:52,438 - INFO - test loss 0.023828031312844087
2023-06-22 15:14:52,439 - INFO - test acc 0.675000011920929
2023-06-22 15:14:55,248 - INFO - Distilling data from client: Client13
2023-06-22 15:14:55,248 - INFO - train loss: 0.0006205793405977376
2023-06-22 15:14:55,249 - INFO - train acc: 1.0
2023-06-22 15:14:55,302 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.77        63
           4       0.64      0.69      0.66        77
           7       0.60      0.58      0.59        60

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:14:55,304 - INFO - test loss 0.024112208933438517
2023-06-22 15:14:55,304 - INFO - test acc 0.675000011920929
2023-06-22 15:14:58,105 - INFO - Distilling data from client: Client13
2023-06-22 15:14:58,105 - INFO - train loss: 0.000480930148607275
2023-06-22 15:14:58,105 - INFO - train acc: 1.0
2023-06-22 15:14:58,164 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.79      0.79        63
           4       0.66      0.68      0.67        77
           7       0.61      0.58      0.60        60

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.69      0.68       200

2023-06-22 15:14:58,164 - INFO - test loss 0.02369214300615066
2023-06-22 15:14:58,164 - INFO - test acc 0.6850000023841858
2023-06-22 15:15:01,064 - INFO - Distilling data from client: Client13
2023-06-22 15:15:01,064 - INFO - train loss: 0.00038909059619722237
2023-06-22 15:15:01,064 - INFO - train acc: 1.0
2023-06-22 15:15:01,129 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.75      0.75        63
           4       0.68      0.69      0.68        77
           7       0.58      0.58      0.58        60

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:15:01,130 - INFO - test loss 0.024534225870781527
2023-06-22 15:15:01,130 - INFO - test acc 0.675000011920929
2023-06-22 15:15:03,984 - INFO - Distilling data from client: Client13
2023-06-22 15:15:03,984 - INFO - train loss: 0.0004508073852262811
2023-06-22 15:15:03,985 - INFO - train acc: 1.0
2023-06-22 15:15:04,042 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.77        63
           4       0.67      0.66      0.67        77
           7       0.62      0.67      0.64        60

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:15:04,043 - INFO - test loss 0.023933094208377255
2023-06-22 15:15:04,043 - INFO - test acc 0.6899999976158142
2023-06-22 15:15:07,017 - INFO - Distilling data from client: Client13
2023-06-22 15:15:07,018 - INFO - train loss: 0.00040577425765940306
2023-06-22 15:15:07,018 - INFO - train acc: 1.0
2023-06-22 15:15:07,084 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.75      0.76        63
           4       0.65      0.68      0.66        77
           7       0.59      0.58      0.59        60

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:15:07,085 - INFO - test loss 0.023981095167178553
2023-06-22 15:15:07,085 - INFO - test acc 0.6699999570846558
2023-06-22 15:15:09,975 - INFO - Distilling data from client: Client13
2023-06-22 15:15:09,975 - INFO - train loss: 0.0003510612854177767
2023-06-22 15:15:09,975 - INFO - train acc: 1.0
2023-06-22 15:15:10,040 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.76      0.77        63
           4       0.66      0.68      0.67        77
           7       0.63      0.62      0.62        60

    accuracy                           0.69       200
   macro avg       0.69      0.68      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:15:10,042 - INFO - test loss 0.024137711603465357
2023-06-22 15:15:10,042 - INFO - test acc 0.6850000023841858
2023-06-22 15:15:12,916 - INFO - Distilling data from client: Client13
2023-06-22 15:15:12,916 - INFO - train loss: 0.00044018449980488496
2023-06-22 15:15:12,916 - INFO - train acc: 1.0
2023-06-22 15:15:12,966 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.77        63
           4       0.68      0.70      0.69        77
           7       0.63      0.65      0.64        60

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:15:12,967 - INFO - test loss 0.02406503893536537
2023-06-22 15:15:12,967 - INFO - test acc 0.699999988079071
2023-06-22 15:15:15,790 - INFO - Distilling data from client: Client13
2023-06-22 15:15:15,790 - INFO - train loss: 0.0004119023976198588
2023-06-22 15:15:15,791 - INFO - train acc: 1.0
2023-06-22 15:15:15,840 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.76      0.76        63
           4       0.67      0.68      0.67        77
           7       0.61      0.60      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:15:15,841 - INFO - test loss 0.02433702343815066
2023-06-22 15:15:15,841 - INFO - test acc 0.6800000071525574
2023-06-22 15:15:18,684 - INFO - Distilling data from client: Client13
2023-06-22 15:15:18,684 - INFO - train loss: 0.00039571151289335407
2023-06-22 15:15:18,684 - INFO - train acc: 1.0
2023-06-22 15:15:18,737 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.78      0.78        63
           4       0.67      0.64      0.65        77
           7       0.59      0.63      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:15:18,738 - INFO - test loss 0.024786981242827386
2023-06-22 15:15:18,738 - INFO - test acc 0.6800000071525574
2023-06-22 15:15:21,580 - INFO - Distilling data from client: Client13
2023-06-22 15:15:21,581 - INFO - train loss: 0.0003000253309569305
2023-06-22 15:15:21,581 - INFO - train acc: 1.0
2023-06-22 15:15:21,640 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.75      0.76        63
           4       0.65      0.69      0.67        77
           7       0.61      0.58      0.60        60

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.68      0.68      0.67       200

2023-06-22 15:15:21,642 - INFO - test loss 0.02367281934249615
2023-06-22 15:15:21,643 - INFO - test acc 0.675000011920929
2023-06-22 15:15:24,443 - INFO - Distilling data from client: Client13
2023-06-22 15:15:24,443 - INFO - train loss: 0.00031530689451570766
2023-06-22 15:15:24,449 - INFO - train acc: 1.0
2023-06-22 15:15:24,501 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.75      0.76        63
           4       0.65      0.66      0.66        77
           7       0.60      0.62      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:15:24,504 - INFO - test loss 0.024295749092651513
2023-06-22 15:15:24,504 - INFO - test acc 0.675000011920929
2023-06-22 15:15:27,321 - INFO - Distilling data from client: Client13
2023-06-22 15:15:27,322 - INFO - train loss: 0.0002673065688382774
2023-06-22 15:15:27,322 - INFO - train acc: 1.0
2023-06-22 15:15:27,393 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.76      0.77        63
           4       0.65      0.66      0.66        77
           7       0.61      0.62      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:15:27,393 - INFO - test loss 0.02400310971684622
2023-06-22 15:15:27,394 - INFO - test acc 0.6800000071525574
2023-06-22 15:15:30,509 - INFO - Distilling data from client: Client13
2023-06-22 15:15:30,509 - INFO - train loss: 0.000294484362222649
2023-06-22 15:15:30,510 - INFO - train acc: 1.0
2023-06-22 15:15:30,578 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.76      0.78        63
           4       0.65      0.68      0.66        77
           7       0.58      0.58      0.58        60

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:15:30,580 - INFO - test loss 0.02433361708071019
2023-06-22 15:15:30,580 - INFO - test acc 0.675000011920929
2023-06-22 15:15:33,557 - INFO - Distilling data from client: Client13
2023-06-22 15:15:33,557 - INFO - train loss: 0.0002509190132342515
2023-06-22 15:15:33,558 - INFO - train acc: 1.0
2023-06-22 15:15:33,617 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.78      0.80        63
           4       0.66      0.69      0.68        77
           7       0.62      0.63      0.63        60

    accuracy                           0.70       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:15:33,617 - INFO - test loss 0.023998935255882158
2023-06-22 15:15:33,618 - INFO - test acc 0.699999988079071
2023-06-22 15:15:36,511 - INFO - Distilling data from client: Client13
2023-06-22 15:15:36,511 - INFO - train loss: 0.0002918621685391982
2023-06-22 15:15:36,511 - INFO - train acc: 1.0
2023-06-22 15:15:36,557 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.76      0.77        63
           4       0.65      0.66      0.65        77
           7       0.61      0.60      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:15:36,558 - INFO - test loss 0.024158465836236938
2023-06-22 15:15:36,558 - INFO - test acc 0.675000011920929
2023-06-22 15:15:39,420 - INFO - Distilling data from client: Client13
2023-06-22 15:15:39,420 - INFO - train loss: 0.0002790219401769737
2023-06-22 15:15:39,420 - INFO - train acc: 1.0
2023-06-22 15:15:39,474 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.75      0.76        63
           4       0.66      0.69      0.68        77
           7       0.62      0.62      0.62        60

    accuracy                           0.69       200
   macro avg       0.69      0.68      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:15:39,474 - INFO - test loss 0.023910046580980605
2023-06-22 15:15:39,474 - INFO - test acc 0.6850000023841858
2023-06-22 15:15:39,505 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:15:39,524 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:15:39,541 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:15:39,559 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:15:39,576 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:15:39,587 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:15:39,599 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:15:39,610 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:15:39,622 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:15:40,162 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client13//synthetic.png
2023-06-22 15:15:40,182 - INFO - c: 3.0 and total_data_in_this_class: 271
2023-06-22 15:15:40,183 - INFO - c: 6.0 and total_data_in_this_class: 266
2023-06-22 15:15:40,183 - INFO - c: 9.0 and total_data_in_this_class: 262
2023-06-22 15:15:40,183 - INFO - c: 3.0 and total_data_in_this_class: 62
2023-06-22 15:15:40,183 - INFO - c: 6.0 and total_data_in_this_class: 67
2023-06-22 15:15:40,183 - INFO - c: 9.0 and total_data_in_this_class: 71
2023-06-22 15:15:40,302 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07276153564453125 sec
2023-06-22 15:15:40,374 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0713205337524414 sec
2023-06-22 15:15:40,383 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.15623068809509277 sec
2023-06-22 15:15:40,386 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:15:40,440 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.053965091705322266 sec
2023-06-22 15:15:40,440 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:15:40,609 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.16880130767822266 sec
2023-06-22 15:15:40,650 - INFO - initial test loss: 0.0243983823698267
2023-06-22 15:15:40,650 - INFO - initial test acc: 0.6549999713897705
2023-06-22 15:15:40,668 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012700319290161133 sec
2023-06-22 15:15:40,861 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2069087028503418 sec
2023-06-22 15:15:40,866 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:15:40,966 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.09925031661987305 sec
2023-06-22 15:15:40,966 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:15:41,393 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4264986515045166 sec
2023-06-22 15:15:44,397 - INFO - Distilling data from client: Client14
2023-06-22 15:15:44,397 - INFO - train loss: 0.0022603139559721043
2023-06-22 15:15:44,398 - INFO - train acc: 0.9961904883384705
2023-06-22 15:15:44,599 - INFO - report:               precision    recall  f1-score   support

           3       0.56      0.63      0.59        62
           6       0.64      0.73      0.68        67
           9       0.85      0.63      0.73        71

    accuracy                           0.67       200
   macro avg       0.68      0.66      0.67       200
weighted avg       0.69      0.67      0.67       200

2023-06-22 15:15:44,599 - INFO - test loss 0.022061632871994554
2023-06-22 15:15:44,599 - INFO - test acc 0.6649999618530273
2023-06-22 15:15:47,455 - INFO - Distilling data from client: Client14
2023-06-22 15:15:47,455 - INFO - train loss: 0.0011213997602756974
2023-06-22 15:15:47,455 - INFO - train acc: 1.0
2023-06-22 15:15:47,513 - INFO - report:               precision    recall  f1-score   support

           3       0.57      0.61      0.59        62
           6       0.64      0.73      0.68        67
           9       0.82      0.65      0.72        71

    accuracy                           0.67       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:15:47,515 - INFO - test loss 0.02200075789788439
2023-06-22 15:15:47,515 - INFO - test acc 0.6649999618530273
2023-06-22 15:15:50,367 - INFO - Distilling data from client: Client14
2023-06-22 15:15:50,368 - INFO - train loss: 0.0008301813498189483
2023-06-22 15:15:50,368 - INFO - train acc: 1.0
2023-06-22 15:15:50,557 - INFO - report:               precision    recall  f1-score   support

           3       0.59      0.61      0.60        62
           6       0.64      0.75      0.69        67
           9       0.83      0.68      0.74        71

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:15:50,557 - INFO - test loss 0.022273703194163816
2023-06-22 15:15:50,557 - INFO - test acc 0.6800000071525574
2023-06-22 15:15:53,356 - INFO - Distilling data from client: Client14
2023-06-22 15:15:53,356 - INFO - train loss: 0.0006282958032251731
2023-06-22 15:15:53,357 - INFO - train acc: 1.0
2023-06-22 15:15:53,414 - INFO - report:               precision    recall  f1-score   support

           3       0.55      0.60      0.57        62
           6       0.61      0.70      0.65        67
           9       0.86      0.68      0.76        71

    accuracy                           0.66       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-06-22 15:15:53,415 - INFO - test loss 0.022957467830983378
2023-06-22 15:15:53,415 - INFO - test acc 0.6599999666213989
2023-06-22 15:15:56,333 - INFO - Distilling data from client: Client14
2023-06-22 15:15:56,334 - INFO - train loss: 0.0005328468206132111
2023-06-22 15:15:56,334 - INFO - train acc: 1.0
2023-06-22 15:15:56,391 - INFO - report:               precision    recall  f1-score   support

           3       0.58      0.61      0.59        62
           6       0.63      0.73      0.68        67
           9       0.88      0.69      0.77        71

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.70      0.68      0.68       200

2023-06-22 15:15:56,391 - INFO - test loss 0.02272898260145718
2023-06-22 15:15:56,392 - INFO - test acc 0.6800000071525574
2023-06-22 15:15:59,293 - INFO - Distilling data from client: Client14
2023-06-22 15:15:59,293 - INFO - train loss: 0.00046892014865971844
2023-06-22 15:15:59,293 - INFO - train acc: 1.0
2023-06-22 15:15:59,346 - INFO - report:               precision    recall  f1-score   support

           3       0.58      0.61      0.59        62
           6       0.64      0.75      0.69        67
           9       0.82      0.65      0.72        71

    accuracy                           0.67       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:15:59,347 - INFO - test loss 0.02293849577378928
2023-06-22 15:15:59,348 - INFO - test acc 0.6699999570846558
2023-06-22 15:16:02,321 - INFO - Distilling data from client: Client14
2023-06-22 15:16:02,321 - INFO - train loss: 0.0004264617408826358
2023-06-22 15:16:02,321 - INFO - train acc: 1.0
2023-06-22 15:16:02,374 - INFO - report:               precision    recall  f1-score   support

           3       0.56      0.61      0.58        62
           6       0.64      0.72      0.68        67
           9       0.82      0.66      0.73        71

    accuracy                           0.67       200
   macro avg       0.67      0.66      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:16:02,375 - INFO - test loss 0.022738646161902104
2023-06-22 15:16:02,375 - INFO - test acc 0.6649999618530273
2023-06-22 15:16:05,332 - INFO - Distilling data from client: Client14
2023-06-22 15:16:05,332 - INFO - train loss: 0.0004009749772176683
2023-06-22 15:16:05,332 - INFO - train acc: 1.0
2023-06-22 15:16:05,384 - INFO - report:               precision    recall  f1-score   support

           3       0.56      0.61      0.58        62
           6       0.61      0.70      0.65        67
           9       0.85      0.66      0.75        71

    accuracy                           0.66       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-06-22 15:16:05,385 - INFO - test loss 0.02321591374658125
2023-06-22 15:16:05,385 - INFO - test acc 0.6599999666213989
2023-06-22 15:16:08,408 - INFO - Distilling data from client: Client14
2023-06-22 15:16:08,409 - INFO - train loss: 0.0005305061373147891
2023-06-22 15:16:08,409 - INFO - train acc: 1.0
2023-06-22 15:16:08,456 - INFO - report:               precision    recall  f1-score   support

           3       0.51      0.60      0.55        62
           6       0.59      0.67      0.63        67
           9       0.85      0.62      0.72        71

    accuracy                           0.63       200
   macro avg       0.65      0.63      0.63       200
weighted avg       0.66      0.63      0.64       200

2023-06-22 15:16:08,458 - INFO - test loss 0.023783070348075485
2023-06-22 15:16:08,458 - INFO - test acc 0.6299999952316284
2023-06-22 15:16:11,378 - INFO - Distilling data from client: Client14
2023-06-22 15:16:11,378 - INFO - train loss: 0.00037243317844879716
2023-06-22 15:16:11,378 - INFO - train acc: 1.0
2023-06-22 15:16:11,438 - INFO - report:               precision    recall  f1-score   support

           3       0.53      0.58      0.55        62
           6       0.59      0.70      0.64        67
           9       0.87      0.63      0.73        71

    accuracy                           0.64       200
   macro avg       0.66      0.64      0.64       200
weighted avg       0.67      0.64      0.65       200

2023-06-22 15:16:11,439 - INFO - test loss 0.023515252378863746
2023-06-22 15:16:11,439 - INFO - test acc 0.6399999856948853
2023-06-22 15:16:14,311 - INFO - Distilling data from client: Client14
2023-06-22 15:16:14,311 - INFO - train loss: 0.000342779139163402
2023-06-22 15:16:14,311 - INFO - train acc: 1.0
2023-06-22 15:16:14,369 - INFO - report:               precision    recall  f1-score   support

           3       0.58      0.61      0.60        62
           6       0.62      0.75      0.68        67
           9       0.85      0.65      0.74        71

    accuracy                           0.67       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.69      0.67      0.67       200

2023-06-22 15:16:14,370 - INFO - test loss 0.02251196885120877
2023-06-22 15:16:14,371 - INFO - test acc 0.6699999570846558
2023-06-22 15:16:17,413 - INFO - Distilling data from client: Client14
2023-06-22 15:16:17,414 - INFO - train loss: 0.00027227558061223477
2023-06-22 15:16:17,414 - INFO - train acc: 1.0
2023-06-22 15:16:17,471 - INFO - report:               precision    recall  f1-score   support

           3       0.57      0.61      0.59        62
           6       0.61      0.73      0.67        67
           9       0.83      0.62      0.71        71

    accuracy                           0.66       200
   macro avg       0.67      0.65      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-06-22 15:16:17,474 - INFO - test loss 0.02332317119254891
2023-06-22 15:16:17,474 - INFO - test acc 0.6549999713897705
2023-06-22 15:16:20,538 - INFO - Distilling data from client: Client14
2023-06-22 15:16:20,538 - INFO - train loss: 0.0003083313200016914
2023-06-22 15:16:20,538 - INFO - train acc: 1.0
2023-06-22 15:16:20,600 - INFO - report:               precision    recall  f1-score   support

           3       0.59      0.63      0.61        62
           6       0.63      0.76      0.69        67
           9       0.83      0.62      0.71        71

    accuracy                           0.67       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.69      0.67      0.67       200

2023-06-22 15:16:20,600 - INFO - test loss 0.022528209500748398
2023-06-22 15:16:20,600 - INFO - test acc 0.6699999570846558
2023-06-22 15:16:23,590 - INFO - Distilling data from client: Client14
2023-06-22 15:16:23,591 - INFO - train loss: 0.0004975455054878009
2023-06-22 15:16:23,591 - INFO - train acc: 1.0
2023-06-22 15:16:23,654 - INFO - report:               precision    recall  f1-score   support

           3       0.56      0.58      0.57        62
           6       0.57      0.73      0.64        67
           9       0.84      0.59      0.69        71

    accuracy                           0.64       200
   macro avg       0.66      0.63      0.64       200
weighted avg       0.66      0.64      0.64       200

2023-06-22 15:16:23,660 - INFO - test loss 0.023654724604507616
2023-06-22 15:16:23,660 - INFO - test acc 0.6349999904632568
2023-06-22 15:16:26,578 - INFO - Distilling data from client: Client14
2023-06-22 15:16:26,578 - INFO - train loss: 0.00030518022118851163
2023-06-22 15:16:26,578 - INFO - train acc: 1.0
2023-06-22 15:16:26,632 - INFO - report:               precision    recall  f1-score   support

           3       0.53      0.58      0.55        62
           6       0.61      0.73      0.67        67
           9       0.85      0.62      0.72        71

    accuracy                           0.65       200
   macro avg       0.66      0.64      0.65       200
weighted avg       0.67      0.65      0.65       200

2023-06-22 15:16:26,634 - INFO - test loss 0.023728252539958535
2023-06-22 15:16:26,634 - INFO - test acc 0.6449999809265137
2023-06-22 15:16:29,589 - INFO - Distilling data from client: Client14
2023-06-22 15:16:29,589 - INFO - train loss: 0.00025962493100692874
2023-06-22 15:16:29,590 - INFO - train acc: 1.0
2023-06-22 15:16:29,636 - INFO - report:               precision    recall  f1-score   support

           3       0.51      0.60      0.55        62
           6       0.60      0.67      0.63        67
           9       0.85      0.63      0.73        71

    accuracy                           0.64       200
   macro avg       0.65      0.63      0.64       200
weighted avg       0.66      0.64      0.64       200

2023-06-22 15:16:29,636 - INFO - test loss 0.023623738079742752
2023-06-22 15:16:29,637 - INFO - test acc 0.6349999904632568
2023-06-22 15:16:32,704 - INFO - Distilling data from client: Client14
2023-06-22 15:16:32,704 - INFO - train loss: 0.00021602820624753516
2023-06-22 15:16:32,704 - INFO - train acc: 1.0
2023-06-22 15:16:32,763 - INFO - report:               precision    recall  f1-score   support

           3       0.57      0.60      0.58        62
           6       0.62      0.73      0.67        67
           9       0.84      0.66      0.74        71

    accuracy                           0.67       200
   macro avg       0.68      0.66      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:16:32,764 - INFO - test loss 0.023037825530795333
2023-06-22 15:16:32,764 - INFO - test acc 0.6649999618530273
2023-06-22 15:16:35,689 - INFO - Distilling data from client: Client14
2023-06-22 15:16:35,690 - INFO - train loss: 0.0002572632910865765
2023-06-22 15:16:35,690 - INFO - train acc: 1.0
2023-06-22 15:16:35,754 - INFO - report:               precision    recall  f1-score   support

           3       0.55      0.60      0.57        62
           6       0.62      0.73      0.67        67
           9       0.85      0.65      0.74        71

    accuracy                           0.66       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-06-22 15:16:35,756 - INFO - test loss 0.02318690564066512
2023-06-22 15:16:35,756 - INFO - test acc 0.6599999666213989
2023-06-22 15:16:38,723 - INFO - Distilling data from client: Client14
2023-06-22 15:16:38,724 - INFO - train loss: 0.00021576586562933482
2023-06-22 15:16:38,724 - INFO - train acc: 1.0
2023-06-22 15:16:38,782 - INFO - report:               precision    recall  f1-score   support

           3       0.54      0.58      0.56        62
           6       0.59      0.70      0.64        67
           9       0.81      0.62      0.70        71

    accuracy                           0.64       200
   macro avg       0.65      0.63      0.64       200
weighted avg       0.66      0.64      0.64       200

2023-06-22 15:16:38,782 - INFO - test loss 0.023106555847504134
2023-06-22 15:16:38,783 - INFO - test acc 0.6349999904632568
2023-06-22 15:16:41,801 - INFO - Distilling data from client: Client14
2023-06-22 15:16:41,801 - INFO - train loss: 0.00021392534719851151
2023-06-22 15:16:41,802 - INFO - train acc: 1.0
2023-06-22 15:16:41,858 - INFO - report:               precision    recall  f1-score   support

           3       0.55      0.58      0.57        62
           6       0.59      0.72      0.65        67
           9       0.83      0.63      0.72        71

    accuracy                           0.65       200
   macro avg       0.66      0.64      0.65       200
weighted avg       0.67      0.65      0.65       200

2023-06-22 15:16:41,860 - INFO - test loss 0.023377915945745415
2023-06-22 15:16:41,860 - INFO - test acc 0.6449999809265137
2023-06-22 15:16:44,700 - INFO - Distilling data from client: Client14
2023-06-22 15:16:44,701 - INFO - train loss: 0.00021498181238844844
2023-06-22 15:16:44,702 - INFO - train acc: 1.0
2023-06-22 15:16:44,761 - INFO - report:               precision    recall  f1-score   support

           3       0.54      0.60      0.57        62
           6       0.62      0.72      0.66        67
           9       0.85      0.65      0.74        71

    accuracy                           0.66       200
   macro avg       0.67      0.65      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-06-22 15:16:44,763 - INFO - test loss 0.02295696765834604
2023-06-22 15:16:44,763 - INFO - test acc 0.6549999713897705
2023-06-22 15:16:47,787 - INFO - Distilling data from client: Client14
2023-06-22 15:16:47,788 - INFO - train loss: 0.00023233637638687542
2023-06-22 15:16:47,788 - INFO - train acc: 1.0
2023-06-22 15:16:47,845 - INFO - report:               precision    recall  f1-score   support

           3       0.52      0.61      0.56        62
           6       0.61      0.67      0.64        67
           9       0.85      0.63      0.73        71

    accuracy                           0.64       200
   macro avg       0.66      0.64      0.64       200
weighted avg       0.67      0.64      0.65       200

2023-06-22 15:16:47,847 - INFO - test loss 0.02304462919367776
2023-06-22 15:16:47,848 - INFO - test acc 0.6399999856948853
2023-06-22 15:16:51,152 - INFO - Distilling data from client: Client14
2023-06-22 15:16:51,153 - INFO - train loss: 0.00021545742574271957
2023-06-22 15:16:51,153 - INFO - train acc: 1.0
2023-06-22 15:16:51,224 - INFO - report:               precision    recall  f1-score   support

           3       0.51      0.58      0.55        62
           6       0.59      0.70      0.64        67
           9       0.86      0.61      0.71        71

    accuracy                           0.63       200
   macro avg       0.65      0.63      0.63       200
weighted avg       0.66      0.63      0.64       200

2023-06-22 15:16:51,225 - INFO - test loss 0.023829276850148715
2023-06-22 15:16:51,225 - INFO - test acc 0.6299999952316284
2023-06-22 15:16:54,293 - INFO - Distilling data from client: Client14
2023-06-22 15:16:54,294 - INFO - train loss: 0.0001895792411112055
2023-06-22 15:16:54,294 - INFO - train acc: 1.0
2023-06-22 15:16:54,362 - INFO - report:               precision    recall  f1-score   support

           3       0.57      0.63      0.60        62
           6       0.62      0.73      0.67        67
           9       0.87      0.63      0.73        71

    accuracy                           0.67       200
   macro avg       0.68      0.66      0.67       200
weighted avg       0.69      0.67      0.67       200

2023-06-22 15:16:54,363 - INFO - test loss 0.02326693403011723
2023-06-22 15:16:54,363 - INFO - test acc 0.6649999618530273
2023-06-22 15:16:57,299 - INFO - Distilling data from client: Client14
2023-06-22 15:16:57,299 - INFO - train loss: 0.0002261074667352931
2023-06-22 15:16:57,300 - INFO - train acc: 1.0
2023-06-22 15:16:57,374 - INFO - report:               precision    recall  f1-score   support

           3       0.54      0.56      0.55        62
           6       0.59      0.72      0.64        67
           9       0.87      0.65      0.74        71

    accuracy                           0.65       200
   macro avg       0.66      0.64      0.65       200
weighted avg       0.67      0.65      0.65       200

2023-06-22 15:16:57,375 - INFO - test loss 0.023332056523344562
2023-06-22 15:16:57,375 - INFO - test acc 0.6449999809265137
2023-06-22 15:16:57,404 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:16:57,422 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:16:57,441 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:16:57,459 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:16:57,471 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:16:57,482 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:16:57,494 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:16:57,506 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:16:57,517 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:16:58,085 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client14//synthetic.png
2023-06-22 15:16:58,107 - INFO - c: 2.0 and total_data_in_this_class: 271
2023-06-22 15:16:58,107 - INFO - c: 3.0 and total_data_in_this_class: 268
2023-06-22 15:16:58,107 - INFO - c: 8.0 and total_data_in_this_class: 260
2023-06-22 15:16:58,107 - INFO - c: 2.0 and total_data_in_this_class: 62
2023-06-22 15:16:58,107 - INFO - c: 3.0 and total_data_in_this_class: 65
2023-06-22 15:16:58,107 - INFO - c: 8.0 and total_data_in_this_class: 73
2023-06-22 15:16:58,233 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07693886756896973 sec
2023-06-22 15:16:58,313 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07811522483825684 sec
2023-06-22 15:16:58,321 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16738629341125488 sec
2023-06-22 15:16:58,325 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:16:58,383 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05770754814147949 sec
2023-06-22 15:16:58,383 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:16:58,567 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.18382501602172852 sec
2023-06-22 15:16:58,626 - INFO - initial test loss: 0.020395313983975566
2023-06-22 15:16:58,626 - INFO - initial test acc: 0.7450000047683716
2023-06-22 15:16:58,643 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012531518936157227 sec
2023-06-22 15:16:58,836 - WARNING - Finished tracing + transforming update_fn for pjit in 0.20649409294128418 sec
2023-06-22 15:16:58,842 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:16:58,946 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10350871086120605 sec
2023-06-22 15:16:58,946 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:16:59,389 - WARNING - Finished XLA compilation of jit(update_fn) in 0.44263482093811035 sec
2023-06-22 15:17:02,601 - INFO - Distilling data from client: Client15
2023-06-22 15:17:02,602 - INFO - train loss: 0.002777003864354234
2023-06-22 15:17:02,602 - INFO - train acc: 0.9922928810119629
2023-06-22 15:17:02,884 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.58      0.61        62
           3       0.66      0.65      0.65        65
           8       0.76      0.82      0.79        73

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:17:02,884 - INFO - test loss 0.020639451050895416
2023-06-22 15:17:02,884 - INFO - test acc 0.6899999976158142
2023-06-22 15:17:05,979 - INFO - Distilling data from client: Client15
2023-06-22 15:17:05,979 - INFO - train loss: 0.0014171337892590016
2023-06-22 15:17:05,979 - INFO - train acc: 0.9980732202529907
2023-06-22 15:17:06,210 - INFO - report:               precision    recall  f1-score   support

           2       0.65      0.66      0.66        62
           3       0.72      0.68      0.70        65
           8       0.79      0.82      0.81        73

    accuracy                           0.73       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:17:06,210 - INFO - test loss 0.020267110320048538
2023-06-22 15:17:06,210 - INFO - test acc 0.7249999642372131
2023-06-22 15:17:09,314 - INFO - Distilling data from client: Client15
2023-06-22 15:17:09,315 - INFO - train loss: 0.001058088128372557
2023-06-22 15:17:09,315 - INFO - train acc: 1.0
2023-06-22 15:17:09,476 - INFO - report:               precision    recall  f1-score   support

           2       0.60      0.61      0.61        62
           3       0.70      0.65      0.67        65
           8       0.75      0.79      0.77        73

    accuracy                           0.69       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:17:09,481 - INFO - test loss 0.02023062086854651
2023-06-22 15:17:09,483 - INFO - test acc 0.6899999976158142
2023-06-22 15:17:12,414 - INFO - Distilling data from client: Client15
2023-06-22 15:17:12,415 - INFO - train loss: 0.0010256299751134684
2023-06-22 15:17:12,415 - INFO - train acc: 0.9980732202529907
2023-06-22 15:17:12,472 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.61      0.62        62
           3       0.68      0.68      0.68        65
           8       0.77      0.79      0.78        73

    accuracy                           0.70       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:17:12,473 - INFO - test loss 0.020405490623735267
2023-06-22 15:17:12,473 - INFO - test acc 0.699999988079071
2023-06-22 15:17:15,288 - INFO - Distilling data from client: Client15
2023-06-22 15:17:15,288 - INFO - train loss: 0.0008403236550911947
2023-06-22 15:17:15,288 - INFO - train acc: 1.0
2023-06-22 15:17:15,340 - INFO - report:               precision    recall  f1-score   support

           2       0.67      0.66      0.67        62
           3       0.69      0.69      0.69        65
           8       0.80      0.81      0.80        73

    accuracy                           0.73       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:17:15,341 - INFO - test loss 0.020283906100180268
2023-06-22 15:17:15,342 - INFO - test acc 0.7249999642372131
2023-06-22 15:17:18,164 - INFO - Distilling data from client: Client15
2023-06-22 15:17:18,164 - INFO - train loss: 0.0007081562363567383
2023-06-22 15:17:18,164 - INFO - train acc: 1.0
2023-06-22 15:17:18,222 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.68      0.66        62
           3       0.72      0.68      0.70        65
           8       0.81      0.81      0.81        73

    accuracy                           0.73       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.73      0.72      0.73       200

2023-06-22 15:17:18,223 - INFO - test loss 0.020946465106910176
2023-06-22 15:17:18,223 - INFO - test acc 0.7249999642372131
2023-06-22 15:17:21,127 - INFO - Distilling data from client: Client15
2023-06-22 15:17:21,128 - INFO - train loss: 0.0007311589001708183
2023-06-22 15:17:21,128 - INFO - train acc: 1.0
2023-06-22 15:17:21,178 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.60      0.60        62
           3       0.64      0.69      0.67        65
           8       0.77      0.73      0.75        73

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:17:21,178 - INFO - test loss 0.021280762782012182
2023-06-22 15:17:21,179 - INFO - test acc 0.675000011920929
2023-06-22 15:17:24,048 - INFO - Distilling data from client: Client15
2023-06-22 15:17:24,048 - INFO - train loss: 0.0007303340802218236
2023-06-22 15:17:24,048 - INFO - train acc: 1.0
2023-06-22 15:17:24,110 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.68      0.66        62
           3       0.70      0.66      0.68        65
           8       0.79      0.79      0.79        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:17:24,114 - INFO - test loss 0.02076097082240924
2023-06-22 15:17:24,114 - INFO - test acc 0.7149999737739563
2023-06-22 15:17:26,976 - INFO - Distilling data from client: Client15
2023-06-22 15:17:26,977 - INFO - train loss: 0.000534479365221635
2023-06-22 15:17:26,977 - INFO - train acc: 1.0
2023-06-22 15:17:27,033 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.66      0.65        62
           3       0.66      0.66      0.66        65
           8       0.79      0.77      0.78        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:17:27,033 - INFO - test loss 0.021407608134216608
2023-06-22 15:17:27,033 - INFO - test acc 0.699999988079071
2023-06-22 15:17:29,858 - INFO - Distilling data from client: Client15
2023-06-22 15:17:29,859 - INFO - train loss: 0.0004734686604381729
2023-06-22 15:17:29,860 - INFO - train acc: 1.0
2023-06-22 15:17:29,909 - INFO - report:               precision    recall  f1-score   support

           2       0.65      0.68      0.66        62
           3       0.70      0.65      0.67        65
           8       0.76      0.78      0.77        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:17:29,910 - INFO - test loss 0.021322260942626698
2023-06-22 15:17:29,910 - INFO - test acc 0.7049999833106995
2023-06-22 15:17:32,794 - INFO - Distilling data from client: Client15
2023-06-22 15:17:32,795 - INFO - train loss: 0.0005343373342218777
2023-06-22 15:17:32,795 - INFO - train acc: 1.0
2023-06-22 15:17:32,857 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.63      0.62        62
           3       0.66      0.68      0.67        65
           8       0.79      0.75      0.77        73

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:17:32,859 - INFO - test loss 0.021441037362651964
2023-06-22 15:17:32,859 - INFO - test acc 0.6899999976158142
2023-06-22 15:17:35,718 - INFO - Distilling data from client: Client15
2023-06-22 15:17:35,719 - INFO - train loss: 0.00044637291383444193
2023-06-22 15:17:35,719 - INFO - train acc: 1.0
2023-06-22 15:17:35,779 - INFO - report:               precision    recall  f1-score   support

           2       0.65      0.73      0.69        62
           3       0.71      0.63      0.67        65
           8       0.79      0.79      0.79        73

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:17:35,780 - INFO - test loss 0.02132448711494528
2023-06-22 15:17:35,780 - INFO - test acc 0.7199999690055847
2023-06-22 15:17:38,708 - INFO - Distilling data from client: Client15
2023-06-22 15:17:38,708 - INFO - train loss: 0.00043265835441325156
2023-06-22 15:17:38,708 - INFO - train acc: 1.0
2023-06-22 15:17:38,757 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.63      0.63        62
           3       0.66      0.69      0.68        65
           8       0.81      0.78      0.80        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-06-22 15:17:38,758 - INFO - test loss 0.02136718545617898
2023-06-22 15:17:38,758 - INFO - test acc 0.7049999833106995
2023-06-22 15:17:41,611 - INFO - Distilling data from client: Client15
2023-06-22 15:17:41,611 - INFO - train loss: 0.00041368762981602126
2023-06-22 15:17:41,612 - INFO - train acc: 1.0
2023-06-22 15:17:41,671 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.63      0.63        62
           3       0.64      0.65      0.64        65
           8       0.79      0.78      0.79        73

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:17:41,672 - INFO - test loss 0.021703313659379605
2023-06-22 15:17:41,672 - INFO - test acc 0.6899999976158142
2023-06-22 15:17:44,424 - INFO - Distilling data from client: Client15
2023-06-22 15:17:44,425 - INFO - train loss: 0.0003745622472331528
2023-06-22 15:17:44,426 - INFO - train acc: 1.0
2023-06-22 15:17:44,590 - INFO - report:               precision    recall  f1-score   support

           2       0.67      0.69      0.68        62
           3       0.71      0.71      0.71        65
           8       0.83      0.81      0.82        73

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:17:44,590 - INFO - test loss 0.021707798236600403
2023-06-22 15:17:44,590 - INFO - test acc 0.7400000095367432
2023-06-22 15:17:47,515 - INFO - Distilling data from client: Client15
2023-06-22 15:17:47,515 - INFO - train loss: 0.0003422559110794829
2023-06-22 15:17:47,515 - INFO - train acc: 1.0
2023-06-22 15:17:47,568 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.68      0.65        62
           3       0.70      0.66      0.68        65
           8       0.81      0.79      0.80        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:17:47,568 - INFO - test loss 0.020770073811145887
2023-06-22 15:17:47,569 - INFO - test acc 0.7149999737739563
2023-06-22 15:17:50,426 - INFO - Distilling data from client: Client15
2023-06-22 15:17:50,426 - INFO - train loss: 0.0003141847872938852
2023-06-22 15:17:50,426 - INFO - train acc: 1.0
2023-06-22 15:17:50,479 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.65      0.63        62
           3       0.68      0.69      0.69        65
           8       0.79      0.75      0.77        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:17:50,479 - INFO - test loss 0.02101834724806821
2023-06-22 15:17:50,479 - INFO - test acc 0.699999988079071
2023-06-22 15:17:53,423 - INFO - Distilling data from client: Client15
2023-06-22 15:17:53,423 - INFO - train loss: 0.00033760037667646104
2023-06-22 15:17:53,423 - INFO - train acc: 1.0
2023-06-22 15:17:53,479 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.68      0.65        62
           3       0.67      0.66      0.67        65
           8       0.81      0.77      0.79        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-06-22 15:17:53,479 - INFO - test loss 0.021416946830276073
2023-06-22 15:17:53,480 - INFO - test acc 0.7049999833106995
2023-06-22 15:17:56,391 - INFO - Distilling data from client: Client15
2023-06-22 15:17:56,392 - INFO - train loss: 0.0003157327171442991
2023-06-22 15:17:56,392 - INFO - train acc: 1.0
2023-06-22 15:17:56,462 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.71      0.68        62
           3       0.71      0.69      0.70        65
           8       0.81      0.78      0.80        73

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-06-22 15:17:56,464 - INFO - test loss 0.020930666085736464
2023-06-22 15:17:56,464 - INFO - test acc 0.7299999594688416
2023-06-22 15:17:59,354 - INFO - Distilling data from client: Client15
2023-06-22 15:17:59,354 - INFO - train loss: 0.00028348537924898523
2023-06-22 15:17:59,354 - INFO - train acc: 1.0
2023-06-22 15:17:59,407 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.69      0.67        62
           3       0.69      0.66      0.68        65
           8       0.80      0.78      0.79        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:17:59,408 - INFO - test loss 0.021025765940073392
2023-06-22 15:17:59,408 - INFO - test acc 0.7149999737739563
2023-06-22 15:18:02,184 - INFO - Distilling data from client: Client15
2023-06-22 15:18:02,184 - INFO - train loss: 0.00033636157026268335
2023-06-22 15:18:02,185 - INFO - train acc: 1.0
2023-06-22 15:18:02,241 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.65      0.65        62
           3       0.68      0.69      0.69        65
           8       0.79      0.79      0.79        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:18:02,241 - INFO - test loss 0.021309831426975415
2023-06-22 15:18:02,241 - INFO - test acc 0.7149999737739563
2023-06-22 15:18:05,087 - INFO - Distilling data from client: Client15
2023-06-22 15:18:05,088 - INFO - train loss: 0.0003637788459175956
2023-06-22 15:18:05,088 - INFO - train acc: 0.9980732202529907
2023-06-22 15:18:05,154 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.69      0.67        62
           3       0.70      0.66      0.68        65
           8       0.78      0.77      0.77        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:18:05,155 - INFO - test loss 0.021594897603487012
2023-06-22 15:18:05,156 - INFO - test acc 0.7099999785423279
2023-06-22 15:18:08,033 - INFO - Distilling data from client: Client15
2023-06-22 15:18:08,033 - INFO - train loss: 0.0003256898058336041
2023-06-22 15:18:08,034 - INFO - train acc: 1.0
2023-06-22 15:18:08,090 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.66      0.66        62
           3       0.68      0.69      0.69        65
           8       0.78      0.77      0.77        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:18:08,091 - INFO - test loss 0.021160905089249206
2023-06-22 15:18:08,092 - INFO - test acc 0.7099999785423279
2023-06-22 15:18:10,981 - INFO - Distilling data from client: Client15
2023-06-22 15:18:10,981 - INFO - train loss: 0.00036023400740972666
2023-06-22 15:18:10,982 - INFO - train acc: 1.0
2023-06-22 15:18:11,042 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.66      0.64        62
           3       0.70      0.65      0.67        65
           8       0.78      0.79      0.79        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-06-22 15:18:11,044 - INFO - test loss 0.02109234608876205
2023-06-22 15:18:11,044 - INFO - test acc 0.7049999833106995
2023-06-22 15:18:13,987 - INFO - Distilling data from client: Client15
2023-06-22 15:18:13,987 - INFO - train loss: 0.000265600443991167
2023-06-22 15:18:13,988 - INFO - train acc: 1.0
2023-06-22 15:18:14,055 - INFO - report:               precision    recall  f1-score   support

           2       0.65      0.63      0.64        62
           3       0.68      0.69      0.69        65
           8       0.77      0.78      0.78        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:18:14,055 - INFO - test loss 0.021125106457137235
2023-06-22 15:18:14,055 - INFO - test acc 0.7049999833106995
2023-06-22 15:18:14,084 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:18:14,101 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:18:14,120 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:18:14,137 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:18:14,152 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:18:14,164 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:18:14,177 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:18:14,189 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:18:14,201 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:18:14,752 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client15//synthetic.png
2023-06-22 15:18:14,773 - INFO - c: 2.0 and total_data_in_this_class: 548
2023-06-22 15:18:14,773 - INFO - c: 5.0 and total_data_in_this_class: 251
2023-06-22 15:18:14,773 - INFO - c: 2.0 and total_data_in_this_class: 118
2023-06-22 15:18:14,774 - INFO - c: 5.0 and total_data_in_this_class: 82
2023-06-22 15:18:14,813 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005030632019042969 sec
2023-06-22 15:18:14,814 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:18:14,816 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0023221969604492188 sec
2023-06-22 15:18:14,816 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:18:14,832 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.015594244003295898 sec
2023-06-22 15:18:14,835 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003209114074707031 sec
2023-06-22 15:18:14,836 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:18:14,838 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0015878677368164062 sec
2023-06-22 15:18:14,838 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:18:14,850 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.012121915817260742 sec
2023-06-22 15:18:14,856 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022864341735839844 sec
2023-06-22 15:18:14,857 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020384788513183594 sec
2023-06-22 15:18:14,859 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000537872314453125 sec
2023-06-22 15:18:14,861 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00035858154296875 sec
2023-06-22 15:18:14,862 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020194053649902344 sec
2023-06-22 15:18:14,863 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004954338073730469 sec
2023-06-22 15:18:14,864 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004177093505859375 sec
2023-06-22 15:18:14,865 - WARNING - Finished tracing + transforming absolute for pjit in 0.00031065940856933594 sec
2023-06-22 15:18:14,866 - WARNING - Finished tracing + transforming fn for pjit in 0.0004646778106689453 sec
2023-06-22 15:18:14,867 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005917549133300781 sec
2023-06-22 15:18:14,869 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00036215782165527344 sec
2023-06-22 15:18:14,871 - WARNING - Finished tracing + transforming fn for pjit in 0.0003972053527832031 sec
2023-06-22 15:18:14,872 - WARNING - Finished tracing + transforming fn for pjit in 0.00045418739318847656 sec
2023-06-22 15:18:14,873 - WARNING - Finished tracing + transforming fn for pjit in 0.00039839744567871094 sec
2023-06-22 15:18:14,874 - WARNING - Finished tracing + transforming fn for pjit in 0.00044989585876464844 sec
2023-06-22 15:18:14,876 - WARNING - Finished tracing + transforming fn for pjit in 0.0003883838653564453 sec
2023-06-22 15:18:14,879 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00029540061950683594 sec
2023-06-22 15:18:14,880 - WARNING - Finished tracing + transforming fn for pjit in 0.0004000663757324219 sec
2023-06-22 15:18:14,882 - WARNING - Finished tracing + transforming fn for pjit in 0.00040650367736816406 sec
2023-06-22 15:18:14,887 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006005764007568359 sec
2023-06-22 15:18:14,888 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016283988952636719 sec
2023-06-22 15:18:14,890 - WARNING - Finished tracing + transforming fn for pjit in 0.00039196014404296875 sec
2023-06-22 15:18:14,891 - WARNING - Finished tracing + transforming fn for pjit in 0.0003991127014160156 sec
2023-06-22 15:18:14,892 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003864765167236328 sec
2023-06-22 15:18:14,894 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004703998565673828 sec
2023-06-22 15:18:14,895 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003273487091064453 sec
2023-06-22 15:18:14,896 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004706382751464844 sec
2023-06-22 15:18:14,898 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004096031188964844 sec
2023-06-22 15:18:14,899 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004067420959472656 sec
2023-06-22 15:18:14,900 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006101131439208984 sec
2023-06-22 15:18:14,901 - WARNING - Finished tracing + transforming _where for pjit in 0.0016810894012451172 sec
2023-06-22 15:18:14,902 - WARNING - Finished tracing + transforming fn for pjit in 0.0004553794860839844 sec
2023-06-22 15:18:14,904 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004546642303466797 sec
2023-06-22 15:18:14,905 - WARNING - Finished tracing + transforming fn for pjit in 0.0004000663757324219 sec
2023-06-22 15:18:14,906 - WARNING - Finished tracing + transforming fn for pjit in 0.00041413307189941406 sec
2023-06-22 15:18:14,907 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003960132598876953 sec
2023-06-22 15:18:14,909 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045037269592285156 sec
2023-06-22 15:18:14,910 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000457763671875 sec
2023-06-22 15:18:14,911 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004680156707763672 sec
2023-06-22 15:18:14,913 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040411949157714844 sec
2023-06-22 15:18:14,914 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039458274841308594 sec
2023-06-22 15:18:14,915 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00047016143798828125 sec
2023-06-22 15:18:14,916 - WARNING - Finished tracing + transforming _where for pjit in 0.0015194416046142578 sec
2023-06-22 15:18:14,917 - WARNING - Finished tracing + transforming fn for pjit in 0.0004563331604003906 sec
2023-06-22 15:18:14,918 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044798851013183594 sec
2023-06-22 15:18:14,921 - WARNING - Finished tracing + transforming fn for pjit in 0.0003867149353027344 sec
2023-06-22 15:18:14,928 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045990943908691406 sec
2023-06-22 15:18:14,929 - WARNING - Finished tracing + transforming fn for pjit in 0.0006062984466552734 sec
2023-06-22 15:18:14,930 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004730224609375 sec
2023-06-22 15:18:14,932 - WARNING - Finished tracing + transforming fn for pjit in 0.0004017353057861328 sec
2023-06-22 15:18:14,938 - WARNING - Finished tracing + transforming fn for pjit in 0.0003287792205810547 sec
2023-06-22 15:18:14,941 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002799034118652344 sec
2023-06-22 15:18:14,942 - WARNING - Finished tracing + transforming fn for pjit in 0.0005590915679931641 sec
2023-06-22 15:18:14,944 - WARNING - Finished tracing + transforming fn for pjit in 0.0004017353057861328 sec
2023-06-22 15:18:14,971 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.1156919002532959 sec
2023-06-22 15:18:14,976 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020003318786621094 sec
2023-06-22 15:18:14,976 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002009868621826172 sec
2023-06-22 15:18:14,977 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004830360412597656 sec
2023-06-22 15:18:14,981 - WARNING - Finished tracing + transforming fn for pjit in 0.0003304481506347656 sec
2023-06-22 15:18:14,982 - WARNING - Finished tracing + transforming fn for pjit in 0.00047087669372558594 sec
2023-06-22 15:18:14,984 - WARNING - Finished tracing + transforming fn for pjit in 0.0003578662872314453 sec
2023-06-22 15:18:14,994 - WARNING - Finished tracing + transforming fn for pjit in 0.00034356117248535156 sec
2023-06-22 15:18:14,996 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003864765167236328 sec
2023-06-22 15:18:14,997 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004553794860839844 sec
2023-06-22 15:18:14,998 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00030922889709472656 sec
2023-06-22 15:18:14,999 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005795955657958984 sec
2023-06-22 15:18:15,001 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040411949157714844 sec
2023-06-22 15:18:15,002 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039839744567871094 sec
2023-06-22 15:18:15,003 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004718303680419922 sec
2023-06-22 15:18:15,004 - WARNING - Finished tracing + transforming _where for pjit in 0.0015683174133300781 sec
2023-06-22 15:18:15,005 - WARNING - Finished tracing + transforming fn for pjit in 0.0004668235778808594 sec
2023-06-22 15:18:15,007 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004525184631347656 sec
2023-06-22 15:18:15,008 - WARNING - Finished tracing + transforming fn for pjit in 0.00041222572326660156 sec
2023-06-22 15:18:15,009 - WARNING - Finished tracing + transforming fn for pjit in 0.000514984130859375 sec
2023-06-22 15:18:15,028 - WARNING - Finished tracing + transforming fn for pjit in 0.0003466606140136719 sec
2023-06-22 15:18:15,059 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08667421340942383 sec
2023-06-22 15:18:15,061 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020551681518554688 sec
2023-06-22 15:18:15,063 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002276897430419922 sec
2023-06-22 15:18:15,063 - WARNING - Finished tracing + transforming _where for pjit in 0.0011191368103027344 sec
2023-06-22 15:18:15,064 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005159378051757812 sec
2023-06-22 15:18:15,065 - WARNING - Finished tracing + transforming trace for pjit in 0.0044345855712890625 sec
2023-06-22 15:18:15,069 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0001888275146484375 sec
2023-06-22 15:18:15,071 - WARNING - Finished tracing + transforming tril for pjit in 0.001110076904296875 sec
2023-06-22 15:18:15,071 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0032050609588623047 sec
2023-06-22 15:18:15,073 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019621849060058594 sec
2023-06-22 15:18:15,074 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00020241737365722656 sec
2023-06-22 15:18:15,077 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0023844242095947266 sec
2023-06-22 15:18:15,083 - WARNING - Finished tracing + transforming _solve for pjit in 0.015860795974731445 sec
2023-06-22 15:18:15,084 - WARNING - Finished tracing + transforming dot for pjit in 0.0005464553833007812 sec
2023-06-22 15:18:15,089 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.23585748672485352 sec
2023-06-22 15:18:15,092 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:18:15,145 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.052593231201171875 sec
2023-06-22 15:18:15,145 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:18:15,316 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1702110767364502 sec
2023-06-22 15:18:15,349 - INFO - initial test loss: 0.022935500093837102
2023-06-22 15:18:15,350 - INFO - initial test acc: 0.7099999785423279
2023-06-22 15:18:15,362 - WARNING - Finished tracing + transforming dot for pjit in 0.0007967948913574219 sec
2023-06-22 15:18:15,364 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006878376007080078 sec
2023-06-22 15:18:15,366 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008132457733154297 sec
2023-06-22 15:18:15,367 - WARNING - Finished tracing + transforming _mean for pjit in 0.002229452133178711 sec
2023-06-22 15:18:15,369 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004303455352783203 sec
2023-06-22 15:18:15,371 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0006279945373535156 sec
2023-06-22 15:18:15,372 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0007627010345458984 sec
2023-06-22 15:18:15,375 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008809566497802734 sec
2023-06-22 15:18:15,377 - WARNING - Finished tracing + transforming _mean for pjit in 0.002900838851928711 sec
2023-06-22 15:18:15,379 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.025412797927856445 sec
2023-06-22 15:18:15,399 - WARNING - Finished tracing + transforming fn for pjit in 0.0006127357482910156 sec
2023-06-22 15:18:15,401 - WARNING - Finished tracing + transforming fn for pjit in 0.00061798095703125 sec
2023-06-22 15:18:15,403 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005266666412353516 sec
2023-06-22 15:18:15,405 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006687641143798828 sec
2023-06-22 15:18:15,407 - WARNING - Finished tracing + transforming _where for pjit in 0.002448558807373047 sec
2023-06-22 15:18:15,426 - WARNING - Finished tracing + transforming fn for pjit in 0.0005950927734375 sec
2023-06-22 15:18:15,428 - WARNING - Finished tracing + transforming fn for pjit in 0.0008308887481689453 sec
2023-06-22 15:18:15,429 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0007007122039794922 sec
2023-06-22 15:18:15,432 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0007390975952148438 sec
2023-06-22 15:18:15,433 - WARNING - Finished tracing + transforming _where for pjit in 0.002512693405151367 sec
2023-06-22 15:18:15,493 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003590583801269531 sec
2023-06-22 15:18:15,573 - WARNING - Finished tracing + transforming fn for pjit in 0.0004551410675048828 sec
2023-06-22 15:18:15,574 - WARNING - Finished tracing + transforming fn for pjit in 0.0003838539123535156 sec
2023-06-22 15:18:15,575 - WARNING - Finished tracing + transforming square for pjit in 0.0003070831298828125 sec
2023-06-22 15:18:15,579 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003802776336669922 sec
2023-06-22 15:18:15,582 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004239082336425781 sec
2023-06-22 15:18:15,583 - WARNING - Finished tracing + transforming fn for pjit in 0.0004611015319824219 sec
2023-06-22 15:18:15,584 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003871917724609375 sec
2023-06-22 15:18:15,585 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040650367736816406 sec
2023-06-22 15:18:15,586 - WARNING - Finished tracing + transforming fn for pjit in 0.00047850608825683594 sec
2023-06-22 15:18:15,587 - WARNING - Finished tracing + transforming fn for pjit in 0.00042319297790527344 sec
2023-06-22 15:18:15,588 - WARNING - Finished tracing + transforming square for pjit in 0.00030422210693359375 sec
2023-06-22 15:18:15,592 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00040340423583984375 sec
2023-06-22 15:18:15,594 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002913475036621094 sec
2023-06-22 15:18:15,595 - WARNING - Finished tracing + transforming fn for pjit in 0.00047516822814941406 sec
2023-06-22 15:18:15,596 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003921985626220703 sec
2023-06-22 15:18:15,597 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004038810729980469 sec
2023-06-22 15:18:15,599 - WARNING - Finished tracing + transforming update_fn for pjit in 0.24640631675720215 sec
2023-06-22 15:18:15,604 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,10]), ShapedArray(float32[334,10]), ShapedArray(float32[334,10]), ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,10]), ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:18:15,704 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.09907650947570801 sec
2023-06-22 15:18:15,704 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:18:16,133 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4286458492279053 sec
2023-06-22 15:18:18,352 - INFO - Distilling data from client: Client16
2023-06-22 15:18:18,352 - INFO - train loss: 0.004994063098728643
2023-06-22 15:18:18,352 - INFO - train acc: 0.9640718698501587
2023-06-22 15:18:18,467 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.83      0.79       118
           5       0.71      0.60      0.65        82

    accuracy                           0.73       200
   macro avg       0.73      0.71      0.72       200
weighted avg       0.73      0.73      0.73       200

2023-06-22 15:18:18,467 - INFO - test loss 0.020567092987194908
2023-06-22 15:18:18,467 - INFO - test acc 0.73499995470047
2023-06-22 15:18:20,552 - INFO - Distilling data from client: Client16
2023-06-22 15:18:20,552 - INFO - train loss: 0.0037964239249229947
2023-06-22 15:18:20,553 - INFO - train acc: 0.9790419340133667
2023-06-22 15:18:20,686 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.85      0.79       118
           5       0.73      0.59      0.65        82

    accuracy                           0.74       200
   macro avg       0.74      0.72      0.72       200
weighted avg       0.74      0.74      0.73       200

2023-06-22 15:18:20,690 - INFO - test loss 0.01942899615319136
2023-06-22 15:18:20,692 - INFO - test acc 0.7400000095367432
2023-06-22 15:18:22,643 - INFO - Distilling data from client: Client16
2023-06-22 15:18:22,644 - INFO - train loss: 0.0031967470391139636
2023-06-22 15:18:22,644 - INFO - train acc: 0.9850299954414368
2023-06-22 15:18:22,694 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.87      0.79       118
           5       0.74      0.52      0.61        82

    accuracy                           0.73       200
   macro avg       0.73      0.70      0.70       200
weighted avg       0.73      0.73      0.72       200

2023-06-22 15:18:22,695 - INFO - test loss 0.019752065663038405
2023-06-22 15:18:22,695 - INFO - test acc 0.7299999594688416
2023-06-22 15:18:24,746 - INFO - Distilling data from client: Client16
2023-06-22 15:18:24,746 - INFO - train loss: 0.002930637621376557
2023-06-22 15:18:24,747 - INFO - train acc: 0.9880239963531494
2023-06-22 15:18:24,800 - INFO - report:               precision    recall  f1-score   support

           2       0.71      0.81      0.76       118
           5       0.66      0.52      0.59        82

    accuracy                           0.69       200
   macro avg       0.69      0.67      0.67       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:18:24,804 - INFO - test loss 0.020417024208913007
2023-06-22 15:18:24,805 - INFO - test acc 0.6949999928474426
2023-06-22 15:18:26,884 - INFO - Distilling data from client: Client16
2023-06-22 15:18:26,885 - INFO - train loss: 0.0026148847758414133
2023-06-22 15:18:26,885 - INFO - train acc: 0.9970059990882874
2023-06-22 15:18:27,006 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.86      0.80       118
           5       0.74      0.59      0.65        82

    accuracy                           0.74       200
   macro avg       0.74      0.72      0.73       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:18:27,006 - INFO - test loss 0.01958974830807986
2023-06-22 15:18:27,006 - INFO - test acc 0.7450000047683716
2023-06-22 15:18:29,152 - INFO - Distilling data from client: Client16
2023-06-22 15:18:29,152 - INFO - train loss: 0.002618385366789542
2023-06-22 15:18:29,153 - INFO - train acc: 0.9970059990882874
2023-06-22 15:18:29,204 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.84      0.78       118
           5       0.70      0.54      0.61        82

    accuracy                           0.71       200
   macro avg       0.71      0.69      0.69       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:18:29,207 - INFO - test loss 0.02035401600736975
2023-06-22 15:18:29,208 - INFO - test acc 0.7149999737739563
2023-06-22 15:18:31,390 - INFO - Distilling data from client: Client16
2023-06-22 15:18:31,391 - INFO - train loss: 0.0024652311292193622
2023-06-22 15:18:31,393 - INFO - train acc: 1.0
2023-06-22 15:18:31,446 - INFO - report:               precision    recall  f1-score   support

           2       0.71      0.83      0.76       118
           5       0.67      0.50      0.57        82

    accuracy                           0.69       200
   macro avg       0.69      0.67      0.67       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:18:31,450 - INFO - test loss 0.020525406895155696
2023-06-22 15:18:31,451 - INFO - test acc 0.6949999928474426
2023-06-22 15:18:33,692 - INFO - Distilling data from client: Client16
2023-06-22 15:18:33,695 - INFO - train loss: 0.0023644691631979145
2023-06-22 15:18:33,695 - INFO - train acc: 1.0
2023-06-22 15:18:33,745 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.83      0.78       118
           5       0.70      0.56      0.62        82

    accuracy                           0.72       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.72      0.72      0.71       200

2023-06-22 15:18:33,749 - INFO - test loss 0.019875354584478398
2023-06-22 15:18:33,749 - INFO - test acc 0.7199999690055847
2023-06-22 15:18:35,988 - INFO - Distilling data from client: Client16
2023-06-22 15:18:35,988 - INFO - train loss: 0.0027931168015791
2023-06-22 15:18:35,989 - INFO - train acc: 0.9910179972648621
2023-06-22 15:18:36,035 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.86      0.80       118
           5       0.74      0.59      0.65        82

    accuracy                           0.74       200
   macro avg       0.74      0.72      0.73       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:18:36,038 - INFO - test loss 0.01877460654243029
2023-06-22 15:18:36,039 - INFO - test acc 0.7450000047683716
2023-06-22 15:18:38,185 - INFO - Distilling data from client: Client16
2023-06-22 15:18:38,186 - INFO - train loss: 0.002682015185135456
2023-06-22 15:18:38,186 - INFO - train acc: 1.0
2023-06-22 15:18:38,234 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.83      0.77       118
           5       0.68      0.52      0.59        82

    accuracy                           0.70       200
   macro avg       0.70      0.68      0.68       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:18:38,238 - INFO - test loss 0.01982328484536309
2023-06-22 15:18:38,239 - INFO - test acc 0.7049999833106995
2023-06-22 15:18:40,460 - INFO - Distilling data from client: Client16
2023-06-22 15:18:40,460 - INFO - train loss: 0.0020498058660813365
2023-06-22 15:18:40,460 - INFO - train acc: 0.9970059990882874
2023-06-22 15:18:40,508 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.84      0.78       118
           5       0.70      0.54      0.61        82

    accuracy                           0.71       200
   macro avg       0.71      0.69      0.69       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:18:40,509 - INFO - test loss 0.021423047683710113
2023-06-22 15:18:40,510 - INFO - test acc 0.7149999737739563
2023-06-22 15:18:42,670 - INFO - Distilling data from client: Client16
2023-06-22 15:18:42,670 - INFO - train loss: 0.002197043095623501
2023-06-22 15:18:42,671 - INFO - train acc: 0.9970059990882874
2023-06-22 15:18:42,729 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.85      0.78       118
           5       0.71      0.54      0.61        82

    accuracy                           0.72       200
   macro avg       0.72      0.69      0.70       200
weighted avg       0.72      0.72      0.71       200

2023-06-22 15:18:42,730 - INFO - test loss 0.02005340344609155
2023-06-22 15:18:42,730 - INFO - test acc 0.7199999690055847
2023-06-22 15:18:44,974 - INFO - Distilling data from client: Client16
2023-06-22 15:18:44,975 - INFO - train loss: 0.002463947341709871
2023-06-22 15:18:44,976 - INFO - train acc: 0.9940119981765747
2023-06-22 15:18:45,025 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.85      0.78       118
           5       0.70      0.52      0.60        82

    accuracy                           0.71       200
   macro avg       0.71      0.69      0.69       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:18:45,029 - INFO - test loss 0.019458780674891273
2023-06-22 15:18:45,030 - INFO - test acc 0.7149999737739563
2023-06-22 15:18:47,130 - INFO - Distilling data from client: Client16
2023-06-22 15:18:47,131 - INFO - train loss: 0.0019491826903156079
2023-06-22 15:18:47,131 - INFO - train acc: 1.0
2023-06-22 15:18:47,189 - INFO - report:               precision    recall  f1-score   support

           2       0.71      0.86      0.78       118
           5       0.71      0.49      0.58        82

    accuracy                           0.71       200
   macro avg       0.71      0.68      0.68       200
weighted avg       0.71      0.71      0.70       200

2023-06-22 15:18:47,192 - INFO - test loss 0.02027453623255829
2023-06-22 15:18:47,193 - INFO - test acc 0.7099999785423279
2023-06-22 15:18:49,449 - INFO - Distilling data from client: Client16
2023-06-22 15:18:49,449 - INFO - train loss: 0.0020494900603955615
2023-06-22 15:18:49,449 - INFO - train acc: 1.0
2023-06-22 15:18:49,503 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.84      0.78       118
           5       0.71      0.56      0.63        82

    accuracy                           0.73       200
   macro avg       0.72      0.70      0.70       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:18:49,508 - INFO - test loss 0.019308815541007694
2023-06-22 15:18:49,509 - INFO - test acc 0.7249999642372131
2023-06-22 15:18:51,616 - INFO - Distilling data from client: Client16
2023-06-22 15:18:51,616 - INFO - train loss: 0.0023974941997145717
2023-06-22 15:18:51,617 - INFO - train acc: 0.9940119981765747
2023-06-22 15:18:51,662 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.82      0.77       118
           5       0.68      0.55      0.61        82

    accuracy                           0.71       200
   macro avg       0.70      0.69      0.69       200
weighted avg       0.71      0.71      0.70       200

2023-06-22 15:18:51,663 - INFO - test loss 0.019696073106565348
2023-06-22 15:18:51,664 - INFO - test acc 0.7099999785423279
2023-06-22 15:18:53,727 - INFO - Distilling data from client: Client16
2023-06-22 15:18:53,727 - INFO - train loss: 0.0023805864815192234
2023-06-22 15:18:53,728 - INFO - train acc: 0.9970059990882874
2023-06-22 15:18:53,776 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.85      0.80       118
           5       0.73      0.60      0.66        82

    accuracy                           0.74       200
   macro avg       0.74      0.72      0.73       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:18:53,780 - INFO - test loss 0.019221046102069712
2023-06-22 15:18:53,781 - INFO - test acc 0.7450000047683716
2023-06-22 15:18:55,834 - INFO - Distilling data from client: Client16
2023-06-22 15:18:55,835 - INFO - train loss: 0.0019485449972270754
2023-06-22 15:18:55,835 - INFO - train acc: 1.0
2023-06-22 15:18:55,882 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.82      0.77       118
           5       0.68      0.55      0.61        82

    accuracy                           0.71       200
   macro avg       0.70      0.69      0.69       200
weighted avg       0.71      0.71      0.70       200

2023-06-22 15:18:55,884 - INFO - test loss 0.020927792185520593
2023-06-22 15:18:55,886 - INFO - test acc 0.7099999785423279
2023-06-22 15:18:58,003 - INFO - Distilling data from client: Client16
2023-06-22 15:18:58,003 - INFO - train loss: 0.0018264229792395866
2023-06-22 15:18:58,004 - INFO - train acc: 0.9940119981765747
2023-06-22 15:18:58,052 - INFO - report:               precision    recall  f1-score   support

           2       0.74      0.85      0.79       118
           5       0.72      0.57      0.64        82

    accuracy                           0.73       200
   macro avg       0.73      0.71      0.71       200
weighted avg       0.73      0.73      0.73       200

2023-06-22 15:18:58,053 - INFO - test loss 0.019558694286281805
2023-06-22 15:18:58,053 - INFO - test acc 0.73499995470047
2023-06-22 15:19:00,217 - INFO - Distilling data from client: Client16
2023-06-22 15:19:00,217 - INFO - train loss: 0.0017038600994858336
2023-06-22 15:19:00,217 - INFO - train acc: 1.0
2023-06-22 15:19:00,263 - INFO - report:               precision    recall  f1-score   support

           2       0.70      0.85      0.77       118
           5       0.68      0.48      0.56        82

    accuracy                           0.69       200
   macro avg       0.69      0.66      0.66       200
weighted avg       0.69      0.69      0.68       200

2023-06-22 15:19:00,264 - INFO - test loss 0.019891080149387758
2023-06-22 15:19:00,265 - INFO - test acc 0.6949999928474426
2023-06-22 15:19:02,418 - INFO - Distilling data from client: Client16
2023-06-22 15:19:02,419 - INFO - train loss: 0.001970387402764329
2023-06-22 15:19:02,419 - INFO - train acc: 1.0
2023-06-22 15:19:02,466 - INFO - report:               precision    recall  f1-score   support

           2       0.74      0.83      0.78       118
           5       0.70      0.57      0.63        82

    accuracy                           0.73       200
   macro avg       0.72      0.70      0.71       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:19:02,468 - INFO - test loss 0.0193795884371997
2023-06-22 15:19:02,468 - INFO - test acc 0.7249999642372131
2023-06-22 15:19:04,605 - INFO - Distilling data from client: Client16
2023-06-22 15:19:04,606 - INFO - train loss: 0.002073448175085556
2023-06-22 15:19:04,606 - INFO - train acc: 1.0
2023-06-22 15:19:04,663 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.86      0.79       118
           5       0.73      0.55      0.63        82

    accuracy                           0.73       200
   macro avg       0.73      0.70      0.71       200
weighted avg       0.73      0.73      0.72       200

2023-06-22 15:19:04,663 - INFO - test loss 0.02048490025198344
2023-06-22 15:19:04,664 - INFO - test acc 0.7299999594688416
2023-06-22 15:19:06,814 - INFO - Distilling data from client: Client16
2023-06-22 15:19:06,815 - INFO - train loss: 0.0019375740280018646
2023-06-22 15:19:06,815 - INFO - train acc: 0.9970059990882874
2023-06-22 15:19:06,925 - INFO - report:               precision    recall  f1-score   support

           2       0.76      0.86      0.80       118
           5       0.75      0.61      0.67        82

    accuracy                           0.76       200
   macro avg       0.75      0.73      0.74       200
weighted avg       0.75      0.76      0.75       200

2023-06-22 15:19:06,926 - INFO - test loss 0.01969978675914551
2023-06-22 15:19:06,927 - INFO - test acc 0.7549999952316284
2023-06-22 15:19:09,043 - INFO - Distilling data from client: Client16
2023-06-22 15:19:09,045 - INFO - train loss: 0.001994501441249487
2023-06-22 15:19:09,046 - INFO - train acc: 1.0
2023-06-22 15:19:09,101 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.87      0.80       118
           5       0.75      0.54      0.62        82

    accuracy                           0.73       200
   macro avg       0.74      0.70      0.71       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:19:09,104 - INFO - test loss 0.019374148633200493
2023-06-22 15:19:09,105 - INFO - test acc 0.73499995470047
2023-06-22 15:19:11,304 - INFO - Distilling data from client: Client16
2023-06-22 15:19:11,306 - INFO - train loss: 0.0018847875689200295
2023-06-22 15:19:11,306 - INFO - train acc: 1.0
2023-06-22 15:19:11,367 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.85      0.78       118
           5       0.71      0.54      0.61        82

    accuracy                           0.72       200
   macro avg       0.72      0.69      0.70       200
weighted avg       0.72      0.72      0.71       200

2023-06-22 15:19:11,371 - INFO - test loss 0.019588968019446696
2023-06-22 15:19:11,372 - INFO - test acc 0.7199999690055847
2023-06-22 15:19:11,382 - WARNING - Finished tracing + transforming jit(gather) in 0.0009582042694091797 sec
2023-06-22 15:19:11,383 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[334,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:19:11,387 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.003537893295288086 sec
2023-06-22 15:19:11,389 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:19:11,417 - WARNING - Finished XLA compilation of jit(gather) in 0.02677440643310547 sec
2023-06-22 15:19:11,457 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:19:11,471 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:19:11,483 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:19:11,495 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:19:11,507 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:19:11,519 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:19:11,942 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client16//synthetic.png
2023-06-22 15:19:11,962 - INFO - c: 4.0 and total_data_in_this_class: 269
2023-06-22 15:19:11,963 - INFO - c: 6.0 and total_data_in_this_class: 268
2023-06-22 15:19:11,963 - INFO - c: 8.0 and total_data_in_this_class: 262
2023-06-22 15:19:11,963 - INFO - c: 4.0 and total_data_in_this_class: 64
2023-06-22 15:19:11,963 - INFO - c: 6.0 and total_data_in_this_class: 65
2023-06-22 15:19:11,963 - INFO - c: 8.0 and total_data_in_this_class: 71
2023-06-22 15:19:12,083 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07460451126098633 sec
2023-06-22 15:19:12,158 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07393646240234375 sec
2023-06-22 15:19:12,166 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.1599259376525879 sec
2023-06-22 15:19:12,169 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:19:12,223 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05355525016784668 sec
2023-06-22 15:19:12,223 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:19:12,401 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17708206176757812 sec
2023-06-22 15:19:12,463 - INFO - initial test loss: 0.02089149142547987
2023-06-22 15:19:12,463 - INFO - initial test acc: 0.7400000095367432
2023-06-22 15:19:12,481 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012733697891235352 sec
2023-06-22 15:19:12,670 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2030317783355713 sec
2023-06-22 15:19:12,675 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:19:12,776 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.0997765064239502 sec
2023-06-22 15:19:12,776 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:19:13,198 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4218604564666748 sec
2023-06-22 15:19:15,993 - INFO - Distilling data from client: Client17
2023-06-22 15:19:15,993 - INFO - train loss: 0.002215122267361705
2023-06-22 15:19:15,994 - INFO - train acc: 1.0
2023-06-22 15:19:16,229 - INFO - report:               precision    recall  f1-score   support

           4       0.64      0.64      0.64        64
           6       0.66      0.71      0.68        65
           8       0.89      0.83      0.86        71

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:19:16,229 - INFO - test loss 0.019651785489525907
2023-06-22 15:19:16,229 - INFO - test acc 0.7299999594688416
2023-06-22 15:19:18,947 - INFO - Distilling data from client: Client17
2023-06-22 15:19:18,947 - INFO - train loss: 0.001234352207606092
2023-06-22 15:19:18,947 - INFO - train acc: 1.0
2023-06-22 15:19:19,157 - INFO - report:               precision    recall  f1-score   support

           4       0.65      0.70      0.68        64
           6       0.68      0.68      0.68        65
           8       0.89      0.83      0.86        71

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:19:19,157 - INFO - test loss 0.019715246240574913
2023-06-22 15:19:19,157 - INFO - test acc 0.7400000095367432
2023-06-22 15:19:21,940 - INFO - Distilling data from client: Client17
2023-06-22 15:19:21,941 - INFO - train loss: 0.0010336298462474236
2023-06-22 15:19:21,941 - INFO - train acc: 1.0
2023-06-22 15:19:21,991 - INFO - report:               precision    recall  f1-score   support

           4       0.67      0.62      0.65        64
           6       0.65      0.71      0.68        65
           8       0.87      0.85      0.86        71

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-06-22 15:19:21,996 - INFO - test loss 0.020195075454745137
2023-06-22 15:19:21,996 - INFO - test acc 0.7299999594688416
2023-06-22 15:19:24,901 - INFO - Distilling data from client: Client17
2023-06-22 15:19:24,902 - INFO - train loss: 0.000877360229502739
2023-06-22 15:19:24,902 - INFO - train acc: 1.0
2023-06-22 15:19:24,966 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.64      0.64        64
           6       0.62      0.65      0.63        65
           8       0.90      0.85      0.87        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:19:24,967 - INFO - test loss 0.020131881821147276
2023-06-22 15:19:24,969 - INFO - test acc 0.7149999737739563
2023-06-22 15:19:27,827 - INFO - Distilling data from client: Client17
2023-06-22 15:19:27,828 - INFO - train loss: 0.0008610826486982505
2023-06-22 15:19:27,828 - INFO - train acc: 0.9980952143669128
2023-06-22 15:19:27,888 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.66      0.64        64
           6       0.66      0.66      0.66        65
           8       0.88      0.83      0.86        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:19:27,889 - INFO - test loss 0.0200631023193415
2023-06-22 15:19:27,890 - INFO - test acc 0.7199999690055847
2023-06-22 15:19:30,827 - INFO - Distilling data from client: Client17
2023-06-22 15:19:30,828 - INFO - train loss: 0.0006488651919622916
2023-06-22 15:19:30,828 - INFO - train acc: 1.0
2023-06-22 15:19:30,898 - INFO - report:               precision    recall  f1-score   support

           4       0.65      0.69      0.67        64
           6       0.69      0.74      0.71        65
           8       0.90      0.79      0.84        71

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:19:30,900 - INFO - test loss 0.020476160770889727
2023-06-22 15:19:30,900 - INFO - test acc 0.7400000095367432
2023-06-22 15:19:33,729 - INFO - Distilling data from client: Client17
2023-06-22 15:19:33,729 - INFO - train loss: 0.0006783506971014994
2023-06-22 15:19:33,729 - INFO - train acc: 0.9980952143669128
2023-06-22 15:19:33,799 - INFO - report:               precision    recall  f1-score   support

           4       0.65      0.62      0.63        64
           6       0.63      0.69      0.66        65
           8       0.88      0.83      0.86        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:19:33,800 - INFO - test loss 0.02028074467660015
2023-06-22 15:19:33,801 - INFO - test acc 0.7199999690055847
2023-06-22 15:19:36,892 - INFO - Distilling data from client: Client17
2023-06-22 15:19:36,894 - INFO - train loss: 0.0005419360526226289
2023-06-22 15:19:36,894 - INFO - train acc: 1.0
2023-06-22 15:19:36,960 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.66      0.64        64
           6       0.63      0.63      0.63        65
           8       0.87      0.83      0.85        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:19:36,960 - INFO - test loss 0.021001519320306927
2023-06-22 15:19:36,961 - INFO - test acc 0.7099999785423279
2023-06-22 15:19:39,895 - INFO - Distilling data from client: Client17
2023-06-22 15:19:39,896 - INFO - train loss: 0.0006544452395627788
2023-06-22 15:19:39,896 - INFO - train acc: 1.0
2023-06-22 15:19:39,958 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.67      0.65        64
           6       0.65      0.65      0.65        65
           8       0.88      0.83      0.86        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:19:39,958 - INFO - test loss 0.020581321100314943
2023-06-22 15:19:39,959 - INFO - test acc 0.7199999690055847
2023-06-22 15:19:42,833 - INFO - Distilling data from client: Client17
2023-06-22 15:19:42,834 - INFO - train loss: 0.0004740980989465091
2023-06-22 15:19:42,834 - INFO - train acc: 1.0
2023-06-22 15:19:42,902 - INFO - report:               precision    recall  f1-score   support

           4       0.60      0.64      0.62        64
           6       0.62      0.62      0.62        65
           8       0.87      0.83      0.85        71

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:19:42,905 - INFO - test loss 0.020820706540256088
2023-06-22 15:19:42,906 - INFO - test acc 0.699999988079071
2023-06-22 15:19:45,809 - INFO - Distilling data from client: Client17
2023-06-22 15:19:45,809 - INFO - train loss: 0.0004866961553846659
2023-06-22 15:19:45,809 - INFO - train acc: 1.0
2023-06-22 15:19:45,862 - INFO - report:               precision    recall  f1-score   support

           4       0.65      0.61      0.63        64
           6       0.63      0.69      0.66        65
           8       0.86      0.83      0.84        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:19:45,863 - INFO - test loss 0.02052215873432675
2023-06-22 15:19:45,863 - INFO - test acc 0.7149999737739563
2023-06-22 15:19:48,745 - INFO - Distilling data from client: Client17
2023-06-22 15:19:48,746 - INFO - train loss: 0.00041643856708110255
2023-06-22 15:19:48,746 - INFO - train acc: 1.0
2023-06-22 15:19:48,840 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.66      0.64        64
           6       0.66      0.65      0.65        65
           8       0.87      0.85      0.86        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:19:48,844 - INFO - test loss 0.02007569612993133
2023-06-22 15:19:48,845 - INFO - test acc 0.7199999690055847
2023-06-22 15:19:51,690 - INFO - Distilling data from client: Client17
2023-06-22 15:19:51,691 - INFO - train loss: 0.00034532821076046626
2023-06-22 15:19:51,691 - INFO - train acc: 1.0
2023-06-22 15:19:51,748 - INFO - report:               precision    recall  f1-score   support

           4       0.60      0.62      0.61        64
           6       0.64      0.65      0.64        65
           8       0.88      0.83      0.86        71

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-06-22 15:19:51,750 - INFO - test loss 0.020887562730501727
2023-06-22 15:19:51,751 - INFO - test acc 0.7049999833106995
2023-06-22 15:19:54,678 - INFO - Distilling data from client: Client17
2023-06-22 15:19:54,678 - INFO - train loss: 0.00044434057918962393
2023-06-22 15:19:54,678 - INFO - train acc: 1.0
2023-06-22 15:19:54,738 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.67      0.65        64
           6       0.68      0.66      0.67        65
           8       0.84      0.82      0.83        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:19:54,739 - INFO - test loss 0.02101038355156074
2023-06-22 15:19:54,740 - INFO - test acc 0.7199999690055847
2023-06-22 15:19:57,642 - INFO - Distilling data from client: Client17
2023-06-22 15:19:57,645 - INFO - train loss: 0.00031400305438954206
2023-06-22 15:19:57,646 - INFO - train acc: 1.0
2023-06-22 15:19:57,724 - INFO - report:               precision    recall  f1-score   support

           4       0.67      0.67      0.67        64
           6       0.67      0.69      0.68        65
           8       0.86      0.83      0.84        71

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.74      0.73      0.74       200

2023-06-22 15:19:57,729 - INFO - test loss 0.020654461967196613
2023-06-22 15:19:57,730 - INFO - test acc 0.73499995470047
2023-06-22 15:20:00,579 - INFO - Distilling data from client: Client17
2023-06-22 15:20:00,579 - INFO - train loss: 0.0005155754541587105
2023-06-22 15:20:00,579 - INFO - train acc: 1.0
2023-06-22 15:20:00,639 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.64      0.64        64
           6       0.66      0.68      0.67        65
           8       0.87      0.83      0.85        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:20:00,639 - INFO - test loss 0.02069582511183017
2023-06-22 15:20:00,639 - INFO - test acc 0.7199999690055847
2023-06-22 15:20:03,531 - INFO - Distilling data from client: Client17
2023-06-22 15:20:03,532 - INFO - train loss: 0.000458124483622576
2023-06-22 15:20:03,532 - INFO - train acc: 1.0
2023-06-22 15:20:03,591 - INFO - report:               precision    recall  f1-score   support

           4       0.60      0.59      0.60        64
           6       0.62      0.66      0.64        65
           8       0.87      0.83      0.85        71

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:20:03,592 - INFO - test loss 0.020466735059370314
2023-06-22 15:20:03,593 - INFO - test acc 0.699999988079071
2023-06-22 15:20:06,467 - INFO - Distilling data from client: Client17
2023-06-22 15:20:06,468 - INFO - train loss: 0.0003105774176665175
2023-06-22 15:20:06,468 - INFO - train acc: 1.0
2023-06-22 15:20:06,539 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.66      0.64        64
           6       0.64      0.66      0.65        65
           8       0.89      0.82      0.85        71

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:20:06,540 - INFO - test loss 0.02083357776315623
2023-06-22 15:20:06,541 - INFO - test acc 0.7149999737739563
2023-06-22 15:20:09,475 - INFO - Distilling data from client: Client17
2023-06-22 15:20:09,476 - INFO - train loss: 0.0003751000105150235
2023-06-22 15:20:09,477 - INFO - train acc: 1.0
2023-06-22 15:20:09,535 - INFO - report:               precision    recall  f1-score   support

           4       0.65      0.66      0.65        64
           6       0.65      0.66      0.66        65
           8       0.86      0.83      0.84        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:20:09,536 - INFO - test loss 0.020425604025872013
2023-06-22 15:20:09,537 - INFO - test acc 0.7199999690055847
2023-06-22 15:20:12,370 - INFO - Distilling data from client: Client17
2023-06-22 15:20:12,370 - INFO - train loss: 0.0003738002843088028
2023-06-22 15:20:12,370 - INFO - train acc: 1.0
2023-06-22 15:20:12,429 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.64      0.63        64
           6       0.64      0.65      0.64        65
           8       0.87      0.83      0.85        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:20:12,434 - INFO - test loss 0.02112101595479277
2023-06-22 15:20:12,434 - INFO - test acc 0.7099999785423279
2023-06-22 15:20:15,299 - INFO - Distilling data from client: Client17
2023-06-22 15:20:15,300 - INFO - train loss: 0.00037606937738457496
2023-06-22 15:20:15,300 - INFO - train acc: 1.0
2023-06-22 15:20:15,356 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.62      0.62        64
           6       0.61      0.65      0.63        65
           8       0.85      0.80      0.83        71

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.70       200

2023-06-22 15:20:15,356 - INFO - test loss 0.020929014255572708
2023-06-22 15:20:15,357 - INFO - test acc 0.6949999928474426
2023-06-22 15:20:18,237 - INFO - Distilling data from client: Client17
2023-06-22 15:20:18,238 - INFO - train loss: 0.0003440355999269335
2023-06-22 15:20:18,238 - INFO - train acc: 1.0
2023-06-22 15:20:18,316 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.67      0.65        64
           6       0.65      0.63      0.64        65
           8       0.86      0.83      0.84        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:20:18,317 - INFO - test loss 0.020795901489668055
2023-06-22 15:20:18,318 - INFO - test acc 0.7149999737739563
2023-06-22 15:20:21,380 - INFO - Distilling data from client: Client17
2023-06-22 15:20:21,380 - INFO - train loss: 0.00032570498042754623
2023-06-22 15:20:21,381 - INFO - train acc: 1.0
2023-06-22 15:20:21,437 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.70      0.66        64
           6       0.65      0.63      0.64        65
           8       0.88      0.80      0.84        71

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:20:21,439 - INFO - test loss 0.020568498221306873
2023-06-22 15:20:21,439 - INFO - test acc 0.7149999737739563
2023-06-22 15:20:24,437 - INFO - Distilling data from client: Client17
2023-06-22 15:20:24,438 - INFO - train loss: 0.00024960294550483087
2023-06-22 15:20:24,438 - INFO - train acc: 1.0
2023-06-22 15:20:24,497 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.64      0.63        64
           6       0.65      0.66      0.66        65
           8       0.87      0.83      0.85        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:20:24,498 - INFO - test loss 0.021267084702477568
2023-06-22 15:20:24,498 - INFO - test acc 0.7149999737739563
2023-06-22 15:20:27,417 - INFO - Distilling data from client: Client17
2023-06-22 15:20:27,418 - INFO - train loss: 0.0002830506491648563
2023-06-22 15:20:27,419 - INFO - train acc: 1.0
2023-06-22 15:20:27,470 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.67      0.65        64
           6       0.67      0.68      0.67        65
           8       0.89      0.83      0.86        71

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:20:27,471 - INFO - test loss 0.021100062651788536
2023-06-22 15:20:27,471 - INFO - test acc 0.7299999594688416
2023-06-22 15:20:27,504 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:27,525 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:27,543 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:27,561 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:27,577 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:27,589 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:27,601 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:27,612 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:27,625 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:28,170 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client17//synthetic.png
2023-06-22 15:20:28,190 - INFO - c: 0.0 and total_data_in_this_class: 3
2023-06-22 15:20:28,190 - INFO - c: 1.0 and total_data_in_this_class: 258
2023-06-22 15:20:28,190 - INFO - c: 2.0 and total_data_in_this_class: 12
2023-06-22 15:20:28,190 - INFO - c: 3.0 and total_data_in_this_class: 253
2023-06-22 15:20:28,190 - INFO - c: 4.0 and total_data_in_this_class: 273
2023-06-22 15:20:28,191 - INFO - c: 0.0 and total_data_in_this_class: 2
2023-06-22 15:20:28,191 - INFO - c: 1.0 and total_data_in_this_class: 70
2023-06-22 15:20:28,191 - INFO - c: 2.0 and total_data_in_this_class: 3
2023-06-22 15:20:28,191 - INFO - c: 3.0 and total_data_in_this_class: 65
2023-06-22 15:20:28,191 - INFO - c: 4.0 and total_data_in_this_class: 60
2023-06-22 15:20:28,227 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005147457122802734 sec
2023-06-22 15:20:28,228 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:20:28,230 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.002324819564819336 sec
2023-06-22 15:20:28,231 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:20:28,244 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01328587532043457 sec
2023-06-22 15:20:28,247 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00030350685119628906 sec
2023-06-22 15:20:28,247 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:20:28,249 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0015769004821777344 sec
2023-06-22 15:20:28,249 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:20:28,260 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011187314987182617 sec
2023-06-22 15:20:28,266 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002148151397705078 sec
2023-06-22 15:20:28,268 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019693374633789062 sec
2023-06-22 15:20:28,269 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005252361297607422 sec
2023-06-22 15:20:28,271 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020837783813476562 sec
2023-06-22 15:20:28,272 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00035834312438964844 sec
2023-06-22 15:20:28,273 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00044846534729003906 sec
2023-06-22 15:20:28,274 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004239082336425781 sec
2023-06-22 15:20:28,275 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003135204315185547 sec
2023-06-22 15:20:28,276 - WARNING - Finished tracing + transforming fn for pjit in 0.00046825408935546875 sec
2023-06-22 15:20:28,278 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0006191730499267578 sec
2023-06-22 15:20:28,279 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002243518829345703 sec
2023-06-22 15:20:28,281 - WARNING - Finished tracing + transforming fn for pjit in 0.0003790855407714844 sec
2023-06-22 15:20:28,282 - WARNING - Finished tracing + transforming fn for pjit in 0.0005953311920166016 sec
2023-06-22 15:20:28,283 - WARNING - Finished tracing + transforming fn for pjit in 0.00040078163146972656 sec
2023-06-22 15:20:28,284 - WARNING - Finished tracing + transforming fn for pjit in 0.00047516822814941406 sec
2023-06-22 15:20:28,286 - WARNING - Finished tracing + transforming fn for pjit in 0.0003674030303955078 sec
2023-06-22 15:20:28,289 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00041031837463378906 sec
2023-06-22 15:20:28,290 - WARNING - Finished tracing + transforming fn for pjit in 0.0003867149353027344 sec
2023-06-22 15:20:28,292 - WARNING - Finished tracing + transforming fn for pjit in 0.0003998279571533203 sec
2023-06-22 15:20:28,298 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005781650543212891 sec
2023-06-22 15:20:28,298 - WARNING - Finished tracing + transforming _mean for pjit in 0.0015990734100341797 sec
2023-06-22 15:20:28,300 - WARNING - Finished tracing + transforming fn for pjit in 0.00041365623474121094 sec
2023-06-22 15:20:28,301 - WARNING - Finished tracing + transforming fn for pjit in 0.00036525726318359375 sec
2023-06-22 15:20:28,302 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00036597251892089844 sec
2023-06-22 15:20:28,304 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006136894226074219 sec
2023-06-22 15:20:28,305 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031638145446777344 sec
2023-06-22 15:20:28,306 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005009174346923828 sec
2023-06-22 15:20:28,308 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003986358642578125 sec
2023-06-22 15:20:28,309 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004055500030517578 sec
2023-06-22 15:20:28,310 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004799365997314453 sec
2023-06-22 15:20:28,311 - WARNING - Finished tracing + transforming _where for pjit in 0.0015633106231689453 sec
2023-06-22 15:20:28,312 - WARNING - Finished tracing + transforming fn for pjit in 0.0004761219024658203 sec
2023-06-22 15:20:28,314 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006005764007568359 sec
2023-06-22 15:20:28,315 - WARNING - Finished tracing + transforming fn for pjit in 0.00040650367736816406 sec
2023-06-22 15:20:28,316 - WARNING - Finished tracing + transforming fn for pjit in 0.00039696693420410156 sec
2023-06-22 15:20:28,318 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003986358642578125 sec
2023-06-22 15:20:28,319 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045943260192871094 sec
2023-06-22 15:20:28,320 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031185150146484375 sec
2023-06-22 15:20:28,321 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004544258117675781 sec
2023-06-22 15:20:28,323 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004112720489501953 sec
2023-06-22 15:20:28,324 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003960132598876953 sec
2023-06-22 15:20:28,325 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004467964172363281 sec
2023-06-22 15:20:28,326 - WARNING - Finished tracing + transforming _where for pjit in 0.001497030258178711 sec
2023-06-22 15:20:28,327 - WARNING - Finished tracing + transforming fn for pjit in 0.00046181678771972656 sec
2023-06-22 15:20:28,328 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000457763671875 sec
2023-06-22 15:20:28,330 - WARNING - Finished tracing + transforming fn for pjit in 0.0003662109375 sec
2023-06-22 15:20:28,337 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004203319549560547 sec
2023-06-22 15:20:28,338 - WARNING - Finished tracing + transforming fn for pjit in 0.0004832744598388672 sec
2023-06-22 15:20:28,340 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00047016143798828125 sec
2023-06-22 15:20:28,341 - WARNING - Finished tracing + transforming fn for pjit in 0.0005228519439697266 sec
2023-06-22 15:20:28,347 - WARNING - Finished tracing + transforming fn for pjit in 0.00036406517028808594 sec
2023-06-22 15:20:28,350 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00027060508728027344 sec
2023-06-22 15:20:28,351 - WARNING - Finished tracing + transforming fn for pjit in 0.0005373954772949219 sec
2023-06-22 15:20:28,353 - WARNING - Finished tracing + transforming fn for pjit in 0.0004074573516845703 sec
2023-06-22 15:20:28,381 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11552286148071289 sec
2023-06-22 15:20:28,386 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020742416381835938 sec
2023-06-22 15:20:28,387 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002014636993408203 sec
2023-06-22 15:20:28,388 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00047779083251953125 sec
2023-06-22 15:20:28,391 - WARNING - Finished tracing + transforming fn for pjit in 0.0003662109375 sec
2023-06-22 15:20:28,392 - WARNING - Finished tracing + transforming fn for pjit in 0.00042748451232910156 sec
2023-06-22 15:20:28,395 - WARNING - Finished tracing + transforming fn for pjit in 0.0003669261932373047 sec
2023-06-22 15:20:28,404 - WARNING - Finished tracing + transforming fn for pjit in 0.00038242340087890625 sec
2023-06-22 15:20:28,406 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00035881996154785156 sec
2023-06-22 15:20:28,407 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004477500915527344 sec
2023-06-22 15:20:28,408 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003266334533691406 sec
2023-06-22 15:20:28,410 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006012916564941406 sec
2023-06-22 15:20:28,411 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003952980041503906 sec
2023-06-22 15:20:28,412 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004076957702636719 sec
2023-06-22 15:20:28,414 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00048422813415527344 sec
2023-06-22 15:20:28,414 - WARNING - Finished tracing + transforming _where for pjit in 0.0015301704406738281 sec
2023-06-22 15:20:28,416 - WARNING - Finished tracing + transforming fn for pjit in 0.00046181678771972656 sec
2023-06-22 15:20:28,417 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004382133483886719 sec
2023-06-22 15:20:28,418 - WARNING - Finished tracing + transforming fn for pjit in 0.00038743019104003906 sec
2023-06-22 15:20:28,419 - WARNING - Finished tracing + transforming fn for pjit in 0.0005326271057128906 sec
2023-06-22 15:20:28,439 - WARNING - Finished tracing + transforming fn for pjit in 0.00040268898010253906 sec
2023-06-22 15:20:28,470 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08794021606445312 sec
2023-06-22 15:20:28,472 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002033710479736328 sec
2023-06-22 15:20:28,474 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00022411346435546875 sec
2023-06-22 15:20:28,475 - WARNING - Finished tracing + transforming _where for pjit in 0.0010557174682617188 sec
2023-06-22 15:20:28,476 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005071163177490234 sec
2023-06-22 15:20:28,476 - WARNING - Finished tracing + transforming trace for pjit in 0.0043523311614990234 sec
2023-06-22 15:20:28,481 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0001747608184814453 sec
2023-06-22 15:20:28,482 - WARNING - Finished tracing + transforming tril for pjit in 0.001092672348022461 sec
2023-06-22 15:20:28,483 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0035066604614257812 sec
2023-06-22 15:20:28,484 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.000186920166015625 sec
2023-06-22 15:20:28,485 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019311904907226562 sec
2023-06-22 15:20:28,488 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.002363920211791992 sec
2023-06-22 15:20:28,495 - WARNING - Finished tracing + transforming _solve for pjit in 0.0157620906829834 sec
2023-06-22 15:20:28,496 - WARNING - Finished tracing + transforming dot for pjit in 0.0005435943603515625 sec
2023-06-22 15:20:28,500 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.2372286319732666 sec
2023-06-22 15:20:28,503 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:20:28,556 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05249452590942383 sec
2023-06-22 15:20:28,556 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:20:28,698 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.14137840270996094 sec
2023-06-22 15:20:28,703 - INFO - initial test loss: 0.039883133295323225
2023-06-22 15:20:28,703 - INFO - initial test acc: 0.38499999046325684
2023-06-22 15:20:28,711 - WARNING - Finished tracing + transforming dot for pjit in 0.0005440711975097656 sec
2023-06-22 15:20:28,712 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00047087669372558594 sec
2023-06-22 15:20:28,714 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006470680236816406 sec
2023-06-22 15:20:28,714 - WARNING - Finished tracing + transforming _mean for pjit in 0.0017123222351074219 sec
2023-06-22 15:20:28,716 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0003256797790527344 sec
2023-06-22 15:20:28,717 - WARNING - Finished tracing + transforming _argmax for pjit in 0.000316619873046875 sec
2023-06-22 15:20:28,718 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039839744567871094 sec
2023-06-22 15:20:28,719 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006175041198730469 sec
2023-06-22 15:20:28,720 - WARNING - Finished tracing + transforming _mean for pjit in 0.0017919540405273438 sec
2023-06-22 15:20:28,721 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.015894651412963867 sec
2023-06-22 15:20:28,734 - WARNING - Finished tracing + transforming fn for pjit in 0.0003764629364013672 sec
2023-06-22 15:20:28,735 - WARNING - Finished tracing + transforming fn for pjit in 0.0004596710205078125 sec
2023-06-22 15:20:28,736 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003657341003417969 sec
2023-06-22 15:20:28,737 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004413127899169922 sec
2023-06-22 15:20:28,738 - WARNING - Finished tracing + transforming _where for pjit in 0.0014338493347167969 sec
2023-06-22 15:20:28,751 - WARNING - Finished tracing + transforming fn for pjit in 0.0004410743713378906 sec
2023-06-22 15:20:28,752 - WARNING - Finished tracing + transforming fn for pjit in 0.000446319580078125 sec
2023-06-22 15:20:28,753 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003750324249267578 sec
2023-06-22 15:20:28,755 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005280971527099609 sec
2023-06-22 15:20:28,755 - WARNING - Finished tracing + transforming _where for pjit in 0.0015234947204589844 sec
2023-06-22 15:20:28,805 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003387928009033203 sec
2023-06-22 15:20:28,885 - WARNING - Finished tracing + transforming fn for pjit in 0.0004131793975830078 sec
2023-06-22 15:20:28,886 - WARNING - Finished tracing + transforming fn for pjit in 0.0003993511199951172 sec
2023-06-22 15:20:28,887 - WARNING - Finished tracing + transforming square for pjit in 0.00030517578125 sec
2023-06-22 15:20:28,891 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003371238708496094 sec
2023-06-22 15:20:28,893 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041794776916503906 sec
2023-06-22 15:20:28,894 - WARNING - Finished tracing + transforming fn for pjit in 0.00043201446533203125 sec
2023-06-22 15:20:28,896 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00038909912109375 sec
2023-06-22 15:20:28,896 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003895759582519531 sec
2023-06-22 15:20:28,898 - WARNING - Finished tracing + transforming fn for pjit in 0.0004761219024658203 sec
2023-06-22 15:20:28,899 - WARNING - Finished tracing + transforming fn for pjit in 0.0003952980041503906 sec
2023-06-22 15:20:28,900 - WARNING - Finished tracing + transforming square for pjit in 0.0002982616424560547 sec
2023-06-22 15:20:28,903 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003681182861328125 sec
2023-06-22 15:20:28,906 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00028705596923828125 sec
2023-06-22 15:20:28,907 - WARNING - Finished tracing + transforming fn for pjit in 0.00048422813415527344 sec
2023-06-22 15:20:28,908 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003962516784667969 sec
2023-06-22 15:20:28,909 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039124488830566406 sec
2023-06-22 15:20:28,910 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2057945728302002 sec
2023-06-22 15:20:28,916 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,10]), ShapedArray(float32[32,10]), ShapedArray(float32[32,10]), ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,10]), ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:20:29,018 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10118889808654785 sec
2023-06-22 15:20:29,018 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:20:29,275 - WARNING - Finished XLA compilation of jit(update_fn) in 0.2571403980255127 sec
2023-06-22 15:20:29,352 - INFO - Distilling data from client: Client18
2023-06-22 15:20:29,352 - INFO - train loss: 0.02620768032592462
2023-06-22 15:20:29,352 - INFO - train acc: 0.65625
2023-06-22 15:20:29,365 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.81      0.79        70
           2       0.08      0.33      0.12         3
           3       0.63      0.63      0.63        65
           4       0.66      0.52      0.58        60

    accuracy                           0.65       200
   macro avg       0.43      0.46      0.42       200
weighted avg       0.67      0.65      0.66       200

2023-06-22 15:20:29,365 - INFO - test loss 0.026903527695651505
2023-06-22 15:20:29,365 - INFO - test acc 0.6499999761581421
2023-06-22 15:20:29,438 - INFO - Distilling data from client: Client18
2023-06-22 15:20:29,438 - INFO - train loss: 0.019379420996279983
2023-06-22 15:20:29,438 - INFO - train acc: 0.78125
2023-06-22 15:20:29,449 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.84      0.80        70
           2       0.07      0.33      0.12         3
           3       0.69      0.58      0.63        65
           4       0.70      0.62      0.65        60

    accuracy                           0.68       200
   macro avg       0.44      0.48      0.44       200
weighted avg       0.70      0.68      0.68       200

2023-06-22 15:20:29,450 - INFO - test loss 0.026148628719318397
2023-06-22 15:20:29,450 - INFO - test acc 0.675000011920929
2023-06-22 15:20:29,529 - INFO - Distilling data from client: Client18
2023-06-22 15:20:29,529 - INFO - train loss: 0.016873206597773704
2023-06-22 15:20:29,529 - INFO - train acc: 0.8125
2023-06-22 15:20:29,537 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.79      0.83      0.81        70
           2       0.00      0.00      0.00         3
           3       0.63      0.52      0.57        65
           4       0.66      0.63      0.64        60

    accuracy                           0.65       200
   macro avg       0.42      0.40      0.41       200
weighted avg       0.68      0.65      0.66       200

2023-06-22 15:20:29,538 - INFO - test loss 0.025929991954932584
2023-06-22 15:20:29,538 - INFO - test acc 0.6499999761581421
2023-06-22 15:20:29,614 - INFO - Distilling data from client: Client18
2023-06-22 15:20:29,614 - INFO - train loss: 0.020226760675173098
2023-06-22 15:20:29,614 - INFO - train acc: 0.8125
2023-06-22 15:20:29,626 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.77      0.87      0.82        70
           2       0.00      0.00      0.00         3
           3       0.69      0.57      0.62        65
           4       0.68      0.67      0.67        60

    accuracy                           0.69       200
   macro avg       0.43      0.42      0.42       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:20:29,626 - INFO - test loss 0.025624343432989846
2023-06-22 15:20:29,626 - INFO - test acc 0.6899999976158142
2023-06-22 15:20:29,709 - INFO - Distilling data from client: Client18
2023-06-22 15:20:29,709 - INFO - train loss: 0.01482200689606657
2023-06-22 15:20:29,709 - INFO - train acc: 0.90625
2023-06-22 15:20:29,718 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.77      0.87      0.82        70
           2       0.00      0.00      0.00         3
           3       0.65      0.52      0.58        65
           4       0.65      0.62      0.63        60

    accuracy                           0.66       200
   macro avg       0.42      0.40      0.41       200
weighted avg       0.68      0.66      0.67       200

2023-06-22 15:20:29,718 - INFO - test loss 0.024842998336330517
2023-06-22 15:20:29,718 - INFO - test acc 0.6599999666213989
2023-06-22 15:20:29,807 - INFO - Distilling data from client: Client18
2023-06-22 15:20:29,808 - INFO - train loss: 0.014610576223456743
2023-06-22 15:20:29,808 - INFO - train acc: 0.875
2023-06-22 15:20:29,817 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.78      0.81      0.80        70
           2       0.00      0.00      0.00         3
           3       0.65      0.65      0.65        65
           4       0.66      0.62      0.64        60

    accuracy                           0.68       200
   macro avg       0.42      0.42      0.42       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:20:29,817 - INFO - test loss 0.025237558765084912
2023-06-22 15:20:29,817 - INFO - test acc 0.6800000071525574
2023-06-22 15:20:29,898 - INFO - Distilling data from client: Client18
2023-06-22 15:20:29,898 - INFO - train loss: 0.020824730498520086
2023-06-22 15:20:29,898 - INFO - train acc: 0.75
2023-06-22 15:20:29,907 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.70      0.83      0.76        70
           2       0.00      0.00      0.00         3
           3       0.66      0.57      0.61        65
           4       0.69      0.60      0.64        60

    accuracy                           0.66       200
   macro avg       0.41      0.40      0.40       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:20:29,907 - INFO - test loss 0.02515886613118454
2023-06-22 15:20:29,907 - INFO - test acc 0.6549999713897705
2023-06-22 15:20:29,985 - INFO - Distilling data from client: Client18
2023-06-22 15:20:29,986 - INFO - train loss: 0.019056323644598964
2023-06-22 15:20:29,986 - INFO - train acc: 0.71875
2023-06-22 15:20:29,999 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.86      0.81        70
           2       0.00      0.00      0.00         3
           3       0.68      0.71      0.69        65
           4       0.74      0.58      0.65        60

    accuracy                           0.70       200
   macro avg       0.44      0.43      0.43       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:20:30,000 - INFO - test loss 0.025329108750903502
2023-06-22 15:20:30,000 - INFO - test acc 0.7049999833106995
2023-06-22 15:20:30,082 - INFO - Distilling data from client: Client18
2023-06-22 15:20:30,082 - INFO - train loss: 0.01441144780710716
2023-06-22 15:20:30,082 - INFO - train acc: 0.90625
2023-06-22 15:20:30,091 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.86      0.81        70
           2       0.00      0.00      0.00         3
           3       0.64      0.57      0.60        65
           4       0.70      0.63      0.67        60

    accuracy                           0.68       200
   macro avg       0.42      0.41      0.41       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:20:30,091 - INFO - test loss 0.02463892173353988
2023-06-22 15:20:30,091 - INFO - test acc 0.675000011920929
2023-06-22 15:20:30,176 - INFO - Distilling data from client: Client18
2023-06-22 15:20:30,176 - INFO - train loss: 0.016058712330533045
2023-06-22 15:20:30,177 - INFO - train acc: 0.875
2023-06-22 15:20:30,185 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.73      0.83      0.78        70
           2       0.00      0.00      0.00         3
           3       0.65      0.54      0.59        65
           4       0.64      0.65      0.64        60

    accuracy                           0.66       200
   macro avg       0.40      0.40      0.40       200
weighted avg       0.66      0.66      0.66       200

2023-06-22 15:20:30,186 - INFO - test loss 0.024465916720384742
2023-06-22 15:20:30,186 - INFO - test acc 0.6599999666213989
2023-06-22 15:20:30,275 - INFO - Distilling data from client: Client18
2023-06-22 15:20:30,275 - INFO - train loss: 0.015061557933427512
2023-06-22 15:20:30,275 - INFO - train acc: 0.84375
2023-06-22 15:20:30,284 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.75      0.89      0.81        70
           2       0.00      0.00      0.00         3
           3       0.68      0.60      0.64        65
           4       0.75      0.60      0.67        60

    accuracy                           0.69       200
   macro avg       0.44      0.42      0.42       200
weighted avg       0.71      0.69      0.69       200

2023-06-22 15:20:30,284 - INFO - test loss 0.025900441108038817
2023-06-22 15:20:30,284 - INFO - test acc 0.6850000023841858
2023-06-22 15:20:30,366 - INFO - Distilling data from client: Client18
2023-06-22 15:20:30,367 - INFO - train loss: 0.015822289061042574
2023-06-22 15:20:30,367 - INFO - train acc: 0.78125
2023-06-22 15:20:30,375 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.75      0.86      0.80        70
           2       0.00      0.00      0.00         3
           3       0.65      0.52      0.58        65
           4       0.68      0.70      0.69        60

    accuracy                           0.68       200
   macro avg       0.42      0.42      0.41       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:20:30,375 - INFO - test loss 0.02512909016604623
2023-06-22 15:20:30,375 - INFO - test acc 0.6800000071525574
2023-06-22 15:20:30,453 - INFO - Distilling data from client: Client18
2023-06-22 15:20:30,453 - INFO - train loss: 0.013733152967016177
2023-06-22 15:20:30,454 - INFO - train acc: 0.875
2023-06-22 15:20:30,467 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.78      0.83      0.81        70
           2       0.00      0.00      0.00         3
           3       0.71      0.71      0.71        65
           4       0.71      0.65      0.68        60

    accuracy                           0.71       200
   macro avg       0.44      0.44      0.44       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:20:30,467 - INFO - test loss 0.023590510687113014
2023-06-22 15:20:30,467 - INFO - test acc 0.7149999737739563
2023-06-22 15:20:30,547 - INFO - Distilling data from client: Client18
2023-06-22 15:20:30,547 - INFO - train loss: 0.019078307363122226
2023-06-22 15:20:30,547 - INFO - train acc: 0.84375
2023-06-22 15:20:30,556 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.75      0.81      0.78        70
           2       0.00      0.00      0.00         3
           3       0.70      0.60      0.64        65
           4       0.67      0.70      0.68        60

    accuracy                           0.69       200
   macro avg       0.42      0.42      0.42       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:20:30,556 - INFO - test loss 0.02517938913695371
2023-06-22 15:20:30,556 - INFO - test acc 0.6899999976158142
2023-06-22 15:20:30,633 - INFO - Distilling data from client: Client18
2023-06-22 15:20:30,634 - INFO - train loss: 0.016757136643165537
2023-06-22 15:20:30,634 - INFO - train acc: 0.875
2023-06-22 15:20:30,642 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.82      0.80      0.81        70
           2       0.00      0.00      0.00         3
           3       0.65      0.68      0.66        65
           4       0.67      0.65      0.66        60

    accuracy                           0.69       200
   macro avg       0.43      0.43      0.43       200
weighted avg       0.70      0.69      0.70       200

2023-06-22 15:20:30,642 - INFO - test loss 0.024286543217378925
2023-06-22 15:20:30,642 - INFO - test acc 0.6949999928474426
2023-06-22 15:20:30,730 - INFO - Distilling data from client: Client18
2023-06-22 15:20:30,730 - INFO - train loss: 0.01710360001031409
2023-06-22 15:20:30,731 - INFO - train acc: 0.75
2023-06-22 15:20:30,739 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.77      0.84      0.80        70
           2       0.00      0.00      0.00         3
           3       0.70      0.68      0.69        65
           4       0.73      0.67      0.70        60

    accuracy                           0.71       200
   macro avg       0.44      0.44      0.44       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:20:30,739 - INFO - test loss 0.023800527539259934
2023-06-22 15:20:30,740 - INFO - test acc 0.7149999737739563
2023-06-22 15:20:30,827 - INFO - Distilling data from client: Client18
2023-06-22 15:20:30,827 - INFO - train loss: 0.017331314108172544
2023-06-22 15:20:30,827 - INFO - train acc: 0.84375
2023-06-22 15:20:30,836 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.86      0.81        70
           2       0.00      0.00      0.00         3
           3       0.66      0.66      0.66        65
           4       0.71      0.58      0.64        60

    accuracy                           0.69       200
   macro avg       0.43      0.42      0.42       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:20:30,836 - INFO - test loss 0.023839331091698255
2023-06-22 15:20:30,836 - INFO - test acc 0.6899999976158142
2023-06-22 15:20:30,919 - INFO - Distilling data from client: Client18
2023-06-22 15:20:30,919 - INFO - train loss: 0.0193096458502915
2023-06-22 15:20:30,919 - INFO - train acc: 0.78125
2023-06-22 15:20:30,928 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.74      0.86      0.79        70
           2       0.00      0.00      0.00         3
           3       0.70      0.60      0.64        65
           4       0.73      0.68      0.71        60

    accuracy                           0.70       200
   macro avg       0.43      0.43      0.43       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:20:30,928 - INFO - test loss 0.024384951497262393
2023-06-22 15:20:30,928 - INFO - test acc 0.699999988079071
2023-06-22 15:20:31,009 - INFO - Distilling data from client: Client18
2023-06-22 15:20:31,009 - INFO - train loss: 0.01754364087350314
2023-06-22 15:20:31,009 - INFO - train acc: 0.84375
2023-06-22 15:20:31,018 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.79      0.79      0.79        70
           2       0.00      0.00      0.00         3
           3       0.64      0.60      0.62        65
           4       0.63      0.65      0.64        60

    accuracy                           0.67       200
   macro avg       0.41      0.41      0.41       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:20:31,018 - INFO - test loss 0.02480563002449911
2023-06-22 15:20:31,018 - INFO - test acc 0.6649999618530273
2023-06-22 15:20:31,101 - INFO - Distilling data from client: Client18
2023-06-22 15:20:31,102 - INFO - train loss: 0.015054898159315246
2023-06-22 15:20:31,102 - INFO - train acc: 0.84375
2023-06-22 15:20:31,110 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.78      0.86      0.82        70
           2       0.00      0.00      0.00         3
           3       0.67      0.65      0.66        65
           4       0.67      0.58      0.63        60

    accuracy                           0.69       200
   macro avg       0.42      0.42      0.42       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:20:31,111 - INFO - test loss 0.02447186639912127
2023-06-22 15:20:31,111 - INFO - test acc 0.6850000023841858
2023-06-22 15:20:31,192 - INFO - Distilling data from client: Client18
2023-06-22 15:20:31,192 - INFO - train loss: 0.017373126177957856
2023-06-22 15:20:31,192 - INFO - train acc: 0.75
2023-06-22 15:20:31,200 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.79      0.81      0.80        70
           2       0.00      0.00      0.00         3
           3       0.69      0.66      0.68        65
           4       0.69      0.72      0.70        60

    accuracy                           0.71       200
   macro avg       0.44      0.44      0.44       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:20:31,200 - INFO - test loss 0.02387580025536329
2023-06-22 15:20:31,201 - INFO - test acc 0.7149999737739563
2023-06-22 15:20:31,283 - INFO - Distilling data from client: Client18
2023-06-22 15:20:31,283 - INFO - train loss: 0.01734671648420246
2023-06-22 15:20:31,283 - INFO - train acc: 0.8125
2023-06-22 15:20:31,291 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.69      0.84      0.76        70
           2       0.12      0.33      0.18         3
           3       0.69      0.58      0.63        65
           4       0.76      0.65      0.70        60

    accuracy                           0.69       200
   macro avg       0.45      0.48      0.45       200
weighted avg       0.70      0.69      0.68       200

2023-06-22 15:20:31,292 - INFO - test loss 0.024532111486091267
2023-06-22 15:20:31,292 - INFO - test acc 0.6850000023841858
2023-06-22 15:20:31,373 - INFO - Distilling data from client: Client18
2023-06-22 15:20:31,374 - INFO - train loss: 0.018207426385008146
2023-06-22 15:20:31,374 - INFO - train acc: 0.84375
2023-06-22 15:20:31,382 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.81      0.79      0.80        70
           2       0.00      0.00      0.00         3
           3       0.63      0.69      0.66        65
           4       0.62      0.60      0.61        60

    accuracy                           0.68       200
   macro avg       0.41      0.42      0.41       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:20:31,382 - INFO - test loss 0.025032126531405113
2023-06-22 15:20:31,383 - INFO - test acc 0.6800000071525574
2023-06-22 15:20:31,465 - INFO - Distilling data from client: Client18
2023-06-22 15:20:31,465 - INFO - train loss: 0.014574527103151026
2023-06-22 15:20:31,465 - INFO - train acc: 0.8125
2023-06-22 15:20:31,474 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.82      0.83      0.82        70
           2       0.00      0.00      0.00         3
           3       0.67      0.60      0.63        65
           4       0.60      0.63      0.62        60

    accuracy                           0.68       200
   macro avg       0.42      0.41      0.41       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:20:31,474 - INFO - test loss 0.024941922081425396
2023-06-22 15:20:31,474 - INFO - test acc 0.675000011920929
2023-06-22 15:20:31,557 - INFO - Distilling data from client: Client18
2023-06-22 15:20:31,557 - INFO - train loss: 0.018939522304807572
2023-06-22 15:20:31,557 - INFO - train acc: 0.84375
2023-06-22 15:20:31,570 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.78      0.81      0.80        70
           2       0.00      0.00      0.00         3
           3       0.66      0.72      0.69        65
           4       0.82      0.67      0.73        60

    accuracy                           0.72       200
   macro avg       0.45      0.44      0.44       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:20:31,570 - INFO - test loss 0.024562585047314494
2023-06-22 15:20:31,571 - INFO - test acc 0.7199999690055847
2023-06-22 15:20:31,575 - WARNING - Finished tracing + transforming jit(gather) in 0.0004458427429199219 sec
2023-06-22 15:20:31,575 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[32,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:20:31,578 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0024700164794921875 sec
2023-06-22 15:20:31,578 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:20:31,596 - WARNING - Finished XLA compilation of jit(gather) in 0.01725935935974121 sec
2023-06-22 15:20:31,612 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:32,150 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:32,162 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:32,174 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:32,190 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:32,205 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:32,218 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:32,230 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:32,242 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:20:32,801 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client18//synthetic.png
2023-06-22 15:20:32,817 - INFO - c: 0.0 and total_data_in_this_class: 273
2023-06-22 15:20:32,817 - INFO - c: 4.0 and total_data_in_this_class: 256
2023-06-22 15:20:32,817 - INFO - c: 9.0 and total_data_in_this_class: 270
2023-06-22 15:20:32,818 - INFO - c: 0.0 and total_data_in_this_class: 60
2023-06-22 15:20:32,818 - INFO - c: 4.0 and total_data_in_this_class: 77
2023-06-22 15:20:32,818 - INFO - c: 9.0 and total_data_in_this_class: 63
2023-06-22 15:20:32,933 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07172799110412598 sec
2023-06-22 15:20:33,007 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07261371612548828 sec
2023-06-22 15:20:33,015 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.1556098461151123 sec
2023-06-22 15:20:33,018 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:20:33,071 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05321145057678223 sec
2023-06-22 15:20:33,072 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:20:33,244 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17204713821411133 sec
2023-06-22 15:20:33,286 - INFO - initial test loss: 0.021735871310927992
2023-06-22 15:20:33,286 - INFO - initial test acc: 0.7249999642372131
2023-06-22 15:20:33,304 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012663841247558594 sec
2023-06-22 15:20:33,500 - WARNING - Finished tracing + transforming update_fn for pjit in 0.20931172370910645 sec
2023-06-22 15:20:33,505 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:20:33,603 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.09781432151794434 sec
2023-06-22 15:20:33,603 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:20:34,028 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4241762161254883 sec
2023-06-22 15:20:36,803 - INFO - Distilling data from client: Client19
2023-06-22 15:20:36,803 - INFO - train loss: 0.002021366588676255
2023-06-22 15:20:36,803 - INFO - train acc: 1.0
2023-06-22 15:20:36,999 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.75      0.77        60
           4       0.77      0.88      0.82        77
           9       0.84      0.73      0.78        63

    accuracy                           0.80       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.80      0.80      0.79       200

2023-06-22 15:20:36,999 - INFO - test loss 0.019784103008676803
2023-06-22 15:20:36,999 - INFO - test acc 0.7949999570846558
2023-06-22 15:20:39,754 - INFO - Distilling data from client: Client19
2023-06-22 15:20:39,754 - INFO - train loss: 0.0009861332277768173
2023-06-22 15:20:39,755 - INFO - train acc: 0.9980506896972656
2023-06-22 15:20:39,806 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.78        60
           4       0.73      0.81      0.77        77
           9       0.78      0.73      0.75        63

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.77       200
weighted avg       0.77      0.77      0.76       200

2023-06-22 15:20:39,807 - INFO - test loss 0.020219124692027135
2023-06-22 15:20:39,807 - INFO - test acc 0.7649999856948853
2023-06-22 15:20:42,495 - INFO - Distilling data from client: Client19
2023-06-22 15:20:42,496 - INFO - train loss: 0.0007887434765955596
2023-06-22 15:20:42,496 - INFO - train acc: 1.0
2023-06-22 15:20:42,552 - INFO - report:               precision    recall  f1-score   support

           0       0.82      0.75      0.78        60
           4       0.77      0.83      0.80        77
           9       0.77      0.76      0.77        63

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.79      0.79      0.78       200

2023-06-22 15:20:42,554 - INFO - test loss 0.019944972036130014
2023-06-22 15:20:42,554 - INFO - test acc 0.7849999666213989
2023-06-22 15:20:45,311 - INFO - Distilling data from client: Client19
2023-06-22 15:20:45,312 - INFO - train loss: 0.0006835917300205466
2023-06-22 15:20:45,312 - INFO - train acc: 1.0
2023-06-22 15:20:45,372 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.73      0.76        60
           4       0.74      0.82      0.78        77
           9       0.80      0.75      0.77        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:20:45,372 - INFO - test loss 0.020350987854670717
2023-06-22 15:20:45,373 - INFO - test acc 0.7699999809265137
2023-06-22 15:20:48,224 - INFO - Distilling data from client: Client19
2023-06-22 15:20:48,224 - INFO - train loss: 0.0005488972681500588
2023-06-22 15:20:48,224 - INFO - train acc: 1.0
2023-06-22 15:20:48,301 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.75      0.77        60
           4       0.72      0.82      0.77        77
           9       0.77      0.68      0.72        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:20:48,301 - INFO - test loss 0.020410292677743875
2023-06-22 15:20:48,302 - INFO - test acc 0.7549999952316284
2023-06-22 15:20:51,097 - INFO - Distilling data from client: Client19
2023-06-22 15:20:51,097 - INFO - train loss: 0.0005127397104157015
2023-06-22 15:20:51,098 - INFO - train acc: 1.0
2023-06-22 15:20:51,164 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.73      0.77        60
           4       0.72      0.81      0.76        77
           9       0.76      0.71      0.74        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:20:51,164 - INFO - test loss 0.02098808346166045
2023-06-22 15:20:51,164 - INFO - test acc 0.7549999952316284
2023-06-22 15:20:53,963 - INFO - Distilling data from client: Client19
2023-06-22 15:20:53,963 - INFO - train loss: 0.0005016543446027315
2023-06-22 15:20:53,964 - INFO - train acc: 1.0
2023-06-22 15:20:54,024 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.73      0.76        60
           4       0.72      0.81      0.76        77
           9       0.78      0.71      0.74        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:20:54,025 - INFO - test loss 0.020796461176041243
2023-06-22 15:20:54,025 - INFO - test acc 0.7549999952316284
2023-06-22 15:20:56,762 - INFO - Distilling data from client: Client19
2023-06-22 15:20:56,763 - INFO - train loss: 0.000518805454343137
2023-06-22 15:20:56,763 - INFO - train acc: 1.0
2023-06-22 15:20:56,826 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.78        60
           4       0.72      0.81      0.76        77
           9       0.78      0.71      0.74        63

    accuracy                           0.76       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:20:56,829 - INFO - test loss 0.02060621735147891
2023-06-22 15:20:56,830 - INFO - test acc 0.7599999904632568
2023-06-22 15:20:59,566 - INFO - Distilling data from client: Client19
2023-06-22 15:20:59,566 - INFO - train loss: 0.0004944005093730553
2023-06-22 15:20:59,566 - INFO - train acc: 1.0
2023-06-22 15:20:59,631 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.72      0.77        60
           4       0.71      0.82      0.76        77
           9       0.76      0.71      0.74        63

    accuracy                           0.76       200
   macro avg       0.77      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:20:59,632 - INFO - test loss 0.02082977185603823
2023-06-22 15:20:59,632 - INFO - test acc 0.7549999952316284
2023-06-22 15:21:02,527 - INFO - Distilling data from client: Client19
2023-06-22 15:21:02,528 - INFO - train loss: 0.00037341475744025654
2023-06-22 15:21:02,529 - INFO - train acc: 1.0
2023-06-22 15:21:02,595 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.73      0.77        60
           4       0.73      0.79      0.76        77
           9       0.77      0.75      0.76        63

    accuracy                           0.76       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:21:02,600 - INFO - test loss 0.02069986034600244
2023-06-22 15:21:02,600 - INFO - test acc 0.7599999904632568
2023-06-22 15:21:05,408 - INFO - Distilling data from client: Client19
2023-06-22 15:21:05,410 - INFO - train loss: 0.0003917396718631925
2023-06-22 15:21:05,411 - INFO - train acc: 1.0
2023-06-22 15:21:05,470 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.72      0.77        60
           4       0.72      0.82      0.76        77
           9       0.77      0.73      0.75        63

    accuracy                           0.76       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:21:05,471 - INFO - test loss 0.020993300722449128
2023-06-22 15:21:05,472 - INFO - test acc 0.7599999904632568
2023-06-22 15:21:08,360 - INFO - Distilling data from client: Client19
2023-06-22 15:21:08,360 - INFO - train loss: 0.00035813073464921035
2023-06-22 15:21:08,360 - INFO - train acc: 1.0
2023-06-22 15:21:08,421 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.72      0.77        60
           4       0.71      0.83      0.77        77
           9       0.76      0.70      0.73        63

    accuracy                           0.76       200
   macro avg       0.77      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:21:08,422 - INFO - test loss 0.020861808093519
2023-06-22 15:21:08,423 - INFO - test acc 0.7549999952316284
2023-06-22 15:21:11,374 - INFO - Distilling data from client: Client19
2023-06-22 15:21:11,375 - INFO - train loss: 0.0003860842479853228
2023-06-22 15:21:11,375 - INFO - train acc: 1.0
2023-06-22 15:21:11,440 - INFO - report:               precision    recall  f1-score   support

           0       0.82      0.75      0.78        60
           4       0.73      0.83      0.78        77
           9       0.77      0.70      0.73        63

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.77      0.76       200

2023-06-22 15:21:11,440 - INFO - test loss 0.020715849877511565
2023-06-22 15:21:11,441 - INFO - test acc 0.7649999856948853
2023-06-22 15:21:14,343 - INFO - Distilling data from client: Client19
2023-06-22 15:21:14,344 - INFO - train loss: 0.00026499743750246665
2023-06-22 15:21:14,344 - INFO - train acc: 1.0
2023-06-22 15:21:14,406 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.73      0.77        60
           4       0.72      0.82      0.77        77
           9       0.76      0.70      0.73        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:21:14,412 - INFO - test loss 0.020914903147018393
2023-06-22 15:21:14,413 - INFO - test acc 0.7549999952316284
2023-06-22 15:21:17,253 - INFO - Distilling data from client: Client19
2023-06-22 15:21:17,253 - INFO - train loss: 0.00040841165119669646
2023-06-22 15:21:17,254 - INFO - train acc: 1.0
2023-06-22 15:21:17,319 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.70      0.73        60
           4       0.72      0.82      0.77        77
           9       0.76      0.70      0.73        63

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:21:17,319 - INFO - test loss 0.02083220103396183
2023-06-22 15:21:17,320 - INFO - test acc 0.7450000047683716
2023-06-22 15:21:20,175 - INFO - Distilling data from client: Client19
2023-06-22 15:21:20,175 - INFO - train loss: 0.00026661891995807883
2023-06-22 15:21:20,176 - INFO - train acc: 1.0
2023-06-22 15:21:20,225 - INFO - report:               precision    recall  f1-score   support

           0       0.81      0.77      0.79        60
           4       0.76      0.82      0.79        77
           9       0.78      0.75      0.76        63

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:21:20,226 - INFO - test loss 0.020421932246221033
2023-06-22 15:21:20,226 - INFO - test acc 0.7799999713897705
2023-06-22 15:21:23,006 - INFO - Distilling data from client: Client19
2023-06-22 15:21:23,006 - INFO - train loss: 0.00022236948881764178
2023-06-22 15:21:23,007 - INFO - train acc: 1.0
2023-06-22 15:21:23,079 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.78        60
           4       0.73      0.82      0.77        77
           9       0.76      0.70      0.73        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:21:23,082 - INFO - test loss 0.02100200875644377
2023-06-22 15:21:23,082 - INFO - test acc 0.7599999904632568
2023-06-22 15:21:25,896 - INFO - Distilling data from client: Client19
2023-06-22 15:21:25,896 - INFO - train loss: 0.00019068764188506784
2023-06-22 15:21:25,897 - INFO - train acc: 1.0
2023-06-22 15:21:25,960 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.75      0.79        60
           4       0.72      0.83      0.77        77
           9       0.77      0.70      0.73        63

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.77      0.76       200

2023-06-22 15:21:25,961 - INFO - test loss 0.02070065293184782
2023-06-22 15:21:25,961 - INFO - test acc 0.7649999856948853
2023-06-22 15:21:28,770 - INFO - Distilling data from client: Client19
2023-06-22 15:21:28,770 - INFO - train loss: 0.00019366088481149663
2023-06-22 15:21:28,770 - INFO - train acc: 1.0
2023-06-22 15:21:28,825 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.78        60
           4       0.74      0.83      0.79        77
           9       0.78      0.71      0.74        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:21:28,826 - INFO - test loss 0.020625018272104122
2023-06-22 15:21:28,827 - INFO - test acc 0.7699999809265137
2023-06-22 15:21:31,658 - INFO - Distilling data from client: Client19
2023-06-22 15:21:31,658 - INFO - train loss: 0.0002451988837846068
2023-06-22 15:21:31,658 - INFO - train acc: 1.0
2023-06-22 15:21:31,716 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.75      0.76        60
           4       0.73      0.84      0.78        77
           9       0.75      0.63      0.69        63

    accuracy                           0.75       200
   macro avg       0.75      0.74      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-06-22 15:21:31,717 - INFO - test loss 0.021154388699964416
2023-06-22 15:21:31,717 - INFO - test acc 0.75
2023-06-22 15:21:34,580 - INFO - Distilling data from client: Client19
2023-06-22 15:21:34,581 - INFO - train loss: 0.00024111262191162493
2023-06-22 15:21:34,581 - INFO - train acc: 1.0
2023-06-22 15:21:34,634 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.73      0.76        60
           4       0.74      0.82      0.78        77
           9       0.78      0.73      0.75        63

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.77      0.76       200

2023-06-22 15:21:34,635 - INFO - test loss 0.02051026704138977
2023-06-22 15:21:34,635 - INFO - test acc 0.7649999856948853
2023-06-22 15:21:37,385 - INFO - Distilling data from client: Client19
2023-06-22 15:21:37,386 - INFO - train loss: 0.0002770189301182644
2023-06-22 15:21:37,386 - INFO - train acc: 1.0
2023-06-22 15:21:37,449 - INFO - report:               precision    recall  f1-score   support

           0       0.81      0.72      0.76        60
           4       0.72      0.82      0.76        77
           9       0.76      0.71      0.74        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:21:37,449 - INFO - test loss 0.020983001513945405
2023-06-22 15:21:37,450 - INFO - test acc 0.7549999952316284
2023-06-22 15:21:40,234 - INFO - Distilling data from client: Client19
2023-06-22 15:21:40,234 - INFO - train loss: 0.00023859021946886664
2023-06-22 15:21:40,235 - INFO - train acc: 1.0
2023-06-22 15:21:40,310 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.73      0.75        60
           4       0.74      0.82      0.78        77
           9       0.79      0.71      0.75        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:21:40,312 - INFO - test loss 0.020807154595074248
2023-06-22 15:21:40,312 - INFO - test acc 0.7599999904632568
2023-06-22 15:21:43,146 - INFO - Distilling data from client: Client19
2023-06-22 15:21:43,146 - INFO - train loss: 0.00027364614181647854
2023-06-22 15:21:43,146 - INFO - train acc: 1.0
2023-06-22 15:21:43,211 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.72      0.75        60
           4       0.72      0.84      0.78        77
           9       0.77      0.68      0.72        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:21:43,212 - INFO - test loss 0.02105578538865917
2023-06-22 15:21:43,214 - INFO - test acc 0.7549999952316284
2023-06-22 15:21:45,988 - INFO - Distilling data from client: Client19
2023-06-22 15:21:45,989 - INFO - train loss: 0.00019575957511988061
2023-06-22 15:21:45,991 - INFO - train acc: 1.0
2023-06-22 15:21:46,046 - INFO - report:               precision    recall  f1-score   support

           0       0.84      0.72      0.77        60
           4       0.70      0.83      0.76        77
           9       0.76      0.70      0.73        63

    accuracy                           0.76       200
   macro avg       0.77      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:21:46,046 - INFO - test loss 0.02122058953897887
2023-06-22 15:21:46,047 - INFO - test acc 0.7549999952316284
2023-06-22 15:21:46,078 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:21:46,108 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:21:46,123 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:21:46,142 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:21:46,154 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:21:46,165 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:21:46,177 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:21:46,189 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:21:46,200 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:21:46,751 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client19//synthetic.png
2023-06-22 15:21:46,772 - INFO - c: 0.0 and total_data_in_this_class: 260
2023-06-22 15:21:46,772 - INFO - c: 4.0 and total_data_in_this_class: 539
2023-06-22 15:21:46,772 - INFO - c: 0.0 and total_data_in_this_class: 73
2023-06-22 15:21:46,772 - INFO - c: 4.0 and total_data_in_this_class: 127
2023-06-22 15:21:46,814 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0004935264587402344 sec
2023-06-22 15:21:46,815 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:21:46,817 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.002274751663208008 sec
2023-06-22 15:21:46,818 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:21:46,833 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.015594720840454102 sec
2023-06-22 15:21:46,837 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.000316619873046875 sec
2023-06-22 15:21:46,837 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:21:46,839 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001566171646118164 sec
2023-06-22 15:21:46,839 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:21:46,851 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011449813842773438 sec
2023-06-22 15:21:46,856 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022220611572265625 sec
2023-06-22 15:21:46,858 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002071857452392578 sec
2023-06-22 15:21:46,859 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005288124084472656 sec
2023-06-22 15:21:46,861 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003440380096435547 sec
2023-06-22 15:21:46,862 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001900196075439453 sec
2023-06-22 15:21:46,863 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004858970642089844 sec
2023-06-22 15:21:46,864 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004215240478515625 sec
2023-06-22 15:21:46,865 - WARNING - Finished tracing + transforming absolute for pjit in 0.00031495094299316406 sec
2023-06-22 15:21:46,866 - WARNING - Finished tracing + transforming fn for pjit in 0.0004582405090332031 sec
2023-06-22 15:21:46,867 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005733966827392578 sec
2023-06-22 15:21:46,869 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003669261932373047 sec
2023-06-22 15:21:46,871 - WARNING - Finished tracing + transforming fn for pjit in 0.0004048347473144531 sec
2023-06-22 15:21:46,872 - WARNING - Finished tracing + transforming fn for pjit in 0.00045680999755859375 sec
2023-06-22 15:21:46,873 - WARNING - Finished tracing + transforming fn for pjit in 0.00038933753967285156 sec
2023-06-22 15:21:46,874 - WARNING - Finished tracing + transforming fn for pjit in 0.00046753883361816406 sec
2023-06-22 15:21:46,876 - WARNING - Finished tracing + transforming fn for pjit in 0.0003781318664550781 sec
2023-06-22 15:21:46,879 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002779960632324219 sec
2023-06-22 15:21:46,880 - WARNING - Finished tracing + transforming fn for pjit in 0.00039887428283691406 sec
2023-06-22 15:21:46,882 - WARNING - Finished tracing + transforming fn for pjit in 0.0003981590270996094 sec
2023-06-22 15:21:46,888 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005955696105957031 sec
2023-06-22 15:21:46,888 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016498565673828125 sec
2023-06-22 15:21:46,890 - WARNING - Finished tracing + transforming fn for pjit in 0.0004017353057861328 sec
2023-06-22 15:21:46,891 - WARNING - Finished tracing + transforming fn for pjit in 0.0003943443298339844 sec
2023-06-22 15:21:46,893 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005097389221191406 sec
2023-06-22 15:21:46,894 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004715919494628906 sec
2023-06-22 15:21:46,895 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003192424774169922 sec
2023-06-22 15:21:46,896 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004718303680419922 sec
2023-06-22 15:21:46,898 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040602684020996094 sec
2023-06-22 15:21:46,899 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041174888610839844 sec
2023-06-22 15:21:46,901 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006148815155029297 sec
2023-06-22 15:21:46,901 - WARNING - Finished tracing + transforming _where for pjit in 0.0016927719116210938 sec
2023-06-22 15:21:46,903 - WARNING - Finished tracing + transforming fn for pjit in 0.00045418739318847656 sec
2023-06-22 15:21:46,904 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046563148498535156 sec
2023-06-22 15:21:46,905 - WARNING - Finished tracing + transforming fn for pjit in 0.00040411949157714844 sec
2023-06-22 15:21:46,906 - WARNING - Finished tracing + transforming fn for pjit in 0.0003933906555175781 sec
2023-06-22 15:21:46,908 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003859996795654297 sec
2023-06-22 15:21:46,909 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004425048828125 sec
2023-06-22 15:21:46,910 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005319118499755859 sec
2023-06-22 15:21:46,911 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045228004455566406 sec
2023-06-22 15:21:46,913 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004031658172607422 sec
2023-06-22 15:21:46,914 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003998279571533203 sec
2023-06-22 15:21:46,915 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004763603210449219 sec
2023-06-22 15:21:46,916 - WARNING - Finished tracing + transforming _where for pjit in 0.0015397071838378906 sec
2023-06-22 15:21:46,917 - WARNING - Finished tracing + transforming fn for pjit in 0.000453948974609375 sec
2023-06-22 15:21:46,918 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004553794860839844 sec
2023-06-22 15:21:46,920 - WARNING - Finished tracing + transforming fn for pjit in 0.00037169456481933594 sec
2023-06-22 15:21:46,927 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004527568817138672 sec
2023-06-22 15:21:46,929 - WARNING - Finished tracing + transforming fn for pjit in 0.0006277561187744141 sec
2023-06-22 15:21:46,930 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004875659942626953 sec
2023-06-22 15:21:46,931 - WARNING - Finished tracing + transforming fn for pjit in 0.0003876686096191406 sec
2023-06-22 15:21:46,938 - WARNING - Finished tracing + transforming fn for pjit in 0.00032830238342285156 sec
2023-06-22 15:21:46,941 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002789497375488281 sec
2023-06-22 15:21:46,943 - WARNING - Finished tracing + transforming fn for pjit in 0.0018317699432373047 sec
2023-06-22 15:21:46,945 - WARNING - Finished tracing + transforming fn for pjit in 0.00041985511779785156 sec
2023-06-22 15:21:46,972 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11691522598266602 sec
2023-06-22 15:21:46,977 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000186920166015625 sec
2023-06-22 15:21:46,978 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002028942108154297 sec
2023-06-22 15:21:46,979 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00048041343688964844 sec
2023-06-22 15:21:46,983 - WARNING - Finished tracing + transforming fn for pjit in 0.0003304481506347656 sec
2023-06-22 15:21:46,984 - WARNING - Finished tracing + transforming fn for pjit in 0.0004425048828125 sec
2023-06-22 15:21:46,986 - WARNING - Finished tracing + transforming fn for pjit in 0.0003616809844970703 sec
2023-06-22 15:21:46,995 - WARNING - Finished tracing + transforming fn for pjit in 0.00037288665771484375 sec
2023-06-22 15:21:46,997 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00036787986755371094 sec
2023-06-22 15:21:46,998 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00043702125549316406 sec
2023-06-22 15:21:46,999 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003113746643066406 sec
2023-06-22 15:21:47,001 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005762577056884766 sec
2023-06-22 15:21:47,002 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000392913818359375 sec
2023-06-22 15:21:47,003 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003936290740966797 sec
2023-06-22 15:21:47,005 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004734992980957031 sec
2023-06-22 15:21:47,005 - WARNING - Finished tracing + transforming _where for pjit in 0.0015137195587158203 sec
2023-06-22 15:21:47,007 - WARNING - Finished tracing + transforming fn for pjit in 0.00045943260192871094 sec
2023-06-22 15:21:47,008 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004582405090332031 sec
2023-06-22 15:21:47,009 - WARNING - Finished tracing + transforming fn for pjit in 0.0003936290740966797 sec
2023-06-22 15:21:47,010 - WARNING - Finished tracing + transforming fn for pjit in 0.0005161762237548828 sec
2023-06-22 15:21:47,030 - WARNING - Finished tracing + transforming fn for pjit in 0.0003600120544433594 sec
2023-06-22 15:21:47,060 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0863797664642334 sec
2023-06-22 15:21:47,062 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020170211791992188 sec
2023-06-22 15:21:47,064 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002307891845703125 sec
2023-06-22 15:21:47,064 - WARNING - Finished tracing + transforming _where for pjit in 0.001071929931640625 sec
2023-06-22 15:21:47,065 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005080699920654297 sec
2023-06-22 15:21:47,066 - WARNING - Finished tracing + transforming trace for pjit in 0.0043718814849853516 sec
2023-06-22 15:21:47,070 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00018739700317382812 sec
2023-06-22 15:21:47,071 - WARNING - Finished tracing + transforming tril for pjit in 0.0010907649993896484 sec
2023-06-22 15:21:47,072 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.002977609634399414 sec
2023-06-22 15:21:47,074 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019550323486328125 sec
2023-06-22 15:21:47,074 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019049644470214844 sec
2023-06-22 15:21:47,078 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0022928714752197266 sec
2023-06-22 15:21:47,084 - WARNING - Finished tracing + transforming _solve for pjit in 0.015502452850341797 sec
2023-06-22 15:21:47,085 - WARNING - Finished tracing + transforming dot for pjit in 0.0005345344543457031 sec
2023-06-22 15:21:47,089 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.23642230033874512 sec
2023-06-22 15:21:47,093 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:21:47,146 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05274248123168945 sec
2023-06-22 15:21:47,146 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:21:47,318 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17140460014343262 sec
2023-06-22 15:21:47,348 - INFO - initial test loss: 0.017331103488643827
2023-06-22 15:21:47,349 - INFO - initial test acc: 0.8050000071525574
2023-06-22 15:21:47,360 - WARNING - Finished tracing + transforming dot for pjit in 0.0008199214935302734 sec
2023-06-22 15:21:47,362 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006797313690185547 sec
2023-06-22 15:21:47,364 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006682872772216797 sec
2023-06-22 15:21:47,365 - WARNING - Finished tracing + transforming _mean for pjit in 0.0023796558380126953 sec
2023-06-22 15:21:47,367 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004336833953857422 sec
2023-06-22 15:21:47,369 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0005557537078857422 sec
2023-06-22 15:21:47,370 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005695819854736328 sec
2023-06-22 15:21:47,372 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008261203765869141 sec
2023-06-22 15:21:47,374 - WARNING - Finished tracing + transforming _mean for pjit in 0.002463102340698242 sec
2023-06-22 15:21:47,375 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.022695064544677734 sec
2023-06-22 15:21:47,394 - WARNING - Finished tracing + transforming fn for pjit in 0.0005965232849121094 sec
2023-06-22 15:21:47,396 - WARNING - Finished tracing + transforming fn for pjit in 0.0007560253143310547 sec
2023-06-22 15:21:47,397 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005128383636474609 sec
2023-06-22 15:21:47,399 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006287097930908203 sec
2023-06-22 15:21:47,400 - WARNING - Finished tracing + transforming _where for pjit in 0.0020437240600585938 sec
2023-06-22 15:21:47,420 - WARNING - Finished tracing + transforming fn for pjit in 0.0006079673767089844 sec
2023-06-22 15:21:47,422 - WARNING - Finished tracing + transforming fn for pjit in 0.0006353855133056641 sec
2023-06-22 15:21:47,423 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005221366882324219 sec
2023-06-22 15:21:47,425 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006172657012939453 sec
2023-06-22 15:21:47,426 - WARNING - Finished tracing + transforming _where for pjit in 0.002088308334350586 sec
2023-06-22 15:21:47,486 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003383159637451172 sec
2023-06-22 15:21:47,565 - WARNING - Finished tracing + transforming fn for pjit in 0.0004584789276123047 sec
2023-06-22 15:21:47,567 - WARNING - Finished tracing + transforming fn for pjit in 0.00039577484130859375 sec
2023-06-22 15:21:47,568 - WARNING - Finished tracing + transforming square for pjit in 0.00030350685119628906 sec
2023-06-22 15:21:47,571 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00035309791564941406 sec
2023-06-22 15:21:47,574 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004534721374511719 sec
2023-06-22 15:21:47,575 - WARNING - Finished tracing + transforming fn for pjit in 0.0004634857177734375 sec
2023-06-22 15:21:47,576 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00039076805114746094 sec
2023-06-22 15:21:47,577 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003809928894042969 sec
2023-06-22 15:21:47,578 - WARNING - Finished tracing + transforming fn for pjit in 0.0004763603210449219 sec
2023-06-22 15:21:47,580 - WARNING - Finished tracing + transforming fn for pjit in 0.0003962516784667969 sec
2023-06-22 15:21:47,581 - WARNING - Finished tracing + transforming square for pjit in 0.0003161430358886719 sec
2023-06-22 15:21:47,584 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00036525726318359375 sec
2023-06-22 15:21:47,587 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032520294189453125 sec
2023-06-22 15:21:47,588 - WARNING - Finished tracing + transforming fn for pjit in 0.0004649162292480469 sec
2023-06-22 15:21:47,589 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003864765167236328 sec
2023-06-22 15:21:47,590 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040721893310546875 sec
2023-06-22 15:21:47,591 - WARNING - Finished tracing + transforming update_fn for pjit in 0.23988032341003418 sec
2023-06-22 15:21:47,597 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,10]), ShapedArray(float32[346,10]), ShapedArray(float32[346,10]), ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,10]), ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:21:47,695 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.09791946411132812 sec
2023-06-22 15:21:47,696 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:21:48,118 - WARNING - Finished XLA compilation of jit(update_fn) in 0.42160868644714355 sec
2023-06-22 15:21:50,164 - INFO - Distilling data from client: Client20
2023-06-22 15:21:50,164 - INFO - train loss: 0.0028150287832134748
2023-06-22 15:21:50,165 - INFO - train acc: 0.9913294315338135
2023-06-22 15:21:50,329 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.67      0.73        73
           4       0.83      0.91      0.86       127

    accuracy                           0.82       200
   macro avg       0.82      0.79      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-06-22 15:21:50,329 - INFO - test loss 0.014632735471335371
2023-06-22 15:21:50,329 - INFO - test acc 0.8199999928474426
2023-06-22 15:21:52,551 - INFO - Distilling data from client: Client20
2023-06-22 15:21:52,551 - INFO - train loss: 0.0019477142435065206
2023-06-22 15:21:52,552 - INFO - train acc: 0.9942196607589722
2023-06-22 15:21:52,595 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.67      0.70        73
           4       0.82      0.86      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:21:52,595 - INFO - test loss 0.015688207310096833
2023-06-22 15:21:52,596 - INFO - test acc 0.7899999618530273
2023-06-22 15:21:54,826 - INFO - Distilling data from client: Client20
2023-06-22 15:21:54,827 - INFO - train loss: 0.0017571281800469428
2023-06-22 15:21:54,828 - INFO - train acc: 0.9971098303794861
2023-06-22 15:21:54,880 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.63      0.68        73
           4       0.80      0.87      0.83       127

    accuracy                           0.78       200
   macro avg       0.77      0.75      0.75       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:21:54,881 - INFO - test loss 0.015798278237620283
2023-06-22 15:21:54,882 - INFO - test acc 0.7799999713897705
2023-06-22 15:21:57,146 - INFO - Distilling data from client: Client20
2023-06-22 15:21:57,147 - INFO - train loss: 0.001461559735697069
2023-06-22 15:21:57,147 - INFO - train acc: 1.0
2023-06-22 15:21:57,213 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.63      0.70        73
           4       0.81      0.91      0.86       127

    accuracy                           0.81       200
   macro avg       0.80      0.77      0.78       200
weighted avg       0.80      0.81      0.80       200

2023-06-22 15:21:57,215 - INFO - test loss 0.015608014950782895
2023-06-22 15:21:57,216 - INFO - test acc 0.8050000071525574
2023-06-22 15:21:59,451 - INFO - Distilling data from client: Client20
2023-06-22 15:21:59,452 - INFO - train loss: 0.0013942868252659261
2023-06-22 15:21:59,452 - INFO - train acc: 0.9913294315338135
2023-06-22 15:21:59,505 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.66      0.71        73
           4       0.82      0.89      0.85       127

    accuracy                           0.81       200
   macro avg       0.80      0.77      0.78       200
weighted avg       0.80      0.81      0.80       200

2023-06-22 15:21:59,505 - INFO - test loss 0.015018256857969259
2023-06-22 15:21:59,505 - INFO - test acc 0.8050000071525574
2023-06-22 15:22:01,627 - INFO - Distilling data from client: Client20
2023-06-22 15:22:01,627 - INFO - train loss: 0.0011063022530328269
2023-06-22 15:22:01,628 - INFO - train acc: 1.0
2023-06-22 15:22:01,670 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.64      0.70        73
           4       0.81      0.88      0.85       127

    accuracy                           0.80       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-06-22 15:22:01,670 - INFO - test loss 0.015277591876334477
2023-06-22 15:22:01,670 - INFO - test acc 0.7949999570846558
2023-06-22 15:22:03,813 - INFO - Distilling data from client: Client20
2023-06-22 15:22:03,814 - INFO - train loss: 0.001186050953024519
2023-06-22 15:22:03,815 - INFO - train acc: 1.0
2023-06-22 15:22:03,880 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.67      0.71        73
           4       0.82      0.87      0.84       127

    accuracy                           0.80       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-06-22 15:22:03,881 - INFO - test loss 0.016052383254096282
2023-06-22 15:22:03,881 - INFO - test acc 0.7949999570846558
2023-06-22 15:22:06,155 - INFO - Distilling data from client: Client20
2023-06-22 15:22:06,155 - INFO - train loss: 0.001227529153104655
2023-06-22 15:22:06,155 - INFO - train acc: 0.9971098303794861
2023-06-22 15:22:06,214 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.62      0.68        73
           4       0.80      0.88      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.75      0.76       200
weighted avg       0.78      0.79      0.78       200

2023-06-22 15:22:06,215 - INFO - test loss 0.01580815297941291
2023-06-22 15:22:06,216 - INFO - test acc 0.7849999666213989
2023-06-22 15:22:08,294 - INFO - Distilling data from client: Client20
2023-06-22 15:22:08,294 - INFO - train loss: 0.00116777365312104
2023-06-22 15:22:08,294 - INFO - train acc: 1.0
2023-06-22 15:22:08,345 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.66      0.72        73
           4       0.82      0.91      0.86       127

    accuracy                           0.81       200
   macro avg       0.81      0.78      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:22:08,348 - INFO - test loss 0.015958284811911895
2023-06-22 15:22:08,349 - INFO - test acc 0.8149999976158142
2023-06-22 15:22:10,664 - INFO - Distilling data from client: Client20
2023-06-22 15:22:10,664 - INFO - train loss: 0.0009689227375530853
2023-06-22 15:22:10,664 - INFO - train acc: 1.0
2023-06-22 15:22:10,724 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.60      0.68        73
           4       0.80      0.90      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.75      0.76       200
weighted avg       0.79      0.79      0.78       200

2023-06-22 15:22:10,724 - INFO - test loss 0.01628507433065039
2023-06-22 15:22:10,725 - INFO - test acc 0.7899999618530273
2023-06-22 15:22:12,939 - INFO - Distilling data from client: Client20
2023-06-22 15:22:12,939 - INFO - train loss: 0.000988548487043062
2023-06-22 15:22:12,939 - INFO - train acc: 1.0
2023-06-22 15:22:12,982 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.64      0.69        73
           4       0.81      0.87      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:22:12,982 - INFO - test loss 0.01631970043668822
2023-06-22 15:22:12,982 - INFO - test acc 0.7899999618530273
2023-06-22 15:22:15,109 - INFO - Distilling data from client: Client20
2023-06-22 15:22:15,110 - INFO - train loss: 0.0009784071982406764
2023-06-22 15:22:15,111 - INFO - train acc: 1.0
2023-06-22 15:22:15,177 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.58      0.66        73
           4       0.79      0.91      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.74      0.75       200
weighted avg       0.78      0.79      0.78       200

2023-06-22 15:22:15,177 - INFO - test loss 0.01620035604364994
2023-06-22 15:22:15,178 - INFO - test acc 0.7849999666213989
2023-06-22 15:22:17,415 - INFO - Distilling data from client: Client20
2023-06-22 15:22:17,415 - INFO - train loss: 0.0008104220515516891
2023-06-22 15:22:17,415 - INFO - train acc: 1.0
2023-06-22 15:22:17,468 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.63      0.69        73
           4       0.81      0.88      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.76      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:22:17,468 - INFO - test loss 0.01621222093759926
2023-06-22 15:22:17,469 - INFO - test acc 0.7899999618530273
2023-06-22 15:22:19,685 - INFO - Distilling data from client: Client20
2023-06-22 15:22:19,685 - INFO - train loss: 0.0009598423581389901
2023-06-22 15:22:19,685 - INFO - train acc: 1.0
2023-06-22 15:22:19,737 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.62      0.69        73
           4       0.80      0.90      0.85       127

    accuracy                           0.80       200
   macro avg       0.79      0.76      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-06-22 15:22:19,739 - INFO - test loss 0.015769055439498438
2023-06-22 15:22:19,740 - INFO - test acc 0.7949999570846558
2023-06-22 15:22:21,906 - INFO - Distilling data from client: Client20
2023-06-22 15:22:21,906 - INFO - train loss: 0.0008515473856247879
2023-06-22 15:22:21,907 - INFO - train acc: 1.0
2023-06-22 15:22:21,959 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.66      0.70        73
           4       0.81      0.87      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:22:21,960 - INFO - test loss 0.016450702743647135
2023-06-22 15:22:21,961 - INFO - test acc 0.7899999618530273
2023-06-22 15:22:24,066 - INFO - Distilling data from client: Client20
2023-06-22 15:22:24,066 - INFO - train loss: 0.0007858540793142815
2023-06-22 15:22:24,066 - INFO - train acc: 0.9971098303794861
2023-06-22 15:22:24,119 - INFO - report:               precision    recall  f1-score   support

           0       0.81      0.66      0.73        73
           4       0.82      0.91      0.87       127

    accuracy                           0.82       200
   macro avg       0.82      0.79      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-06-22 15:22:24,120 - INFO - test loss 0.016696614993456153
2023-06-22 15:22:24,120 - INFO - test acc 0.8199999928474426
2023-06-22 15:22:26,347 - INFO - Distilling data from client: Client20
2023-06-22 15:22:26,348 - INFO - train loss: 0.0008276240950127898
2023-06-22 15:22:26,348 - INFO - train acc: 1.0
2023-06-22 15:22:26,410 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.63      0.69        73
           4       0.81      0.89      0.85       127

    accuracy                           0.80       200
   macro avg       0.79      0.76      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-06-22 15:22:26,413 - INFO - test loss 0.01616317612768067
2023-06-22 15:22:26,414 - INFO - test acc 0.7949999570846558
2023-06-22 15:22:28,791 - INFO - Distilling data from client: Client20
2023-06-22 15:22:28,792 - INFO - train loss: 0.0009155206528025027
2023-06-22 15:22:28,793 - INFO - train acc: 1.0
2023-06-22 15:22:28,855 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.62      0.69        73
           4       0.80      0.90      0.85       127

    accuracy                           0.80       200
   macro avg       0.79      0.76      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-06-22 15:22:28,856 - INFO - test loss 0.016086354914759255
2023-06-22 15:22:28,857 - INFO - test acc 0.7949999570846558
2023-06-22 15:22:31,030 - INFO - Distilling data from client: Client20
2023-06-22 15:22:31,030 - INFO - train loss: 0.0009354473326380723
2023-06-22 15:22:31,030 - INFO - train acc: 1.0
2023-06-22 15:22:31,070 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.62      0.71        73
           4       0.81      0.93      0.86       127

    accuracy                           0.81       200
   macro avg       0.82      0.77      0.79       200
weighted avg       0.82      0.81      0.81       200

2023-06-22 15:22:31,070 - INFO - test loss 0.016060616428782204
2023-06-22 15:22:31,071 - INFO - test acc 0.8149999976158142
2023-06-22 15:22:33,364 - INFO - Distilling data from client: Client20
2023-06-22 15:22:33,364 - INFO - train loss: 0.0007342933231867455
2023-06-22 15:22:33,365 - INFO - train acc: 1.0
2023-06-22 15:22:33,407 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.60      0.68        73
           4       0.80      0.91      0.85       127

    accuracy                           0.80       200
   macro avg       0.79      0.75      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-06-22 15:22:33,408 - INFO - test loss 0.015583203325484551
2023-06-22 15:22:33,408 - INFO - test acc 0.7949999570846558
2023-06-22 15:22:35,627 - INFO - Distilling data from client: Client20
2023-06-22 15:22:35,627 - INFO - train loss: 0.0007509305678992433
2023-06-22 15:22:35,628 - INFO - train acc: 1.0
2023-06-22 15:22:35,677 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.64      0.71        73
           4       0.82      0.91      0.86       127

    accuracy                           0.81       200
   macro avg       0.81      0.77      0.79       200
weighted avg       0.81      0.81      0.80       200

2023-06-22 15:22:35,678 - INFO - test loss 0.016203046843355395
2023-06-22 15:22:35,679 - INFO - test acc 0.8100000023841858
2023-06-22 15:22:37,860 - INFO - Distilling data from client: Client20
2023-06-22 15:22:37,861 - INFO - train loss: 0.0007841716190400917
2023-06-22 15:22:37,862 - INFO - train acc: 1.0
2023-06-22 15:22:37,921 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.63      0.68        73
           4       0.80      0.87      0.84       127

    accuracy                           0.79       200
   macro avg       0.77      0.75      0.76       200
weighted avg       0.78      0.79      0.78       200

2023-06-22 15:22:37,924 - INFO - test loss 0.015613798937798792
2023-06-22 15:22:37,925 - INFO - test acc 0.7849999666213989
2023-06-22 15:22:40,065 - INFO - Distilling data from client: Client20
2023-06-22 15:22:40,065 - INFO - train loss: 0.0008563750107319399
2023-06-22 15:22:40,065 - INFO - train acc: 1.0
2023-06-22 15:22:40,111 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.62      0.69        73
           4       0.80      0.90      0.85       127

    accuracy                           0.80       200
   macro avg       0.79      0.76      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-06-22 15:22:40,112 - INFO - test loss 0.016464388977255957
2023-06-22 15:22:40,112 - INFO - test acc 0.7949999570846558
2023-06-22 15:22:42,345 - INFO - Distilling data from client: Client20
2023-06-22 15:22:42,345 - INFO - train loss: 0.0006478134279834824
2023-06-22 15:22:42,346 - INFO - train acc: 1.0
2023-06-22 15:22:42,403 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.58      0.65        73
           4       0.78      0.88      0.83       127

    accuracy                           0.77       200
   macro avg       0.76      0.73      0.74       200
weighted avg       0.77      0.77      0.76       200

2023-06-22 15:22:42,404 - INFO - test loss 0.01653225905039409
2023-06-22 15:22:42,404 - INFO - test acc 0.7699999809265137
2023-06-22 15:22:44,774 - INFO - Distilling data from client: Client20
2023-06-22 15:22:44,774 - INFO - train loss: 0.0008422484920669985
2023-06-22 15:22:44,774 - INFO - train acc: 1.0
2023-06-22 15:22:44,827 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.53      0.63        73
           4       0.77      0.91      0.83       127

    accuracy                           0.77       200
   macro avg       0.77      0.72      0.73       200
weighted avg       0.77      0.77      0.76       200

2023-06-22 15:22:44,829 - INFO - test loss 0.01678692108265406
2023-06-22 15:22:44,830 - INFO - test acc 0.7699999809265137
2023-06-22 15:22:44,839 - WARNING - Finished tracing + transforming jit(gather) in 0.0007824897766113281 sec
2023-06-22 15:22:44,841 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[346,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:22:44,845 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.003828763961791992 sec
2023-06-22 15:22:44,846 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:22:44,871 - WARNING - Finished XLA compilation of jit(gather) in 0.023510217666625977 sec
2023-06-22 15:22:44,900 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:22:44,919 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:22:44,934 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:22:44,946 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:22:44,960 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:22:44,971 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:22:45,453 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client20//synthetic.png
2023-06-22 15:22:45,476 - INFO - c: 0.0 and total_data_in_this_class: 271
2023-06-22 15:22:45,476 - INFO - c: 6.0 and total_data_in_this_class: 259
2023-06-22 15:22:45,476 - INFO - c: 7.0 and total_data_in_this_class: 269
2023-06-22 15:22:45,476 - INFO - c: 0.0 and total_data_in_this_class: 62
2023-06-22 15:22:45,477 - INFO - c: 6.0 and total_data_in_this_class: 74
2023-06-22 15:22:45,477 - INFO - c: 7.0 and total_data_in_this_class: 64
2023-06-22 15:22:45,609 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08017754554748535 sec
2023-06-22 15:22:45,688 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07765698432922363 sec
2023-06-22 15:22:45,697 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16997432708740234 sec
2023-06-22 15:22:45,700 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:22:45,760 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05946516990661621 sec
2023-06-22 15:22:45,760 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:22:45,942 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.18124866485595703 sec
2023-06-22 15:22:46,023 - INFO - initial test loss: 0.019305157060978972
2023-06-22 15:22:46,024 - INFO - initial test acc: 0.7949999570846558
2023-06-22 15:22:46,052 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.02373051643371582 sec
2023-06-22 15:22:46,251 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2236804962158203 sec
2023-06-22 15:22:46,256 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:22:46,357 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.09990811347961426 sec
2023-06-22 15:22:46,357 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:22:46,788 - WARNING - Finished XLA compilation of jit(update_fn) in 0.43048691749572754 sec
2023-06-22 15:22:49,711 - INFO - Distilling data from client: Client21
2023-06-22 15:22:49,712 - INFO - train loss: 0.0018944305972606148
2023-06-22 15:22:49,712 - INFO - train acc: 0.9961464405059814
2023-06-22 15:22:49,923 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.82      0.86        62
           6       0.79      0.86      0.83        74
           7       0.75      0.73      0.74        64

    accuracy                           0.81       200
   macro avg       0.82      0.81      0.81       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:22:49,924 - INFO - test loss 0.01804620116977969
2023-06-22 15:22:49,924 - INFO - test acc 0.8100000023841858
2023-06-22 15:22:52,818 - INFO - Distilling data from client: Client21
2023-06-22 15:22:52,819 - INFO - train loss: 0.0009076072831034369
2023-06-22 15:22:52,819 - INFO - train acc: 1.0
2023-06-22 15:22:52,881 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.82      0.86        62
           6       0.77      0.85      0.81        74
           7       0.74      0.72      0.73        64

    accuracy                           0.80       200
   macro avg       0.81      0.80      0.80       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:22:52,883 - INFO - test loss 0.01825387055138422
2023-06-22 15:22:52,885 - INFO - test acc 0.7999999523162842
2023-06-22 15:22:55,704 - INFO - Distilling data from client: Client21
2023-06-22 15:22:55,705 - INFO - train loss: 0.0006474536326955771
2023-06-22 15:22:55,705 - INFO - train acc: 1.0
2023-06-22 15:22:55,755 - INFO - report:               precision    recall  f1-score   support

           0       0.89      0.81      0.85        62
           6       0.76      0.85      0.80        74
           7       0.70      0.67      0.69        64

    accuracy                           0.78       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:22:55,756 - INFO - test loss 0.018275775645771528
2023-06-22 15:22:55,756 - INFO - test acc 0.7799999713897705
2023-06-22 15:22:58,611 - INFO - Distilling data from client: Client21
2023-06-22 15:22:58,612 - INFO - train loss: 0.0006253505408027862
2023-06-22 15:22:58,612 - INFO - train acc: 1.0
2023-06-22 15:22:58,666 - INFO - report:               precision    recall  f1-score   support

           0       0.86      0.82      0.84        62
           6       0.75      0.81      0.78        74
           7       0.70      0.67      0.69        64

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:22:58,666 - INFO - test loss 0.01907913190947536
2023-06-22 15:22:58,666 - INFO - test acc 0.7699999809265137
2023-06-22 15:23:01,489 - INFO - Distilling data from client: Client21
2023-06-22 15:23:01,490 - INFO - train loss: 0.0005866024580110752
2023-06-22 15:23:01,490 - INFO - train acc: 1.0
2023-06-22 15:23:01,536 - INFO - report:               precision    recall  f1-score   support

           0       0.86      0.81      0.83        62
           6       0.77      0.82      0.80        74
           7       0.70      0.69      0.69        64

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:23:01,536 - INFO - test loss 0.019073849104376636
2023-06-22 15:23:01,537 - INFO - test acc 0.7749999761581421
2023-06-22 15:23:04,347 - INFO - Distilling data from client: Client21
2023-06-22 15:23:04,347 - INFO - train loss: 0.0004762472484747295
2023-06-22 15:23:04,347 - INFO - train acc: 1.0
2023-06-22 15:23:04,401 - INFO - report:               precision    recall  f1-score   support

           0       0.86      0.81      0.83        62
           6       0.82      0.84      0.83        74
           7       0.70      0.72      0.71        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:23:04,401 - INFO - test loss 0.019029731459796813
2023-06-22 15:23:04,401 - INFO - test acc 0.7899999618530273
2023-06-22 15:23:07,206 - INFO - Distilling data from client: Client21
2023-06-22 15:23:07,206 - INFO - train loss: 0.00041428882272049266
2023-06-22 15:23:07,206 - INFO - train acc: 1.0
2023-06-22 15:23:07,261 - INFO - report:               precision    recall  f1-score   support

           0       0.86      0.82      0.84        62
           6       0.80      0.82      0.81        74
           7       0.71      0.72      0.71        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:23:07,262 - INFO - test loss 0.018636751568997342
2023-06-22 15:23:07,263 - INFO - test acc 0.7899999618530273
2023-06-22 15:23:10,052 - INFO - Distilling data from client: Client21
2023-06-22 15:23:10,053 - INFO - train loss: 0.00037713146881010825
2023-06-22 15:23:10,053 - INFO - train acc: 1.0
2023-06-22 15:23:10,111 - INFO - report:               precision    recall  f1-score   support

           0       0.85      0.82      0.84        62
           6       0.79      0.84      0.82        74
           7       0.73      0.70      0.71        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:23:10,113 - INFO - test loss 0.018648196853587522
2023-06-22 15:23:10,114 - INFO - test acc 0.7899999618530273
2023-06-22 15:23:12,923 - INFO - Distilling data from client: Client21
2023-06-22 15:23:12,923 - INFO - train loss: 0.0003272158071959789
2023-06-22 15:23:12,924 - INFO - train acc: 1.0
2023-06-22 15:23:12,978 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.85      0.87        62
           6       0.79      0.84      0.82        74
           7       0.73      0.70      0.71        64

    accuracy                           0.80       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:23:12,980 - INFO - test loss 0.018563330892337623
2023-06-22 15:23:12,980 - INFO - test acc 0.7999999523162842
2023-06-22 15:23:15,822 - INFO - Distilling data from client: Client21
2023-06-22 15:23:15,823 - INFO - train loss: 0.0002798068464394291
2023-06-22 15:23:15,824 - INFO - train acc: 1.0
2023-06-22 15:23:15,871 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.84      0.86        62
           6       0.78      0.84      0.81        74
           7       0.73      0.70      0.71        64

    accuracy                           0.80       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.80      0.80      0.79       200

2023-06-22 15:23:15,872 - INFO - test loss 0.018310426182592224
2023-06-22 15:23:15,873 - INFO - test acc 0.7949999570846558
2023-06-22 15:23:18,699 - INFO - Distilling data from client: Client21
2023-06-22 15:23:18,700 - INFO - train loss: 0.00029652810461273406
2023-06-22 15:23:18,700 - INFO - train acc: 1.0
2023-06-22 15:23:18,761 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.81      0.84        62
           6       0.78      0.82      0.80        74
           7       0.72      0.73      0.73        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:23:18,767 - INFO - test loss 0.01875182669798545
2023-06-22 15:23:18,768 - INFO - test acc 0.7899999618530273
2023-06-22 15:23:21,584 - INFO - Distilling data from client: Client21
2023-06-22 15:23:21,584 - INFO - train loss: 0.0002343722402947139
2023-06-22 15:23:21,585 - INFO - train acc: 1.0
2023-06-22 15:23:21,642 - INFO - report:               precision    recall  f1-score   support

           0       0.89      0.82      0.86        62
           6       0.77      0.81      0.79        74
           7       0.72      0.73      0.73        64

    accuracy                           0.79       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:23:21,643 - INFO - test loss 0.018632689455520067
2023-06-22 15:23:21,643 - INFO - test acc 0.7899999618530273
2023-06-22 15:23:24,402 - INFO - Distilling data from client: Client21
2023-06-22 15:23:24,402 - INFO - train loss: 0.0002437487203880923
2023-06-22 15:23:24,402 - INFO - train acc: 1.0
2023-06-22 15:23:24,458 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.84      0.86        62
           6       0.77      0.82      0.80        74
           7       0.73      0.70      0.71        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:23:24,458 - INFO - test loss 0.018602750924395374
2023-06-22 15:23:24,458 - INFO - test acc 0.7899999618530273
2023-06-22 15:23:27,211 - INFO - Distilling data from client: Client21
2023-06-22 15:23:27,212 - INFO - train loss: 0.00021652457438576367
2023-06-22 15:23:27,213 - INFO - train acc: 1.0
2023-06-22 15:23:27,270 - INFO - report:               precision    recall  f1-score   support

           0       0.89      0.82      0.86        62
           6       0.77      0.85      0.81        74
           7       0.70      0.67      0.69        64

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.79      0.79      0.78       200

2023-06-22 15:23:27,271 - INFO - test loss 0.018403408137885994
2023-06-22 15:23:27,272 - INFO - test acc 0.7849999666213989
2023-06-22 15:23:30,147 - INFO - Distilling data from client: Client21
2023-06-22 15:23:30,148 - INFO - train loss: 0.00020474497259330448
2023-06-22 15:23:30,148 - INFO - train acc: 1.0
2023-06-22 15:23:30,219 - INFO - report:               precision    recall  f1-score   support

           0       0.90      0.85      0.88        62
           6       0.79      0.88      0.83        74
           7       0.75      0.69      0.72        64

    accuracy                           0.81       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:23:30,219 - INFO - test loss 0.01874506833903792
2023-06-22 15:23:30,219 - INFO - test acc 0.8100000023841858
2023-06-22 15:23:33,019 - INFO - Distilling data from client: Client21
2023-06-22 15:23:33,019 - INFO - train loss: 0.00019083176001749775
2023-06-22 15:23:33,019 - INFO - train acc: 1.0
2023-06-22 15:23:33,073 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.81      0.84        62
           6       0.77      0.82      0.80        74
           7       0.69      0.69      0.69        64

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:23:33,074 - INFO - test loss 0.018672935322277453
2023-06-22 15:23:33,074 - INFO - test acc 0.7749999761581421
2023-06-22 15:23:36,027 - INFO - Distilling data from client: Client21
2023-06-22 15:23:36,028 - INFO - train loss: 0.00018036013230335603
2023-06-22 15:23:36,028 - INFO - train acc: 1.0
2023-06-22 15:23:36,083 - INFO - report:               precision    recall  f1-score   support

           0       0.84      0.84      0.84        62
           6       0.81      0.84      0.82        74
           7       0.72      0.69      0.70        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:23:36,084 - INFO - test loss 0.018416977747228957
2023-06-22 15:23:36,085 - INFO - test acc 0.7899999618530273
2023-06-22 15:23:38,911 - INFO - Distilling data from client: Client21
2023-06-22 15:23:38,911 - INFO - train loss: 0.00020058973013048717
2023-06-22 15:23:38,912 - INFO - train acc: 1.0
2023-06-22 15:23:38,968 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.82      0.86        62
           6       0.77      0.85      0.81        74
           7       0.71      0.69      0.70        64

    accuracy                           0.79       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:23:38,969 - INFO - test loss 0.018436320449430384
2023-06-22 15:23:38,969 - INFO - test acc 0.7899999618530273
2023-06-22 15:23:41,886 - INFO - Distilling data from client: Client21
2023-06-22 15:23:41,887 - INFO - train loss: 0.00016289915384291016
2023-06-22 15:23:41,887 - INFO - train acc: 1.0
2023-06-22 15:23:41,935 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.82      0.86        62
           6       0.78      0.84      0.81        74
           7       0.72      0.72      0.72        64

    accuracy                           0.80       200
   macro avg       0.80      0.79      0.80       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:23:41,936 - INFO - test loss 0.018689345123706805
2023-06-22 15:23:41,938 - INFO - test acc 0.7949999570846558
2023-06-22 15:23:44,800 - INFO - Distilling data from client: Client21
2023-06-22 15:23:44,801 - INFO - train loss: 0.00018128240427569558
2023-06-22 15:23:44,801 - INFO - train acc: 1.0
2023-06-22 15:23:44,859 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.82      0.86        62
           6       0.78      0.82      0.80        74
           7       0.70      0.72      0.71        64

    accuracy                           0.79       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:23:44,861 - INFO - test loss 0.018135211785351095
2023-06-22 15:23:44,861 - INFO - test acc 0.7899999618530273
2023-06-22 15:23:47,666 - INFO - Distilling data from client: Client21
2023-06-22 15:23:47,667 - INFO - train loss: 0.00017141417030605525
2023-06-22 15:23:47,668 - INFO - train acc: 1.0
2023-06-22 15:23:47,828 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.84      0.87        62
           6       0.79      0.85      0.82        74
           7       0.76      0.75      0.76        64

    accuracy                           0.81       200
   macro avg       0.82      0.81      0.82       200
weighted avg       0.82      0.81      0.82       200

2023-06-22 15:23:47,828 - INFO - test loss 0.01856646225188087
2023-06-22 15:23:47,828 - INFO - test acc 0.8149999976158142
2023-06-22 15:23:50,611 - INFO - Distilling data from client: Client21
2023-06-22 15:23:50,611 - INFO - train loss: 0.00017522169680790725
2023-06-22 15:23:50,611 - INFO - train acc: 1.0
2023-06-22 15:23:50,683 - INFO - report:               precision    recall  f1-score   support

           0       0.89      0.81      0.85        62
           6       0.80      0.85      0.82        74
           7       0.71      0.72      0.71        64

    accuracy                           0.80       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:23:50,683 - INFO - test loss 0.01888296718170539
2023-06-22 15:23:50,684 - INFO - test acc 0.7949999570846558
2023-06-22 15:23:53,580 - INFO - Distilling data from client: Client21
2023-06-22 15:23:53,580 - INFO - train loss: 0.00016174444614752375
2023-06-22 15:23:53,581 - INFO - train acc: 1.0
2023-06-22 15:23:53,640 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.84      0.87        62
           6       0.77      0.85      0.81        74
           7       0.72      0.69      0.70        64

    accuracy                           0.80       200
   macro avg       0.80      0.79      0.80       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:23:53,642 - INFO - test loss 0.018590232499040148
2023-06-22 15:23:53,642 - INFO - test acc 0.7949999570846558
2023-06-22 15:23:56,460 - INFO - Distilling data from client: Client21
2023-06-22 15:23:56,461 - INFO - train loss: 0.0001762463151283361
2023-06-22 15:23:56,461 - INFO - train acc: 1.0
2023-06-22 15:23:56,518 - INFO - report:               precision    recall  f1-score   support

           0       0.90      0.85      0.88        62
           6       0.79      0.85      0.82        74
           7       0.72      0.69      0.70        64

    accuracy                           0.80       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:23:56,519 - INFO - test loss 0.018653850820060484
2023-06-22 15:23:56,520 - INFO - test acc 0.7999999523162842
2023-06-22 15:23:59,328 - INFO - Distilling data from client: Client21
2023-06-22 15:23:59,328 - INFO - train loss: 0.00014696384059062942
2023-06-22 15:23:59,329 - INFO - train acc: 1.0
2023-06-22 15:23:59,386 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.84      0.86        62
           6       0.78      0.80      0.79        74
           7       0.69      0.70      0.70        64

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:23:59,389 - INFO - test loss 0.019024162391475207
2023-06-22 15:23:59,390 - INFO - test acc 0.7799999713897705
2023-06-22 15:23:59,421 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:23:59,440 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:23:59,458 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:23:59,477 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:23:59,492 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:23:59,503 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:23:59,516 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:23:59,527 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:23:59,540 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:24:00,100 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client21//synthetic.png
2023-06-22 15:24:00,120 - INFO - c: 9.0 and total_data_in_this_class: 799
2023-06-22 15:24:00,121 - INFO - c: 9.0 and total_data_in_this_class: 200
2023-06-22 15:24:00,160 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00047326087951660156 sec
2023-06-22 15:24:00,160 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:24:00,163 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0022897720336914062 sec
2023-06-22 15:24:00,163 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:24:00,179 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01553964614868164 sec
2023-06-22 15:24:00,183 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00032329559326171875 sec
2023-06-22 15:24:00,183 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:24:00,185 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0015912055969238281 sec
2023-06-22 15:24:00,185 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:24:00,197 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011348247528076172 sec
2023-06-22 15:24:00,202 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020122528076171875 sec
2023-06-22 15:24:00,203 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021982192993164062 sec
2023-06-22 15:24:00,205 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005273818969726562 sec
2023-06-22 15:24:00,208 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0012722015380859375 sec
2023-06-22 15:24:00,209 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020575523376464844 sec
2023-06-22 15:24:00,210 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004894733428955078 sec
2023-06-22 15:24:00,211 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042557716369628906 sec
2023-06-22 15:24:00,212 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003197193145751953 sec
2023-06-22 15:24:00,213 - WARNING - Finished tracing + transforming fn for pjit in 0.0004706382751464844 sec
2023-06-22 15:24:00,214 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005931854248046875 sec
2023-06-22 15:24:00,216 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022077560424804688 sec
2023-06-22 15:24:00,218 - WARNING - Finished tracing + transforming fn for pjit in 0.0003917217254638672 sec
2023-06-22 15:24:00,219 - WARNING - Finished tracing + transforming fn for pjit in 0.0006196498870849609 sec
2023-06-22 15:24:00,220 - WARNING - Finished tracing + transforming fn for pjit in 0.000392913818359375 sec
2023-06-22 15:24:00,221 - WARNING - Finished tracing + transforming fn for pjit in 0.0004467964172363281 sec
2023-06-22 15:24:00,223 - WARNING - Finished tracing + transforming fn for pjit in 0.00035834312438964844 sec
2023-06-22 15:24:00,226 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0004138946533203125 sec
2023-06-22 15:24:00,227 - WARNING - Finished tracing + transforming fn for pjit in 0.00039458274841308594 sec
2023-06-22 15:24:00,229 - WARNING - Finished tracing + transforming fn for pjit in 0.0003986358642578125 sec
2023-06-22 15:24:00,235 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006206035614013672 sec
2023-06-22 15:24:00,236 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016872882843017578 sec
2023-06-22 15:24:00,237 - WARNING - Finished tracing + transforming fn for pjit in 0.0004413127899169922 sec
2023-06-22 15:24:00,239 - WARNING - Finished tracing + transforming fn for pjit in 0.0003921985626220703 sec
2023-06-22 15:24:00,240 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000385284423828125 sec
2023-06-22 15:24:00,242 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006155967712402344 sec
2023-06-22 15:24:00,243 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032067298889160156 sec
2023-06-22 15:24:00,244 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004687309265136719 sec
2023-06-22 15:24:00,246 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004069805145263672 sec
2023-06-22 15:24:00,247 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039958953857421875 sec
2023-06-22 15:24:00,248 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004889965057373047 sec
2023-06-22 15:24:00,249 - WARNING - Finished tracing + transforming _where for pjit in 0.0015730857849121094 sec
2023-06-22 15:24:00,250 - WARNING - Finished tracing + transforming fn for pjit in 0.0004570484161376953 sec
2023-06-22 15:24:00,251 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005834102630615234 sec
2023-06-22 15:24:00,253 - WARNING - Finished tracing + transforming fn for pjit in 0.00039577484130859375 sec
2023-06-22 15:24:00,254 - WARNING - Finished tracing + transforming fn for pjit in 0.00040030479431152344 sec
2023-06-22 15:24:00,255 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003921985626220703 sec
2023-06-22 15:24:00,257 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000461578369140625 sec
2023-06-22 15:24:00,258 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003123283386230469 sec
2023-06-22 15:24:00,259 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004725456237792969 sec
2023-06-22 15:24:00,261 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004124641418457031 sec
2023-06-22 15:24:00,262 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003857612609863281 sec
2023-06-22 15:24:00,263 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00046825408935546875 sec
2023-06-22 15:24:00,264 - WARNING - Finished tracing + transforming _where for pjit in 0.0015442371368408203 sec
2023-06-22 15:24:00,265 - WARNING - Finished tracing + transforming fn for pjit in 0.00047850608825683594 sec
2023-06-22 15:24:00,266 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004563331604003906 sec
2023-06-22 15:24:00,268 - WARNING - Finished tracing + transforming fn for pjit in 0.00039267539978027344 sec
2023-06-22 15:24:00,275 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004456043243408203 sec
2023-06-22 15:24:00,276 - WARNING - Finished tracing + transforming fn for pjit in 0.0004706382751464844 sec
2023-06-22 15:24:00,278 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046753883361816406 sec
2023-06-22 15:24:00,279 - WARNING - Finished tracing + transforming fn for pjit in 0.0005564689636230469 sec
2023-06-22 15:24:00,286 - WARNING - Finished tracing + transforming fn for pjit in 0.0003414154052734375 sec
2023-06-22 15:24:00,289 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002810955047607422 sec
2023-06-22 15:24:00,290 - WARNING - Finished tracing + transforming fn for pjit in 0.000530242919921875 sec
2023-06-22 15:24:00,291 - WARNING - Finished tracing + transforming fn for pjit in 0.0004050731658935547 sec
2023-06-22 15:24:00,319 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11815047264099121 sec
2023-06-22 15:24:00,324 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021338462829589844 sec
2023-06-22 15:24:00,325 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002048015594482422 sec
2023-06-22 15:24:00,326 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004706382751464844 sec
2023-06-22 15:24:00,330 - WARNING - Finished tracing + transforming fn for pjit in 0.0003311634063720703 sec
2023-06-22 15:24:00,331 - WARNING - Finished tracing + transforming fn for pjit in 0.0004596710205078125 sec
2023-06-22 15:24:00,333 - WARNING - Finished tracing + transforming fn for pjit in 0.0003619194030761719 sec
2023-06-22 15:24:00,343 - WARNING - Finished tracing + transforming fn for pjit in 0.0003771781921386719 sec
2023-06-22 15:24:00,344 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00036716461181640625 sec
2023-06-22 15:24:00,346 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044155120849609375 sec
2023-06-22 15:24:00,347 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003070831298828125 sec
2023-06-22 15:24:00,348 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005745887756347656 sec
2023-06-22 15:24:00,350 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003905296325683594 sec
2023-06-22 15:24:00,351 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004150867462158203 sec
2023-06-22 15:24:00,352 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004601478576660156 sec
2023-06-22 15:24:00,353 - WARNING - Finished tracing + transforming _where for pjit in 0.0015354156494140625 sec
2023-06-22 15:24:00,354 - WARNING - Finished tracing + transforming fn for pjit in 0.0004534721374511719 sec
2023-06-22 15:24:00,355 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004470348358154297 sec
2023-06-22 15:24:00,357 - WARNING - Finished tracing + transforming fn for pjit in 0.0003859996795654297 sec
2023-06-22 15:24:00,358 - WARNING - Finished tracing + transforming fn for pjit in 0.0005247592926025391 sec
2023-06-22 15:24:00,377 - WARNING - Finished tracing + transforming fn for pjit in 0.00032806396484375 sec
2023-06-22 15:24:00,407 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08690476417541504 sec
2023-06-22 15:24:00,410 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002148151397705078 sec
2023-06-22 15:24:00,412 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002143383026123047 sec
2023-06-22 15:24:00,412 - WARNING - Finished tracing + transforming _where for pjit in 0.0010838508605957031 sec
2023-06-22 15:24:00,413 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005013942718505859 sec
2023-06-22 15:24:00,414 - WARNING - Finished tracing + transforming trace for pjit in 0.00447845458984375 sec
2023-06-22 15:24:00,418 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00018358230590820312 sec
2023-06-22 15:24:00,419 - WARNING - Finished tracing + transforming tril for pjit in 0.001104116439819336 sec
2023-06-22 15:24:00,420 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.003023386001586914 sec
2023-06-22 15:24:00,421 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00020051002502441406 sec
2023-06-22 15:24:00,422 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019478797912597656 sec
2023-06-22 15:24:00,426 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0026941299438476562 sec
2023-06-22 15:24:00,433 - WARNING - Finished tracing + transforming _solve for pjit in 0.01629948616027832 sec
2023-06-22 15:24:00,434 - WARNING - Finished tracing + transforming dot for pjit in 0.000537872314453125 sec
2023-06-22 15:24:00,438 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.2390613555908203 sec
2023-06-22 15:24:00,441 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:24:00,493 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05209684371948242 sec
2023-06-22 15:24:00,494 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:24:00,664 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.16965579986572266 sec
2023-06-22 15:24:00,708 - INFO - initial test loss: 0.0008100661618863905
2023-06-22 15:24:00,708 - INFO - initial test acc: 1.0
2023-06-22 15:24:00,720 - WARNING - Finished tracing + transforming dot for pjit in 0.0008127689361572266 sec
2023-06-22 15:24:00,722 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006766319274902344 sec
2023-06-22 15:24:00,725 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006763935089111328 sec
2023-06-22 15:24:00,726 - WARNING - Finished tracing + transforming _mean for pjit in 0.002336263656616211 sec
2023-06-22 15:24:00,728 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004336833953857422 sec
2023-06-22 15:24:00,730 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0005609989166259766 sec
2023-06-22 15:24:00,732 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005786418914794922 sec
2023-06-22 15:24:00,734 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008203983306884766 sec
2023-06-22 15:24:00,735 - WARNING - Finished tracing + transforming _mean for pjit in 0.0026345252990722656 sec
2023-06-22 15:24:00,737 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.02395319938659668 sec
2023-06-22 15:24:00,756 - WARNING - Finished tracing + transforming fn for pjit in 0.0005886554718017578 sec
2023-06-22 15:24:00,758 - WARNING - Finished tracing + transforming fn for pjit in 0.0007607936859130859 sec
2023-06-22 15:24:00,759 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005121231079101562 sec
2023-06-22 15:24:00,762 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006039142608642578 sec
2023-06-22 15:24:00,763 - WARNING - Finished tracing + transforming _where for pjit in 0.002184152603149414 sec
2023-06-22 15:24:00,782 - WARNING - Finished tracing + transforming fn for pjit in 0.0005953311920166016 sec
2023-06-22 15:24:00,784 - WARNING - Finished tracing + transforming fn for pjit in 0.0006136894226074219 sec
2023-06-22 15:24:00,786 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005130767822265625 sec
2023-06-22 15:24:00,788 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005736351013183594 sec
2023-06-22 15:24:00,789 - WARNING - Finished tracing + transforming _where for pjit in 0.0021424293518066406 sec
2023-06-22 15:24:00,849 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00033736228942871094 sec
2023-06-22 15:24:00,928 - WARNING - Finished tracing + transforming fn for pjit in 0.0004150867462158203 sec
2023-06-22 15:24:00,930 - WARNING - Finished tracing + transforming fn for pjit in 0.0003955364227294922 sec
2023-06-22 15:24:00,931 - WARNING - Finished tracing + transforming square for pjit in 0.00029349327087402344 sec
2023-06-22 15:24:00,934 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003523826599121094 sec
2023-06-22 15:24:00,937 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045990943908691406 sec
2023-06-22 15:24:00,938 - WARNING - Finished tracing + transforming fn for pjit in 0.00043582916259765625 sec
2023-06-22 15:24:00,939 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00037097930908203125 sec
2023-06-22 15:24:00,940 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037670135498046875 sec
2023-06-22 15:24:00,941 - WARNING - Finished tracing + transforming fn for pjit in 0.0004417896270751953 sec
2023-06-22 15:24:00,943 - WARNING - Finished tracing + transforming fn for pjit in 0.0003991127014160156 sec
2023-06-22 15:24:00,944 - WARNING - Finished tracing + transforming square for pjit in 0.00030350685119628906 sec
2023-06-22 15:24:00,947 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004067420959472656 sec
2023-06-22 15:24:00,950 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003151893615722656 sec
2023-06-22 15:24:00,951 - WARNING - Finished tracing + transforming fn for pjit in 0.0004868507385253906 sec
2023-06-22 15:24:00,952 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003833770751953125 sec
2023-06-22 15:24:00,953 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004115104675292969 sec
2023-06-22 15:24:00,955 - WARNING - Finished tracing + transforming update_fn for pjit in 0.24272966384887695 sec
2023-06-22 15:24:00,960 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,10]), ShapedArray(float32[533,10]), ShapedArray(float32[533,10]), ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,10]), ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:24:01,062 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10094022750854492 sec
2023-06-22 15:24:01,062 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:24:01,495 - WARNING - Finished XLA compilation of jit(update_fn) in 0.43295741081237793 sec
2023-06-22 15:24:04,280 - INFO - Distilling data from client: Client22
2023-06-22 15:24:04,281 - INFO - train loss: 1.9665643557038127e-05
2023-06-22 15:24:04,281 - INFO - train acc: 1.0
2023-06-22 15:24:04,463 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:04,464 - INFO - test loss 0.0005955596628900242
2023-06-22 15:24:04,464 - INFO - test acc 1.0
2023-06-22 15:24:07,253 - INFO - Distilling data from client: Client22
2023-06-22 15:24:07,253 - INFO - train loss: 2.845089973810877e-06
2023-06-22 15:24:07,253 - INFO - train acc: 1.0
2023-06-22 15:24:07,317 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:07,330 - INFO - test loss 0.0005300773343065798
2023-06-22 15:24:07,330 - INFO - test acc 1.0
2023-06-22 15:24:10,221 - INFO - Distilling data from client: Client22
2023-06-22 15:24:10,222 - INFO - train loss: 5.88123628526206e-07
2023-06-22 15:24:10,222 - INFO - train acc: 1.0
2023-06-22 15:24:10,273 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:10,273 - INFO - test loss 0.0004933397542983565
2023-06-22 15:24:10,274 - INFO - test acc 1.0
2023-06-22 15:24:13,184 - INFO - Distilling data from client: Client22
2023-06-22 15:24:13,185 - INFO - train loss: 1.0513028627037969e-07
2023-06-22 15:24:13,185 - INFO - train acc: 1.0
2023-06-22 15:24:13,237 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:13,238 - INFO - test loss 0.00048626308153972323
2023-06-22 15:24:13,238 - INFO - test acc 1.0
2023-06-22 15:24:16,038 - INFO - Distilling data from client: Client22
2023-06-22 15:24:16,038 - INFO - train loss: 3.725878756137138e-08
2023-06-22 15:24:16,038 - INFO - train acc: 1.0
2023-06-22 15:24:16,086 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:16,087 - INFO - test loss 0.0004873774255125196
2023-06-22 15:24:16,087 - INFO - test acc 1.0
2023-06-22 15:24:19,050 - INFO - Distilling data from client: Client22
2023-06-22 15:24:19,051 - INFO - train loss: 1.0996833720636086e-08
2023-06-22 15:24:19,051 - INFO - train acc: 1.0
2023-06-22 15:24:19,093 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:19,094 - INFO - test loss 0.00048818283700756014
2023-06-22 15:24:19,095 - INFO - test acc 1.0
2023-06-22 15:24:22,003 - INFO - Distilling data from client: Client22
2023-06-22 15:24:22,003 - INFO - train loss: 4.227222007999991e-09
2023-06-22 15:24:22,003 - INFO - train acc: 1.0
2023-06-22 15:24:22,059 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:22,060 - INFO - test loss 0.0004890697904705543
2023-06-22 15:24:22,061 - INFO - test acc 1.0
2023-06-22 15:24:25,025 - INFO - Distilling data from client: Client22
2023-06-22 15:24:25,026 - INFO - train loss: 8.426326766312222e-10
2023-06-22 15:24:25,026 - INFO - train acc: 1.0
2023-06-22 15:24:25,084 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:25,084 - INFO - test loss 0.0004887421724260278
2023-06-22 15:24:25,084 - INFO - test acc 1.0
2023-06-22 15:24:27,991 - INFO - Distilling data from client: Client22
2023-06-22 15:24:27,991 - INFO - train loss: 3.063828891143645e-10
2023-06-22 15:24:27,991 - INFO - train acc: 1.0
2023-06-22 15:24:28,046 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:28,046 - INFO - test loss 0.0004889358191996543
2023-06-22 15:24:28,046 - INFO - test acc 1.0
2023-06-22 15:24:31,153 - INFO - Distilling data from client: Client22
2023-06-22 15:24:31,153 - INFO - train loss: 8.076433439208353e-11
2023-06-22 15:24:31,153 - INFO - train acc: 1.0
2023-06-22 15:24:31,216 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:31,217 - INFO - test loss 0.0004888788073908227
2023-06-22 15:24:31,217 - INFO - test acc 1.0
2023-06-22 15:24:34,111 - INFO - Distilling data from client: Client22
2023-06-22 15:24:34,112 - INFO - train loss: 2.427250884637362e-11
2023-06-22 15:24:34,112 - INFO - train acc: 1.0
2023-06-22 15:24:34,180 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:34,182 - INFO - test loss 0.0004889078883774773
2023-06-22 15:24:34,182 - INFO - test acc 1.0
2023-06-22 15:24:37,111 - INFO - Distilling data from client: Client22
2023-06-22 15:24:37,112 - INFO - train loss: 7.689023998548014e-12
2023-06-22 15:24:37,113 - INFO - train acc: 1.0
2023-06-22 15:24:37,155 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:37,156 - INFO - test loss 0.0004889075789926778
2023-06-22 15:24:37,156 - INFO - test acc 1.0
2023-06-22 15:24:40,137 - INFO - Distilling data from client: Client22
2023-06-22 15:24:40,138 - INFO - train loss: 2.9846876564600045e-12
2023-06-22 15:24:40,138 - INFO - train acc: 1.0
2023-06-22 15:24:40,197 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:40,200 - INFO - test loss 0.0004889095979393222
2023-06-22 15:24:40,201 - INFO - test acc 1.0
2023-06-22 15:24:43,050 - INFO - Distilling data from client: Client22
2023-06-22 15:24:43,050 - INFO - train loss: 9.133312508677196e-13
2023-06-22 15:24:43,050 - INFO - train acc: 1.0
2023-06-22 15:24:43,096 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:43,096 - INFO - test loss 0.000488905146667915
2023-06-22 15:24:43,097 - INFO - test acc 1.0
2023-06-22 15:24:45,915 - INFO - Distilling data from client: Client22
2023-06-22 15:24:45,915 - INFO - train loss: 2.605525434578664e-13
2023-06-22 15:24:45,916 - INFO - train acc: 1.0
2023-06-22 15:24:45,973 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:45,974 - INFO - test loss 0.0004889117481491833
2023-06-22 15:24:45,975 - INFO - test acc 1.0
2023-06-22 15:24:48,962 - INFO - Distilling data from client: Client22
2023-06-22 15:24:48,962 - INFO - train loss: 8.67661321550621e-14
2023-06-22 15:24:48,962 - INFO - train acc: 1.0
2023-06-22 15:24:49,015 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:49,015 - INFO - test loss 0.0004889101181291055
2023-06-22 15:24:49,016 - INFO - test acc 1.0
2023-06-22 15:24:51,947 - INFO - Distilling data from client: Client22
2023-06-22 15:24:51,948 - INFO - train loss: 4.851887346092938e-14
2023-06-22 15:24:51,949 - INFO - train acc: 1.0
2023-06-22 15:24:51,996 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:51,999 - INFO - test loss 0.0004889087794516635
2023-06-22 15:24:52,000 - INFO - test acc 1.0
2023-06-22 15:24:54,863 - INFO - Distilling data from client: Client22
2023-06-22 15:24:54,864 - INFO - train loss: 1.7965486582330437e-14
2023-06-22 15:24:54,865 - INFO - train acc: 1.0
2023-06-22 15:24:54,917 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:54,918 - INFO - test loss 0.000488910351441718
2023-06-22 15:24:54,918 - INFO - test acc 1.0
2023-06-22 15:24:57,710 - INFO - Distilling data from client: Client22
2023-06-22 15:24:57,711 - INFO - train loss: 5.370024587273078e-15
2023-06-22 15:24:57,712 - INFO - train acc: 1.0
2023-06-22 15:24:57,779 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:24:57,782 - INFO - test loss 0.0004889098071831828
2023-06-22 15:24:57,782 - INFO - test acc 1.0
2023-06-22 15:25:00,766 - INFO - Distilling data from client: Client22
2023-06-22 15:25:00,767 - INFO - train loss: 2.176868555989119e-15
2023-06-22 15:25:00,767 - INFO - train acc: 1.0
2023-06-22 15:25:00,833 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:25:00,834 - INFO - test loss 0.0004889098833752387
2023-06-22 15:25:00,834 - INFO - test acc 1.0
2023-06-22 15:25:03,750 - INFO - Distilling data from client: Client22
2023-06-22 15:25:03,751 - INFO - train loss: 6.036649657344363e-16
2023-06-22 15:25:03,751 - INFO - train acc: 1.0
2023-06-22 15:25:03,796 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:25:03,797 - INFO - test loss 0.0004889099143959063
2023-06-22 15:25:03,797 - INFO - test acc 1.0
2023-06-22 15:25:06,752 - INFO - Distilling data from client: Client22
2023-06-22 15:25:06,753 - INFO - train loss: 1.2490835601495423e-16
2023-06-22 15:25:06,753 - INFO - train acc: 1.0
2023-06-22 15:25:06,813 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:25:06,817 - INFO - test loss 0.0004889097481309493
2023-06-22 15:25:06,818 - INFO - test acc 1.0
2023-06-22 15:25:09,706 - INFO - Distilling data from client: Client22
2023-06-22 15:25:09,706 - INFO - train loss: 2.300528951561873e-17
2023-06-22 15:25:09,707 - INFO - train acc: 1.0
2023-06-22 15:25:09,761 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:25:09,761 - INFO - test loss 0.0004889097475334942
2023-06-22 15:25:09,762 - INFO - test acc 1.0
2023-06-22 15:25:12,736 - INFO - Distilling data from client: Client22
2023-06-22 15:25:12,736 - INFO - train loss: 4.451180935967686e-18
2023-06-22 15:25:12,737 - INFO - train acc: 1.0
2023-06-22 15:25:12,808 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:25:12,808 - INFO - test loss 0.0004889097235032879
2023-06-22 15:25:12,809 - INFO - test acc 1.0
2023-06-22 15:25:15,817 - INFO - Distilling data from client: Client22
2023-06-22 15:25:15,818 - INFO - train loss: 6.225311648643076e-19
2023-06-22 15:25:15,818 - INFO - train acc: 1.0
2023-06-22 15:25:15,869 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-06-22 15:25:15,870 - INFO - test loss 0.0004889097327288489
2023-06-22 15:25:15,870 - INFO - test acc 1.0
2023-06-22 15:25:15,875 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.0009889602661132812 sec
2023-06-22 15:25:15,884 - WARNING - Finished tracing + transforming fn for pjit in 0.0011217594146728516 sec
2023-06-22 15:25:15,885 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(int64[1]), ShapedArray(int64[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:25:15,890 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.00428009033203125 sec
2023-06-22 15:25:15,891 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:25:15,906 - WARNING - Finished XLA compilation of jit(fn) in 0.014406681060791016 sec
2023-06-22 15:25:15,908 - WARNING - Finished tracing + transforming jit(add) in 0.0011246204376220703 sec
2023-06-22 15:25:15,909 - DEBUG - Compiling add for with global shapes and types [ShapedArray(int64[1]), ShapedArray(int64[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:25:15,912 - WARNING - Finished jaxpr to MLIR module conversion jit(add) in 0.0025436878204345703 sec
2023-06-22 15:25:15,913 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:25:15,926 - WARNING - Finished XLA compilation of jit(add) in 0.0126495361328125 sec
2023-06-22 15:25:15,928 - WARNING - Finished tracing + transforming jit(select_n) in 0.0004642009735107422 sec
2023-06-22 15:25:15,929 - DEBUG - Compiling select_n for with global shapes and types [ShapedArray(bool[1]), ShapedArray(int64[1]), ShapedArray(int64[1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:25:15,932 - WARNING - Finished jaxpr to MLIR module conversion jit(select_n) in 0.002391338348388672 sec
2023-06-22 15:25:15,932 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:25:15,946 - WARNING - Finished XLA compilation of jit(select_n) in 0.012769460678100586 sec
2023-06-22 15:25:15,948 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00038361549377441406 sec
2023-06-22 15:25:15,951 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.00039196014404296875 sec
2023-06-22 15:25:15,951 - DEBUG - Compiling convert_element_type for with global shapes and types [ShapedArray(int64[1])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:25:15,954 - WARNING - Finished jaxpr to MLIR module conversion jit(convert_element_type) in 0.002213001251220703 sec
2023-06-22 15:25:15,955 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:25:15,967 - WARNING - Finished XLA compilation of jit(convert_element_type) in 0.011573314666748047 sec
2023-06-22 15:25:15,969 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003731250762939453 sec
2023-06-22 15:25:15,969 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(int32[1])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:25:15,971 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0016722679138183594 sec
2023-06-22 15:25:15,971 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:25:15,979 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.007805347442626953 sec
2023-06-22 15:25:15,981 - WARNING - Finished tracing + transforming jit(gather) in 0.0006587505340576172 sec
2023-06-22 15:25:15,982 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[533,3,32,32]), ShapedArray(int32[1,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:25:15,985 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.002736330032348633 sec
2023-06-22 15:25:15,985 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:25:16,005 - WARNING - Finished XLA compilation of jit(gather) in 0.019330978393554688 sec
2023-06-22 15:25:16,008 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005795955657958984 sec
2023-06-22 15:25:16,009 - WARNING - Finished tracing + transforming jit(copy) in 0.0003190040588378906 sec
2023-06-22 15:25:16,009 - DEBUG - Compiling copy for with global shapes and types [ShapedArray(float32[1,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:25:16,012 - WARNING - Finished jaxpr to MLIR module conversion jit(copy) in 0.0022630691528320312 sec
2023-06-22 15:25:16,012 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:25:16,023 - WARNING - Finished XLA compilation of jit(copy) in 0.009827136993408203 sec
2023-06-22 15:25:16,038 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:25:16,051 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:25:16,053 - WARNING - Finished tracing + transforming _unstack for pjit in 0.0007145404815673828 sec
2023-06-22 15:25:16,053 - DEBUG - Compiling _unstack for with global shapes and types [ShapedArray(float32[1,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:25:16,056 - WARNING - Finished jaxpr to MLIR module conversion jit(_unstack) in 0.0021212100982666016 sec
2023-06-22 15:25:16,056 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:25:16,065 - WARNING - Finished XLA compilation of jit(_unstack) in 0.008481979370117188 sec
2023-06-22 15:25:16,077 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:25:16,379 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client22//synthetic.png
2023-06-22 15:25:16,400 - INFO - c: 0.0 and total_data_in_this_class: 258
2023-06-22 15:25:16,400 - INFO - c: 6.0 and total_data_in_this_class: 277
2023-06-22 15:25:16,400 - INFO - c: 9.0 and total_data_in_this_class: 264
2023-06-22 15:25:16,400 - INFO - c: 0.0 and total_data_in_this_class: 75
2023-06-22 15:25:16,400 - INFO - c: 6.0 and total_data_in_this_class: 56
2023-06-22 15:25:16,401 - INFO - c: 9.0 and total_data_in_this_class: 69
2023-06-22 15:25:16,526 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07681107521057129 sec
2023-06-22 15:25:16,603 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07538104057312012 sec
2023-06-22 15:25:16,611 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16390132904052734 sec
2023-06-22 15:25:16,614 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:25:16,670 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.055611610412597656 sec
2023-06-22 15:25:16,671 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:25:16,850 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17867183685302734 sec
2023-06-22 15:25:16,959 - INFO - initial test loss: 0.020960359422923416
2023-06-22 15:25:16,959 - INFO - initial test acc: 0.75
2023-06-22 15:25:16,979 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.0137939453125 sec
2023-06-22 15:25:17,188 - WARNING - Finished tracing + transforming update_fn for pjit in 0.22341465950012207 sec
2023-06-22 15:25:17,193 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:25:17,300 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10616254806518555 sec
2023-06-22 15:25:17,301 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:25:17,711 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4098811149597168 sec
2023-06-22 15:25:20,482 - INFO - Distilling data from client: Client23
2023-06-22 15:25:20,483 - INFO - train loss: 0.002270636225600723
2023-06-22 15:25:20,483 - INFO - train acc: 0.9941860437393188
2023-06-22 15:25:20,629 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.77      0.76        75
           6       0.74      0.86      0.79        56
           9       0.84      0.71      0.77        69

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.77       200

2023-06-22 15:25:20,630 - INFO - test loss 0.018589126381900103
2023-06-22 15:25:20,630 - INFO - test acc 0.7749999761581421
2023-06-22 15:25:23,316 - INFO - Distilling data from client: Client23
2023-06-22 15:25:23,317 - INFO - train loss: 0.0011513408439326603
2023-06-22 15:25:23,317 - INFO - train acc: 0.998062014579773
2023-06-22 15:25:23,370 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.74        75
           6       0.73      0.88      0.80        56
           9       0.85      0.72      0.78        69

    accuracy                           0.77       200
   macro avg       0.77      0.78      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-06-22 15:25:23,371 - INFO - test loss 0.018760463600020417
2023-06-22 15:25:23,372 - INFO - test acc 0.7699999809265137
2023-06-22 15:25:26,051 - INFO - Distilling data from client: Client23
2023-06-22 15:25:26,052 - INFO - train loss: 0.000749994339617544
2023-06-22 15:25:26,052 - INFO - train acc: 1.0
2023-06-22 15:25:26,115 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.72      0.74        75
           6       0.68      0.88      0.77        56
           9       0.83      0.70      0.76        69

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.77      0.76      0.75       200

2023-06-22 15:25:26,117 - INFO - test loss 0.019078147995753674
2023-06-22 15:25:26,133 - INFO - test acc 0.7549999952316284
2023-06-22 15:25:28,907 - INFO - Distilling data from client: Client23
2023-06-22 15:25:28,908 - INFO - train loss: 0.0005424743308351278
2023-06-22 15:25:28,909 - INFO - train acc: 1.0
2023-06-22 15:25:28,953 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.74        75
           6       0.69      0.88      0.77        56
           9       0.84      0.67      0.74        69

    accuracy                           0.75       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-06-22 15:25:28,955 - INFO - test loss 0.019427907461561898
2023-06-22 15:25:28,956 - INFO - test acc 0.75
2023-06-22 15:25:31,759 - INFO - Distilling data from client: Client23
2023-06-22 15:25:31,760 - INFO - train loss: 0.0007191668983848202
2023-06-22 15:25:31,760 - INFO - train acc: 1.0
2023-06-22 15:25:31,812 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.75      0.75        75
           6       0.74      0.86      0.79        56
           9       0.84      0.74      0.78        69

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.77       200

2023-06-22 15:25:31,813 - INFO - test loss 0.019078280488431056
2023-06-22 15:25:31,813 - INFO - test acc 0.7749999761581421
2023-06-22 15:25:34,611 - INFO - Distilling data from client: Client23
2023-06-22 15:25:34,612 - INFO - train loss: 0.0004909669057378848
2023-06-22 15:25:34,612 - INFO - train acc: 1.0
2023-06-22 15:25:34,694 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.72      0.72        75
           6       0.69      0.86      0.76        56
           9       0.84      0.68      0.75        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.76      0.74      0.74       200

2023-06-22 15:25:34,694 - INFO - test loss 0.019625343355349507
2023-06-22 15:25:34,695 - INFO - test acc 0.7450000047683716
2023-06-22 15:25:37,454 - INFO - Distilling data from client: Client23
2023-06-22 15:25:37,454 - INFO - train loss: 0.0003815450234232481
2023-06-22 15:25:37,454 - INFO - train acc: 1.0
2023-06-22 15:25:37,504 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        75
           6       0.66      0.86      0.74        56
           9       0.81      0.64      0.72        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.75      0.73      0.73       200

2023-06-22 15:25:37,505 - INFO - test loss 0.019523923705049177
2023-06-22 15:25:37,505 - INFO - test acc 0.73499995470047
2023-06-22 15:25:40,279 - INFO - Distilling data from client: Client23
2023-06-22 15:25:40,279 - INFO - train loss: 0.00042656538041717916
2023-06-22 15:25:40,279 - INFO - train acc: 1.0
2023-06-22 15:25:40,334 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.75      0.76        75
           6       0.66      0.82      0.73        56
           9       0.81      0.67      0.73        69

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:25:40,335 - INFO - test loss 0.01976894756259133
2023-06-22 15:25:40,336 - INFO - test acc 0.7400000095367432
2023-06-22 15:25:43,114 - INFO - Distilling data from client: Client23
2023-06-22 15:25:43,114 - INFO - train loss: 0.00039829211361774836
2023-06-22 15:25:43,114 - INFO - train acc: 1.0
2023-06-22 15:25:43,172 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.75      0.75        75
           6       0.70      0.84      0.76        56
           9       0.81      0.70      0.75        69

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.75       200

2023-06-22 15:25:43,179 - INFO - test loss 0.019630803446200353
2023-06-22 15:25:43,180 - INFO - test acc 0.7549999952316284
2023-06-22 15:25:45,940 - INFO - Distilling data from client: Client23
2023-06-22 15:25:45,941 - INFO - train loss: 0.000366247726794068
2023-06-22 15:25:45,941 - INFO - train acc: 1.0
2023-06-22 15:25:45,997 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.75      0.74        75
           6       0.70      0.86      0.77        56
           9       0.82      0.65      0.73        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:25:46,003 - INFO - test loss 0.019916050821540885
2023-06-22 15:25:46,003 - INFO - test acc 0.7450000047683716
2023-06-22 15:25:48,734 - INFO - Distilling data from client: Client23
2023-06-22 15:25:48,735 - INFO - train loss: 0.0002672605467147369
2023-06-22 15:25:48,736 - INFO - train acc: 1.0
2023-06-22 15:25:48,788 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.72      0.73        75
           6       0.65      0.82      0.72        56
           9       0.80      0.65      0.72        69

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.74      0.72      0.72       200

2023-06-22 15:25:48,789 - INFO - test loss 0.019763754558610527
2023-06-22 15:25:48,789 - INFO - test acc 0.7249999642372131
2023-06-22 15:25:51,650 - INFO - Distilling data from client: Client23
2023-06-22 15:25:51,651 - INFO - train loss: 0.0002169808413930773
2023-06-22 15:25:51,652 - INFO - train acc: 1.0
2023-06-22 15:25:51,719 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.74        75
           6       0.66      0.88      0.75        56
           9       0.83      0.62      0.71        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.75      0.73      0.73       200

2023-06-22 15:25:51,721 - INFO - test loss 0.019657310602542104
2023-06-22 15:25:51,721 - INFO - test acc 0.73499995470047
2023-06-22 15:25:54,477 - INFO - Distilling data from client: Client23
2023-06-22 15:25:54,478 - INFO - train loss: 0.00023406736757439648
2023-06-22 15:25:54,479 - INFO - train acc: 1.0
2023-06-22 15:25:54,550 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.76      0.75        75
           6       0.70      0.86      0.77        56
           9       0.85      0.67      0.75        69

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.77      0.76      0.75       200

2023-06-22 15:25:54,553 - INFO - test loss 0.019842602307142562
2023-06-22 15:25:54,554 - INFO - test acc 0.7549999952316284
2023-06-22 15:25:57,296 - INFO - Distilling data from client: Client23
2023-06-22 15:25:57,296 - INFO - train loss: 0.00026310976814834764
2023-06-22 15:25:57,296 - INFO - train acc: 1.0
2023-06-22 15:25:57,359 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        75
           6       0.70      0.86      0.77        56
           9       0.84      0.68      0.75        69

    accuracy                           0.75       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-06-22 15:25:57,359 - INFO - test loss 0.020022468951593445
2023-06-22 15:25:57,360 - INFO - test acc 0.75
2023-06-22 15:26:00,260 - INFO - Distilling data from client: Client23
2023-06-22 15:26:00,261 - INFO - train loss: 0.00022051559703017604
2023-06-22 15:26:00,261 - INFO - train acc: 1.0
2023-06-22 15:26:00,314 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.75      0.75        75
           6       0.69      0.84      0.76        56
           9       0.82      0.68      0.75        69

    accuracy                           0.75       200
   macro avg       0.75      0.76      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-06-22 15:26:00,317 - INFO - test loss 0.019879510006628926
2023-06-22 15:26:00,318 - INFO - test acc 0.75
2023-06-22 15:26:03,194 - INFO - Distilling data from client: Client23
2023-06-22 15:26:03,195 - INFO - train loss: 0.00023447107909908323
2023-06-22 15:26:03,195 - INFO - train acc: 1.0
2023-06-22 15:26:03,243 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.75      0.75        75
           6       0.67      0.84      0.75        56
           9       0.82      0.67      0.74        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.76      0.74      0.74       200

2023-06-22 15:26:03,244 - INFO - test loss 0.01947731524886389
2023-06-22 15:26:03,244 - INFO - test acc 0.7450000047683716
2023-06-22 15:26:06,132 - INFO - Distilling data from client: Client23
2023-06-22 15:26:06,132 - INFO - train loss: 0.0001862420548092451
2023-06-22 15:26:06,132 - INFO - train acc: 1.0
2023-06-22 15:26:06,185 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        75
           6       0.66      0.88      0.75        56
           9       0.85      0.65      0.74        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.76      0.74      0.74       200

2023-06-22 15:26:06,186 - INFO - test loss 0.019337616769167926
2023-06-22 15:26:06,187 - INFO - test acc 0.7450000047683716
2023-06-22 15:26:09,128 - INFO - Distilling data from client: Client23
2023-06-22 15:26:09,129 - INFO - train loss: 0.0002243385041391265
2023-06-22 15:26:09,129 - INFO - train acc: 1.0
2023-06-22 15:26:09,190 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.72      0.72        75
           6       0.67      0.88      0.76        56
           9       0.84      0.62      0.72        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.75      0.73      0.73       200

2023-06-22 15:26:09,193 - INFO - test loss 0.02007326799731034
2023-06-22 15:26:09,194 - INFO - test acc 0.7299999594688416
2023-06-22 15:26:12,022 - INFO - Distilling data from client: Client23
2023-06-22 15:26:12,022 - INFO - train loss: 0.00023602997443699224
2023-06-22 15:26:12,023 - INFO - train acc: 1.0
2023-06-22 15:26:12,083 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.69      0.71        75
           6       0.67      0.88      0.76        56
           9       0.82      0.65      0.73        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:26:12,084 - INFO - test loss 0.019760804185611765
2023-06-22 15:26:12,085 - INFO - test acc 0.7299999594688416
2023-06-22 15:26:14,934 - INFO - Distilling data from client: Client23
2023-06-22 15:26:14,934 - INFO - train loss: 0.00017351936478317737
2023-06-22 15:26:14,934 - INFO - train acc: 1.0
2023-06-22 15:26:14,982 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        75
           6       0.68      0.89      0.78        56
           9       0.85      0.64      0.73        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.76      0.74      0.74       200

2023-06-22 15:26:14,984 - INFO - test loss 0.01946302643814552
2023-06-22 15:26:14,984 - INFO - test acc 0.7450000047683716
2023-06-22 15:26:17,756 - INFO - Distilling data from client: Client23
2023-06-22 15:26:17,756 - INFO - train loss: 0.0001788602586035564
2023-06-22 15:26:17,756 - INFO - train acc: 1.0
2023-06-22 15:26:17,820 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.72      0.73        75
           6       0.66      0.86      0.74        56
           9       0.83      0.65      0.73        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.75      0.73      0.73       200

2023-06-22 15:26:17,820 - INFO - test loss 0.019691030635203968
2023-06-22 15:26:17,826 - INFO - test acc 0.73499995470047
2023-06-22 15:26:20,662 - INFO - Distilling data from client: Client23
2023-06-22 15:26:20,662 - INFO - train loss: 0.00015061727412976646
2023-06-22 15:26:20,662 - INFO - train acc: 1.0
2023-06-22 15:26:20,731 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.74        75
           6       0.69      0.86      0.76        56
           9       0.82      0.67      0.74        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:26:20,732 - INFO - test loss 0.01969322182472437
2023-06-22 15:26:20,732 - INFO - test acc 0.7450000047683716
2023-06-22 15:26:23,530 - INFO - Distilling data from client: Client23
2023-06-22 15:26:23,533 - INFO - train loss: 0.00016249309699503697
2023-06-22 15:26:23,533 - INFO - train acc: 1.0
2023-06-22 15:26:23,600 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.75      0.73        75
           6       0.70      0.89      0.79        56
           9       0.84      0.62      0.72        69

    accuracy                           0.74       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.74      0.74       200

2023-06-22 15:26:23,601 - INFO - test loss 0.01989289875593741
2023-06-22 15:26:23,601 - INFO - test acc 0.7450000047683716
2023-06-22 15:26:26,402 - INFO - Distilling data from client: Client23
2023-06-22 15:26:26,403 - INFO - train loss: 0.00015478375776235647
2023-06-22 15:26:26,403 - INFO - train acc: 1.0
2023-06-22 15:26:26,457 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.75      0.75        75
           6       0.68      0.86      0.76        56
           9       0.85      0.67      0.75        69

    accuracy                           0.75       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-06-22 15:26:26,457 - INFO - test loss 0.01971378904881907
2023-06-22 15:26:26,457 - INFO - test acc 0.75
2023-06-22 15:26:29,183 - INFO - Distilling data from client: Client23
2023-06-22 15:26:29,184 - INFO - train loss: 0.00015891343649594405
2023-06-22 15:26:29,184 - INFO - train acc: 1.0
2023-06-22 15:26:29,254 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.69      0.71        75
           6       0.66      0.86      0.74        56
           9       0.82      0.67      0.74        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:26:29,255 - INFO - test loss 0.01963972535169241
2023-06-22 15:26:29,255 - INFO - test acc 0.7299999594688416
2023-06-22 15:26:29,346 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:26:29,358 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:26:29,372 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:26:29,383 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:26:29,394 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:26:29,405 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:26:29,418 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:26:29,430 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:26:29,442 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:26:30,004 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client23//synthetic.png
2023-06-22 15:26:30,033 - INFO - c: 1.0 and total_data_in_this_class: 268
2023-06-22 15:26:30,033 - INFO - c: 8.0 and total_data_in_this_class: 531
2023-06-22 15:26:30,033 - INFO - c: 1.0 and total_data_in_this_class: 65
2023-06-22 15:26:30,033 - INFO - c: 8.0 and total_data_in_this_class: 135
2023-06-22 15:26:30,168 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07902669906616211 sec
2023-06-22 15:26:30,244 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0747992992401123 sec
2023-06-22 15:26:30,253 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16712641716003418 sec
2023-06-22 15:26:30,256 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:26:30,315 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05762362480163574 sec
2023-06-22 15:26:30,315 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:26:30,491 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17537736892700195 sec
2023-06-22 15:26:30,523 - INFO - initial test loss: 0.015404913136465817
2023-06-22 15:26:30,523 - INFO - initial test acc: 0.8399999737739563
2023-06-22 15:26:30,541 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012764215469360352 sec
2023-06-22 15:26:31,521 - WARNING - Finished tracing + transforming update_fn for pjit in 0.9948039054870605 sec
2023-06-22 15:26:31,527 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:26:31,627 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.1003885269165039 sec
2023-06-22 15:26:31,628 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:26:32,062 - WARNING - Finished XLA compilation of jit(update_fn) in 0.43402886390686035 sec
2023-06-22 15:26:34,287 - INFO - Distilling data from client: Client24
2023-06-22 15:26:34,289 - INFO - train loss: 0.0028023811989082515
2023-06-22 15:26:34,290 - INFO - train acc: 0.9888268113136292
2023-06-22 15:26:34,434 - INFO - report:               precision    recall  f1-score   support

           1       0.67      0.78      0.72        65
           8       0.89      0.81      0.85       135

    accuracy                           0.81       200
   macro avg       0.78      0.80      0.79       200
weighted avg       0.82      0.81      0.81       200

2023-06-22 15:26:34,435 - INFO - test loss 0.014855018312971474
2023-06-22 15:26:34,436 - INFO - test acc 0.8050000071525574
2023-06-22 15:26:36,580 - INFO - Distilling data from client: Client24
2023-06-22 15:26:36,581 - INFO - train loss: 0.0017222539607648771
2023-06-22 15:26:36,582 - INFO - train acc: 0.9972066879272461
2023-06-22 15:26:36,700 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.83      0.75        65
           8       0.91      0.81      0.86       135

    accuracy                           0.82       200
   macro avg       0.80      0.82      0.80       200
weighted avg       0.84      0.82      0.82       200

2023-06-22 15:26:36,701 - INFO - test loss 0.013649562496052184
2023-06-22 15:26:36,701 - INFO - test acc 0.8199999928474426
2023-06-22 15:26:38,771 - INFO - Distilling data from client: Client24
2023-06-22 15:26:38,771 - INFO - train loss: 0.0017502692357745364
2023-06-22 15:26:38,771 - INFO - train acc: 0.9972066879272461
2023-06-22 15:26:38,872 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.82      0.76        65
           8       0.90      0.84      0.87       135

    accuracy                           0.83       200
   macro avg       0.81      0.83      0.82       200
weighted avg       0.84      0.83      0.84       200

2023-06-22 15:26:38,873 - INFO - test loss 0.013510097446600866
2023-06-22 15:26:38,873 - INFO - test acc 0.8349999785423279
2023-06-22 15:26:41,237 - INFO - Distilling data from client: Client24
2023-06-22 15:26:41,239 - INFO - train loss: 0.001464798338807437
2023-06-22 15:26:41,240 - INFO - train acc: 0.9972066879272461
2023-06-22 15:26:41,284 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.82      0.76        65
           8       0.90      0.84      0.87       135

    accuracy                           0.83       200
   macro avg       0.81      0.83      0.82       200
weighted avg       0.84      0.83      0.84       200

2023-06-22 15:26:41,284 - INFO - test loss 0.013899067487379664
2023-06-22 15:26:41,284 - INFO - test acc 0.8349999785423279
2023-06-22 15:26:43,557 - INFO - Distilling data from client: Client24
2023-06-22 15:26:43,557 - INFO - train loss: 0.0013465336370791583
2023-06-22 15:26:43,558 - INFO - train acc: 1.0
2023-06-22 15:26:43,605 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.74      0.71        65
           8       0.87      0.83      0.85       135

    accuracy                           0.80       200
   macro avg       0.77      0.78      0.78       200
weighted avg       0.81      0.80      0.80       200

2023-06-22 15:26:43,607 - INFO - test loss 0.014485845852251886
2023-06-22 15:26:43,607 - INFO - test acc 0.7999999523162842
2023-06-22 15:26:45,964 - INFO - Distilling data from client: Client24
2023-06-22 15:26:45,964 - INFO - train loss: 0.0010017413406460361
2023-06-22 15:26:45,964 - INFO - train acc: 1.0
2023-06-22 15:26:46,076 - INFO - report:               precision    recall  f1-score   support

           1       0.74      0.82      0.77        65
           8       0.91      0.86      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.84      0.83       200
weighted avg       0.85      0.84      0.85       200

2023-06-22 15:26:46,079 - INFO - test loss 0.013651651194491063
2023-06-22 15:26:46,080 - INFO - test acc 0.8449999690055847
2023-06-22 15:26:48,540 - INFO - Distilling data from client: Client24
2023-06-22 15:26:48,541 - INFO - train loss: 0.0011006866715042498
2023-06-22 15:26:48,542 - INFO - train acc: 0.9972066879272461
2023-06-22 15:26:48,590 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.75      0.73        65
           8       0.88      0.84      0.86       135

    accuracy                           0.81       200
   macro avg       0.79      0.80      0.79       200
weighted avg       0.82      0.81      0.82       200

2023-06-22 15:26:48,592 - INFO - test loss 0.013754402049273839
2023-06-22 15:26:48,593 - INFO - test acc 0.8149999976158142
2023-06-22 15:26:51,057 - INFO - Distilling data from client: Client24
2023-06-22 15:26:51,058 - INFO - train loss: 0.0013666720584898007
2023-06-22 15:26:51,059 - INFO - train acc: 0.9972066879272461
2023-06-22 15:26:51,109 - INFO - report:               precision    recall  f1-score   support

           1       0.73      0.80      0.76        65
           8       0.90      0.86      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.83      0.82       200
weighted avg       0.85      0.84      0.84       200

2023-06-22 15:26:51,141 - INFO - test loss 0.013948683971696447
2023-06-22 15:26:51,143 - INFO - test acc 0.8399999737739563
2023-06-22 15:26:53,584 - INFO - Distilling data from client: Client24
2023-06-22 15:26:53,585 - INFO - train loss: 0.0012106557888501978
2023-06-22 15:26:53,585 - INFO - train acc: 0.9972066879272461
2023-06-22 15:26:53,647 - INFO - report:               precision    recall  f1-score   support

           1       0.71      0.75      0.73        65
           8       0.88      0.85      0.86       135

    accuracy                           0.82       200
   macro avg       0.79      0.80      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-06-22 15:26:53,648 - INFO - test loss 0.014364692298416951
2023-06-22 15:26:53,649 - INFO - test acc 0.8199999928474426
2023-06-22 15:26:56,009 - INFO - Distilling data from client: Client24
2023-06-22 15:26:56,010 - INFO - train loss: 0.0010484371898657134
2023-06-22 15:26:56,010 - INFO - train acc: 1.0
2023-06-22 15:26:56,093 - INFO - report:               precision    recall  f1-score   support

           1       0.73      0.83      0.78        65
           8       0.91      0.85      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.84      0.83       200
weighted avg       0.85      0.84      0.85       200

2023-06-22 15:26:56,094 - INFO - test loss 0.013470632202025754
2023-06-22 15:26:56,094 - INFO - test acc 0.8449999690055847
2023-06-22 15:26:58,513 - INFO - Distilling data from client: Client24
2023-06-22 15:26:58,514 - INFO - train loss: 0.001030600379976911
2023-06-22 15:26:58,515 - INFO - train acc: 1.0
2023-06-22 15:26:58,626 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.82      0.78        65
           8       0.91      0.87      0.89       135

    accuracy                           0.85       200
   macro avg       0.83      0.84      0.83       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:26:58,628 - INFO - test loss 0.013161793985717582
2023-06-22 15:26:58,628 - INFO - test acc 0.8499999642372131
2023-06-22 15:27:00,947 - INFO - Distilling data from client: Client24
2023-06-22 15:27:00,949 - INFO - train loss: 0.0010047217174908977
2023-06-22 15:27:00,950 - INFO - train acc: 1.0
2023-06-22 15:27:01,003 - INFO - report:               precision    recall  f1-score   support

           1       0.76      0.77      0.76        65
           8       0.89      0.88      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.83      0.82       200
weighted avg       0.85      0.84      0.85       200

2023-06-22 15:27:01,007 - INFO - test loss 0.013231398262910378
2023-06-22 15:27:01,009 - INFO - test acc 0.8449999690055847
2023-06-22 15:27:03,209 - INFO - Distilling data from client: Client24
2023-06-22 15:27:03,210 - INFO - train loss: 0.0008013998112149261
2023-06-22 15:27:03,211 - INFO - train acc: 1.0
2023-06-22 15:27:03,260 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.74      0.71        65
           8       0.87      0.84      0.85       135

    accuracy                           0.81       200
   macro avg       0.78      0.79      0.78       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:27:03,260 - INFO - test loss 0.01336983026630285
2023-06-22 15:27:03,261 - INFO - test acc 0.8050000071525574
2023-06-22 15:27:05,942 - INFO - Distilling data from client: Client24
2023-06-22 15:27:05,943 - INFO - train loss: 0.0008698716743559032
2023-06-22 15:27:05,943 - INFO - train acc: 1.0
2023-06-22 15:27:06,086 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.75      0.73        65
           8       0.88      0.84      0.86       135

    accuracy                           0.81       200
   macro avg       0.79      0.80      0.79       200
weighted avg       0.82      0.81      0.82       200

2023-06-22 15:27:06,087 - INFO - test loss 0.014345316575366987
2023-06-22 15:27:06,087 - INFO - test acc 0.8149999976158142
2023-06-22 15:27:09,491 - INFO - Distilling data from client: Client24
2023-06-22 15:27:09,491 - INFO - train loss: 0.0008956731396375727
2023-06-22 15:27:09,491 - INFO - train acc: 1.0
2023-06-22 15:27:09,547 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.77      0.75        65
           8       0.89      0.86      0.87       135

    accuracy                           0.83       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.83      0.83      0.83       200

2023-06-22 15:27:09,551 - INFO - test loss 0.013681698646406499
2023-06-22 15:27:09,551 - INFO - test acc 0.8299999833106995
2023-06-22 15:27:12,324 - INFO - Distilling data from client: Client24
2023-06-22 15:27:12,324 - INFO - train loss: 0.0007692034950986702
2023-06-22 15:27:12,325 - INFO - train acc: 1.0
2023-06-22 15:27:12,371 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.74      0.73        65
           8       0.87      0.86      0.87       135

    accuracy                           0.82       200
   macro avg       0.79      0.80      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-06-22 15:27:12,372 - INFO - test loss 0.012951455350281879
2023-06-22 15:27:12,373 - INFO - test acc 0.8199999928474426
2023-06-22 15:27:14,725 - INFO - Distilling data from client: Client24
2023-06-22 15:27:14,725 - INFO - train loss: 0.0008591248365131937
2023-06-22 15:27:14,725 - INFO - train acc: 1.0
2023-06-22 15:27:14,800 - INFO - report:               precision    recall  f1-score   support

           1       0.73      0.74      0.73        65
           8       0.87      0.87      0.87       135

    accuracy                           0.82       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.83      0.82      0.83       200

2023-06-22 15:27:14,801 - INFO - test loss 0.014324271051980994
2023-06-22 15:27:14,802 - INFO - test acc 0.824999988079071
2023-06-22 15:27:17,123 - INFO - Distilling data from client: Client24
2023-06-22 15:27:17,124 - INFO - train loss: 0.0007210883843156762
2023-06-22 15:27:17,124 - INFO - train acc: 1.0
2023-06-22 15:27:17,180 - INFO - report:               precision    recall  f1-score   support

           1       0.74      0.83      0.78        65
           8       0.91      0.86      0.89       135

    accuracy                           0.85       200
   macro avg       0.83      0.85      0.83       200
weighted avg       0.86      0.85      0.85       200

2023-06-22 15:27:17,180 - INFO - test loss 0.013165038113481402
2023-06-22 15:27:17,181 - INFO - test acc 0.8499999642372131
2023-06-22 15:27:19,472 - INFO - Distilling data from client: Client24
2023-06-22 15:27:19,473 - INFO - train loss: 0.0008560948658242616
2023-06-22 15:27:19,474 - INFO - train acc: 1.0
2023-06-22 15:27:19,516 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.82      0.78        65
           8       0.91      0.87      0.89       135

    accuracy                           0.85       200
   macro avg       0.83      0.84      0.83       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:27:19,517 - INFO - test loss 0.01331000295797474
2023-06-22 15:27:19,517 - INFO - test acc 0.8499999642372131
2023-06-22 15:27:21,764 - INFO - Distilling data from client: Client24
2023-06-22 15:27:21,765 - INFO - train loss: 0.0007150580854078624
2023-06-22 15:27:21,765 - INFO - train acc: 1.0
2023-06-22 15:27:21,891 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.80      0.79        65
           8       0.90      0.89      0.90       135

    accuracy                           0.86       200
   macro avg       0.84      0.84      0.84       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:27:21,893 - INFO - test loss 0.013349677836755756
2023-06-22 15:27:21,893 - INFO - test acc 0.85999995470047
2023-06-22 15:27:24,110 - INFO - Distilling data from client: Client24
2023-06-22 15:27:24,112 - INFO - train loss: 0.0006248865159383466
2023-06-22 15:27:24,113 - INFO - train acc: 1.0
2023-06-22 15:27:24,156 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.77      0.75        65
           8       0.89      0.86      0.87       135

    accuracy                           0.83       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.83      0.83      0.83       200

2023-06-22 15:27:24,156 - INFO - test loss 0.013358245118861679
2023-06-22 15:27:24,157 - INFO - test acc 0.8299999833106995
2023-06-22 15:27:26,350 - INFO - Distilling data from client: Client24
2023-06-22 15:27:26,351 - INFO - train loss: 0.0008424809211377839
2023-06-22 15:27:26,351 - INFO - train acc: 0.9972066879272461
2023-06-22 15:27:26,419 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.78      0.77        65
           8       0.89      0.87      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.83      0.83       200
weighted avg       0.85      0.84      0.85       200

2023-06-22 15:27:26,424 - INFO - test loss 0.012996144015632505
2023-06-22 15:27:26,425 - INFO - test acc 0.8449999690055847
2023-06-22 15:27:28,736 - INFO - Distilling data from client: Client24
2023-06-22 15:27:28,740 - INFO - train loss: 0.000760587128535246
2023-06-22 15:27:28,741 - INFO - train acc: 1.0
2023-06-22 15:27:28,786 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.78      0.75        65
           8       0.89      0.85      0.87       135

    accuracy                           0.83       200
   macro avg       0.80      0.82      0.81       200
weighted avg       0.84      0.83      0.83       200

2023-06-22 15:27:28,788 - INFO - test loss 0.013699467017688257
2023-06-22 15:27:28,789 - INFO - test acc 0.8299999833106995
2023-06-22 15:27:31,038 - INFO - Distilling data from client: Client24
2023-06-22 15:27:31,040 - INFO - train loss: 0.0007074443762788629
2023-06-22 15:27:31,041 - INFO - train acc: 1.0
2023-06-22 15:27:31,076 - INFO - report:               precision    recall  f1-score   support

           1       0.76      0.74      0.75        65
           8       0.88      0.89      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.82       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:27:31,077 - INFO - test loss 0.013648056817226457
2023-06-22 15:27:31,078 - INFO - test acc 0.8399999737739563
2023-06-22 15:27:33,491 - INFO - Distilling data from client: Client24
2023-06-22 15:27:33,492 - INFO - train loss: 0.000744929850085099
2023-06-22 15:27:33,492 - INFO - train acc: 1.0
2023-06-22 15:27:33,567 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.78      0.77        65
           8       0.89      0.87      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.83      0.83       200
weighted avg       0.85      0.84      0.85       200

2023-06-22 15:27:33,568 - INFO - test loss 0.012951090546469125
2023-06-22 15:27:33,568 - INFO - test acc 0.8449999690055847
2023-06-22 15:27:33,601 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:27:33,622 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:27:33,641 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:27:33,658 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:27:33,676 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:27:33,691 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:27:34,134 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client24//synthetic.png
2023-06-22 15:27:34,157 - INFO - c: 2.0 and total_data_in_this_class: 269
2023-06-22 15:27:34,157 - INFO - c: 4.0 and total_data_in_this_class: 256
2023-06-22 15:27:34,157 - INFO - c: 7.0 and total_data_in_this_class: 274
2023-06-22 15:27:34,157 - INFO - c: 2.0 and total_data_in_this_class: 64
2023-06-22 15:27:34,157 - INFO - c: 4.0 and total_data_in_this_class: 77
2023-06-22 15:27:34,157 - INFO - c: 7.0 and total_data_in_this_class: 59
2023-06-22 15:27:34,286 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08107829093933105 sec
2023-06-22 15:27:34,361 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07340168952941895 sec
2023-06-22 15:27:34,369 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16656064987182617 sec
2023-06-22 15:27:34,373 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:27:34,429 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.0559689998626709 sec
2023-06-22 15:27:34,429 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:27:34,608 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1785447597503662 sec
2023-06-22 15:27:34,650 - INFO - initial test loss: 0.03176735245531477
2023-06-22 15:27:34,650 - INFO - initial test acc: 0.5649999976158142
2023-06-22 15:27:34,669 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.013941049575805664 sec
2023-06-22 15:27:34,867 - WARNING - Finished tracing + transforming update_fn for pjit in 0.21302127838134766 sec
2023-06-22 15:27:34,873 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:27:34,975 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10203170776367188 sec
2023-06-22 15:27:34,975 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:27:35,408 - WARNING - Finished XLA compilation of jit(update_fn) in 0.43181777000427246 sec
2023-06-22 15:27:38,450 - INFO - Distilling data from client: Client25
2023-06-22 15:27:38,451 - INFO - train loss: 0.003845636258327781
2023-06-22 15:27:38,451 - INFO - train acc: 0.9902534484863281
2023-06-22 15:27:38,654 - INFO - report:               precision    recall  f1-score   support

           2       0.46      0.48      0.47        64
           4       0.60      0.62      0.61        77
           7       0.66      0.59      0.62        59

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-06-22 15:27:38,654 - INFO - test loss 0.0292080282571868
2023-06-22 15:27:38,655 - INFO - test acc 0.5699999928474426
2023-06-22 15:27:41,790 - INFO - Distilling data from client: Client25
2023-06-22 15:27:41,791 - INFO - train loss: 0.0024139143256655888
2023-06-22 15:27:41,791 - INFO - train acc: 0.9961013793945312
2023-06-22 15:27:41,951 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.59      0.56        64
           4       0.67      0.62      0.64        77
           7       0.71      0.68      0.70        59

    accuracy                           0.63       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.64      0.63      0.63       200

2023-06-22 15:27:41,952 - INFO - test loss 0.028855667104344326
2023-06-22 15:27:41,952 - INFO - test acc 0.6299999952316284
2023-06-22 15:27:45,251 - INFO - Distilling data from client: Client25
2023-06-22 15:27:45,251 - INFO - train loss: 0.002016962492087914
2023-06-22 15:27:45,251 - INFO - train acc: 0.9980506896972656
2023-06-22 15:27:45,457 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.61      0.55        64
           4       0.69      0.64      0.66        77
           7       0.75      0.66      0.70        59

    accuracy                           0.64       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.64      0.64       200

2023-06-22 15:27:45,457 - INFO - test loss 0.028923481115383832
2023-06-22 15:27:45,457 - INFO - test acc 0.6349999904632568
2023-06-22 15:27:48,637 - INFO - Distilling data from client: Client25
2023-06-22 15:27:48,638 - INFO - train loss: 0.001786893043646836
2023-06-22 15:27:48,638 - INFO - train acc: 0.9980506896972656
2023-06-22 15:27:48,700 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.66      0.58        64
           4       0.69      0.60      0.64        77
           7       0.74      0.66      0.70        59

    accuracy                           0.64       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.64      0.64       200

2023-06-22 15:27:48,702 - INFO - test loss 0.02989261520763385
2023-06-22 15:27:48,702 - INFO - test acc 0.6349999904632568
2023-06-22 15:27:51,779 - INFO - Distilling data from client: Client25
2023-06-22 15:27:51,780 - INFO - train loss: 0.0015221695767445047
2023-06-22 15:27:51,781 - INFO - train acc: 1.0
2023-06-22 15:27:51,835 - INFO - report:               precision    recall  f1-score   support

           2       0.49      0.58      0.53        64
           4       0.69      0.64      0.66        77
           7       0.72      0.66      0.69        59

    accuracy                           0.62       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.64      0.62      0.63       200

2023-06-22 15:27:51,835 - INFO - test loss 0.029916420955418126
2023-06-22 15:27:51,835 - INFO - test acc 0.625
2023-06-22 15:27:54,932 - INFO - Distilling data from client: Client25
2023-06-22 15:27:54,933 - INFO - train loss: 0.0013890567058723947
2023-06-22 15:27:54,933 - INFO - train acc: 0.9980506896972656
2023-06-22 15:27:55,147 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.61      0.56        64
           4       0.71      0.64      0.67        77
           7       0.71      0.68      0.70        59

    accuracy                           0.64       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.64      0.64       200

2023-06-22 15:27:55,147 - INFO - test loss 0.029757417899986004
2023-06-22 15:27:55,147 - INFO - test acc 0.6399999856948853
2023-06-22 15:27:58,092 - INFO - Distilling data from client: Client25
2023-06-22 15:27:58,093 - INFO - train loss: 0.0013089196675936724
2023-06-22 15:27:58,093 - INFO - train acc: 1.0
2023-06-22 15:27:58,155 - INFO - report:               precision    recall  f1-score   support

           2       0.49      0.61      0.55        64
           4       0.67      0.61      0.64        77
           7       0.78      0.68      0.73        59

    accuracy                           0.63       200
   macro avg       0.65      0.63      0.64       200
weighted avg       0.65      0.63      0.64       200

2023-06-22 15:27:58,157 - INFO - test loss 0.029624309826936878
2023-06-22 15:27:58,158 - INFO - test acc 0.6299999952316284
2023-06-22 15:28:01,336 - INFO - Distilling data from client: Client25
2023-06-22 15:28:01,338 - INFO - train loss: 0.0011300291122066128
2023-06-22 15:28:01,339 - INFO - train acc: 1.0
2023-06-22 15:28:01,408 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.55      0.53        64
           4       0.68      0.64      0.66        77
           7       0.70      0.71      0.71        59

    accuracy                           0.63       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.63      0.63       200

2023-06-22 15:28:01,411 - INFO - test loss 0.029489264919683508
2023-06-22 15:28:01,412 - INFO - test acc 0.6299999952316284
2023-06-22 15:28:04,565 - INFO - Distilling data from client: Client25
2023-06-22 15:28:04,566 - INFO - train loss: 0.0012476165135635164
2023-06-22 15:28:04,567 - INFO - train acc: 1.0
2023-06-22 15:28:04,722 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.66      0.59        64
           4       0.72      0.65      0.68        77
           7       0.72      0.64      0.68        59

    accuracy                           0.65       200
   macro avg       0.66      0.65      0.65       200
weighted avg       0.66      0.65      0.65       200

2023-06-22 15:28:04,722 - INFO - test loss 0.029853902311659755
2023-06-22 15:28:04,722 - INFO - test acc 0.6499999761581421
2023-06-22 15:28:07,679 - INFO - Distilling data from client: Client25
2023-06-22 15:28:07,679 - INFO - train loss: 0.000986096784816119
2023-06-22 15:28:07,679 - INFO - train acc: 1.0
2023-06-22 15:28:07,731 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.58      0.56        64
           4       0.70      0.66      0.68        77
           7       0.71      0.69      0.70        59

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:28:07,732 - INFO - test loss 0.030703630621864344
2023-06-22 15:28:07,732 - INFO - test acc 0.6449999809265137
2023-06-22 15:28:10,762 - INFO - Distilling data from client: Client25
2023-06-22 15:28:10,763 - INFO - train loss: 0.0011668766028428634
2023-06-22 15:28:10,763 - INFO - train acc: 1.0
2023-06-22 15:28:10,820 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.62      0.58        64
           4       0.69      0.62      0.65        77
           7       0.71      0.68      0.70        59

    accuracy                           0.64       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.64      0.64       200

2023-06-22 15:28:10,820 - INFO - test loss 0.0313853418970693
2023-06-22 15:28:10,820 - INFO - test acc 0.6399999856948853
2023-06-22 15:28:13,696 - INFO - Distilling data from client: Client25
2023-06-22 15:28:13,696 - INFO - train loss: 0.0010938487455227713
2023-06-22 15:28:13,697 - INFO - train acc: 1.0
2023-06-22 15:28:13,764 - INFO - report:               precision    recall  f1-score   support

           2       0.50      0.58      0.54        64
           4       0.65      0.61      0.63        77
           7       0.76      0.69      0.73        59

    accuracy                           0.62       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.64      0.62      0.63       200

2023-06-22 15:28:13,766 - INFO - test loss 0.030187474026293692
2023-06-22 15:28:13,767 - INFO - test acc 0.625
2023-06-22 15:28:16,813 - INFO - Distilling data from client: Client25
2023-06-22 15:28:16,813 - INFO - train loss: 0.0009944234102944897
2023-06-22 15:28:16,813 - INFO - train acc: 1.0
2023-06-22 15:28:16,875 - INFO - report:               precision    recall  f1-score   support

           2       0.47      0.56      0.51        64
           4       0.65      0.58      0.62        77
           7       0.72      0.66      0.69        59

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.61       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:28:16,875 - INFO - test loss 0.030751636573464764
2023-06-22 15:28:16,876 - INFO - test acc 0.5999999642372131
2023-06-22 15:28:20,106 - INFO - Distilling data from client: Client25
2023-06-22 15:28:20,106 - INFO - train loss: 0.0010977186136394833
2023-06-22 15:28:20,106 - INFO - train acc: 1.0
2023-06-22 15:28:20,226 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.61      0.57        64
           4       0.71      0.68      0.69        77
           7       0.70      0.64      0.67        59

    accuracy                           0.65       200
   macro avg       0.65      0.64      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:28:20,228 - INFO - test loss 0.029658955422718118
2023-06-22 15:28:20,228 - INFO - test acc 0.6449999809265137
2023-06-22 15:28:23,217 - INFO - Distilling data from client: Client25
2023-06-22 15:28:23,217 - INFO - train loss: 0.000937070201684997
2023-06-22 15:28:23,218 - INFO - train acc: 1.0
2023-06-22 15:28:23,282 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.61      0.56        64
           4       0.67      0.58      0.62        77
           7       0.68      0.66      0.67        59

    accuracy                           0.61       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.62      0.61      0.62       200

2023-06-22 15:28:23,284 - INFO - test loss 0.03087132625932698
2023-06-22 15:28:23,284 - INFO - test acc 0.6150000095367432
2023-06-22 15:28:26,655 - INFO - Distilling data from client: Client25
2023-06-22 15:28:26,655 - INFO - train loss: 0.0008715839773085164
2023-06-22 15:28:26,655 - INFO - train acc: 1.0
2023-06-22 15:28:26,713 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.59      0.55        64
           4       0.67      0.62      0.64        77
           7       0.71      0.66      0.68        59

    accuracy                           0.62       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.62      0.63       200

2023-06-22 15:28:26,713 - INFO - test loss 0.03074337625474614
2023-06-22 15:28:26,714 - INFO - test acc 0.625
2023-06-22 15:28:29,767 - INFO - Distilling data from client: Client25
2023-06-22 15:28:29,767 - INFO - train loss: 0.0007889642658992152
2023-06-22 15:28:29,767 - INFO - train acc: 1.0
2023-06-22 15:28:29,830 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.58      0.55        64
           4       0.69      0.66      0.68        77
           7       0.68      0.64      0.66        59

    accuracy                           0.63       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.63      0.63       200

2023-06-22 15:28:29,830 - INFO - test loss 0.03061791190240709
2023-06-22 15:28:29,830 - INFO - test acc 0.6299999952316284
2023-06-22 15:28:32,913 - INFO - Distilling data from client: Client25
2023-06-22 15:28:32,914 - INFO - train loss: 0.0009204161219622046
2023-06-22 15:28:32,914 - INFO - train acc: 1.0
2023-06-22 15:28:32,967 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.59      0.55        64
           4       0.69      0.62      0.65        77
           7       0.70      0.68      0.69        59

    accuracy                           0.63       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.64      0.63      0.63       200

2023-06-22 15:28:32,968 - INFO - test loss 0.030535756241117838
2023-06-22 15:28:32,968 - INFO - test acc 0.6299999952316284
2023-06-22 15:28:35,753 - INFO - Distilling data from client: Client25
2023-06-22 15:28:35,754 - INFO - train loss: 0.0007917535953448724
2023-06-22 15:28:35,754 - INFO - train acc: 1.0
2023-06-22 15:28:35,809 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.62      0.57        64
           4       0.67      0.60      0.63        77
           7       0.71      0.66      0.68        59

    accuracy                           0.62       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.62      0.63       200

2023-06-22 15:28:35,809 - INFO - test loss 0.030508527624557982
2023-06-22 15:28:35,810 - INFO - test acc 0.625
2023-06-22 15:28:38,787 - INFO - Distilling data from client: Client25
2023-06-22 15:28:38,788 - INFO - train loss: 0.0008813248818519531
2023-06-22 15:28:38,788 - INFO - train acc: 1.0
2023-06-22 15:28:38,870 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.56      0.54        64
           4       0.67      0.65      0.66        77
           7       0.71      0.66      0.68        59

    accuracy                           0.62       200
   macro avg       0.63      0.62      0.63       200
weighted avg       0.63      0.62      0.63       200

2023-06-22 15:28:38,871 - INFO - test loss 0.0318916660694123
2023-06-22 15:28:38,872 - INFO - test acc 0.625
2023-06-22 15:28:41,816 - INFO - Distilling data from client: Client25
2023-06-22 15:28:41,816 - INFO - train loss: 0.0007348115274173457
2023-06-22 15:28:41,817 - INFO - train acc: 1.0
2023-06-22 15:28:41,888 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.62      0.58        64
           4       0.70      0.62      0.66        77
           7       0.68      0.66      0.67        59

    accuracy                           0.64       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:28:41,889 - INFO - test loss 0.0306007640830077
2023-06-22 15:28:41,889 - INFO - test acc 0.6349999904632568
2023-06-22 15:28:44,762 - INFO - Distilling data from client: Client25
2023-06-22 15:28:44,762 - INFO - train loss: 0.000778489570596121
2023-06-22 15:28:44,762 - INFO - train acc: 1.0
2023-06-22 15:28:44,812 - INFO - report:               precision    recall  f1-score   support

           2       0.50      0.55      0.52        64
           4       0.66      0.62      0.64        77
           7       0.67      0.64      0.66        59

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.61       200
weighted avg       0.61      0.60      0.61       200

2023-06-22 15:28:44,813 - INFO - test loss 0.030693193509294888
2023-06-22 15:28:44,813 - INFO - test acc 0.6049999594688416
2023-06-22 15:28:47,674 - INFO - Distilling data from client: Client25
2023-06-22 15:28:47,675 - INFO - train loss: 0.000719611200258052
2023-06-22 15:28:47,675 - INFO - train acc: 1.0
2023-06-22 15:28:47,726 - INFO - report:               precision    recall  f1-score   support

           2       0.49      0.58      0.53        64
           4       0.67      0.62      0.64        77
           7       0.74      0.66      0.70        59

    accuracy                           0.62       200
   macro avg       0.63      0.62      0.62       200
weighted avg       0.63      0.62      0.62       200

2023-06-22 15:28:47,726 - INFO - test loss 0.030724270006831263
2023-06-22 15:28:47,727 - INFO - test acc 0.6200000047683716
2023-06-22 15:28:50,620 - INFO - Distilling data from client: Client25
2023-06-22 15:28:50,620 - INFO - train loss: 0.0006874123288118667
2023-06-22 15:28:50,620 - INFO - train acc: 1.0
2023-06-22 15:28:50,685 - INFO - report:               precision    recall  f1-score   support

           2       0.50      0.56      0.53        64
           4       0.67      0.62      0.64        77
           7       0.71      0.68      0.70        59

    accuracy                           0.62       200
   macro avg       0.63      0.62      0.62       200
weighted avg       0.63      0.62      0.62       200

2023-06-22 15:28:50,686 - INFO - test loss 0.03199361516891281
2023-06-22 15:28:50,686 - INFO - test acc 0.6200000047683716
2023-06-22 15:28:54,044 - INFO - Distilling data from client: Client25
2023-06-22 15:28:54,045 - INFO - train loss: 0.000677511676783144
2023-06-22 15:28:54,045 - INFO - train acc: 1.0
2023-06-22 15:28:54,119 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.59      0.55        64
           4       0.68      0.58      0.63        77
           7       0.63      0.63      0.63        59

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:28:54,121 - INFO - test loss 0.031068786419966125
2023-06-22 15:28:54,121 - INFO - test acc 0.5999999642372131
2023-06-22 15:28:54,153 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:28:54,173 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:28:54,191 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:28:54,209 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:28:54,222 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:28:54,233 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:28:54,246 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:28:54,259 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:28:54,272 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:28:54,871 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client25//synthetic.png
2023-06-22 15:28:54,896 - INFO - c: 5.0 and total_data_in_this_class: 26
2023-06-22 15:28:54,896 - INFO - c: 6.0 and total_data_in_this_class: 246
2023-06-22 15:28:54,897 - INFO - c: 8.0 and total_data_in_this_class: 527
2023-06-22 15:28:54,897 - INFO - c: 5.0 and total_data_in_this_class: 4
2023-06-22 15:28:54,897 - INFO - c: 6.0 and total_data_in_this_class: 57
2023-06-22 15:28:54,897 - INFO - c: 8.0 and total_data_in_this_class: 139
2023-06-22 15:28:54,939 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005881786346435547 sec
2023-06-22 15:28:54,940 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:28:54,942 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0025720596313476562 sec
2023-06-22 15:28:54,943 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:28:54,960 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01704859733581543 sec
2023-06-22 15:28:54,963 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00043129920959472656 sec
2023-06-22 15:28:54,964 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:28:54,966 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0017647743225097656 sec
2023-06-22 15:28:54,966 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:28:54,979 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01263570785522461 sec
2023-06-22 15:28:54,985 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00023889541625976562 sec
2023-06-22 15:28:54,987 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020647048950195312 sec
2023-06-22 15:28:54,988 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005559921264648438 sec
2023-06-22 15:28:54,990 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0004131793975830078 sec
2023-06-22 15:28:54,991 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000209808349609375 sec
2023-06-22 15:28:54,992 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005035400390625 sec
2023-06-22 15:28:54,993 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004410743713378906 sec
2023-06-22 15:28:54,994 - WARNING - Finished tracing + transforming absolute for pjit in 0.00032138824462890625 sec
2023-06-22 15:28:54,995 - WARNING - Finished tracing + transforming fn for pjit in 0.0004851818084716797 sec
2023-06-22 15:28:54,997 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005831718444824219 sec
2023-06-22 15:28:55,000 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0017843246459960938 sec
2023-06-22 15:28:55,002 - WARNING - Finished tracing + transforming fn for pjit in 0.0004210472106933594 sec
2023-06-22 15:28:55,003 - WARNING - Finished tracing + transforming fn for pjit in 0.00045013427734375 sec
2023-06-22 15:28:55,004 - WARNING - Finished tracing + transforming fn for pjit in 0.00040602684020996094 sec
2023-06-22 15:28:55,005 - WARNING - Finished tracing + transforming fn for pjit in 0.00046062469482421875 sec
2023-06-22 15:28:55,007 - WARNING - Finished tracing + transforming fn for pjit in 0.0003943443298339844 sec
2023-06-22 15:28:55,010 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00031566619873046875 sec
2023-06-22 15:28:55,011 - WARNING - Finished tracing + transforming fn for pjit in 0.0004127025604248047 sec
2023-06-22 15:28:55,013 - WARNING - Finished tracing + transforming fn for pjit in 0.00041961669921875 sec
2023-06-22 15:28:55,019 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006923675537109375 sec
2023-06-22 15:28:55,020 - WARNING - Finished tracing + transforming _mean for pjit in 0.0017781257629394531 sec
2023-06-22 15:28:55,022 - WARNING - Finished tracing + transforming fn for pjit in 0.00041985511779785156 sec
2023-06-22 15:28:55,023 - WARNING - Finished tracing + transforming fn for pjit in 0.00040650367736816406 sec
2023-06-22 15:28:55,025 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005524158477783203 sec
2023-06-22 15:28:55,026 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046944618225097656 sec
2023-06-22 15:28:55,027 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003452301025390625 sec
2023-06-22 15:28:55,028 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004932880401611328 sec
2023-06-22 15:28:55,030 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004069805145263672 sec
2023-06-22 15:28:55,031 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041866302490234375 sec
2023-06-22 15:28:55,033 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0007517337799072266 sec
2023-06-22 15:28:55,033 - WARNING - Finished tracing + transforming _where for pjit in 0.0018262863159179688 sec
2023-06-22 15:28:55,035 - WARNING - Finished tracing + transforming fn for pjit in 0.0004763603210449219 sec
2023-06-22 15:28:55,036 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045752525329589844 sec
2023-06-22 15:28:55,037 - WARNING - Finished tracing + transforming fn for pjit in 0.0003943443298339844 sec
2023-06-22 15:28:55,038 - WARNING - Finished tracing + transforming fn for pjit in 0.00040268898010253906 sec
2023-06-22 15:28:55,040 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039124488830566406 sec
2023-06-22 15:28:55,041 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046539306640625 sec
2023-06-22 15:28:55,042 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005657672882080078 sec
2023-06-22 15:28:55,044 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004749298095703125 sec
2023-06-22 15:28:55,045 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003814697265625 sec
2023-06-22 15:28:55,046 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038433074951171875 sec
2023-06-22 15:28:55,048 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004954338073730469 sec
2023-06-22 15:28:55,048 - WARNING - Finished tracing + transforming _where for pjit in 0.0015408992767333984 sec
2023-06-22 15:28:55,050 - WARNING - Finished tracing + transforming fn for pjit in 0.0004265308380126953 sec
2023-06-22 15:28:55,051 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004305839538574219 sec
2023-06-22 15:28:55,053 - WARNING - Finished tracing + transforming fn for pjit in 0.0003762245178222656 sec
2023-06-22 15:28:55,060 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004837512969970703 sec
2023-06-22 15:28:55,061 - WARNING - Finished tracing + transforming fn for pjit in 0.0005924701690673828 sec
2023-06-22 15:28:55,062 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00047659873962402344 sec
2023-06-22 15:28:55,064 - WARNING - Finished tracing + transforming fn for pjit in 0.0003972053527832031 sec
2023-06-22 15:28:55,070 - WARNING - Finished tracing + transforming fn for pjit in 0.0003592967987060547 sec
2023-06-22 15:28:55,073 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002868175506591797 sec
2023-06-22 15:28:55,074 - WARNING - Finished tracing + transforming fn for pjit in 0.0005257129669189453 sec
2023-06-22 15:28:55,076 - WARNING - Finished tracing + transforming fn for pjit in 0.00041604042053222656 sec
2023-06-22 15:28:55,104 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.12039923667907715 sec
2023-06-22 15:28:55,110 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020432472229003906 sec
2023-06-22 15:28:55,110 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019621849060058594 sec
2023-06-22 15:28:55,111 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00048279762268066406 sec
2023-06-22 15:28:55,116 - WARNING - Finished tracing + transforming fn for pjit in 0.0003933906555175781 sec
2023-06-22 15:28:55,117 - WARNING - Finished tracing + transforming fn for pjit in 0.00042629241943359375 sec
2023-06-22 15:28:55,119 - WARNING - Finished tracing + transforming fn for pjit in 0.00040650367736816406 sec
2023-06-22 15:28:55,129 - WARNING - Finished tracing + transforming fn for pjit in 0.00038886070251464844 sec
2023-06-22 15:28:55,131 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003635883331298828 sec
2023-06-22 15:28:55,132 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004520416259765625 sec
2023-06-22 15:28:55,133 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029349327087402344 sec
2023-06-22 15:28:55,134 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005545616149902344 sec
2023-06-22 15:28:55,136 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003733634948730469 sec
2023-06-22 15:28:55,136 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00036907196044921875 sec
2023-06-22 15:28:55,138 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004584789276123047 sec
2023-06-22 15:28:55,138 - WARNING - Finished tracing + transforming _where for pjit in 0.001466989517211914 sec
2023-06-22 15:28:55,140 - WARNING - Finished tracing + transforming fn for pjit in 0.0004646778106689453 sec
2023-06-22 15:28:55,141 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046539306640625 sec
2023-06-22 15:28:55,142 - WARNING - Finished tracing + transforming fn for pjit in 0.00039315223693847656 sec
2023-06-22 15:28:55,144 - WARNING - Finished tracing + transforming fn for pjit in 0.0005192756652832031 sec
2023-06-22 15:28:55,164 - WARNING - Finished tracing + transforming fn for pjit in 0.000370025634765625 sec
2023-06-22 15:28:55,196 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08991694450378418 sec
2023-06-22 15:28:55,198 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002028942108154297 sec
2023-06-22 15:28:55,200 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002422332763671875 sec
2023-06-22 15:28:55,200 - WARNING - Finished tracing + transforming _where for pjit in 0.0011031627655029297 sec
2023-06-22 15:28:55,201 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005152225494384766 sec
2023-06-22 15:28:55,202 - WARNING - Finished tracing + transforming trace for pjit in 0.004395961761474609 sec
2023-06-22 15:28:55,206 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0001819133758544922 sec
2023-06-22 15:28:55,207 - WARNING - Finished tracing + transforming tril for pjit in 0.0010936260223388672 sec
2023-06-22 15:28:55,208 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.003139019012451172 sec
2023-06-22 15:28:55,209 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00018262863159179688 sec
2023-06-22 15:28:55,210 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00018310546875 sec
2023-06-22 15:28:55,214 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0026144981384277344 sec
2023-06-22 15:28:55,221 - WARNING - Finished tracing + transforming _solve for pjit in 0.016321182250976562 sec
2023-06-22 15:28:55,222 - WARNING - Finished tracing + transforming dot for pjit in 0.0004985332489013672 sec
2023-06-22 15:28:55,226 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.24449896812438965 sec
2023-06-22 15:28:55,229 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:28:55,285 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.055587053298950195 sec
2023-06-22 15:28:55,285 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:28:55,444 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1587238311767578 sec
2023-06-22 15:28:55,452 - INFO - initial test loss: 0.02339624576835876
2023-06-22 15:28:55,452 - INFO - initial test acc: 0.6649999618530273
2023-06-22 15:28:55,460 - WARNING - Finished tracing + transforming dot for pjit in 0.0006034374237060547 sec
2023-06-22 15:28:55,461 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004706382751464844 sec
2023-06-22 15:28:55,463 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006763935089111328 sec
2023-06-22 15:28:55,464 - WARNING - Finished tracing + transforming _mean for pjit in 0.0017931461334228516 sec
2023-06-22 15:28:55,466 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0003349781036376953 sec
2023-06-22 15:28:55,467 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00030684471130371094 sec
2023-06-22 15:28:55,468 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004105567932128906 sec
2023-06-22 15:28:55,469 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006029605865478516 sec
2023-06-22 15:28:55,470 - WARNING - Finished tracing + transforming _mean for pjit in 0.0017347335815429688 sec
2023-06-22 15:28:55,471 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.016347169876098633 sec
2023-06-22 15:28:55,485 - WARNING - Finished tracing + transforming fn for pjit in 0.0004723072052001953 sec
2023-06-22 15:28:55,486 - WARNING - Finished tracing + transforming fn for pjit in 0.0004374980926513672 sec
2023-06-22 15:28:55,487 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003905296325683594 sec
2023-06-22 15:28:55,488 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004966259002685547 sec
2023-06-22 15:28:55,489 - WARNING - Finished tracing + transforming _where for pjit in 0.0015187263488769531 sec
2023-06-22 15:28:55,502 - WARNING - Finished tracing + transforming fn for pjit in 0.0004074573516845703 sec
2023-06-22 15:28:55,503 - WARNING - Finished tracing + transforming fn for pjit in 0.00045990943908691406 sec
2023-06-22 15:28:55,504 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003807544708251953 sec
2023-06-22 15:28:55,506 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005583763122558594 sec
2023-06-22 15:28:55,507 - WARNING - Finished tracing + transforming _where for pjit in 0.001569986343383789 sec
2023-06-22 15:28:55,560 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004074573516845703 sec
2023-06-22 15:28:55,644 - WARNING - Finished tracing + transforming fn for pjit in 0.0005056858062744141 sec
2023-06-22 15:28:55,645 - WARNING - Finished tracing + transforming fn for pjit in 0.00040721893310546875 sec
2023-06-22 15:28:55,646 - WARNING - Finished tracing + transforming square for pjit in 0.0003132820129394531 sec
2023-06-22 15:28:55,650 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003714561462402344 sec
2023-06-22 15:28:55,653 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0009019374847412109 sec
2023-06-22 15:28:55,654 - WARNING - Finished tracing + transforming fn for pjit in 0.0004763603210449219 sec
2023-06-22 15:28:55,655 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004038810729980469 sec
2023-06-22 15:28:55,656 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003952980041503906 sec
2023-06-22 15:28:55,657 - WARNING - Finished tracing + transforming fn for pjit in 0.0004410743713378906 sec
2023-06-22 15:28:55,659 - WARNING - Finished tracing + transforming fn for pjit in 0.0003571510314941406 sec
2023-06-22 15:28:55,659 - WARNING - Finished tracing + transforming square for pjit in 0.0003101825714111328 sec
2023-06-22 15:28:55,663 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004031658172607422 sec
2023-06-22 15:28:55,666 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003063678741455078 sec
2023-06-22 15:28:55,667 - WARNING - Finished tracing + transforming fn for pjit in 0.0004425048828125 sec
2023-06-22 15:28:55,668 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003933906555175781 sec
2023-06-22 15:28:55,669 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003676414489746094 sec
2023-06-22 15:28:55,670 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2161695957183838 sec
2023-06-22 15:28:55,676 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,10]), ShapedArray(float32[51,10]), ShapedArray(float32[51,10]), ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,10]), ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:28:55,781 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10501670837402344 sec
2023-06-22 15:28:55,782 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:28:56,166 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3842017650604248 sec
2023-06-22 15:28:56,309 - INFO - Distilling data from client: Client26
2023-06-22 15:28:56,310 - INFO - train loss: 0.012904934644385265
2023-06-22 15:28:56,310 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:56,369 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.73      0.89      0.80        57
           8       0.97      0.86      0.91       139

    accuracy                           0.85       200
   macro avg       0.57      0.59      0.57       200
weighted avg       0.88      0.85      0.86       200

2023-06-22 15:28:56,369 - INFO - test loss 0.015117193824758741
2023-06-22 15:28:56,369 - INFO - test acc 0.8549999594688416
2023-06-22 15:28:56,505 - INFO - Distilling data from client: Client26
2023-06-22 15:28:56,505 - INFO - train loss: 0.008967877071387305
2023-06-22 15:28:56,505 - INFO - train acc: 0.9215686321258545
2023-06-22 15:28:56,522 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.72      0.89      0.80        57
           8       0.96      0.87      0.91       139

    accuracy                           0.86       200
   macro avg       0.56      0.59      0.57       200
weighted avg       0.87      0.86      0.86       200

2023-06-22 15:28:56,522 - INFO - test loss 0.013951253635384166
2023-06-22 15:28:56,523 - INFO - test acc 0.85999995470047
2023-06-22 15:28:56,664 - INFO - Distilling data from client: Client26
2023-06-22 15:28:56,664 - INFO - train loss: 0.009817210185609417
2023-06-22 15:28:56,664 - INFO - train acc: 0.9411765336990356
2023-06-22 15:28:56,682 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.76      0.88      0.81        57
           8       0.96      0.89      0.93       139

    accuracy                           0.87       200
   macro avg       0.57      0.59      0.58       200
weighted avg       0.88      0.87      0.87       200

2023-06-22 15:28:56,682 - INFO - test loss 0.014194614973580278
2023-06-22 15:28:56,683 - INFO - test acc 0.8700000047683716
2023-06-22 15:28:56,819 - INFO - Distilling data from client: Client26
2023-06-22 15:28:56,819 - INFO - train loss: 0.008354273900262669
2023-06-22 15:28:56,820 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:56,837 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.83      0.86      0.84        57
           8       0.94      0.93      0.93       139

    accuracy                           0.89       200
   macro avg       0.59      0.60      0.59       200
weighted avg       0.89      0.89      0.89       200

2023-06-22 15:28:56,837 - INFO - test loss 0.013091809454141703
2023-06-22 15:28:56,837 - INFO - test acc 0.8899999856948853
2023-06-22 15:28:56,987 - INFO - Distilling data from client: Client26
2023-06-22 15:28:56,987 - INFO - train loss: 0.006087414081159127
2023-06-22 15:28:56,988 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:56,997 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.77      0.89      0.83        57
           8       0.95      0.91      0.93       139

    accuracy                           0.89       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.88      0.89      0.88       200

2023-06-22 15:28:56,997 - INFO - test loss 0.013101233241991462
2023-06-22 15:28:56,998 - INFO - test acc 0.8849999904632568
2023-06-22 15:28:57,131 - INFO - Distilling data from client: Client26
2023-06-22 15:28:57,131 - INFO - train loss: 0.00700648690230612
2023-06-22 15:28:57,131 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:57,141 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.77      0.86      0.81        57
           8       0.94      0.90      0.92       139

    accuracy                           0.87       200
   macro avg       0.57      0.59      0.58       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:28:57,141 - INFO - test loss 0.012990850920277488
2023-06-22 15:28:57,141 - INFO - test acc 0.8700000047683716
2023-06-22 15:28:57,277 - INFO - Distilling data from client: Client26
2023-06-22 15:28:57,277 - INFO - train loss: 0.007958808825906265
2023-06-22 15:28:57,277 - INFO - train acc: 0.960784375667572
2023-06-22 15:28:57,286 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.77      0.89      0.83        57
           8       0.96      0.90      0.93       139

    accuracy                           0.88       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.89      0.88      0.88       200

2023-06-22 15:28:57,287 - INFO - test loss 0.013490656921729653
2023-06-22 15:28:57,287 - INFO - test acc 0.8799999952316284
2023-06-22 15:28:57,426 - INFO - Distilling data from client: Client26
2023-06-22 15:28:57,426 - INFO - train loss: 0.006724519159697502
2023-06-22 15:28:57,427 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:57,436 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.83      0.84      0.83        57
           8       0.95      0.94      0.94       139

    accuracy                           0.89       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.90      0.89      0.89       200

2023-06-22 15:28:57,436 - INFO - test loss 0.012485299380407471
2023-06-22 15:28:57,436 - INFO - test acc 0.8899999856948853
2023-06-22 15:28:57,571 - INFO - Distilling data from client: Client26
2023-06-22 15:28:57,571 - INFO - train loss: 0.007144381481710369
2023-06-22 15:28:57,572 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:57,581 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.75      0.91      0.83        57
           8       0.95      0.88      0.92       139

    accuracy                           0.88       200
   macro avg       0.57      0.60      0.58       200
weighted avg       0.88      0.88      0.87       200

2023-06-22 15:28:57,581 - INFO - test loss 0.012893155421420962
2023-06-22 15:28:57,581 - INFO - test acc 0.875
2023-06-22 15:28:57,716 - INFO - Distilling data from client: Client26
2023-06-22 15:28:57,716 - INFO - train loss: 0.007195021835152817
2023-06-22 15:28:57,717 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:57,726 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.78      0.88      0.83        57
           8       0.94      0.91      0.92       139

    accuracy                           0.88       200
   macro avg       0.57      0.59      0.58       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:28:57,726 - INFO - test loss 0.012309286875908978
2023-06-22 15:28:57,726 - INFO - test acc 0.8799999952316284
2023-06-22 15:28:57,866 - INFO - Distilling data from client: Client26
2023-06-22 15:28:57,866 - INFO - train loss: 0.007746878881845845
2023-06-22 15:28:57,866 - INFO - train acc: 0.960784375667572
2023-06-22 15:28:57,876 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.80      0.82      0.81        57
           8       0.93      0.91      0.92       139

    accuracy                           0.87       200
   macro avg       0.57      0.58      0.58       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:28:57,876 - INFO - test loss 0.013583302522636377
2023-06-22 15:28:57,876 - INFO - test acc 0.8700000047683716
2023-06-22 15:28:58,016 - INFO - Distilling data from client: Client26
2023-06-22 15:28:58,016 - INFO - train loss: 0.006157736262188726
2023-06-22 15:28:58,016 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:58,026 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.80      0.84      0.82        57
           8       0.95      0.91      0.93       139

    accuracy                           0.87       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.89      0.87      0.88       200

2023-06-22 15:28:58,026 - INFO - test loss 0.01385902003767882
2023-06-22 15:28:58,026 - INFO - test acc 0.8700000047683716
2023-06-22 15:28:58,161 - INFO - Distilling data from client: Client26
2023-06-22 15:28:58,161 - INFO - train loss: 0.004047588767426203
2023-06-22 15:28:58,162 - INFO - train acc: 1.0
2023-06-22 15:28:58,171 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.79      0.88      0.83        57
           8       0.96      0.92      0.94       139

    accuracy                           0.89       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.89      0.89      0.89       200

2023-06-22 15:28:58,171 - INFO - test loss 0.012872696510058573
2023-06-22 15:28:58,171 - INFO - test acc 0.8899999856948853
2023-06-22 15:28:58,309 - INFO - Distilling data from client: Client26
2023-06-22 15:28:58,309 - INFO - train loss: 0.0049169998806693265
2023-06-22 15:28:58,309 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:58,319 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.78      0.88      0.83        57
           8       0.95      0.91      0.93       139

    accuracy                           0.88       200
   macro avg       0.58      0.59      0.59       200
weighted avg       0.89      0.88      0.88       200

2023-06-22 15:28:58,319 - INFO - test loss 0.013766090085583734
2023-06-22 15:28:58,320 - INFO - test acc 0.8799999952316284
2023-06-22 15:28:58,455 - INFO - Distilling data from client: Client26
2023-06-22 15:28:58,455 - INFO - train loss: 0.004720020595171744
2023-06-22 15:28:58,455 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:58,464 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.79      0.86      0.82        57
           8       0.94      0.91      0.92       139

    accuracy                           0.88       200
   macro avg       0.58      0.59      0.58       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:28:58,465 - INFO - test loss 0.013044160783536981
2023-06-22 15:28:58,465 - INFO - test acc 0.875
2023-06-22 15:28:58,604 - INFO - Distilling data from client: Client26
2023-06-22 15:28:58,604 - INFO - train loss: 0.006012696992196733
2023-06-22 15:28:58,604 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:58,618 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.77      0.88      0.82        57
           8       0.94      0.91      0.93       139

    accuracy                           0.89       200
   macro avg       0.57      0.60      0.58       200
weighted avg       0.87      0.89      0.88       200

2023-06-22 15:28:58,618 - INFO - test loss 0.012840059103534068
2023-06-22 15:28:58,618 - INFO - test acc 0.8849999904632568
2023-06-22 15:28:58,756 - INFO - Distilling data from client: Client26
2023-06-22 15:28:58,756 - INFO - train loss: 0.005397190107142906
2023-06-22 15:28:58,756 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:58,765 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.75      0.91      0.83        57
           8       0.95      0.88      0.92       139

    accuracy                           0.88       200
   macro avg       0.57      0.60      0.58       200
weighted avg       0.88      0.88      0.87       200

2023-06-22 15:28:58,765 - INFO - test loss 0.012558633454065785
2023-06-22 15:28:58,766 - INFO - test acc 0.875
2023-06-22 15:28:58,901 - INFO - Distilling data from client: Client26
2023-06-22 15:28:58,901 - INFO - train loss: 0.004211001253010567
2023-06-22 15:28:58,901 - INFO - train acc: 1.0
2023-06-22 15:28:58,910 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.79      0.88      0.83        57
           8       0.95      0.91      0.93       139

    accuracy                           0.89       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.89      0.89      0.89       200

2023-06-22 15:28:58,910 - INFO - test loss 0.012646061602572062
2023-06-22 15:28:58,910 - INFO - test acc 0.8849999904632568
2023-06-22 15:28:59,048 - INFO - Distilling data from client: Client26
2023-06-22 15:28:59,048 - INFO - train loss: 0.005123297864766635
2023-06-22 15:28:59,048 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:59,064 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.80      0.86      0.83        57
           8       0.95      0.94      0.94       139

    accuracy                           0.90       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.89      0.90      0.89       200

2023-06-22 15:28:59,065 - INFO - test loss 0.013016167164030194
2023-06-22 15:28:59,065 - INFO - test acc 0.8949999809265137
2023-06-22 15:28:59,202 - INFO - Distilling data from client: Client26
2023-06-22 15:28:59,202 - INFO - train loss: 0.007159875392304506
2023-06-22 15:28:59,202 - INFO - train acc: 0.9411765336990356
2023-06-22 15:28:59,212 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.80      0.89      0.84        57
           8       0.96      0.92      0.94       139

    accuracy                           0.90       200
   macro avg       0.59      0.61      0.59       200
weighted avg       0.90      0.90      0.89       200

2023-06-22 15:28:59,212 - INFO - test loss 0.012201042238832826
2023-06-22 15:28:59,212 - INFO - test acc 0.8949999809265137
2023-06-22 15:28:59,352 - INFO - Distilling data from client: Client26
2023-06-22 15:28:59,352 - INFO - train loss: 0.004790854108450285
2023-06-22 15:28:59,352 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:59,362 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.74      0.88      0.80        57
           8       0.94      0.89      0.92       139

    accuracy                           0.87       200
   macro avg       0.56      0.59      0.57       200
weighted avg       0.86      0.87      0.86       200

2023-06-22 15:28:59,362 - INFO - test loss 0.012969998125172978
2023-06-22 15:28:59,363 - INFO - test acc 0.8700000047683716
2023-06-22 15:28:59,499 - INFO - Distilling data from client: Client26
2023-06-22 15:28:59,499 - INFO - train loss: 0.005560169366243417
2023-06-22 15:28:59,499 - INFO - train acc: 0.960784375667572
2023-06-22 15:28:59,509 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.83      0.86      0.84        57
           8       0.94      0.94      0.94       139

    accuracy                           0.90       200
   macro avg       0.59      0.60      0.59       200
weighted avg       0.89      0.90      0.89       200

2023-06-22 15:28:59,509 - INFO - test loss 0.012098058811743891
2023-06-22 15:28:59,509 - INFO - test acc 0.8949999809265137
2023-06-22 15:28:59,645 - INFO - Distilling data from client: Client26
2023-06-22 15:28:59,645 - INFO - train loss: 0.0036720777150176086
2023-06-22 15:28:59,645 - INFO - train acc: 1.0
2023-06-22 15:28:59,654 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.78      0.89      0.84        57
           8       0.95      0.91      0.93       139

    accuracy                           0.89       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.89      0.89      0.89       200

2023-06-22 15:28:59,654 - INFO - test loss 0.012300247868084273
2023-06-22 15:28:59,654 - INFO - test acc 0.8899999856948853
2023-06-22 15:28:59,794 - INFO - Distilling data from client: Client26
2023-06-22 15:28:59,794 - INFO - train loss: 0.0035534240573134443
2023-06-22 15:28:59,794 - INFO - train acc: 0.9803922176361084
2023-06-22 15:28:59,804 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.76      0.88      0.81        57
           8       0.95      0.90      0.92       139

    accuracy                           0.88       200
   macro avg       0.57      0.59      0.58       200
weighted avg       0.87      0.88      0.87       200

2023-06-22 15:28:59,804 - INFO - test loss 0.01273711034177824
2023-06-22 15:28:59,804 - INFO - test acc 0.875
2023-06-22 15:28:59,943 - INFO - Distilling data from client: Client26
2023-06-22 15:28:59,943 - INFO - train loss: 0.0046248782294568205
2023-06-22 15:28:59,943 - INFO - train acc: 0.960784375667572
2023-06-22 15:28:59,953 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.77      0.86      0.81        57
           8       0.95      0.91      0.93       139

    accuracy                           0.88       200
   macro avg       0.57      0.59      0.58       200
weighted avg       0.88      0.88      0.87       200

2023-06-22 15:28:59,953 - INFO - test loss 0.012672305668505121
2023-06-22 15:28:59,953 - INFO - test acc 0.875
2023-06-22 15:28:59,958 - WARNING - Finished tracing + transforming jit(gather) in 0.0004944801330566406 sec
2023-06-22 15:28:59,958 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[51,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:28:59,961 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0024902820587158203 sec
2023-06-22 15:28:59,961 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:28:59,980 - WARNING - Finished XLA compilation of jit(gather) in 0.01797032356262207 sec
2023-06-22 15:28:59,996 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:00,011 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:00,026 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:00,040 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:00,055 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:00,070 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:00,602 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client26//synthetic.png
2023-06-22 15:29:00,623 - INFO - c: 3.0 and total_data_in_this_class: 531
2023-06-22 15:29:00,623 - INFO - c: 4.0 and total_data_in_this_class: 268
2023-06-22 15:29:00,623 - INFO - c: 3.0 and total_data_in_this_class: 135
2023-06-22 15:29:00,623 - INFO - c: 4.0 and total_data_in_this_class: 65
2023-06-22 15:29:00,750 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0793464183807373 sec
2023-06-22 15:29:00,829 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07666850090026855 sec
2023-06-22 15:29:00,837 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16783952713012695 sec
2023-06-22 15:29:00,840 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:29:00,896 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.055693864822387695 sec
2023-06-22 15:29:00,896 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:29:01,073 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17647123336791992 sec
2023-06-22 15:29:01,113 - INFO - initial test loss: 0.023536299380305218
2023-06-22 15:29:01,114 - INFO - initial test acc: 0.7049999833106995
2023-06-22 15:29:01,136 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.016038179397583008 sec
2023-06-22 15:29:01,376 - WARNING - Finished tracing + transforming update_fn for pjit in 0.257462739944458 sec
2023-06-22 15:29:01,381 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:29:01,484 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10185718536376953 sec
2023-06-22 15:29:01,484 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:29:01,913 - WARNING - Finished XLA compilation of jit(update_fn) in 0.42911553382873535 sec
2023-06-22 15:29:04,085 - INFO - Distilling data from client: Client27
2023-06-22 15:29:04,086 - INFO - train loss: 0.005287578125610058
2023-06-22 15:29:04,086 - INFO - train acc: 0.9720669984817505
2023-06-22 15:29:04,214 - INFO - report:               precision    recall  f1-score   support

           3       0.84      0.84      0.84       135
           4       0.67      0.66      0.67        65

    accuracy                           0.79       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.78      0.79      0.78       200

2023-06-22 15:29:04,216 - INFO - test loss 0.016583086942535907
2023-06-22 15:29:04,216 - INFO - test acc 0.7849999666213989
2023-06-22 15:29:06,587 - INFO - Distilling data from client: Client27
2023-06-22 15:29:06,588 - INFO - train loss: 0.0038260670164334803
2023-06-22 15:29:06,588 - INFO - train acc: 0.9832401871681213
2023-06-22 15:29:06,636 - INFO - report:               precision    recall  f1-score   support

           3       0.82      0.83      0.83       135
           4       0.64      0.63      0.64        65

    accuracy                           0.77       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.76      0.77      0.76       200

2023-06-22 15:29:06,636 - INFO - test loss 0.017386508446984947
2023-06-22 15:29:06,637 - INFO - test acc 0.7649999856948853
2023-06-22 15:29:08,842 - INFO - Distilling data from client: Client27
2023-06-22 15:29:08,843 - INFO - train loss: 0.0035405033867148042
2023-06-22 15:29:08,844 - INFO - train acc: 0.9804468750953674
2023-06-22 15:29:08,900 - INFO - report:               precision    recall  f1-score   support

           3       0.82      0.86      0.84       135
           4       0.67      0.60      0.63        65

    accuracy                           0.78       200
   macro avg       0.74      0.73      0.74       200
weighted avg       0.77      0.78      0.77       200

2023-06-22 15:29:08,901 - INFO - test loss 0.017657386852565828
2023-06-22 15:29:08,902 - INFO - test acc 0.7749999761581421
2023-06-22 15:29:11,095 - INFO - Distilling data from client: Client27
2023-06-22 15:29:11,096 - INFO - train loss: 0.0034450581335954346
2023-06-22 15:29:11,097 - INFO - train acc: 0.9832401871681213
2023-06-22 15:29:11,142 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.60      0.60      0.60        65

    accuracy                           0.74       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:29:11,145 - INFO - test loss 0.017689932292718966
2023-06-22 15:29:11,146 - INFO - test acc 0.7400000095367432
2023-06-22 15:29:13,406 - INFO - Distilling data from client: Client27
2023-06-22 15:29:13,407 - INFO - train loss: 0.002972172162359762
2023-06-22 15:29:13,407 - INFO - train acc: 0.9972066879272461
2023-06-22 15:29:13,470 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.82      0.81       135
           4       0.61      0.58      0.60        65

    accuracy                           0.74       200
   macro avg       0.71      0.70      0.71       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:29:13,472 - INFO - test loss 0.018276778820418352
2023-06-22 15:29:13,473 - INFO - test acc 0.7450000047683716
2023-06-22 15:29:15,807 - INFO - Distilling data from client: Client27
2023-06-22 15:29:15,807 - INFO - train loss: 0.002654650188939376
2023-06-22 15:29:15,807 - INFO - train acc: 0.9916200637817383
2023-06-22 15:29:15,857 - INFO - report:               precision    recall  f1-score   support

           3       0.84      0.83      0.83       135
           4       0.65      0.66      0.66        65

    accuracy                           0.78       200
   macro avg       0.74      0.75      0.74       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:29:15,858 - INFO - test loss 0.017694394111025403
2023-06-22 15:29:15,858 - INFO - test acc 0.7749999761581421
2023-06-22 15:29:17,409 - INFO - Distilling data from client: Client27
2023-06-22 15:29:17,409 - INFO - train loss: 0.0023802265464001123
2023-06-22 15:29:17,409 - INFO - train acc: 0.9972066879272461
2023-06-22 15:29:17,440 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.83      0.82       135
           4       0.63      0.60      0.61        65

    accuracy                           0.76       200
   macro avg       0.72      0.71      0.72       200
weighted avg       0.75      0.76      0.75       200

2023-06-22 15:29:17,440 - INFO - test loss 0.01775463067122608
2023-06-22 15:29:17,440 - INFO - test acc 0.7549999952316284
2023-06-22 15:29:19,064 - INFO - Distilling data from client: Client27
2023-06-22 15:29:19,064 - INFO - train loss: 0.0026562170061860627
2023-06-22 15:29:19,064 - INFO - train acc: 0.9916200637817383
2023-06-22 15:29:19,099 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.62      0.62      0.62        65

    accuracy                           0.75       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.75      0.75      0.75       200

2023-06-22 15:29:19,100 - INFO - test loss 0.018528907116057005
2023-06-22 15:29:19,100 - INFO - test acc 0.75
2023-06-22 15:29:20,906 - INFO - Distilling data from client: Client27
2023-06-22 15:29:20,906 - INFO - train loss: 0.0023208130894452946
2023-06-22 15:29:20,907 - INFO - train acc: 1.0
2023-06-22 15:29:20,947 - INFO - report:               precision    recall  f1-score   support

           3       0.85      0.79      0.82       135
           4       0.61      0.71      0.66        65

    accuracy                           0.76       200
   macro avg       0.73      0.75      0.74       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:29:20,948 - INFO - test loss 0.018024561802955155
2023-06-22 15:29:20,949 - INFO - test acc 0.7599999904632568
2023-06-22 15:29:23,212 - INFO - Distilling data from client: Client27
2023-06-22 15:29:23,212 - INFO - train loss: 0.002569126810596318
2023-06-22 15:29:23,212 - INFO - train acc: 0.9916200637817383
2023-06-22 15:29:23,267 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.60      0.60      0.60        65

    accuracy                           0.74       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:29:23,270 - INFO - test loss 0.018552816183430115
2023-06-22 15:29:23,270 - INFO - test acc 0.7400000095367432
2023-06-22 15:29:25,691 - INFO - Distilling data from client: Client27
2023-06-22 15:29:25,692 - INFO - train loss: 0.00254180398070976
2023-06-22 15:29:25,692 - INFO - train acc: 0.9944133758544922
2023-06-22 15:29:25,754 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.80      0.81       135
           4       0.60      0.62      0.61        65

    accuracy                           0.74       200
   macro avg       0.70      0.71      0.71       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:29:25,755 - INFO - test loss 0.01818758369294393
2023-06-22 15:29:25,755 - INFO - test acc 0.7400000095367432
2023-06-22 15:29:28,125 - INFO - Distilling data from client: Client27
2023-06-22 15:29:28,126 - INFO - train loss: 0.002405491900083346
2023-06-22 15:29:28,126 - INFO - train acc: 1.0
2023-06-22 15:29:28,179 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.61      0.62      0.61        65

    accuracy                           0.74       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.75      0.74      0.75       200

2023-06-22 15:29:28,180 - INFO - test loss 0.017885062044121425
2023-06-22 15:29:28,181 - INFO - test acc 0.7450000047683716
2023-06-22 15:29:30,387 - INFO - Distilling data from client: Client27
2023-06-22 15:29:30,388 - INFO - train loss: 0.0028994225953986397
2023-06-22 15:29:30,388 - INFO - train acc: 0.9888268113136292
2023-06-22 15:29:30,443 - INFO - report:               precision    recall  f1-score   support

           3       0.83      0.84      0.84       135
           4       0.67      0.65      0.66        65

    accuracy                           0.78       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:29:30,444 - INFO - test loss 0.017672815932772576
2023-06-22 15:29:30,444 - INFO - test acc 0.7799999713897705
2023-06-22 15:29:32,706 - INFO - Distilling data from client: Client27
2023-06-22 15:29:32,706 - INFO - train loss: 0.0026382276427115254
2023-06-22 15:29:32,706 - INFO - train acc: 0.9972066879272461
2023-06-22 15:29:32,774 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.82      0.82       135
           4       0.62      0.60      0.61        65

    accuracy                           0.75       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.75      0.75      0.75       200

2023-06-22 15:29:32,776 - INFO - test loss 0.018610979517860112
2023-06-22 15:29:32,776 - INFO - test acc 0.75
2023-06-22 15:29:35,044 - INFO - Distilling data from client: Client27
2023-06-22 15:29:35,044 - INFO - train loss: 0.002325175928446175
2023-06-22 15:29:35,046 - INFO - train acc: 1.0
2023-06-22 15:29:35,098 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.61      0.62      0.61        65

    accuracy                           0.74       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.75      0.74      0.75       200

2023-06-22 15:29:35,101 - INFO - test loss 0.01889068950217562
2023-06-22 15:29:35,102 - INFO - test acc 0.7450000047683716
2023-06-22 15:29:37,257 - INFO - Distilling data from client: Client27
2023-06-22 15:29:37,257 - INFO - train loss: 0.0021781773183889384
2023-06-22 15:29:37,257 - INFO - train acc: 0.9972066879272461
2023-06-22 15:29:37,308 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.60      0.60      0.60        65

    accuracy                           0.74       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:29:37,310 - INFO - test loss 0.01792716766842525
2023-06-22 15:29:37,310 - INFO - test acc 0.7400000095367432
2023-06-22 15:29:39,406 - INFO - Distilling data from client: Client27
2023-06-22 15:29:39,408 - INFO - train loss: 0.002222147519438072
2023-06-22 15:29:39,408 - INFO - train acc: 0.9972066879272461
2023-06-22 15:29:39,467 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.76      0.79       135
           4       0.56      0.63      0.59        65

    accuracy                           0.72       200
   macro avg       0.69      0.70      0.69       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:29:39,469 - INFO - test loss 0.0189948733693139
2023-06-22 15:29:39,470 - INFO - test acc 0.7199999690055847
2023-06-22 15:29:41,765 - INFO - Distilling data from client: Client27
2023-06-22 15:29:41,765 - INFO - train loss: 0.001656835190464698
2023-06-22 15:29:41,765 - INFO - train acc: 1.0
2023-06-22 15:29:41,815 - INFO - report:               precision    recall  f1-score   support

           3       0.83      0.79      0.81       135
           4       0.60      0.66      0.63        65

    accuracy                           0.74       200
   macro avg       0.71      0.72      0.72       200
weighted avg       0.75      0.74      0.75       200

2023-06-22 15:29:41,816 - INFO - test loss 0.018957524720785487
2023-06-22 15:29:41,818 - INFO - test acc 0.7450000047683716
2023-06-22 15:29:44,106 - INFO - Distilling data from client: Client27
2023-06-22 15:29:44,107 - INFO - train loss: 0.0021027496074843928
2023-06-22 15:29:44,107 - INFO - train acc: 0.9972066879272461
2023-06-22 15:29:44,159 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.84      0.82       135
           4       0.63      0.57      0.60        65

    accuracy                           0.75       200
   macro avg       0.71      0.70      0.71       200
weighted avg       0.74      0.75      0.75       200

2023-06-22 15:29:44,160 - INFO - test loss 0.018766924336092616
2023-06-22 15:29:44,160 - INFO - test acc 0.75
2023-06-22 15:29:46,438 - INFO - Distilling data from client: Client27
2023-06-22 15:29:46,439 - INFO - train loss: 0.002028584995982095
2023-06-22 15:29:46,439 - INFO - train acc: 1.0
2023-06-22 15:29:46,498 - INFO - report:               precision    recall  f1-score   support

           3       0.82      0.81      0.82       135
           4       0.62      0.63      0.63        65

    accuracy                           0.76       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:29:46,503 - INFO - test loss 0.017717703482799062
2023-06-22 15:29:46,504 - INFO - test acc 0.7549999952316284
2023-06-22 15:29:48,740 - INFO - Distilling data from client: Client27
2023-06-22 15:29:48,740 - INFO - train loss: 0.0021445390158742698
2023-06-22 15:29:48,741 - INFO - train acc: 0.9972066879272461
2023-06-22 15:29:48,887 - INFO - report:               precision    recall  f1-score   support

           3       0.83      0.87      0.85       135
           4       0.70      0.62      0.66        65

    accuracy                           0.79       200
   macro avg       0.76      0.74      0.75       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:29:48,887 - INFO - test loss 0.018326020426676748
2023-06-22 15:29:48,888 - INFO - test acc 0.7899999618530273
2023-06-22 15:29:51,131 - INFO - Distilling data from client: Client27
2023-06-22 15:29:51,131 - INFO - train loss: 0.0022739387485618684
2023-06-22 15:29:51,131 - INFO - train acc: 1.0
2023-06-22 15:29:51,182 - INFO - report:               precision    recall  f1-score   support

           3       0.83      0.81      0.82       135
           4       0.63      0.65      0.64        65

    accuracy                           0.76       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:29:51,183 - INFO - test loss 0.018792652927381838
2023-06-22 15:29:51,184 - INFO - test acc 0.7599999904632568
2023-06-22 15:29:53,510 - INFO - Distilling data from client: Client27
2023-06-22 15:29:53,510 - INFO - train loss: 0.0019855507046422846
2023-06-22 15:29:53,511 - INFO - train acc: 0.9972066879272461
2023-06-22 15:29:53,552 - INFO - report:               precision    recall  f1-score   support

           3       0.82      0.82      0.82       135
           4       0.63      0.63      0.63        65

    accuracy                           0.76       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:29:53,553 - INFO - test loss 0.018229660204858646
2023-06-22 15:29:53,553 - INFO - test acc 0.7599999904632568
2023-06-22 15:29:55,842 - INFO - Distilling data from client: Client27
2023-06-22 15:29:55,843 - INFO - train loss: 0.0020577137235769406
2023-06-22 15:29:55,843 - INFO - train acc: 1.0
2023-06-22 15:29:55,900 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.80      0.80       135
           4       0.59      0.60      0.60        65

    accuracy                           0.73       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.73      0.74       200

2023-06-22 15:29:55,901 - INFO - test loss 0.02007697930911838
2023-06-22 15:29:55,902 - INFO - test acc 0.73499995470047
2023-06-22 15:29:58,150 - INFO - Distilling data from client: Client27
2023-06-22 15:29:58,151 - INFO - train loss: 0.002271454866046849
2023-06-22 15:29:58,151 - INFO - train acc: 0.9972066879272461
2023-06-22 15:29:58,199 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.60      0.60      0.60        65

    accuracy                           0.74       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:29:58,200 - INFO - test loss 0.019836308314954657
2023-06-22 15:29:58,201 - INFO - test acc 0.7400000095367432
2023-06-22 15:29:58,234 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:58,252 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:58,274 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:58,292 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:58,308 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:58,321 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:29:58,763 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client27//synthetic.png
2023-06-22 15:29:58,787 - INFO - c: 3.0 and total_data_in_this_class: 271
2023-06-22 15:29:58,787 - INFO - c: 5.0 and total_data_in_this_class: 264
2023-06-22 15:29:58,788 - INFO - c: 9.0 and total_data_in_this_class: 264
2023-06-22 15:29:58,788 - INFO - c: 3.0 and total_data_in_this_class: 62
2023-06-22 15:29:58,788 - INFO - c: 5.0 and total_data_in_this_class: 69
2023-06-22 15:29:58,788 - INFO - c: 9.0 and total_data_in_this_class: 69
2023-06-22 15:29:58,924 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0797281265258789 sec
2023-06-22 15:29:59,001 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07542800903320312 sec
2023-06-22 15:29:59,010 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.1683788299560547 sec
2023-06-22 15:29:59,013 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:29:59,069 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05541229248046875 sec
2023-06-22 15:29:59,069 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:29:59,232 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.16226935386657715 sec
2023-06-22 15:29:59,287 - INFO - initial test loss: 0.02772336870197458
2023-06-22 15:29:59,287 - INFO - initial test acc: 0.5600000023841858
2023-06-22 15:29:59,305 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.013002157211303711 sec
2023-06-22 15:29:59,502 - WARNING - Finished tracing + transforming update_fn for pjit in 0.21067118644714355 sec
2023-06-22 15:29:59,507 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[528,10]), ShapedArray(float32[528,10]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:29:59,612 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10445594787597656 sec
2023-06-22 15:29:59,612 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:30:00,003 - WARNING - Finished XLA compilation of jit(update_fn) in 0.39092540740966797 sec
2023-06-22 15:30:02,761 - INFO - Distilling data from client: Client28
2023-06-22 15:30:02,761 - INFO - train loss: 0.0030743036870972632
2023-06-22 15:30:02,762 - INFO - train acc: 0.9962121248245239
2023-06-22 15:30:02,958 - INFO - report:               precision    recall  f1-score   support

           3       0.47      0.50      0.48        62
           5       0.59      0.49      0.54        69
           9       0.74      0.81      0.77        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.60      0.60       200

2023-06-22 15:30:02,958 - INFO - test loss 0.026157521897632686
2023-06-22 15:30:02,958 - INFO - test acc 0.6049999594688416
2023-06-22 15:30:05,880 - INFO - Distilling data from client: Client28
2023-06-22 15:30:05,880 - INFO - train loss: 0.0015777508633440347
2023-06-22 15:30:05,880 - INFO - train acc: 1.0
2023-06-22 15:30:05,936 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.50      0.47        62
           5       0.60      0.51      0.55        69
           9       0.76      0.80      0.78        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:30:05,937 - INFO - test loss 0.025939572424242646
2023-06-22 15:30:05,937 - INFO - test acc 0.6049999594688416
2023-06-22 15:30:08,873 - INFO - Distilling data from client: Client28
2023-06-22 15:30:08,874 - INFO - train loss: 0.0013231197333494313
2023-06-22 15:30:08,874 - INFO - train acc: 1.0
2023-06-22 15:30:08,934 - INFO - report:               precision    recall  f1-score   support

           3       0.47      0.52      0.49        62
           5       0.60      0.52      0.56        69
           9       0.74      0.77      0.75        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:30:08,934 - INFO - test loss 0.026035168561493693
2023-06-22 15:30:08,934 - INFO - test acc 0.6049999594688416
2023-06-22 15:30:11,850 - INFO - Distilling data from client: Client28
2023-06-22 15:30:11,851 - INFO - train loss: 0.0010576796557266082
2023-06-22 15:30:11,851 - INFO - train acc: 1.0
2023-06-22 15:30:11,928 - INFO - report:               precision    recall  f1-score   support

           3       0.46      0.53      0.50        62
           5       0.60      0.49      0.54        69
           9       0.75      0.78      0.77        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:30:11,929 - INFO - test loss 0.026589711596378962
2023-06-22 15:30:11,929 - INFO - test acc 0.6049999594688416
2023-06-22 15:30:14,773 - INFO - Distilling data from client: Client28
2023-06-22 15:30:14,773 - INFO - train loss: 0.0012014638469410406
2023-06-22 15:30:14,773 - INFO - train acc: 0.998106062412262
2023-06-22 15:30:14,829 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.52      0.48        62
           5       0.61      0.52      0.56        69
           9       0.74      0.74      0.74        69

    accuracy                           0.59       200
   macro avg       0.60      0.59      0.59       200
weighted avg       0.60      0.59      0.60       200

2023-06-22 15:30:14,830 - INFO - test loss 0.02625580110624744
2023-06-22 15:30:14,830 - INFO - test acc 0.5949999690055847
2023-06-22 15:30:17,685 - INFO - Distilling data from client: Client28
2023-06-22 15:30:17,685 - INFO - train loss: 0.0009578256982290995
2023-06-22 15:30:17,685 - INFO - train acc: 1.0
2023-06-22 15:30:17,741 - INFO - report:               precision    recall  f1-score   support

           3       0.46      0.52      0.48        62
           5       0.61      0.52      0.56        69
           9       0.73      0.75      0.74        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.60      0.60       200

2023-06-22 15:30:17,743 - INFO - test loss 0.025724850967898436
2023-06-22 15:30:17,743 - INFO - test acc 0.5999999642372131
2023-06-22 15:30:20,670 - INFO - Distilling data from client: Client28
2023-06-22 15:30:20,670 - INFO - train loss: 0.0007099812980375369
2023-06-22 15:30:20,670 - INFO - train acc: 1.0
2023-06-22 15:30:20,729 - INFO - report:               precision    recall  f1-score   support

           3       0.46      0.53      0.49        62
           5       0.62      0.48      0.54        69
           9       0.72      0.78      0.75        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.59       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:30:20,729 - INFO - test loss 0.026477494772049068
2023-06-22 15:30:20,730 - INFO - test acc 0.5999999642372131
2023-06-22 15:30:23,496 - INFO - Distilling data from client: Client28
2023-06-22 15:30:23,497 - INFO - train loss: 0.0007096061094034542
2023-06-22 15:30:23,498 - INFO - train acc: 1.0
2023-06-22 15:30:23,551 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.50      0.47        62
           5       0.61      0.49      0.54        69
           9       0.75      0.80      0.77        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.59       200
weighted avg       0.60      0.60      0.60       200

2023-06-22 15:30:23,551 - INFO - test loss 0.026535947750083225
2023-06-22 15:30:23,551 - INFO - test acc 0.5999999642372131
2023-06-22 15:30:26,417 - INFO - Distilling data from client: Client28
2023-06-22 15:30:26,418 - INFO - train loss: 0.0006113732460844134
2023-06-22 15:30:26,418 - INFO - train acc: 1.0
2023-06-22 15:30:26,469 - INFO - report:               precision    recall  f1-score   support

           3       0.42      0.48      0.45        62
           5       0.56      0.48      0.52        69
           9       0.73      0.74      0.73        69

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-06-22 15:30:26,470 - INFO - test loss 0.027107599526482944
2023-06-22 15:30:26,470 - INFO - test acc 0.5699999928474426
2023-06-22 15:30:29,283 - INFO - Distilling data from client: Client28
2023-06-22 15:30:29,284 - INFO - train loss: 0.0006591206398399896
2023-06-22 15:30:29,284 - INFO - train acc: 1.0
2023-06-22 15:30:29,338 - INFO - report:               precision    recall  f1-score   support

           3       0.45      0.53      0.49        62
           5       0.61      0.51      0.56        69
           9       0.75      0.75      0.75        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:30:29,338 - INFO - test loss 0.02688093250883521
2023-06-22 15:30:29,338 - INFO - test acc 0.5999999642372131
2023-06-22 15:30:32,179 - INFO - Distilling data from client: Client28
2023-06-22 15:30:32,179 - INFO - train loss: 0.0006566590344068451
2023-06-22 15:30:32,180 - INFO - train acc: 1.0
2023-06-22 15:30:32,239 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.52      0.47        62
           5       0.62      0.48      0.54        69
           9       0.70      0.75      0.73        69

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-06-22 15:30:32,241 - INFO - test loss 0.02677228775898873
2023-06-22 15:30:32,242 - INFO - test acc 0.5849999785423279
2023-06-22 15:30:35,085 - INFO - Distilling data from client: Client28
2023-06-22 15:30:35,085 - INFO - train loss: 0.0006322892156487342
2023-06-22 15:30:35,086 - INFO - train acc: 1.0
2023-06-22 15:30:35,134 - INFO - report:               precision    recall  f1-score   support

           3       0.45      0.48      0.47        62
           5       0.62      0.54      0.57        69
           9       0.72      0.77      0.74        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.59       200
weighted avg       0.60      0.60      0.60       200

2023-06-22 15:30:35,134 - INFO - test loss 0.026687126899614128
2023-06-22 15:30:35,134 - INFO - test acc 0.5999999642372131
2023-06-22 15:30:38,040 - INFO - Distilling data from client: Client28
2023-06-22 15:30:38,040 - INFO - train loss: 0.0005974731760717445
2023-06-22 15:30:38,041 - INFO - train acc: 1.0
2023-06-22 15:30:38,089 - INFO - report:               precision    recall  f1-score   support

           3       0.42      0.50      0.46        62
           5       0.56      0.48      0.52        69
           9       0.72      0.71      0.72        69

    accuracy                           0.56       200
   macro avg       0.57      0.56      0.56       200
weighted avg       0.57      0.56      0.57       200

2023-06-22 15:30:38,091 - INFO - test loss 0.027245230330351738
2023-06-22 15:30:38,091 - INFO - test acc 0.5649999976158142
2023-06-22 15:30:40,933 - INFO - Distilling data from client: Client28
2023-06-22 15:30:40,934 - INFO - train loss: 0.0006741299627851405
2023-06-22 15:30:40,934 - INFO - train acc: 0.9962121248245239
2023-06-22 15:30:40,992 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.52      0.47        62
           5       0.62      0.51      0.56        69
           9       0.72      0.74      0.73        69

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-06-22 15:30:40,993 - INFO - test loss 0.02724213522215369
2023-06-22 15:30:40,993 - INFO - test acc 0.5899999737739563
2023-06-22 15:30:43,780 - INFO - Distilling data from client: Client28
2023-06-22 15:30:43,780 - INFO - train loss: 0.0005564916084020537
2023-06-22 15:30:43,781 - INFO - train acc: 1.0
2023-06-22 15:30:43,829 - INFO - report:               precision    recall  f1-score   support

           3       0.43      0.48      0.46        62
           5       0.59      0.48      0.53        69
           9       0.72      0.78      0.75        69

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-06-22 15:30:43,829 - INFO - test loss 0.027519685590293434
2023-06-22 15:30:43,830 - INFO - test acc 0.5849999785423279
2023-06-22 15:30:46,652 - INFO - Distilling data from client: Client28
2023-06-22 15:30:46,652 - INFO - train loss: 0.0004571122868749902
2023-06-22 15:30:46,652 - INFO - train acc: 1.0
2023-06-22 15:30:46,714 - INFO - report:               precision    recall  f1-score   support

           3       0.46      0.52      0.49        62
           5       0.54      0.46      0.50        69
           9       0.72      0.75      0.74        69

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-06-22 15:30:46,714 - INFO - test loss 0.027474032555454806
2023-06-22 15:30:46,714 - INFO - test acc 0.5799999833106995
2023-06-22 15:30:49,584 - INFO - Distilling data from client: Client28
2023-06-22 15:30:49,584 - INFO - train loss: 0.0005983862945070898
2023-06-22 15:30:49,585 - INFO - train acc: 1.0
2023-06-22 15:30:49,642 - INFO - report:               precision    recall  f1-score   support

           3       0.45      0.55      0.49        62
           5       0.57      0.45      0.50        69
           9       0.71      0.72      0.72        69

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-06-22 15:30:49,643 - INFO - test loss 0.027528017740601905
2023-06-22 15:30:49,643 - INFO - test acc 0.574999988079071
2023-06-22 15:30:52,477 - INFO - Distilling data from client: Client28
2023-06-22 15:30:52,477 - INFO - train loss: 0.0004542212787121456
2023-06-22 15:30:52,477 - INFO - train acc: 1.0
2023-06-22 15:30:52,543 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.50      0.47        62
           5       0.58      0.46      0.52        69
           9       0.73      0.78      0.76        69

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-06-22 15:30:52,544 - INFO - test loss 0.02653996668951404
2023-06-22 15:30:52,544 - INFO - test acc 0.5849999785423279
2023-06-22 15:30:55,369 - INFO - Distilling data from client: Client28
2023-06-22 15:30:55,369 - INFO - train loss: 0.0004634834457524056
2023-06-22 15:30:55,370 - INFO - train acc: 1.0
2023-06-22 15:30:55,527 - INFO - report:               precision    recall  f1-score   support

           3       0.48      0.50      0.49        62
           5       0.60      0.55      0.58        69
           9       0.73      0.77      0.75        69

    accuracy                           0.61       200
   macro avg       0.60      0.61      0.60       200
weighted avg       0.61      0.61      0.61       200

2023-06-22 15:30:55,528 - INFO - test loss 0.02709917415479589
2023-06-22 15:30:55,528 - INFO - test acc 0.6100000143051147
2023-06-22 15:30:58,309 - INFO - Distilling data from client: Client28
2023-06-22 15:30:58,309 - INFO - train loss: 0.0003557291963786757
2023-06-22 15:30:58,310 - INFO - train acc: 1.0
2023-06-22 15:30:58,363 - INFO - report:               precision    recall  f1-score   support

           3       0.45      0.53      0.49        62
           5       0.61      0.49      0.54        69
           9       0.70      0.72      0.71        69

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.59       200

2023-06-22 15:30:58,364 - INFO - test loss 0.027795343111317424
2023-06-22 15:30:58,364 - INFO - test acc 0.5849999785423279
2023-06-22 15:31:01,090 - INFO - Distilling data from client: Client28
2023-06-22 15:31:01,091 - INFO - train loss: 0.000400091706739708
2023-06-22 15:31:01,091 - INFO - train acc: 1.0
2023-06-22 15:31:01,150 - INFO - report:               precision    recall  f1-score   support

           3       0.42      0.48      0.45        62
           5       0.59      0.46      0.52        69
           9       0.70      0.75      0.73        69

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-06-22 15:31:01,151 - INFO - test loss 0.02763840610098402
2023-06-22 15:31:01,151 - INFO - test acc 0.5699999928474426
2023-06-22 15:31:03,925 - INFO - Distilling data from client: Client28
2023-06-22 15:31:03,926 - INFO - train loss: 0.0004651269756076343
2023-06-22 15:31:03,926 - INFO - train acc: 1.0
2023-06-22 15:31:03,980 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.52      0.48        62
           5       0.59      0.48      0.53        69
           9       0.71      0.74      0.72        69

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-06-22 15:31:03,980 - INFO - test loss 0.02717055644895055
2023-06-22 15:31:03,980 - INFO - test acc 0.5799999833106995
2023-06-22 15:31:06,778 - INFO - Distilling data from client: Client28
2023-06-22 15:31:06,778 - INFO - train loss: 0.00043709934981900334
2023-06-22 15:31:06,780 - INFO - train acc: 1.0
2023-06-22 15:31:06,833 - INFO - report:               precision    recall  f1-score   support

           3       0.46      0.52      0.49        62
           5       0.60      0.48      0.53        69
           9       0.68      0.75      0.72        69

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-06-22 15:31:06,833 - INFO - test loss 0.027028299261503016
2023-06-22 15:31:06,834 - INFO - test acc 0.5849999785423279
2023-06-22 15:31:09,672 - INFO - Distilling data from client: Client28
2023-06-22 15:31:09,672 - INFO - train loss: 0.00039128053378283036
2023-06-22 15:31:09,672 - INFO - train acc: 1.0
2023-06-22 15:31:09,716 - INFO - report:               precision    recall  f1-score   support

           3       0.45      0.52      0.48        62
           5       0.57      0.46      0.51        69
           9       0.70      0.74      0.72        69

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-06-22 15:31:09,716 - INFO - test loss 0.0278388586145549
2023-06-22 15:31:09,716 - INFO - test acc 0.574999988079071
2023-06-22 15:31:12,538 - INFO - Distilling data from client: Client28
2023-06-22 15:31:12,538 - INFO - train loss: 0.00042157163966181547
2023-06-22 15:31:12,539 - INFO - train acc: 1.0
2023-06-22 15:31:12,593 - INFO - report:               precision    recall  f1-score   support

           3       0.43      0.53      0.48        62
           5       0.58      0.45      0.51        69
           9       0.70      0.72      0.71        69

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-06-22 15:31:12,593 - INFO - test loss 0.027603848571937723
2023-06-22 15:31:12,593 - INFO - test acc 0.5699999928474426
2023-06-22 15:31:12,614 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:31:12,627 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:31:12,641 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:31:12,659 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:31:12,676 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:31:12,690 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:31:12,703 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:31:12,716 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:31:12,727 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:31:13,283 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client28//synthetic.png
2023-06-22 15:31:13,308 - INFO - c: 2.0 and total_data_in_this_class: 270
2023-06-22 15:31:13,308 - INFO - c: 7.0 and total_data_in_this_class: 529
2023-06-22 15:31:13,308 - INFO - c: 2.0 and total_data_in_this_class: 63
2023-06-22 15:31:13,308 - INFO - c: 7.0 and total_data_in_this_class: 137
2023-06-22 15:31:13,354 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005495548248291016 sec
2023-06-22 15:31:13,354 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:31:13,357 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.002604961395263672 sec
2023-06-22 15:31:13,357 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:31:13,375 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01722431182861328 sec
2023-06-22 15:31:13,379 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003769397735595703 sec
2023-06-22 15:31:13,379 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:31:13,381 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0017631053924560547 sec
2023-06-22 15:31:13,382 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:31:13,394 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.012221574783325195 sec
2023-06-22 15:31:13,400 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002384185791015625 sec
2023-06-22 15:31:13,401 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022029876708984375 sec
2023-06-22 15:31:13,403 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0006012916564941406 sec
2023-06-22 15:31:13,405 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00038361549377441406 sec
2023-06-22 15:31:13,406 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002009868621826172 sec
2023-06-22 15:31:13,407 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004744529724121094 sec
2023-06-22 15:31:13,408 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042557716369628906 sec
2023-06-22 15:31:13,409 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003139972686767578 sec
2023-06-22 15:31:13,410 - WARNING - Finished tracing + transforming fn for pjit in 0.0004591941833496094 sec
2023-06-22 15:31:13,411 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0006084442138671875 sec
2023-06-22 15:31:13,413 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003504753112792969 sec
2023-06-22 15:31:13,415 - WARNING - Finished tracing + transforming fn for pjit in 0.00044345855712890625 sec
2023-06-22 15:31:13,416 - WARNING - Finished tracing + transforming fn for pjit in 0.0004630088806152344 sec
2023-06-22 15:31:13,417 - WARNING - Finished tracing + transforming fn for pjit in 0.0003960132598876953 sec
2023-06-22 15:31:13,418 - WARNING - Finished tracing + transforming fn for pjit in 0.0004668235778808594 sec
2023-06-22 15:31:13,420 - WARNING - Finished tracing + transforming fn for pjit in 0.0003650188446044922 sec
2023-06-22 15:31:13,424 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003399848937988281 sec
2023-06-22 15:31:13,424 - WARNING - Finished tracing + transforming fn for pjit in 0.00039505958557128906 sec
2023-06-22 15:31:13,426 - WARNING - Finished tracing + transforming fn for pjit in 0.0004107952117919922 sec
2023-06-22 15:31:13,432 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006279945373535156 sec
2023-06-22 15:31:13,433 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016438961029052734 sec
2023-06-22 15:31:13,435 - WARNING - Finished tracing + transforming fn for pjit in 0.00046896934509277344 sec
2023-06-22 15:31:13,436 - WARNING - Finished tracing + transforming fn for pjit in 0.00039577484130859375 sec
2023-06-22 15:31:13,437 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005333423614501953 sec
2023-06-22 15:31:13,439 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00047087669372558594 sec
2023-06-22 15:31:13,440 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031757354736328125 sec
2023-06-22 15:31:13,441 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000461578369140625 sec
2023-06-22 15:31:13,442 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004076957702636719 sec
2023-06-22 15:31:13,443 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041794776916503906 sec
2023-06-22 15:31:13,445 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006577968597412109 sec
2023-06-22 15:31:13,446 - WARNING - Finished tracing + transforming _where for pjit in 0.0017344951629638672 sec
2023-06-22 15:31:13,447 - WARNING - Finished tracing + transforming fn for pjit in 0.0004813671112060547 sec
2023-06-22 15:31:13,448 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045800209045410156 sec
2023-06-22 15:31:13,450 - WARNING - Finished tracing + transforming fn for pjit in 0.0003991127014160156 sec
2023-06-22 15:31:13,451 - WARNING - Finished tracing + transforming fn for pjit in 0.00039505958557128906 sec
2023-06-22 15:31:13,452 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039458274841308594 sec
2023-06-22 15:31:13,453 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004546642303466797 sec
2023-06-22 15:31:13,454 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046062469482421875 sec
2023-06-22 15:31:13,456 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045680999755859375 sec
2023-06-22 15:31:13,457 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004074573516845703 sec
2023-06-22 15:31:13,458 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003998279571533203 sec
2023-06-22 15:31:13,460 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00047326087951660156 sec
2023-06-22 15:31:13,460 - WARNING - Finished tracing + transforming _where for pjit in 0.0015459060668945312 sec
2023-06-22 15:31:13,462 - WARNING - Finished tracing + transforming fn for pjit in 0.0004565715789794922 sec
2023-06-22 15:31:13,463 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004658699035644531 sec
2023-06-22 15:31:13,465 - WARNING - Finished tracing + transforming fn for pjit in 0.00041961669921875 sec
2023-06-22 15:31:13,472 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004794597625732422 sec
2023-06-22 15:31:13,473 - WARNING - Finished tracing + transforming fn for pjit in 0.0006024837493896484 sec
2023-06-22 15:31:13,474 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004749298095703125 sec
2023-06-22 15:31:13,476 - WARNING - Finished tracing + transforming fn for pjit in 0.0004203319549560547 sec
2023-06-22 15:31:13,482 - WARNING - Finished tracing + transforming fn for pjit in 0.00035643577575683594 sec
2023-06-22 15:31:13,485 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002932548522949219 sec
2023-06-22 15:31:13,486 - WARNING - Finished tracing + transforming fn for pjit in 0.0005185604095458984 sec
2023-06-22 15:31:13,487 - WARNING - Finished tracing + transforming fn for pjit in 0.00039124488830566406 sec
2023-06-22 15:31:13,518 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11881279945373535 sec
2023-06-22 15:31:13,523 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022459030151367188 sec
2023-06-22 15:31:13,524 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00018906593322753906 sec
2023-06-22 15:31:13,525 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00047588348388671875 sec
2023-06-22 15:31:13,528 - WARNING - Finished tracing + transforming fn for pjit in 0.00036644935607910156 sec
2023-06-22 15:31:13,529 - WARNING - Finished tracing + transforming fn for pjit in 0.0004506111145019531 sec
2023-06-22 15:31:13,532 - WARNING - Finished tracing + transforming fn for pjit in 0.00040435791015625 sec
2023-06-22 15:31:13,542 - WARNING - Finished tracing + transforming fn for pjit in 0.0003898143768310547 sec
2023-06-22 15:31:13,544 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003871917724609375 sec
2023-06-22 15:31:13,545 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045180320739746094 sec
2023-06-22 15:31:13,546 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003178119659423828 sec
2023-06-22 15:31:13,547 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005841255187988281 sec
2023-06-22 15:31:13,549 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003972053527832031 sec
2023-06-22 15:31:13,550 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039267539978027344 sec
2023-06-22 15:31:13,551 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004894733428955078 sec
2023-06-22 15:31:13,552 - WARNING - Finished tracing + transforming _where for pjit in 0.0015583038330078125 sec
2023-06-22 15:31:13,553 - WARNING - Finished tracing + transforming fn for pjit in 0.00043892860412597656 sec
2023-06-22 15:31:13,555 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004532337188720703 sec
2023-06-22 15:31:13,556 - WARNING - Finished tracing + transforming fn for pjit in 0.0003943443298339844 sec
2023-06-22 15:31:13,557 - WARNING - Finished tracing + transforming fn for pjit in 0.0005195140838623047 sec
2023-06-22 15:31:13,578 - WARNING - Finished tracing + transforming fn for pjit in 0.0003676414489746094 sec
2023-06-22 15:31:13,611 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.09155488014221191 sec
2023-06-22 15:31:13,613 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002224445343017578 sec
2023-06-22 15:31:13,615 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002422332763671875 sec
2023-06-22 15:31:13,615 - WARNING - Finished tracing + transforming _where for pjit in 0.0010864734649658203 sec
2023-06-22 15:31:13,616 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005352497100830078 sec
2023-06-22 15:31:13,617 - WARNING - Finished tracing + transforming trace for pjit in 0.004487276077270508 sec
2023-06-22 15:31:13,621 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00019741058349609375 sec
2023-06-22 15:31:13,623 - WARNING - Finished tracing + transforming tril for pjit in 0.0011167526245117188 sec
2023-06-22 15:31:13,623 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0032241344451904297 sec
2023-06-22 15:31:13,625 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00020813941955566406 sec
2023-06-22 15:31:13,626 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001785755157470703 sec
2023-06-22 15:31:13,629 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0024938583374023438 sec
2023-06-22 15:31:13,636 - WARNING - Finished tracing + transforming _solve for pjit in 0.01619124412536621 sec
2023-06-22 15:31:13,637 - WARNING - Finished tracing + transforming dot for pjit in 0.0005178451538085938 sec
2023-06-22 15:31:13,641 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.24473094940185547 sec
2023-06-22 15:31:13,644 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:31:13,700 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05531811714172363 sec
2023-06-22 15:31:13,700 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:31:13,878 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17685222625732422 sec
2023-06-22 15:31:13,909 - INFO - initial test loss: 0.019547657417633243
2023-06-22 15:31:13,909 - INFO - initial test acc: 0.7450000047683716
2023-06-22 15:31:13,922 - WARNING - Finished tracing + transforming dot for pjit in 0.0008516311645507812 sec
2023-06-22 15:31:13,924 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006954669952392578 sec
2023-06-22 15:31:13,926 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008525848388671875 sec
2023-06-22 15:31:13,927 - WARNING - Finished tracing + transforming _mean for pjit in 0.0022957324981689453 sec
2023-06-22 15:31:13,929 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004456043243408203 sec
2023-06-22 15:31:13,931 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00042366981506347656 sec
2023-06-22 15:31:13,932 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005736351013183594 sec
2023-06-22 15:31:13,934 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008397102355957031 sec
2023-06-22 15:31:13,935 - WARNING - Finished tracing + transforming _mean for pjit in 0.0024993419647216797 sec
2023-06-22 15:31:13,937 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.022573232650756836 sec
2023-06-22 15:31:13,956 - WARNING - Finished tracing + transforming fn for pjit in 0.0006003379821777344 sec
2023-06-22 15:31:13,958 - WARNING - Finished tracing + transforming fn for pjit in 0.0006151199340820312 sec
2023-06-22 15:31:13,959 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005185604095458984 sec
2023-06-22 15:31:13,961 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006303787231445312 sec
2023-06-22 15:31:13,962 - WARNING - Finished tracing + transforming _where for pjit in 0.001977682113647461 sec
2023-06-22 15:31:13,981 - WARNING - Finished tracing + transforming fn for pjit in 0.0005946159362792969 sec
2023-06-22 15:31:13,983 - WARNING - Finished tracing + transforming fn for pjit in 0.0006196498870849609 sec
2023-06-22 15:31:13,984 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005142688751220703 sec
2023-06-22 15:31:13,988 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0022475719451904297 sec
2023-06-22 15:31:13,988 - WARNING - Finished tracing + transforming _where for pjit in 0.003583669662475586 sec
2023-06-22 15:31:14,047 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0007226467132568359 sec
2023-06-22 15:31:14,128 - WARNING - Finished tracing + transforming fn for pjit in 0.00044345855712890625 sec
2023-06-22 15:31:14,129 - WARNING - Finished tracing + transforming fn for pjit in 0.0004291534423828125 sec
2023-06-22 15:31:14,130 - WARNING - Finished tracing + transforming square for pjit in 0.00028634071350097656 sec
2023-06-22 15:31:14,134 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003314018249511719 sec
2023-06-22 15:31:14,137 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000396728515625 sec
2023-06-22 15:31:14,138 - WARNING - Finished tracing + transforming fn for pjit in 0.0004444122314453125 sec
2023-06-22 15:31:14,139 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00036907196044921875 sec
2023-06-22 15:31:14,139 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003762245178222656 sec
2023-06-22 15:31:14,140 - WARNING - Finished tracing + transforming fn for pjit in 0.0004456043243408203 sec
2023-06-22 15:31:14,142 - WARNING - Finished tracing + transforming fn for pjit in 0.0004105567932128906 sec
2023-06-22 15:31:14,143 - WARNING - Finished tracing + transforming square for pjit in 0.00029659271240234375 sec
2023-06-22 15:31:14,146 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00038242340087890625 sec
2023-06-22 15:31:14,149 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027441978454589844 sec
2023-06-22 15:31:14,150 - WARNING - Finished tracing + transforming fn for pjit in 0.00046706199645996094 sec
2023-06-22 15:31:14,151 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003719329833984375 sec
2023-06-22 15:31:14,152 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003807544708251953 sec
2023-06-22 15:31:14,153 - WARNING - Finished tracing + transforming update_fn for pjit in 0.24011468887329102 sec
2023-06-22 15:31:14,159 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[360,10]), ShapedArray(float32[360,10]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:31:14,261 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.1021115779876709 sec
2023-06-22 15:31:14,261 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:31:14,654 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3925132751464844 sec
2023-06-22 15:31:16,672 - INFO - Distilling data from client: Client29
2023-06-22 15:31:16,673 - INFO - train loss: 0.005300567522449331
2023-06-22 15:31:16,673 - INFO - train acc: 0.9555555582046509
2023-06-22 15:31:16,786 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.70      0.71        63
           7       0.86      0.88      0.87       137

    accuracy                           0.82       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.82      0.82      0.82       200

2023-06-22 15:31:16,789 - INFO - test loss 0.01566201821605943
2023-06-22 15:31:16,790 - INFO - test acc 0.8199999928474426
2023-06-22 15:31:18,800 - INFO - Distilling data from client: Client29
2023-06-22 15:31:18,800 - INFO - train loss: 0.0032428840013927216
2023-06-22 15:31:18,801 - INFO - train acc: 0.9916666746139526
2023-06-22 15:31:18,851 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.65      0.69        63
           7       0.85      0.90      0.87       137

    accuracy                           0.82       200
   macro avg       0.80      0.77      0.78       200
weighted avg       0.82      0.82      0.82       200

2023-06-22 15:31:18,853 - INFO - test loss 0.016284150249464703
2023-06-22 15:31:18,853 - INFO - test acc 0.8199999928474426
2023-06-22 15:31:20,913 - INFO - Distilling data from client: Client29
2023-06-22 15:31:20,915 - INFO - train loss: 0.0027225568548146987
2023-06-22 15:31:20,916 - INFO - train acc: 0.9861111640930176
2023-06-22 15:31:20,975 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.71      0.69        63
           7       0.86      0.83      0.85       137

    accuracy                           0.80       200
   macro avg       0.76      0.77      0.77       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:31:20,980 - INFO - test loss 0.016661278244736663
2023-06-22 15:31:20,982 - INFO - test acc 0.7949999570846558
2023-06-22 15:31:22,938 - INFO - Distilling data from client: Client29
2023-06-22 15:31:22,938 - INFO - train loss: 0.0023911242884076768
2023-06-22 15:31:22,938 - INFO - train acc: 0.9944444894790649
2023-06-22 15:31:22,987 - INFO - report:               precision    recall  f1-score   support

           2       0.67      0.62      0.64        63
           7       0.83      0.86      0.85       137

    accuracy                           0.79       200
   macro avg       0.75      0.74      0.75       200
weighted avg       0.78      0.79      0.78       200

2023-06-22 15:31:22,989 - INFO - test loss 0.017578435728675162
2023-06-22 15:31:22,990 - INFO - test acc 0.7849999666213989
2023-06-22 15:31:24,899 - INFO - Distilling data from client: Client29
2023-06-22 15:31:24,899 - INFO - train loss: 0.0021505975868778096
2023-06-22 15:31:24,899 - INFO - train acc: 0.9972222447395325
2023-06-22 15:31:24,960 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.62      0.64        63
           7       0.83      0.85      0.84       137

    accuracy                           0.78       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:31:24,962 - INFO - test loss 0.01778101949576963
2023-06-22 15:31:24,962 - INFO - test acc 0.7799999713897705
2023-06-22 15:31:27,085 - INFO - Distilling data from client: Client29
2023-06-22 15:31:27,085 - INFO - train loss: 0.002431245588777061
2023-06-22 15:31:27,086 - INFO - train acc: 0.9916666746139526
2023-06-22 15:31:27,132 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.63      0.68        63
           7       0.84      0.89      0.87       137

    accuracy                           0.81       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:31:27,137 - INFO - test loss 0.01681592623279432
2023-06-22 15:31:27,138 - INFO - test acc 0.8100000023841858
2023-06-22 15:31:29,206 - INFO - Distilling data from client: Client29
2023-06-22 15:31:29,206 - INFO - train loss: 0.0022052894407482136
2023-06-22 15:31:29,207 - INFO - train acc: 0.9944444894790649
2023-06-22 15:31:29,243 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.63      0.68        63
           7       0.84      0.89      0.87       137

    accuracy                           0.81       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:31:29,244 - INFO - test loss 0.017488167396050865
2023-06-22 15:31:29,244 - INFO - test acc 0.8100000023841858
2023-06-22 15:31:31,309 - INFO - Distilling data from client: Client29
2023-06-22 15:31:31,309 - INFO - train loss: 0.0019709973339160905
2023-06-22 15:31:31,309 - INFO - train acc: 0.9861111640930176
2023-06-22 15:31:31,349 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.63      0.68        63
           7       0.84      0.89      0.87       137

    accuracy                           0.81       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:31:31,350 - INFO - test loss 0.01666424202640416
2023-06-22 15:31:31,350 - INFO - test acc 0.8100000023841858
2023-06-22 15:31:33,542 - INFO - Distilling data from client: Client29
2023-06-22 15:31:33,543 - INFO - train loss: 0.002017980078888204
2023-06-22 15:31:33,543 - INFO - train acc: 0.9916666746139526
2023-06-22 15:31:33,576 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.59      0.61        63
           7       0.82      0.85      0.83       137

    accuracy                           0.77       200
   macro avg       0.73      0.72      0.72       200
weighted avg       0.76      0.77      0.76       200

2023-06-22 15:31:33,576 - INFO - test loss 0.018210929419180705
2023-06-22 15:31:33,576 - INFO - test acc 0.7649999856948853
2023-06-22 15:31:35,530 - INFO - Distilling data from client: Client29
2023-06-22 15:31:35,532 - INFO - train loss: 0.002093320409497054
2023-06-22 15:31:35,533 - INFO - train acc: 1.0
2023-06-22 15:31:35,590 - INFO - report:               precision    recall  f1-score   support

           2       0.69      0.57      0.63        63
           7       0.82      0.88      0.85       137

    accuracy                           0.79       200
   macro avg       0.75      0.73      0.74       200
weighted avg       0.78      0.79      0.78       200

2023-06-22 15:31:35,594 - INFO - test loss 0.018208717742755592
2023-06-22 15:31:35,595 - INFO - test acc 0.7849999666213989
2023-06-22 15:31:37,599 - INFO - Distilling data from client: Client29
2023-06-22 15:31:37,600 - INFO - train loss: 0.0017647572436900471
2023-06-22 15:31:37,600 - INFO - train acc: 1.0
2023-06-22 15:31:37,640 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.63      0.68        63
           7       0.84      0.89      0.87       137

    accuracy                           0.81       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:31:37,642 - INFO - test loss 0.017176148062649874
2023-06-22 15:31:37,643 - INFO - test acc 0.8100000023841858
2023-06-22 15:31:39,745 - INFO - Distilling data from client: Client29
2023-06-22 15:31:39,746 - INFO - train loss: 0.0015830604318176312
2023-06-22 15:31:39,746 - INFO - train acc: 1.0
2023-06-22 15:31:39,804 - INFO - report:               precision    recall  f1-score   support

           2       0.68      0.62      0.65        63
           7       0.83      0.87      0.85       137

    accuracy                           0.79       200
   macro avg       0.76      0.74      0.75       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:31:39,805 - INFO - test loss 0.017444666033574478
2023-06-22 15:31:39,805 - INFO - test acc 0.7899999618530273
2023-06-22 15:31:42,019 - INFO - Distilling data from client: Client29
2023-06-22 15:31:42,019 - INFO - train loss: 0.001911919218652924
2023-06-22 15:31:42,020 - INFO - train acc: 1.0
2023-06-22 15:31:42,075 - INFO - report:               precision    recall  f1-score   support

           2       0.69      0.65      0.67        63
           7       0.84      0.87      0.86       137

    accuracy                           0.80       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:31:42,076 - INFO - test loss 0.017282872560184494
2023-06-22 15:31:42,077 - INFO - test acc 0.7999999523162842
2023-06-22 15:31:44,028 - INFO - Distilling data from client: Client29
2023-06-22 15:31:44,028 - INFO - train loss: 0.0015907471867765946
2023-06-22 15:31:44,029 - INFO - train acc: 0.9944444894790649
2023-06-22 15:31:44,067 - INFO - report:               precision    recall  f1-score   support

           2       0.71      0.65      0.68        63
           7       0.85      0.88      0.86       137

    accuracy                           0.81       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.80      0.81      0.80       200

2023-06-22 15:31:44,067 - INFO - test loss 0.01716729473665925
2023-06-22 15:31:44,067 - INFO - test acc 0.8050000071525574
2023-06-22 15:31:45,986 - INFO - Distilling data from client: Client29
2023-06-22 15:31:45,986 - INFO - train loss: 0.0016939319520504442
2023-06-22 15:31:45,986 - INFO - train acc: 1.0
2023-06-22 15:31:46,025 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.62      0.67        63
           7       0.84      0.89      0.86       137

    accuracy                           0.81       200
   macro avg       0.78      0.75      0.76       200
weighted avg       0.80      0.81      0.80       200

2023-06-22 15:31:46,025 - INFO - test loss 0.017449798397793002
2023-06-22 15:31:46,026 - INFO - test acc 0.8050000071525574
2023-06-22 15:31:48,049 - INFO - Distilling data from client: Client29
2023-06-22 15:31:48,049 - INFO - train loss: 0.0016217103074083442
2023-06-22 15:31:48,050 - INFO - train acc: 0.9944444894790649
2023-06-22 15:31:48,109 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.62      0.68        63
           7       0.84      0.91      0.87       137

    accuracy                           0.81       200
   macro avg       0.79      0.76      0.77       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:31:48,112 - INFO - test loss 0.01747323908600697
2023-06-22 15:31:48,112 - INFO - test acc 0.8149999976158142
2023-06-22 15:31:50,125 - INFO - Distilling data from client: Client29
2023-06-22 15:31:50,126 - INFO - train loss: 0.001994701422519174
2023-06-22 15:31:50,126 - INFO - train acc: 1.0
2023-06-22 15:31:50,169 - INFO - report:               precision    recall  f1-score   support

           2       0.67      0.57      0.62        63
           7       0.82      0.87      0.84       137

    accuracy                           0.78       200
   macro avg       0.74      0.72      0.73       200
weighted avg       0.77      0.78      0.77       200

2023-06-22 15:31:50,170 - INFO - test loss 0.01732382345479462
2023-06-22 15:31:50,170 - INFO - test acc 0.7749999761581421
2023-06-22 15:31:52,377 - INFO - Distilling data from client: Client29
2023-06-22 15:31:52,378 - INFO - train loss: 0.0017105778371684108
2023-06-22 15:31:52,378 - INFO - train acc: 1.0
2023-06-22 15:31:52,428 - INFO - report:               precision    recall  f1-score   support

           2       0.70      0.63      0.67        63
           7       0.84      0.88      0.86       137

    accuracy                           0.80       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:31:52,429 - INFO - test loss 0.017221181494250993
2023-06-22 15:31:52,429 - INFO - test acc 0.7999999523162842
2023-06-22 15:31:54,686 - INFO - Distilling data from client: Client29
2023-06-22 15:31:54,686 - INFO - train loss: 0.0016674954833498221
2023-06-22 15:31:54,687 - INFO - train acc: 0.9972222447395325
2023-06-22 15:31:54,737 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.60      0.66        63
           7       0.83      0.89      0.86       137

    accuracy                           0.80       200
   macro avg       0.77      0.75      0.76       200
weighted avg       0.79      0.80      0.79       200

2023-06-22 15:31:54,739 - INFO - test loss 0.01712687096297663
2023-06-22 15:31:54,740 - INFO - test acc 0.7999999523162842
2023-06-22 15:31:56,854 - INFO - Distilling data from client: Client29
2023-06-22 15:31:56,855 - INFO - train loss: 0.0017824795750785458
2023-06-22 15:31:56,855 - INFO - train acc: 0.9944444894790649
2023-06-22 15:31:56,906 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.62      0.64        63
           7       0.83      0.85      0.84       137

    accuracy                           0.78       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:31:56,907 - INFO - test loss 0.01675801803152537
2023-06-22 15:31:56,908 - INFO - test acc 0.7799999713897705
2023-06-22 15:31:59,098 - INFO - Distilling data from client: Client29
2023-06-22 15:31:59,099 - INFO - train loss: 0.0013227740333999918
2023-06-22 15:31:59,099 - INFO - train acc: 1.0
2023-06-22 15:31:59,153 - INFO - report:               precision    recall  f1-score   support

           2       0.69      0.67      0.68        63
           7       0.85      0.86      0.86       137

    accuracy                           0.80       200
   macro avg       0.77      0.76      0.77       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:31:59,154 - INFO - test loss 0.017070979937203395
2023-06-22 15:31:59,154 - INFO - test acc 0.7999999523162842
2023-06-22 15:32:01,346 - INFO - Distilling data from client: Client29
2023-06-22 15:32:01,346 - INFO - train loss: 0.0018174479879147954
2023-06-22 15:32:01,346 - INFO - train acc: 0.9972222447395325
2023-06-22 15:32:01,396 - INFO - report:               precision    recall  f1-score   support

           2       0.67      0.59      0.63        63
           7       0.82      0.87      0.84       137

    accuracy                           0.78       200
   macro avg       0.75      0.73      0.74       200
weighted avg       0.77      0.78      0.78       200

2023-06-22 15:32:01,399 - INFO - test loss 0.018178149236487993
2023-06-22 15:32:01,400 - INFO - test acc 0.7799999713897705
2023-06-22 15:32:03,594 - INFO - Distilling data from client: Client29
2023-06-22 15:32:03,595 - INFO - train loss: 0.0016511466376852727
2023-06-22 15:32:03,595 - INFO - train acc: 1.0
2023-06-22 15:32:03,640 - INFO - report:               precision    recall  f1-score   support

           2       0.68      0.62      0.65        63
           7       0.83      0.87      0.85       137

    accuracy                           0.79       200
   macro avg       0.76      0.74      0.75       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:32:03,642 - INFO - test loss 0.017846444133680573
2023-06-22 15:32:03,642 - INFO - test acc 0.7899999618530273
2023-06-22 15:32:05,759 - INFO - Distilling data from client: Client29
2023-06-22 15:32:05,760 - INFO - train loss: 0.0015455878853029873
2023-06-22 15:32:05,761 - INFO - train acc: 0.9972222447395325
2023-06-22 15:32:05,810 - INFO - report:               precision    recall  f1-score   support

           2       0.68      0.57      0.62        63
           7       0.82      0.88      0.85       137

    accuracy                           0.78       200
   macro avg       0.75      0.72      0.73       200
weighted avg       0.77      0.78      0.77       200

2023-06-22 15:32:05,811 - INFO - test loss 0.01794833406822408
2023-06-22 15:32:05,811 - INFO - test acc 0.7799999713897705
2023-06-22 15:32:07,953 - INFO - Distilling data from client: Client29
2023-06-22 15:32:07,954 - INFO - train loss: 0.0012374754989565146
2023-06-22 15:32:07,954 - INFO - train acc: 1.0
2023-06-22 15:32:08,001 - INFO - report:               precision    recall  f1-score   support

           2       0.69      0.63      0.66        63
           7       0.84      0.87      0.85       137

    accuracy                           0.80       200
   macro avg       0.76      0.75      0.76       200
weighted avg       0.79      0.80      0.79       200

2023-06-22 15:32:08,002 - INFO - test loss 0.01683104698164705
2023-06-22 15:32:08,002 - INFO - test acc 0.7949999570846558
2023-06-22 15:32:08,011 - WARNING - Finished tracing + transforming jit(gather) in 0.0010106563568115234 sec
2023-06-22 15:32:08,012 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[360,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:32:08,017 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.003947257995605469 sec
2023-06-22 15:32:08,017 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:32:08,041 - WARNING - Finished XLA compilation of jit(gather) in 0.022683382034301758 sec
2023-06-22 15:32:08,067 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:32:08,090 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:32:08,105 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:32:08,116 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:32:08,130 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:32:08,144 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:32:08,578 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client29//synthetic.png
2023-06-22 15:32:08,600 - INFO - c: 1.0 and total_data_in_this_class: 538
2023-06-22 15:32:08,600 - INFO - c: 7.0 and total_data_in_this_class: 261
2023-06-22 15:32:08,600 - INFO - c: 1.0 and total_data_in_this_class: 128
2023-06-22 15:32:08,600 - INFO - c: 7.0 and total_data_in_this_class: 72
2023-06-22 15:32:08,649 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0004086494445800781 sec
2023-06-22 15:32:08,649 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:32:08,651 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.002038717269897461 sec
2023-06-22 15:32:08,652 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:32:08,668 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.015643596649169922 sec
2023-06-22 15:32:08,671 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003619194030761719 sec
2023-06-22 15:32:08,671 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:32:08,673 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0016193389892578125 sec
2023-06-22 15:32:08,674 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:32:08,686 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011671781539916992 sec
2023-06-22 15:32:08,691 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002238750457763672 sec
2023-06-22 15:32:08,693 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019598007202148438 sec
2023-06-22 15:32:08,694 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005199909210205078 sec
2023-06-22 15:32:08,696 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00035071372985839844 sec
2023-06-22 15:32:08,697 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019168853759765625 sec
2023-06-22 15:32:08,698 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004513263702392578 sec
2023-06-22 15:32:08,699 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040602684020996094 sec
2023-06-22 15:32:08,700 - WARNING - Finished tracing + transforming absolute for pjit in 0.00028967857360839844 sec
2023-06-22 15:32:08,701 - WARNING - Finished tracing + transforming fn for pjit in 0.0004436969757080078 sec
2023-06-22 15:32:08,702 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005540847778320312 sec
2023-06-22 15:32:08,703 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003349781036376953 sec
2023-06-22 15:32:08,705 - WARNING - Finished tracing + transforming fn for pjit in 0.00037360191345214844 sec
2023-06-22 15:32:08,706 - WARNING - Finished tracing + transforming fn for pjit in 0.00045108795166015625 sec
2023-06-22 15:32:08,707 - WARNING - Finished tracing + transforming fn for pjit in 0.000408172607421875 sec
2023-06-22 15:32:08,708 - WARNING - Finished tracing + transforming fn for pjit in 0.0004456043243408203 sec
2023-06-22 15:32:08,710 - WARNING - Finished tracing + transforming fn for pjit in 0.0003609657287597656 sec
2023-06-22 15:32:08,713 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002791881561279297 sec
2023-06-22 15:32:08,714 - WARNING - Finished tracing + transforming fn for pjit in 0.00039577484130859375 sec
2023-06-22 15:32:08,716 - WARNING - Finished tracing + transforming fn for pjit in 0.00040149688720703125 sec
2023-06-22 15:32:08,722 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005795955657958984 sec
2023-06-22 15:32:08,722 - WARNING - Finished tracing + transforming _mean for pjit in 0.0015571117401123047 sec
2023-06-22 15:32:08,724 - WARNING - Finished tracing + transforming fn for pjit in 0.000408172607421875 sec
2023-06-22 15:32:08,725 - WARNING - Finished tracing + transforming fn for pjit in 0.00035834312438964844 sec
2023-06-22 15:32:08,727 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005204677581787109 sec
2023-06-22 15:32:08,728 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044918060302734375 sec
2023-06-22 15:32:08,729 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031685829162597656 sec
2023-06-22 15:32:08,730 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00043773651123046875 sec
2023-06-22 15:32:08,732 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004000663757324219 sec
2023-06-22 15:32:08,733 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003955364227294922 sec
2023-06-22 15:32:08,734 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006077289581298828 sec
2023-06-22 15:32:08,735 - WARNING - Finished tracing + transforming _where for pjit in 0.0016644001007080078 sec
2023-06-22 15:32:08,736 - WARNING - Finished tracing + transforming fn for pjit in 0.0004305839538574219 sec
2023-06-22 15:32:08,737 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004181861877441406 sec
2023-06-22 15:32:08,739 - WARNING - Finished tracing + transforming fn for pjit in 0.0004394054412841797 sec
2023-06-22 15:32:08,740 - WARNING - Finished tracing + transforming fn for pjit in 0.0003859996795654297 sec
2023-06-22 15:32:08,741 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003917217254638672 sec
2023-06-22 15:32:08,742 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004546642303466797 sec
2023-06-22 15:32:08,743 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045037269592285156 sec
2023-06-22 15:32:08,745 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006275177001953125 sec
2023-06-22 15:32:08,747 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005483627319335938 sec
2023-06-22 15:32:08,749 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005376338958740234 sec
2023-06-22 15:32:08,751 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006134510040283203 sec
2023-06-22 15:32:08,751 - WARNING - Finished tracing + transforming _where for pjit in 0.0020797252655029297 sec
2023-06-22 15:32:08,753 - WARNING - Finished tracing + transforming fn for pjit in 0.0006256103515625 sec
2023-06-22 15:32:08,755 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006277561187744141 sec
2023-06-22 15:32:08,759 - WARNING - Finished tracing + transforming fn for pjit in 0.0005543231964111328 sec
2023-06-22 15:32:08,769 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006594657897949219 sec
2023-06-22 15:32:08,771 - WARNING - Finished tracing + transforming fn for pjit in 0.0008203983306884766 sec
2023-06-22 15:32:08,772 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006618499755859375 sec
2023-06-22 15:32:08,774 - WARNING - Finished tracing + transforming fn for pjit in 0.00054168701171875 sec
2023-06-22 15:32:08,783 - WARNING - Finished tracing + transforming fn for pjit in 0.0005433559417724609 sec
2023-06-22 15:32:08,788 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003936290740966797 sec
2023-06-22 15:32:08,789 - WARNING - Finished tracing + transforming fn for pjit in 0.0007355213165283203 sec
2023-06-22 15:32:08,791 - WARNING - Finished tracing + transforming fn for pjit in 0.0005633831024169922 sec
2023-06-22 15:32:08,833 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.14310741424560547 sec
2023-06-22 15:32:08,842 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002961158752441406 sec
2023-06-22 15:32:08,843 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00027370452880859375 sec
2023-06-22 15:32:08,844 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0006513595581054688 sec
2023-06-22 15:32:08,850 - WARNING - Finished tracing + transforming fn for pjit in 0.0005345344543457031 sec
2023-06-22 15:32:08,851 - WARNING - Finished tracing + transforming fn for pjit in 0.0006389617919921875 sec
2023-06-22 15:32:08,855 - WARNING - Finished tracing + transforming fn for pjit in 0.0005290508270263672 sec
2023-06-22 15:32:08,868 - WARNING - Finished tracing + transforming fn for pjit in 0.0003714561462402344 sec
2023-06-22 15:32:08,870 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039696693420410156 sec
2023-06-22 15:32:08,871 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046133995056152344 sec
2023-06-22 15:32:08,872 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003104209899902344 sec
2023-06-22 15:32:08,874 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005805492401123047 sec
2023-06-22 15:32:08,875 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040435791015625 sec
2023-06-22 15:32:08,876 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038814544677734375 sec
2023-06-22 15:32:08,878 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00047588348388671875 sec
2023-06-22 15:32:08,878 - WARNING - Finished tracing + transforming _where for pjit in 0.0015065670013427734 sec
2023-06-22 15:32:08,879 - WARNING - Finished tracing + transforming fn for pjit in 0.0004563331604003906 sec
2023-06-22 15:32:08,881 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004382133483886719 sec
2023-06-22 15:32:08,882 - WARNING - Finished tracing + transforming fn for pjit in 0.0003829002380371094 sec
2023-06-22 15:32:08,883 - WARNING - Finished tracing + transforming fn for pjit in 0.0005152225494384766 sec
2023-06-22 15:32:08,903 - WARNING - Finished tracing + transforming fn for pjit in 0.0003540515899658203 sec
2023-06-22 15:32:08,936 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.09964990615844727 sec
2023-06-22 15:32:08,938 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002124309539794922 sec
2023-06-22 15:32:08,940 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00024366378784179688 sec
2023-06-22 15:32:08,940 - WARNING - Finished tracing + transforming _where for pjit in 0.0010895729064941406 sec
2023-06-22 15:32:08,941 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005068778991699219 sec
2023-06-22 15:32:08,942 - WARNING - Finished tracing + transforming trace for pjit in 0.00442957878112793 sec
2023-06-22 15:32:08,946 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00017690658569335938 sec
2023-06-22 15:32:08,947 - WARNING - Finished tracing + transforming tril for pjit in 0.0011076927185058594 sec
2023-06-22 15:32:08,948 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0031201839447021484 sec
2023-06-22 15:32:08,950 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019216537475585938 sec
2023-06-22 15:32:08,950 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001895427703857422 sec
2023-06-22 15:32:08,954 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.002420186996459961 sec
2023-06-22 15:32:08,960 - WARNING - Finished tracing + transforming _solve for pjit in 0.015842199325561523 sec
2023-06-22 15:32:08,961 - WARNING - Finished tracing + transforming dot for pjit in 0.0005400180816650391 sec
2023-06-22 15:32:08,966 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.2777843475341797 sec
2023-06-22 15:32:08,969 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:32:09,025 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05553889274597168 sec
2023-06-22 15:32:09,025 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:32:09,203 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17696595191955566 sec
2023-06-22 15:32:09,229 - INFO - initial test loss: 0.012270341621569114
2023-06-22 15:32:09,229 - INFO - initial test acc: 0.875
2023-06-22 15:32:09,241 - WARNING - Finished tracing + transforming dot for pjit in 0.00083160400390625 sec
2023-06-22 15:32:09,242 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006821155548095703 sec
2023-06-22 15:32:09,245 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008347034454345703 sec
2023-06-22 15:32:09,246 - WARNING - Finished tracing + transforming _mean for pjit in 0.002267122268676758 sec
2023-06-22 15:32:09,248 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004336833953857422 sec
2023-06-22 15:32:09,249 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00042629241943359375 sec
2023-06-22 15:32:09,251 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006186962127685547 sec
2023-06-22 15:32:09,253 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008218288421630859 sec
2023-06-22 15:32:09,254 - WARNING - Finished tracing + transforming _mean for pjit in 0.002450704574584961 sec
2023-06-22 15:32:09,256 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.022677183151245117 sec
2023-06-22 15:32:09,276 - WARNING - Finished tracing + transforming fn for pjit in 0.0006093978881835938 sec
2023-06-22 15:32:09,278 - WARNING - Finished tracing + transforming fn for pjit in 0.0006277561187744141 sec
2023-06-22 15:32:09,279 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005600452423095703 sec
2023-06-22 15:32:09,282 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006213188171386719 sec
2023-06-22 15:32:09,283 - WARNING - Finished tracing + transforming _where for pjit in 0.0022993087768554688 sec
2023-06-22 15:32:09,311 - WARNING - Finished tracing + transforming fn for pjit in 0.0006320476531982422 sec
2023-06-22 15:32:09,320 - WARNING - Finished tracing + transforming fn for pjit in 0.0006403923034667969 sec
2023-06-22 15:32:09,323 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005288124084472656 sec
2023-06-22 15:32:09,326 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0007212162017822266 sec
2023-06-22 15:32:09,330 - WARNING - Finished tracing + transforming _where for pjit in 0.004327535629272461 sec
2023-06-22 15:32:09,384 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00036787986755371094 sec
2023-06-22 15:32:09,465 - WARNING - Finished tracing + transforming fn for pjit in 0.00042724609375 sec
2023-06-22 15:32:09,466 - WARNING - Finished tracing + transforming fn for pjit in 0.0003952980041503906 sec
2023-06-22 15:32:09,467 - WARNING - Finished tracing + transforming square for pjit in 0.0003161430358886719 sec
2023-06-22 15:32:09,471 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003612041473388672 sec
2023-06-22 15:32:09,474 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004360675811767578 sec
2023-06-22 15:32:09,475 - WARNING - Finished tracing + transforming fn for pjit in 0.00044798851013183594 sec
2023-06-22 15:32:09,476 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003902912139892578 sec
2023-06-22 15:32:09,477 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003998279571533203 sec
2023-06-22 15:32:09,478 - WARNING - Finished tracing + transforming fn for pjit in 0.0004591941833496094 sec
2023-06-22 15:32:09,479 - WARNING - Finished tracing + transforming fn for pjit in 0.0004494190216064453 sec
2023-06-22 15:32:09,480 - WARNING - Finished tracing + transforming square for pjit in 0.00028395652770996094 sec
2023-06-22 15:32:09,483 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000362396240234375 sec
2023-06-22 15:32:09,486 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002613067626953125 sec
2023-06-22 15:32:09,487 - WARNING - Finished tracing + transforming fn for pjit in 0.00044536590576171875 sec
2023-06-22 15:32:09,488 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003943443298339844 sec
2023-06-22 15:32:09,489 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038504600524902344 sec
2023-06-22 15:32:09,490 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2583467960357666 sec
2023-06-22 15:32:09,496 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[348,10]), ShapedArray(float32[348,10]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:32:09,597 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.1011955738067627 sec
2023-06-22 15:32:09,598 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:32:10,007 - WARNING - Finished XLA compilation of jit(update_fn) in 0.40890002250671387 sec
2023-06-22 15:32:12,148 - INFO - Distilling data from client: Client30
2023-06-22 15:32:12,148 - INFO - train loss: 0.0025701100842308745
2023-06-22 15:32:12,149 - INFO - train acc: 1.0
2023-06-22 15:32:12,265 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.90      0.89       128
           7       0.81      0.79      0.80        72

    accuracy                           0.86       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:32:12,267 - INFO - test loss 0.01095367294690287
2023-06-22 15:32:12,267 - INFO - test acc 0.85999995470047
2023-06-22 15:32:14,450 - INFO - Distilling data from client: Client30
2023-06-22 15:32:14,450 - INFO - train loss: 0.0016674648572788371
2023-06-22 15:32:14,451 - INFO - train acc: 1.0
2023-06-22 15:32:14,492 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.89      0.89       128
           7       0.80      0.79      0.80        72

    accuracy                           0.85       200
   macro avg       0.84      0.84      0.84       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:32:14,499 - INFO - test loss 0.011169929935103814
2023-06-22 15:32:14,499 - INFO - test acc 0.8549999594688416
2023-06-22 15:32:16,579 - INFO - Distilling data from client: Client30
2023-06-22 15:32:16,579 - INFO - train loss: 0.0014758087566837759
2023-06-22 15:32:16,580 - INFO - train acc: 1.0
2023-06-22 15:32:16,687 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.91      0.90       128
           7       0.83      0.81      0.82        72

    accuracy                           0.87       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:32:16,689 - INFO - test loss 0.011259968749812903
2023-06-22 15:32:16,690 - INFO - test acc 0.8700000047683716
2023-06-22 15:32:18,984 - INFO - Distilling data from client: Client30
2023-06-22 15:32:18,985 - INFO - train loss: 0.0013940836309220947
2023-06-22 15:32:18,987 - INFO - train acc: 0.9971264600753784
2023-06-22 15:32:19,039 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.88      0.89       128
           7       0.79      0.83      0.81        72

    accuracy                           0.86       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:32:19,042 - INFO - test loss 0.011342150944718191
2023-06-22 15:32:19,044 - INFO - test acc 0.85999995470047
2023-06-22 15:32:21,394 - INFO - Distilling data from client: Client30
2023-06-22 15:32:21,395 - INFO - train loss: 0.001335262637805359
2023-06-22 15:32:21,395 - INFO - train acc: 1.0
2023-06-22 15:32:21,439 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.90      0.89       128
           7       0.82      0.81      0.81        72

    accuracy                           0.86       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:32:21,444 - INFO - test loss 0.01072291624255609
2023-06-22 15:32:21,444 - INFO - test acc 0.8650000095367432
2023-06-22 15:32:23,625 - INFO - Distilling data from client: Client30
2023-06-22 15:32:23,626 - INFO - train loss: 0.0012365867908550284
2023-06-22 15:32:23,628 - INFO - train acc: 0.9971264600753784
2023-06-22 15:32:23,765 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.91      0.90       128
           7       0.83      0.82      0.83        72

    accuracy                           0.88       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.87      0.88      0.87       200

2023-06-22 15:32:23,768 - INFO - test loss 0.010606762242249183
2023-06-22 15:32:23,769 - INFO - test acc 0.875
2023-06-22 15:32:25,906 - INFO - Distilling data from client: Client30
2023-06-22 15:32:25,907 - INFO - train loss: 0.001144437659319152
2023-06-22 15:32:25,907 - INFO - train acc: 0.9971264600753784
2023-06-22 15:32:25,954 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.91      0.89       128
           7       0.83      0.74      0.78        72

    accuracy                           0.85       200
   macro avg       0.84      0.83      0.83       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:32:25,957 - INFO - test loss 0.011041516194129049
2023-06-22 15:32:25,957 - INFO - test acc 0.8499999642372131
2023-06-22 15:32:28,241 - INFO - Distilling data from client: Client30
2023-06-22 15:32:28,241 - INFO - train loss: 0.000852608985925939
2023-06-22 15:32:28,242 - INFO - train acc: 1.0
2023-06-22 15:32:28,301 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.90      0.89       128
           7       0.81      0.78      0.79        72

    accuracy                           0.85       200
   macro avg       0.84      0.84      0.84       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:32:28,302 - INFO - test loss 0.011098349278406241
2023-06-22 15:32:28,302 - INFO - test acc 0.8549999594688416
2023-06-22 15:32:30,588 - INFO - Distilling data from client: Client30
2023-06-22 15:32:30,588 - INFO - train loss: 0.0010227383270491708
2023-06-22 15:32:30,588 - INFO - train acc: 1.0
2023-06-22 15:32:30,633 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.91      0.90       128
           7       0.84      0.79      0.81        72

    accuracy                           0.87       200
   macro avg       0.86      0.85      0.86       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:32:30,634 - INFO - test loss 0.010563488652922665
2023-06-22 15:32:30,634 - INFO - test acc 0.8700000047683716
2023-06-22 15:32:32,873 - INFO - Distilling data from client: Client30
2023-06-22 15:32:32,874 - INFO - train loss: 0.0008524223363566027
2023-06-22 15:32:32,875 - INFO - train acc: 1.0
2023-06-22 15:32:33,009 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.93      0.91       128
           7       0.86      0.79      0.83        72

    accuracy                           0.88       200
   macro avg       0.88      0.86      0.87       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:32:33,012 - INFO - test loss 0.010662855024939102
2023-06-22 15:32:33,013 - INFO - test acc 0.8799999952316284
2023-06-22 15:32:35,311 - INFO - Distilling data from client: Client30
2023-06-22 15:32:35,322 - INFO - train loss: 0.001105034004692219
2023-06-22 15:32:35,323 - INFO - train acc: 1.0
2023-06-22 15:32:35,402 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.91      0.89       128
           7       0.83      0.76      0.80        72

    accuracy                           0.86       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:32:35,403 - INFO - test loss 0.010959142301308301
2023-06-22 15:32:35,404 - INFO - test acc 0.85999995470047
2023-06-22 15:32:37,980 - INFO - Distilling data from client: Client30
2023-06-22 15:32:37,980 - INFO - train loss: 0.0009135116752277876
2023-06-22 15:32:37,981 - INFO - train acc: 1.0
2023-06-22 15:32:38,038 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.91      0.90       128
           7       0.83      0.81      0.82        72

    accuracy                           0.87       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:32:38,038 - INFO - test loss 0.01091436774123677
2023-06-22 15:32:38,039 - INFO - test acc 0.8700000047683716
2023-06-22 15:32:40,125 - INFO - Distilling data from client: Client30
2023-06-22 15:32:40,125 - INFO - train loss: 0.0007187026110588446
2023-06-22 15:32:40,125 - INFO - train acc: 1.0
2023-06-22 15:32:40,256 - INFO - report:               precision    recall  f1-score   support

           1       0.91      0.94      0.92       128
           7       0.88      0.83      0.86        72

    accuracy                           0.90       200
   macro avg       0.90      0.89      0.89       200
weighted avg       0.90      0.90      0.90       200

2023-06-22 15:32:40,256 - INFO - test loss 0.009489094817536011
2023-06-22 15:32:40,256 - INFO - test acc 0.8999999761581421
2023-06-22 15:32:42,586 - INFO - Distilling data from client: Client30
2023-06-22 15:32:42,587 - INFO - train loss: 0.0007465562550250884
2023-06-22 15:32:42,588 - INFO - train acc: 1.0
2023-06-22 15:32:42,646 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.91      0.88       128
           7       0.81      0.72      0.76        72

    accuracy                           0.84       200
   macro avg       0.83      0.81      0.82       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:32:42,648 - INFO - test loss 0.011430336184230295
2023-06-22 15:32:42,649 - INFO - test acc 0.8399999737739563
2023-06-22 15:32:44,860 - INFO - Distilling data from client: Client30
2023-06-22 15:32:44,860 - INFO - train loss: 0.0008574812781505529
2023-06-22 15:32:44,860 - INFO - train acc: 1.0
2023-06-22 15:32:44,996 - INFO - report:               precision    recall  f1-score   support

           1       0.92      0.88      0.90       128
           7       0.79      0.86      0.83        72

    accuracy                           0.87       200
   macro avg       0.86      0.87      0.86       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:32:44,997 - INFO - test loss 0.010342072066385804
2023-06-22 15:32:44,998 - INFO - test acc 0.8700000047683716
2023-06-22 15:32:48,109 - INFO - Distilling data from client: Client30
2023-06-22 15:32:48,110 - INFO - train loss: 0.0008120468439455745
2023-06-22 15:32:48,110 - INFO - train acc: 1.0
2023-06-22 15:32:48,167 - INFO - report:               precision    recall  f1-score   support

           1       0.91      0.91      0.91       128
           7       0.83      0.83      0.83        72

    accuracy                           0.88       200
   macro avg       0.87      0.87      0.87       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:32:48,168 - INFO - test loss 0.010915943135623687
2023-06-22 15:32:48,168 - INFO - test acc 0.8799999952316284
2023-06-22 15:32:51,221 - INFO - Distilling data from client: Client30
2023-06-22 15:32:51,221 - INFO - train loss: 0.0007285988429150177
2023-06-22 15:32:51,222 - INFO - train acc: 1.0
2023-06-22 15:32:51,279 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.92      0.91       128
           7       0.86      0.82      0.84        72

    accuracy                           0.89       200
   macro avg       0.88      0.87      0.87       200
weighted avg       0.88      0.89      0.88       200

2023-06-22 15:32:51,280 - INFO - test loss 0.010598095227491175
2023-06-22 15:32:51,281 - INFO - test acc 0.8849999904632568
2023-06-22 15:32:53,989 - INFO - Distilling data from client: Client30
2023-06-22 15:32:53,990 - INFO - train loss: 0.0007673740310398975
2023-06-22 15:32:53,991 - INFO - train acc: 1.0
2023-06-22 15:32:54,042 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.91      0.89       128
           7       0.82      0.78      0.80        72

    accuracy                           0.86       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:32:54,044 - INFO - test loss 0.010608808582191468
2023-06-22 15:32:54,045 - INFO - test acc 0.85999995470047
2023-06-22 15:32:56,341 - INFO - Distilling data from client: Client30
2023-06-22 15:32:56,341 - INFO - train loss: 0.0008321234857126109
2023-06-22 15:32:56,342 - INFO - train acc: 1.0
2023-06-22 15:32:56,407 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.91      0.90       128
           7       0.83      0.79      0.81        72

    accuracy                           0.86       200
   macro avg       0.86      0.85      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:32:56,408 - INFO - test loss 0.011164453967748051
2023-06-22 15:32:56,409 - INFO - test acc 0.8650000095367432
2023-06-22 15:32:58,573 - INFO - Distilling data from client: Client30
2023-06-22 15:32:58,573 - INFO - train loss: 0.0007064464551970701
2023-06-22 15:32:58,574 - INFO - train acc: 1.0
2023-06-22 15:32:58,628 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.89      0.89       128
           7       0.81      0.81      0.81        72

    accuracy                           0.86       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:32:58,630 - INFO - test loss 0.01119399703418712
2023-06-22 15:32:58,631 - INFO - test acc 0.85999995470047
2023-06-22 15:33:00,845 - INFO - Distilling data from client: Client30
2023-06-22 15:33:00,845 - INFO - train loss: 0.0008442450610523614
2023-06-22 15:33:00,846 - INFO - train acc: 1.0
2023-06-22 15:33:00,948 - INFO - report:               precision    recall  f1-score   support

           1       0.92      0.94      0.93       128
           7       0.88      0.85      0.87        72

    accuracy                           0.91       200
   macro avg       0.90      0.89      0.90       200
weighted avg       0.90      0.91      0.90       200

2023-06-22 15:33:00,950 - INFO - test loss 0.010397595980865612
2023-06-22 15:33:00,951 - INFO - test acc 0.9049999713897705
2023-06-22 15:33:03,294 - INFO - Distilling data from client: Client30
2023-06-22 15:33:03,295 - INFO - train loss: 0.0008227602839483002
2023-06-22 15:33:03,296 - INFO - train acc: 1.0
2023-06-22 15:33:03,345 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.91      0.89       128
           7       0.82      0.76      0.79        72

    accuracy                           0.85       200
   macro avg       0.85      0.84      0.84       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:33:03,348 - INFO - test loss 0.011507094171455009
2023-06-22 15:33:03,348 - INFO - test acc 0.8549999594688416
2023-06-22 15:33:05,616 - INFO - Distilling data from client: Client30
2023-06-22 15:33:05,617 - INFO - train loss: 0.0007806728014316336
2023-06-22 15:33:05,618 - INFO - train acc: 1.0
2023-06-22 15:33:05,661 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.88      0.89       128
           7       0.79      0.81      0.80        72

    accuracy                           0.85       200
   macro avg       0.84      0.84      0.84       200
weighted avg       0.86      0.85      0.86       200

2023-06-22 15:33:05,664 - INFO - test loss 0.010362016098003253
2023-06-22 15:33:05,665 - INFO - test acc 0.8549999594688416
2023-06-22 15:33:08,043 - INFO - Distilling data from client: Client30
2023-06-22 15:33:08,043 - INFO - train loss: 0.0007970921297190482
2023-06-22 15:33:08,043 - INFO - train acc: 1.0
2023-06-22 15:33:08,091 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.92      0.89       128
           7       0.84      0.75      0.79        72

    accuracy                           0.86       200
   macro avg       0.86      0.84      0.84       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:33:08,095 - INFO - test loss 0.010403668624513519
2023-06-22 15:33:08,096 - INFO - test acc 0.85999995470047
2023-06-22 15:33:10,417 - INFO - Distilling data from client: Client30
2023-06-22 15:33:10,418 - INFO - train loss: 0.0007979276522860009
2023-06-22 15:33:10,418 - INFO - train acc: 1.0
2023-06-22 15:33:10,475 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.91      0.90       128
           7       0.83      0.82      0.83        72

    accuracy                           0.88       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.87      0.88      0.87       200

2023-06-22 15:33:10,477 - INFO - test loss 0.010411419419319782
2023-06-22 15:33:10,478 - INFO - test acc 0.875
2023-06-22 15:33:10,487 - WARNING - Finished tracing + transforming jit(gather) in 0.0009872913360595703 sec
2023-06-22 15:33:10,488 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[348,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:33:10,493 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.003814220428466797 sec
2023-06-22 15:33:10,494 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:33:10,540 - WARNING - Finished XLA compilation of jit(gather) in 0.04469633102416992 sec
2023-06-22 15:33:10,578 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:33:10,589 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:33:10,600 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:33:10,611 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:33:10,625 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:33:10,637 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:33:11,065 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client30//synthetic.png
2023-06-22 15:33:11,087 - INFO - c: 8.0 and total_data_in_this_class: 261
2023-06-22 15:33:11,087 - INFO - c: 9.0 and total_data_in_this_class: 538
2023-06-22 15:33:11,087 - INFO - c: 8.0 and total_data_in_this_class: 72
2023-06-22 15:33:11,087 - INFO - c: 9.0 and total_data_in_this_class: 128
2023-06-22 15:33:12,247 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 1.1073076725006104 sec
2023-06-22 15:33:12,323 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07429170608520508 sec
2023-06-22 15:33:12,331 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 1.193446397781372 sec
2023-06-22 15:33:12,334 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:33:12,389 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.054752349853515625 sec
2023-06-22 15:33:12,390 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:33:12,572 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1820068359375 sec
2023-06-22 15:33:12,616 - INFO - initial test loss: 0.01617797664615259
2023-06-22 15:33:12,616 - INFO - initial test acc: 0.824999988079071
2023-06-22 15:33:12,635 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012743949890136719 sec
2023-06-22 15:33:12,830 - WARNING - Finished tracing + transforming update_fn for pjit in 0.20883393287658691 sec
2023-06-22 15:33:12,835 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[348,10]), ShapedArray(float32[348,10]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:33:12,940 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.1044001579284668 sec
2023-06-22 15:33:12,940 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:33:13,345 - WARNING - Finished XLA compilation of jit(update_fn) in 0.40465569496154785 sec
2023-06-22 15:33:15,505 - INFO - Distilling data from client: Client31
2023-06-22 15:33:15,506 - INFO - train loss: 0.00385115895028734
2023-06-22 15:33:15,507 - INFO - train acc: 0.9856321811676025
2023-06-22 15:33:15,622 - INFO - report:               precision    recall  f1-score   support

           8       0.72      0.75      0.73        72
           9       0.86      0.84      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:33:15,625 - INFO - test loss 0.014396134654543695
2023-06-22 15:33:15,626 - INFO - test acc 0.8050000071525574
2023-06-22 15:33:17,826 - INFO - Distilling data from client: Client31
2023-06-22 15:33:17,826 - INFO - train loss: 0.0026328467515050777
2023-06-22 15:33:17,826 - INFO - train acc: 0.9971264600753784
2023-06-22 15:33:17,933 - INFO - report:               precision    recall  f1-score   support

           8       0.76      0.74      0.75        72
           9       0.85      0.87      0.86       128

    accuracy                           0.82       200
   macro avg       0.81      0.80      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-06-22 15:33:17,940 - INFO - test loss 0.013644742219938343
2023-06-22 15:33:17,941 - INFO - test acc 0.8199999928474426
2023-06-22 15:33:20,206 - INFO - Distilling data from client: Client31
2023-06-22 15:33:20,207 - INFO - train loss: 0.0023437612875146776
2023-06-22 15:33:20,208 - INFO - train acc: 1.0
2023-06-22 15:33:20,287 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.71      0.73        72
           9       0.84      0.87      0.85       128

    accuracy                           0.81       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:33:20,288 - INFO - test loss 0.014100645386547997
2023-06-22 15:33:20,288 - INFO - test acc 0.8100000023841858
2023-06-22 15:33:22,554 - INFO - Distilling data from client: Client31
2023-06-22 15:33:22,554 - INFO - train loss: 0.002310464204387676
2023-06-22 15:33:22,555 - INFO - train acc: 1.0
2023-06-22 15:33:22,677 - INFO - report:               precision    recall  f1-score   support

           8       0.77      0.78      0.77        72
           9       0.87      0.87      0.87       128

    accuracy                           0.83       200
   macro avg       0.82      0.82      0.82       200
weighted avg       0.84      0.83      0.84       200

2023-06-22 15:33:22,681 - INFO - test loss 0.013080481312883412
2023-06-22 15:33:22,682 - INFO - test acc 0.8349999785423279
2023-06-22 15:33:24,830 - INFO - Distilling data from client: Client31
2023-06-22 15:33:24,830 - INFO - train loss: 0.0023278397578130098
2023-06-22 15:33:24,830 - INFO - train acc: 0.9942528605461121
2023-06-22 15:33:24,931 - INFO - report:               precision    recall  f1-score   support

           8       0.83      0.75      0.79        72
           9       0.87      0.91      0.89       128

    accuracy                           0.85       200
   macro avg       0.85      0.83      0.84       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:33:24,932 - INFO - test loss 0.012613051626565877
2023-06-22 15:33:24,932 - INFO - test acc 0.8549999594688416
2023-06-22 15:33:27,176 - INFO - Distilling data from client: Client31
2023-06-22 15:33:27,176 - INFO - train loss: 0.0022572788844098875
2023-06-22 15:33:27,177 - INFO - train acc: 0.9971264600753784
2023-06-22 15:33:27,222 - INFO - report:               precision    recall  f1-score   support

           8       0.76      0.69      0.72        72
           9       0.84      0.88      0.85       128

    accuracy                           0.81       200
   macro avg       0.80      0.78      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:33:27,222 - INFO - test loss 0.014439496231208774
2023-06-22 15:33:27,223 - INFO - test acc 0.8100000023841858
2023-06-22 15:33:29,590 - INFO - Distilling data from client: Client31
2023-06-22 15:33:29,590 - INFO - train loss: 0.0019695752731001756
2023-06-22 15:33:29,590 - INFO - train acc: 1.0
2023-06-22 15:33:29,637 - INFO - report:               precision    recall  f1-score   support

           8       0.73      0.72      0.73        72
           9       0.84      0.85      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.80      0.81      0.80       200

2023-06-22 15:33:29,638 - INFO - test loss 0.01392073939527507
2023-06-22 15:33:29,638 - INFO - test acc 0.8050000071525574
2023-06-22 15:33:31,879 - INFO - Distilling data from client: Client31
2023-06-22 15:33:31,880 - INFO - train loss: 0.001454790817811199
2023-06-22 15:33:31,880 - INFO - train acc: 1.0
2023-06-22 15:33:31,918 - INFO - report:               precision    recall  f1-score   support

           8       0.80      0.74      0.77        72
           9       0.86      0.90      0.88       128

    accuracy                           0.84       200
   macro avg       0.83      0.82      0.82       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:33:31,919 - INFO - test loss 0.013854015421510383
2023-06-22 15:33:31,919 - INFO - test acc 0.8399999737739563
2023-06-22 15:33:34,153 - INFO - Distilling data from client: Client31
2023-06-22 15:33:34,154 - INFO - train loss: 0.0017450841960116772
2023-06-22 15:33:34,154 - INFO - train acc: 1.0
2023-06-22 15:33:34,199 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.79      0.77        72
           9       0.88      0.85      0.87       128

    accuracy                           0.83       200
   macro avg       0.81      0.82      0.82       200
weighted avg       0.83      0.83      0.83       200

2023-06-22 15:33:34,200 - INFO - test loss 0.013546381776340983
2023-06-22 15:33:34,200 - INFO - test acc 0.8299999833106995
2023-06-22 15:33:36,633 - INFO - Distilling data from client: Client31
2023-06-22 15:33:36,633 - INFO - train loss: 0.001756376840694006
2023-06-22 15:33:36,634 - INFO - train acc: 1.0
2023-06-22 15:33:36,690 - INFO - report:               precision    recall  f1-score   support

           8       0.73      0.72      0.73        72
           9       0.84      0.85      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.80      0.81      0.80       200

2023-06-22 15:33:36,696 - INFO - test loss 0.014179764720994635
2023-06-22 15:33:36,697 - INFO - test acc 0.8050000071525574
2023-06-22 15:33:38,951 - INFO - Distilling data from client: Client31
2023-06-22 15:33:38,952 - INFO - train loss: 0.0020376186386985126
2023-06-22 15:33:38,953 - INFO - train acc: 0.9942528605461121
2023-06-22 15:33:38,995 - INFO - report:               precision    recall  f1-score   support

           8       0.71      0.72      0.72        72
           9       0.84      0.84      0.84       128

    accuracy                           0.80       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:33:38,996 - INFO - test loss 0.013962336481192324
2023-06-22 15:33:38,997 - INFO - test acc 0.7949999570846558
2023-06-22 15:33:41,262 - INFO - Distilling data from client: Client31
2023-06-22 15:33:41,263 - INFO - train loss: 0.0017496828501735477
2023-06-22 15:33:41,263 - INFO - train acc: 1.0
2023-06-22 15:33:41,308 - INFO - report:               precision    recall  f1-score   support

           8       0.78      0.78      0.78        72
           9       0.88      0.88      0.88       128

    accuracy                           0.84       200
   macro avg       0.83      0.83      0.83       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:33:41,309 - INFO - test loss 0.013336333977632247
2023-06-22 15:33:41,309 - INFO - test acc 0.8399999737739563
2023-06-22 15:33:43,545 - INFO - Distilling data from client: Client31
2023-06-22 15:33:43,545 - INFO - train loss: 0.0019087685559380587
2023-06-22 15:33:43,547 - INFO - train acc: 0.9971264600753784
2023-06-22 15:33:43,596 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.74      0.74        72
           9       0.85      0.86      0.86       128

    accuracy                           0.81       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:33:43,597 - INFO - test loss 0.014049969173755998
2023-06-22 15:33:43,597 - INFO - test acc 0.8149999976158142
2023-06-22 15:33:45,852 - INFO - Distilling data from client: Client31
2023-06-22 15:33:45,852 - INFO - train loss: 0.0018383359636559742
2023-06-22 15:33:45,853 - INFO - train acc: 0.9971264600753784
2023-06-22 15:33:45,899 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.75      0.75        72
           9       0.86      0.86      0.86       128

    accuracy                           0.82       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-06-22 15:33:45,900 - INFO - test loss 0.014222444984295106
2023-06-22 15:33:45,901 - INFO - test acc 0.8199999928474426
2023-06-22 15:33:48,186 - INFO - Distilling data from client: Client31
2023-06-22 15:33:48,186 - INFO - train loss: 0.0016237073210679336
2023-06-22 15:33:48,187 - INFO - train acc: 1.0
2023-06-22 15:33:48,237 - INFO - report:               precision    recall  f1-score   support

           8       0.79      0.78      0.78        72
           9       0.88      0.88      0.88       128

    accuracy                           0.84       200
   macro avg       0.83      0.83      0.83       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:33:48,237 - INFO - test loss 0.013304912126920443
2023-06-22 15:33:48,238 - INFO - test acc 0.8449999690055847
2023-06-22 15:33:50,606 - INFO - Distilling data from client: Client31
2023-06-22 15:33:50,606 - INFO - train loss: 0.0017157506288305376
2023-06-22 15:33:50,607 - INFO - train acc: 1.0
2023-06-22 15:33:50,665 - INFO - report:               precision    recall  f1-score   support

           8       0.74      0.78      0.76        72
           9       0.87      0.84      0.86       128

    accuracy                           0.82       200
   macro avg       0.80      0.81      0.81       200
weighted avg       0.82      0.82      0.82       200

2023-06-22 15:33:50,668 - INFO - test loss 0.013143162540667741
2023-06-22 15:33:50,669 - INFO - test acc 0.8199999928474426
2023-06-22 15:33:52,942 - INFO - Distilling data from client: Client31
2023-06-22 15:33:52,943 - INFO - train loss: 0.001724438674104255
2023-06-22 15:33:52,943 - INFO - train acc: 1.0
2023-06-22 15:33:52,992 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.71      0.73        72
           9       0.84      0.87      0.85       128

    accuracy                           0.81       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:33:52,993 - INFO - test loss 0.014489480069061825
2023-06-22 15:33:52,993 - INFO - test acc 0.8100000023841858
2023-06-22 15:33:55,204 - INFO - Distilling data from client: Client31
2023-06-22 15:33:55,205 - INFO - train loss: 0.0015405932122851541
2023-06-22 15:33:55,205 - INFO - train acc: 0.9971264600753784
2023-06-22 15:33:55,274 - INFO - report:               precision    recall  f1-score   support

           8       0.72      0.76      0.74        72
           9       0.86      0.84      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.80      0.80       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:33:55,274 - INFO - test loss 0.012701577424915378
2023-06-22 15:33:55,274 - INFO - test acc 0.8100000023841858
2023-06-22 15:33:57,395 - INFO - Distilling data from client: Client31
2023-06-22 15:33:57,395 - INFO - train loss: 0.0017262119092417468
2023-06-22 15:33:57,396 - INFO - train acc: 0.9942528605461121
2023-06-22 15:33:57,457 - INFO - report:               precision    recall  f1-score   support

           8       0.77      0.78      0.77        72
           9       0.87      0.87      0.87       128

    accuracy                           0.83       200
   macro avg       0.82      0.82      0.82       200
weighted avg       0.84      0.83      0.84       200

2023-06-22 15:33:57,459 - INFO - test loss 0.012876003366401257
2023-06-22 15:33:57,460 - INFO - test acc 0.8349999785423279
2023-06-22 15:33:59,710 - INFO - Distilling data from client: Client31
2023-06-22 15:33:59,711 - INFO - train loss: 0.0017557750152502431
2023-06-22 15:33:59,711 - INFO - train acc: 0.9971264600753784
2023-06-22 15:33:59,779 - INFO - report:               precision    recall  f1-score   support

           8       0.74      0.72      0.73        72
           9       0.85      0.86      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:33:59,783 - INFO - test loss 0.01454989921325935
2023-06-22 15:33:59,785 - INFO - test acc 0.8100000023841858
2023-06-22 15:34:01,918 - INFO - Distilling data from client: Client31
2023-06-22 15:34:01,919 - INFO - train loss: 0.0013922093465965181
2023-06-22 15:34:01,919 - INFO - train acc: 1.0
2023-06-22 15:34:01,958 - INFO - report:               precision    recall  f1-score   support

           8       0.77      0.74      0.75        72
           9       0.85      0.88      0.86       128

    accuracy                           0.82       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.82      0.82      0.82       200

2023-06-22 15:34:01,958 - INFO - test loss 0.013158235437617556
2023-06-22 15:34:01,958 - INFO - test acc 0.824999988079071
2023-06-22 15:34:04,307 - INFO - Distilling data from client: Client31
2023-06-22 15:34:04,307 - INFO - train loss: 0.0017689853888363445
2023-06-22 15:34:04,307 - INFO - train acc: 0.9971264600753784
2023-06-22 15:34:04,351 - INFO - report:               precision    recall  f1-score   support

           8       0.76      0.76      0.76        72
           9       0.87      0.87      0.87       128

    accuracy                           0.83       200
   macro avg       0.82      0.82      0.82       200
weighted avg       0.83      0.83      0.83       200

2023-06-22 15:34:04,352 - INFO - test loss 0.013176067515118213
2023-06-22 15:34:04,352 - INFO - test acc 0.8299999833106995
2023-06-22 15:34:06,655 - INFO - Distilling data from client: Client31
2023-06-22 15:34:06,656 - INFO - train loss: 0.0018949019096606097
2023-06-22 15:34:06,656 - INFO - train acc: 1.0
2023-06-22 15:34:06,699 - INFO - report:               precision    recall  f1-score   support

           8       0.74      0.72      0.73        72
           9       0.85      0.86      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:34:06,700 - INFO - test loss 0.01471018823955125
2023-06-22 15:34:06,700 - INFO - test acc 0.8100000023841858
2023-06-22 15:34:09,019 - INFO - Distilling data from client: Client31
2023-06-22 15:34:09,019 - INFO - train loss: 0.0014203627187832201
2023-06-22 15:34:09,020 - INFO - train acc: 1.0
2023-06-22 15:34:09,074 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.76      0.76        72
           9       0.87      0.86      0.86       128

    accuracy                           0.82       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.83      0.82      0.83       200

2023-06-22 15:34:09,079 - INFO - test loss 0.01409264118792824
2023-06-22 15:34:09,080 - INFO - test acc 0.824999988079071
2023-06-22 15:34:11,589 - INFO - Distilling data from client: Client31
2023-06-22 15:34:11,589 - INFO - train loss: 0.0013775365715558143
2023-06-22 15:34:11,590 - INFO - train acc: 1.0
2023-06-22 15:34:11,646 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.78      0.76        72
           9       0.87      0.85      0.86       128

    accuracy                           0.82       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.83      0.82      0.83       200

2023-06-22 15:34:11,649 - INFO - test loss 0.014170146395898689
2023-06-22 15:34:11,650 - INFO - test acc 0.824999988079071
2023-06-22 15:34:11,683 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:34:11,702 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:34:11,722 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:34:11,743 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:34:11,757 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:34:11,771 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:34:12,222 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client31//synthetic.png
2023-06-22 15:34:12,244 - INFO - c: 0.0 and total_data_in_this_class: 255
2023-06-22 15:34:12,244 - INFO - c: 2.0 and total_data_in_this_class: 273
2023-06-22 15:34:12,244 - INFO - c: 3.0 and total_data_in_this_class: 271
2023-06-22 15:34:12,245 - INFO - c: 0.0 and total_data_in_this_class: 78
2023-06-22 15:34:12,245 - INFO - c: 2.0 and total_data_in_this_class: 60
2023-06-22 15:34:12,245 - INFO - c: 3.0 and total_data_in_this_class: 62
2023-06-22 15:34:12,298 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005474090576171875 sec
2023-06-22 15:34:12,298 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:34:12,301 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0025320053100585938 sec
2023-06-22 15:34:12,301 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:34:12,319 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.016869306564331055 sec
2023-06-22 15:34:12,323 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003826618194580078 sec
2023-06-22 15:34:12,324 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:34:12,325 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0016942024230957031 sec
2023-06-22 15:34:12,326 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:34:12,339 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.012914657592773438 sec
2023-06-22 15:34:12,344 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022149085998535156 sec
2023-06-22 15:34:12,346 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001990795135498047 sec
2023-06-22 15:34:12,347 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005373954772949219 sec
2023-06-22 15:34:12,350 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0012111663818359375 sec
2023-06-22 15:34:12,351 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021314620971679688 sec
2023-06-22 15:34:12,352 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00046324729919433594 sec
2023-06-22 15:34:12,353 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004525184631347656 sec
2023-06-22 15:34:12,354 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003139972686767578 sec
2023-06-22 15:34:12,355 - WARNING - Finished tracing + transforming fn for pjit in 0.0004973411560058594 sec
2023-06-22 15:34:12,357 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0006232261657714844 sec
2023-06-22 15:34:12,358 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021004676818847656 sec
2023-06-22 15:34:12,360 - WARNING - Finished tracing + transforming fn for pjit in 0.0004265308380126953 sec
2023-06-22 15:34:12,361 - WARNING - Finished tracing + transforming fn for pjit in 0.0006034374237060547 sec
2023-06-22 15:34:12,362 - WARNING - Finished tracing + transforming fn for pjit in 0.0003902912139892578 sec
2023-06-22 15:34:12,363 - WARNING - Finished tracing + transforming fn for pjit in 0.0004658699035644531 sec
2023-06-22 15:34:12,366 - WARNING - Finished tracing + transforming fn for pjit in 0.0003781318664550781 sec
2023-06-22 15:34:12,369 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00041484832763671875 sec
2023-06-22 15:34:12,370 - WARNING - Finished tracing + transforming fn for pjit in 0.0003802776336669922 sec
2023-06-22 15:34:12,371 - WARNING - Finished tracing + transforming fn for pjit in 0.0004367828369140625 sec
2023-06-22 15:34:12,377 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006215572357177734 sec
2023-06-22 15:34:12,378 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016360282897949219 sec
2023-06-22 15:34:12,379 - WARNING - Finished tracing + transforming fn for pjit in 0.0004024505615234375 sec
2023-06-22 15:34:12,381 - WARNING - Finished tracing + transforming fn for pjit in 0.0003972053527832031 sec
2023-06-22 15:34:12,382 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039124488830566406 sec
2023-06-22 15:34:12,384 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006201267242431641 sec
2023-06-22 15:34:12,385 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002999305725097656 sec
2023-06-22 15:34:12,386 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046062469482421875 sec
2023-06-22 15:34:12,387 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003981590270996094 sec
2023-06-22 15:34:12,388 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004057884216308594 sec
2023-06-22 15:34:12,390 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00046515464782714844 sec
2023-06-22 15:34:12,390 - WARNING - Finished tracing + transforming _where for pjit in 0.0014584064483642578 sec
2023-06-22 15:34:12,392 - WARNING - Finished tracing + transforming fn for pjit in 0.0004286766052246094 sec
2023-06-22 15:34:12,393 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0008273124694824219 sec
2023-06-22 15:34:12,395 - WARNING - Finished tracing + transforming fn for pjit in 0.0003654956817626953 sec
2023-06-22 15:34:12,396 - WARNING - Finished tracing + transforming fn for pjit in 0.00039196014404296875 sec
2023-06-22 15:34:12,397 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004093647003173828 sec
2023-06-22 15:34:12,398 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004596710205078125 sec
2023-06-22 15:34:12,399 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003190040588378906 sec
2023-06-22 15:34:12,401 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004658699035644531 sec
2023-06-22 15:34:12,402 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038313865661621094 sec
2023-06-22 15:34:12,403 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041031837463378906 sec
2023-06-22 15:34:12,405 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004913806915283203 sec
2023-06-22 15:34:12,405 - WARNING - Finished tracing + transforming _where for pjit in 0.001535654067993164 sec
2023-06-22 15:34:12,407 - WARNING - Finished tracing + transforming fn for pjit in 0.0004723072052001953 sec
2023-06-22 15:34:12,408 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004627704620361328 sec
2023-06-22 15:34:12,410 - WARNING - Finished tracing + transforming fn for pjit in 0.0003948211669921875 sec
2023-06-22 15:34:12,417 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046515464782714844 sec
2023-06-22 15:34:12,418 - WARNING - Finished tracing + transforming fn for pjit in 0.00046324729919433594 sec
2023-06-22 15:34:12,419 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00049591064453125 sec
2023-06-22 15:34:12,421 - WARNING - Finished tracing + transforming fn for pjit in 0.0005173683166503906 sec
2023-06-22 15:34:12,427 - WARNING - Finished tracing + transforming fn for pjit in 0.00037097930908203125 sec
2023-06-22 15:34:12,430 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00029349327087402344 sec
2023-06-22 15:34:12,432 - WARNING - Finished tracing + transforming fn for pjit in 0.0005528926849365234 sec
2023-06-22 15:34:12,433 - WARNING - Finished tracing + transforming fn for pjit in 0.00038242340087890625 sec
2023-06-22 15:34:12,464 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.12070894241333008 sec
2023-06-22 15:34:12,470 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022602081298828125 sec
2023-06-22 15:34:12,471 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001990795135498047 sec
2023-06-22 15:34:12,472 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00048542022705078125 sec
2023-06-22 15:34:12,475 - WARNING - Finished tracing + transforming fn for pjit in 0.0003979206085205078 sec
2023-06-22 15:34:12,477 - WARNING - Finished tracing + transforming fn for pjit in 0.00045299530029296875 sec
2023-06-22 15:34:12,479 - WARNING - Finished tracing + transforming fn for pjit in 0.0004050731658935547 sec
2023-06-22 15:34:12,489 - WARNING - Finished tracing + transforming fn for pjit in 0.0003941059112548828 sec
2023-06-22 15:34:12,491 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038313865661621094 sec
2023-06-22 15:34:12,492 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044655799865722656 sec
2023-06-22 15:34:12,493 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003123283386230469 sec
2023-06-22 15:34:12,495 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006029605865478516 sec
2023-06-22 15:34:12,496 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003771781921386719 sec
2023-06-22 15:34:12,497 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037789344787597656 sec
2023-06-22 15:34:12,499 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005061626434326172 sec
2023-06-22 15:34:12,499 - WARNING - Finished tracing + transforming _where for pjit in 0.0015769004821777344 sec
2023-06-22 15:34:12,501 - WARNING - Finished tracing + transforming fn for pjit in 0.0004305839538574219 sec
2023-06-22 15:34:12,502 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042176246643066406 sec
2023-06-22 15:34:12,503 - WARNING - Finished tracing + transforming fn for pjit in 0.0003914833068847656 sec
2023-06-22 15:34:12,504 - WARNING - Finished tracing + transforming fn for pjit in 0.0005156993865966797 sec
2023-06-22 15:34:12,524 - WARNING - Finished tracing + transforming fn for pjit in 0.0003819465637207031 sec
2023-06-22 15:34:12,557 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.09155130386352539 sec
2023-06-22 15:34:12,560 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022220611572265625 sec
2023-06-22 15:34:12,561 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00022339820861816406 sec
2023-06-22 15:34:12,562 - WARNING - Finished tracing + transforming _where for pjit in 0.0010731220245361328 sec
2023-06-22 15:34:12,563 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005421638488769531 sec
2023-06-22 15:34:12,564 - WARNING - Finished tracing + transforming trace for pjit in 0.0045626163482666016 sec
2023-06-22 15:34:12,568 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00021004676818847656 sec
2023-06-22 15:34:12,569 - WARNING - Finished tracing + transforming tril for pjit in 0.0011203289031982422 sec
2023-06-22 15:34:12,570 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.00325775146484375 sec
2023-06-22 15:34:12,572 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001823902130126953 sec
2023-06-22 15:34:12,572 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00017833709716796875 sec
2023-06-22 15:34:12,577 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.003076791763305664 sec
2023-06-22 15:34:12,583 - WARNING - Finished tracing + transforming _solve for pjit in 0.016658782958984375 sec
2023-06-22 15:34:12,584 - WARNING - Finished tracing + transforming dot for pjit in 0.0005667209625244141 sec
2023-06-22 15:34:12,589 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.24714112281799316 sec
2023-06-22 15:34:12,592 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:34:12,649 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.057070016860961914 sec
2023-06-22 15:34:12,650 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:34:12,831 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.18063068389892578 sec
2023-06-22 15:34:12,873 - INFO - initial test loss: 0.027587318950061574
2023-06-22 15:34:12,873 - INFO - initial test acc: 0.6200000047683716
2023-06-22 15:34:12,885 - WARNING - Finished tracing + transforming dot for pjit in 0.0008094310760498047 sec
2023-06-22 15:34:12,887 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006835460662841797 sec
2023-06-22 15:34:12,889 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008225440979003906 sec
2023-06-22 15:34:12,890 - WARNING - Finished tracing + transforming _mean for pjit in 0.0022385120391845703 sec
2023-06-22 15:34:12,892 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004374980926513672 sec
2023-06-22 15:34:12,894 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00042319297790527344 sec
2023-06-22 15:34:12,895 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005767345428466797 sec
2023-06-22 15:34:12,897 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00081634521484375 sec
2023-06-22 15:34:12,898 - WARNING - Finished tracing + transforming _mean for pjit in 0.0023872852325439453 sec
2023-06-22 15:34:12,900 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.021815776824951172 sec
2023-06-22 15:34:12,919 - WARNING - Finished tracing + transforming fn for pjit in 0.0005943775177001953 sec
2023-06-22 15:34:12,920 - WARNING - Finished tracing + transforming fn for pjit in 0.0006160736083984375 sec
2023-06-22 15:34:12,922 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0010464191436767578 sec
2023-06-22 15:34:12,925 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006330013275146484 sec
2023-06-22 15:34:12,926 - WARNING - Finished tracing + transforming _where for pjit in 0.0024085044860839844 sec
2023-06-22 15:34:12,945 - WARNING - Finished tracing + transforming fn for pjit in 0.0005991458892822266 sec
2023-06-22 15:34:12,947 - WARNING - Finished tracing + transforming fn for pjit in 0.0006291866302490234 sec
2023-06-22 15:34:12,949 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000762939453125 sec
2023-06-22 15:34:12,951 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0007574558258056641 sec
2023-06-22 15:34:12,952 - WARNING - Finished tracing + transforming _where for pjit in 0.0022165775299072266 sec
2023-06-22 15:34:13,014 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003886222839355469 sec
2023-06-22 15:34:13,099 - WARNING - Finished tracing + transforming fn for pjit in 0.0005025863647460938 sec
2023-06-22 15:34:13,100 - WARNING - Finished tracing + transforming fn for pjit in 0.0004112720489501953 sec
2023-06-22 15:34:13,101 - WARNING - Finished tracing + transforming square for pjit in 0.0003287792205810547 sec
2023-06-22 15:34:13,105 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003669261932373047 sec
2023-06-22 15:34:13,108 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004870891571044922 sec
2023-06-22 15:34:13,109 - WARNING - Finished tracing + transforming fn for pjit in 0.0004756450653076172 sec
2023-06-22 15:34:13,110 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00040721893310546875 sec
2023-06-22 15:34:13,111 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004181861877441406 sec
2023-06-22 15:34:13,112 - WARNING - Finished tracing + transforming fn for pjit in 0.00046896934509277344 sec
2023-06-22 15:34:13,114 - WARNING - Finished tracing + transforming fn for pjit in 0.00042366981506347656 sec
2023-06-22 15:34:13,114 - WARNING - Finished tracing + transforming square for pjit in 0.0003101825714111328 sec
2023-06-22 15:34:13,118 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003666877746582031 sec
2023-06-22 15:34:13,121 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002951622009277344 sec
2023-06-22 15:34:13,122 - WARNING - Finished tracing + transforming fn for pjit in 0.00047326087951660156 sec
2023-06-22 15:34:13,123 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003676414489746094 sec
2023-06-22 15:34:13,124 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041103363037109375 sec
2023-06-22 15:34:13,125 - WARNING - Finished tracing + transforming update_fn for pjit in 0.24837851524353027 sec
2023-06-22 15:34:13,131 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[510,10]), ShapedArray(float32[510,10]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:34:13,237 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.105804443359375 sec
2023-06-22 15:34:13,238 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:34:13,681 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4433712959289551 sec
2023-06-22 15:34:16,496 - INFO - Distilling data from client: Client32
2023-06-22 15:34:16,496 - INFO - train loss: 0.0032230524262755137
2023-06-22 15:34:16,496 - INFO - train acc: 0.988235354423523
2023-06-22 15:34:16,658 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.74      0.75        78
           2       0.61      0.68      0.65        60
           3       0.67      0.61      0.64        62

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:34:16,658 - INFO - test loss 0.02463763804045076
2023-06-22 15:34:16,658 - INFO - test acc 0.6850000023841858
2023-06-22 15:34:19,437 - INFO - Distilling data from client: Client32
2023-06-22 15:34:19,438 - INFO - train loss: 0.0019212738775336734
2023-06-22 15:34:19,438 - INFO - train acc: 0.9980392456054688
2023-06-22 15:34:19,482 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.73      0.75        78
           2       0.55      0.63      0.59        60
           3       0.62      0.56      0.59        62

    accuracy                           0.65       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.66      0.65      0.65       200

2023-06-22 15:34:19,483 - INFO - test loss 0.02589162458511565
2023-06-22 15:34:19,483 - INFO - test acc 0.6499999761581421
2023-06-22 15:34:22,381 - INFO - Distilling data from client: Client32
2023-06-22 15:34:22,382 - INFO - train loss: 0.0015530079676246745
2023-06-22 15:34:22,382 - INFO - train acc: 1.0
2023-06-22 15:34:22,446 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.68      0.71        78
           2       0.56      0.62      0.59        60
           3       0.59      0.60      0.59        62

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:34:22,447 - INFO - test loss 0.02556258788193119
2023-06-22 15:34:22,447 - INFO - test acc 0.6349999904632568
2023-06-22 15:34:25,343 - INFO - Distilling data from client: Client32
2023-06-22 15:34:25,344 - INFO - train loss: 0.0011056396990824697
2023-06-22 15:34:25,344 - INFO - train acc: 1.0
2023-06-22 15:34:25,391 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.71      0.72        78
           2       0.58      0.63      0.60        60
           3       0.63      0.61      0.62        62

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-06-22 15:34:25,393 - INFO - test loss 0.02635549395433246
2023-06-22 15:34:25,393 - INFO - test acc 0.6549999713897705
2023-06-22 15:34:28,259 - INFO - Distilling data from client: Client32
2023-06-22 15:34:28,259 - INFO - train loss: 0.001105071720173463
2023-06-22 15:34:28,259 - INFO - train acc: 1.0
2023-06-22 15:34:28,309 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.74      0.75        78
           2       0.59      0.65      0.62        60
           3       0.63      0.58      0.61        62

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:34:28,309 - INFO - test loss 0.02608146218046154
2023-06-22 15:34:28,309 - INFO - test acc 0.6649999618530273
2023-06-22 15:34:31,121 - INFO - Distilling data from client: Client32
2023-06-22 15:34:31,121 - INFO - train loss: 0.0009705583663306844
2023-06-22 15:34:31,121 - INFO - train acc: 1.0
2023-06-22 15:34:31,173 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.69      0.71        78
           2       0.57      0.60      0.59        60
           3       0.60      0.61      0.61        62

    accuracy                           0.64       200
   macro avg       0.63      0.64      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:34:31,174 - INFO - test loss 0.02641613506362063
2023-06-22 15:34:31,174 - INFO - test acc 0.6399999856948853
2023-06-22 15:34:34,268 - INFO - Distilling data from client: Client32
2023-06-22 15:34:34,269 - INFO - train loss: 0.0008848403402229649
2023-06-22 15:34:34,269 - INFO - train acc: 1.0
2023-06-22 15:34:34,327 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.73      0.75        78
           2       0.60      0.67      0.63        60
           3       0.63      0.61      0.62        62

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:34:34,328 - INFO - test loss 0.02531818001880638
2023-06-22 15:34:34,328 - INFO - test acc 0.675000011920929
2023-06-22 15:34:37,140 - INFO - Distilling data from client: Client32
2023-06-22 15:34:37,141 - INFO - train loss: 0.0007777000474032678
2023-06-22 15:34:37,143 - INFO - train acc: 1.0
2023-06-22 15:34:37,207 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.69      0.72        78
           2       0.53      0.62      0.57        60
           3       0.59      0.55      0.57        62

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.63      0.62      0.63       200

2023-06-22 15:34:37,208 - INFO - test loss 0.026944317725688432
2023-06-22 15:34:37,209 - INFO - test acc 0.625
2023-06-22 15:34:40,122 - INFO - Distilling data from client: Client32
2023-06-22 15:34:40,122 - INFO - train loss: 0.0009710680785788539
2023-06-22 15:34:40,122 - INFO - train acc: 1.0
2023-06-22 15:34:40,184 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.68      0.71        78
           2       0.54      0.63      0.58        60
           3       0.59      0.55      0.57        62

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.63      0.62      0.63       200

2023-06-22 15:34:40,185 - INFO - test loss 0.02610284805786742
2023-06-22 15:34:40,185 - INFO - test acc 0.625
2023-06-22 15:34:42,993 - INFO - Distilling data from client: Client32
2023-06-22 15:34:42,993 - INFO - train loss: 0.000752736037164126
2023-06-22 15:34:42,993 - INFO - train acc: 1.0
2023-06-22 15:34:43,053 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        78
           2       0.58      0.65      0.61        60
           3       0.65      0.60      0.62        62

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:34:43,053 - INFO - test loss 0.026762721183184467
2023-06-22 15:34:43,053 - INFO - test acc 0.6649999618530273
2023-06-22 15:34:45,980 - INFO - Distilling data from client: Client32
2023-06-22 15:34:45,980 - INFO - train loss: 0.000735471452200748
2023-06-22 15:34:45,981 - INFO - train acc: 1.0
2023-06-22 15:34:46,034 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.72      0.74        78
           2       0.59      0.68      0.64        60
           3       0.65      0.60      0.62        62

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:34:46,036 - INFO - test loss 0.026156913331153983
2023-06-22 15:34:46,036 - INFO - test acc 0.6699999570846558
2023-06-22 15:34:48,830 - INFO - Distilling data from client: Client32
2023-06-22 15:34:48,830 - INFO - train loss: 0.0006927635680936934
2023-06-22 15:34:48,830 - INFO - train acc: 1.0
2023-06-22 15:34:48,879 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.72      0.74        78
           2       0.54      0.58      0.56        60
           3       0.59      0.58      0.59        62

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:34:48,881 - INFO - test loss 0.026577107664331846
2023-06-22 15:34:48,881 - INFO - test acc 0.6349999904632568
2023-06-22 15:34:51,732 - INFO - Distilling data from client: Client32
2023-06-22 15:34:51,732 - INFO - train loss: 0.000611709703801966
2023-06-22 15:34:51,732 - INFO - train acc: 1.0
2023-06-22 15:34:51,786 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.69      0.72        78
           2       0.54      0.62      0.58        60
           3       0.60      0.58      0.59        62

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:34:51,787 - INFO - test loss 0.026148312412201755
2023-06-22 15:34:51,787 - INFO - test acc 0.6349999904632568
2023-06-22 15:34:54,620 - INFO - Distilling data from client: Client32
2023-06-22 15:34:54,620 - INFO - train loss: 0.0005356804248908573
2023-06-22 15:34:54,620 - INFO - train acc: 1.0
2023-06-22 15:34:54,672 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.71      0.72        78
           2       0.54      0.60      0.57        60
           3       0.58      0.55      0.56        62

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.63      0.62      0.63       200

2023-06-22 15:34:54,672 - INFO - test loss 0.02642262775368393
2023-06-22 15:34:54,673 - INFO - test acc 0.625
2023-06-22 15:34:57,593 - INFO - Distilling data from client: Client32
2023-06-22 15:34:57,593 - INFO - train loss: 0.0005790122575111661
2023-06-22 15:34:57,593 - INFO - train acc: 1.0
2023-06-22 15:34:57,654 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        78
           2       0.53      0.60      0.56        60
           3       0.65      0.56      0.60        62

    accuracy                           0.64       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:34:57,654 - INFO - test loss 0.026394326370938255
2023-06-22 15:34:57,654 - INFO - test acc 0.6399999856948853
2023-06-22 15:35:00,624 - INFO - Distilling data from client: Client32
2023-06-22 15:35:00,625 - INFO - train loss: 0.0006451763818256144
2023-06-22 15:35:00,625 - INFO - train acc: 1.0
2023-06-22 15:35:00,679 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.68      0.71        78
           2       0.55      0.67      0.60        60
           3       0.61      0.55      0.58        62

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:35:00,679 - INFO - test loss 0.026658068446436155
2023-06-22 15:35:00,679 - INFO - test acc 0.6349999904632568
2023-06-22 15:35:03,546 - INFO - Distilling data from client: Client32
2023-06-22 15:35:03,547 - INFO - train loss: 0.0005110910119206161
2023-06-22 15:35:03,547 - INFO - train acc: 1.0
2023-06-22 15:35:03,604 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.71      0.73        78
           2       0.54      0.62      0.58        60
           3       0.60      0.58      0.59        62

    accuracy                           0.64       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.65      0.64      0.64       200

2023-06-22 15:35:03,605 - INFO - test loss 0.02665857996769496
2023-06-22 15:35:03,605 - INFO - test acc 0.6399999856948853
2023-06-22 15:35:06,437 - INFO - Distilling data from client: Client32
2023-06-22 15:35:06,438 - INFO - train loss: 0.0005372183601977672
2023-06-22 15:35:06,439 - INFO - train acc: 1.0
2023-06-22 15:35:06,487 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.71      0.72        78
           2       0.57      0.65      0.61        60
           3       0.64      0.60      0.62        62

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-06-22 15:35:06,487 - INFO - test loss 0.02648702554925364
2023-06-22 15:35:06,488 - INFO - test acc 0.6549999713897705
2023-06-22 15:35:09,337 - INFO - Distilling data from client: Client32
2023-06-22 15:35:09,338 - INFO - train loss: 0.0006076516954071434
2023-06-22 15:35:09,338 - INFO - train acc: 1.0
2023-06-22 15:35:09,400 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.68      0.71        78
           2       0.57      0.65      0.61        60
           3       0.61      0.60      0.60        62

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:35:09,401 - INFO - test loss 0.026638219449459363
2023-06-22 15:35:09,401 - INFO - test acc 0.6449999809265137
2023-06-22 15:35:12,339 - INFO - Distilling data from client: Client32
2023-06-22 15:35:12,340 - INFO - train loss: 0.0005662385522538607
2023-06-22 15:35:12,340 - INFO - train acc: 1.0
2023-06-22 15:35:12,397 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.72      0.73        78
           2       0.55      0.65      0.60        60
           3       0.63      0.55      0.59        62

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:35:12,398 - INFO - test loss 0.026524604821456143
2023-06-22 15:35:12,398 - INFO - test acc 0.6449999809265137
2023-06-22 15:35:15,379 - INFO - Distilling data from client: Client32
2023-06-22 15:35:15,379 - INFO - train loss: 0.0004647215778259563
2023-06-22 15:35:15,380 - INFO - train acc: 1.0
2023-06-22 15:35:15,426 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.72      0.74        78
           2       0.57      0.63      0.60        60
           3       0.63      0.60      0.61        62

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-06-22 15:35:15,427 - INFO - test loss 0.02690515815258459
2023-06-22 15:35:15,427 - INFO - test acc 0.6549999713897705
2023-06-22 15:35:18,394 - INFO - Distilling data from client: Client32
2023-06-22 15:35:18,394 - INFO - train loss: 0.0005903384986242229
2023-06-22 15:35:18,395 - INFO - train acc: 1.0
2023-06-22 15:35:18,451 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.69      0.72        78
           2       0.53      0.62      0.57        60
           3       0.60      0.56      0.58        62

    accuracy                           0.63       200
   macro avg       0.63      0.62      0.62       200
weighted avg       0.64      0.63      0.63       200

2023-06-22 15:35:18,452 - INFO - test loss 0.027018285471704324
2023-06-22 15:35:18,453 - INFO - test acc 0.6299999952316284
2023-06-22 15:35:21,317 - INFO - Distilling data from client: Client32
2023-06-22 15:35:21,318 - INFO - train loss: 0.0005399574093246949
2023-06-22 15:35:21,319 - INFO - train acc: 1.0
2023-06-22 15:35:21,378 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.71      0.72        78
           2       0.57      0.65      0.60        60
           3       0.61      0.56      0.59        62

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:35:21,384 - INFO - test loss 0.026162794614457426
2023-06-22 15:35:21,386 - INFO - test acc 0.6449999809265137
2023-06-22 15:35:24,211 - INFO - Distilling data from client: Client32
2023-06-22 15:35:24,212 - INFO - train loss: 0.00037295304695577475
2023-06-22 15:35:24,213 - INFO - train acc: 1.0
2023-06-22 15:35:24,274 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.73      0.75        78
           2       0.58      0.67      0.62        60
           3       0.61      0.56      0.59        62

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-06-22 15:35:24,275 - INFO - test loss 0.026333127351346457
2023-06-22 15:35:24,276 - INFO - test acc 0.6599999666213989
2023-06-22 15:35:27,126 - INFO - Distilling data from client: Client32
2023-06-22 15:35:27,126 - INFO - train loss: 0.000380823408088532
2023-06-22 15:35:27,127 - INFO - train acc: 1.0
2023-06-22 15:35:27,192 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        78
           2       0.54      0.62      0.58        60
           3       0.62      0.56      0.59        62

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:35:27,193 - INFO - test loss 0.02648602092460255
2023-06-22 15:35:27,194 - INFO - test acc 0.6449999809265137
2023-06-22 15:35:27,202 - WARNING - Finished tracing + transforming jit(gather) in 0.0008258819580078125 sec
2023-06-22 15:35:27,203 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[510,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:35:27,208 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0037860870361328125 sec
2023-06-22 15:35:27,208 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:35:27,231 - WARNING - Finished XLA compilation of jit(gather) in 0.02196502685546875 sec
2023-06-22 15:35:27,254 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:35:27,272 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:35:27,288 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:35:27,300 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:35:27,312 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:35:27,325 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:35:27,339 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:35:27,352 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:35:27,364 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:35:27,969 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client32//synthetic.png
2023-06-22 15:35:27,990 - INFO - c: 3.0 and total_data_in_this_class: 259
2023-06-22 15:35:27,990 - INFO - c: 6.0 and total_data_in_this_class: 279
2023-06-22 15:35:27,990 - INFO - c: 8.0 and total_data_in_this_class: 261
2023-06-22 15:35:27,991 - INFO - c: 3.0 and total_data_in_this_class: 74
2023-06-22 15:35:27,991 - INFO - c: 6.0 and total_data_in_this_class: 54
2023-06-22 15:35:27,991 - INFO - c: 8.0 and total_data_in_this_class: 72
2023-06-22 15:35:28,128 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08219552040100098 sec
2023-06-22 15:35:28,207 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0765998363494873 sec
2023-06-22 15:35:28,215 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.17087507247924805 sec
2023-06-22 15:35:28,218 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:35:28,276 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05700874328613281 sec
2023-06-22 15:35:28,276 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:35:28,457 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.18019533157348633 sec
2023-06-22 15:35:28,503 - INFO - initial test loss: 0.021868363959568243
2023-06-22 15:35:28,504 - INFO - initial test acc: 0.7199999690055847
2023-06-22 15:35:28,526 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.014719724655151367 sec
2023-06-22 15:35:28,764 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2535991668701172 sec
2023-06-22 15:35:28,770 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:35:28,882 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.11227607727050781 sec
2023-06-22 15:35:28,883 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:35:29,373 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4903090000152588 sec
2023-06-22 15:35:32,121 - INFO - Distilling data from client: Client33
2023-06-22 15:35:32,122 - INFO - train loss: 0.0028156150602784927
2023-06-22 15:35:32,122 - INFO - train acc: 0.9961464405059814
2023-06-22 15:35:32,310 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.57      0.64        74
           6       0.61      0.83      0.70        54
           8       0.93      0.89      0.91        72

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.77      0.76      0.75       200

2023-06-22 15:35:32,311 - INFO - test loss 0.019538626907532605
2023-06-22 15:35:32,311 - INFO - test acc 0.7549999952316284
2023-06-22 15:35:35,135 - INFO - Distilling data from client: Client33
2023-06-22 15:35:35,136 - INFO - train loss: 0.0015300804868014109
2023-06-22 15:35:35,137 - INFO - train acc: 0.9961464405059814
2023-06-22 15:35:35,192 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.51      0.61        74
           6       0.57      0.87      0.69        54
           8       0.93      0.88      0.90        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.77      0.74      0.74       200

2023-06-22 15:35:35,192 - INFO - test loss 0.020510627439412964
2023-06-22 15:35:35,192 - INFO - test acc 0.7400000095367432
2023-06-22 15:35:39,026 - INFO - Distilling data from client: Client33
2023-06-22 15:35:39,027 - INFO - train loss: 0.0013627553214204267
2023-06-22 15:35:39,028 - INFO - train acc: 0.9980732202529907
2023-06-22 15:35:39,086 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.46      0.59        74
           6       0.56      0.91      0.70        54
           8       0.90      0.89      0.90        72

    accuracy                           0.73       200
   macro avg       0.76      0.75      0.73       200
weighted avg       0.78      0.73      0.73       200

2023-06-22 15:35:39,087 - INFO - test loss 0.02074870830462916
2023-06-22 15:35:39,087 - INFO - test acc 0.73499995470047
2023-06-22 15:35:41,835 - INFO - Distilling data from client: Client33
2023-06-22 15:35:41,836 - INFO - train loss: 0.0011253728590115686
2023-06-22 15:35:41,836 - INFO - train acc: 1.0
2023-06-22 15:35:41,882 - INFO - report:               precision    recall  f1-score   support

           3       0.78      0.54      0.64        74
           6       0.59      0.87      0.71        54
           8       0.90      0.88      0.89        72

    accuracy                           0.75       200
   macro avg       0.76      0.76      0.74       200
weighted avg       0.77      0.75      0.75       200

2023-06-22 15:35:41,883 - INFO - test loss 0.020780161240267016
2023-06-22 15:35:41,883 - INFO - test acc 0.75
2023-06-22 15:35:44,572 - INFO - Distilling data from client: Client33
2023-06-22 15:35:44,573 - INFO - train loss: 0.0009750359577977767
2023-06-22 15:35:44,573 - INFO - train acc: 1.0
2023-06-22 15:35:44,629 - INFO - report:               precision    recall  f1-score   support

           3       0.79      0.51      0.62        74
           6       0.58      0.89      0.70        54
           8       0.93      0.89      0.91        72

    accuracy                           0.75       200
   macro avg       0.77      0.76      0.74       200
weighted avg       0.78      0.75      0.75       200

2023-06-22 15:35:44,629 - INFO - test loss 0.021251942716618864
2023-06-22 15:35:44,629 - INFO - test acc 0.75
2023-06-22 15:35:47,585 - INFO - Distilling data from client: Client33
2023-06-22 15:35:47,586 - INFO - train loss: 0.0008079834012778126
2023-06-22 15:35:47,586 - INFO - train acc: 1.0
2023-06-22 15:35:47,640 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.49      0.59        74
           6       0.55      0.85      0.67        54
           8       0.91      0.88      0.89        72

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.72       200
weighted avg       0.76      0.72      0.72       200

2023-06-22 15:35:47,642 - INFO - test loss 0.021720463322749434
2023-06-22 15:35:47,643 - INFO - test acc 0.7249999642372131
2023-06-22 15:35:50,624 - INFO - Distilling data from client: Client33
2023-06-22 15:35:50,624 - INFO - train loss: 0.0006478266154271559
2023-06-22 15:35:50,625 - INFO - train acc: 1.0
2023-06-22 15:35:50,677 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.49      0.60        74
           6       0.56      0.85      0.68        54
           8       0.90      0.89      0.90        72

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.72       200
weighted avg       0.76      0.73      0.73       200

2023-06-22 15:35:50,679 - INFO - test loss 0.021876319704840167
2023-06-22 15:35:50,679 - INFO - test acc 0.7299999594688416
2023-06-22 15:35:53,498 - INFO - Distilling data from client: Client33
2023-06-22 15:35:53,498 - INFO - train loss: 0.0006242104613738639
2023-06-22 15:35:53,498 - INFO - train acc: 1.0
2023-06-22 15:35:53,554 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.45      0.57        74
           6       0.56      0.91      0.69        54
           8       0.90      0.89      0.90        72

    accuracy                           0.73       200
   macro avg       0.75      0.75      0.72       200
weighted avg       0.77      0.73      0.72       200

2023-06-22 15:35:53,554 - INFO - test loss 0.02111279010917447
2023-06-22 15:35:53,554 - INFO - test acc 0.7299999594688416
2023-06-22 15:35:56,526 - INFO - Distilling data from client: Client33
2023-06-22 15:35:56,526 - INFO - train loss: 0.0005554912061079316
2023-06-22 15:35:56,526 - INFO - train acc: 1.0
2023-06-22 15:35:56,594 - INFO - report:               precision    recall  f1-score   support

           3       0.78      0.51      0.62        74
           6       0.58      0.85      0.69        54
           8       0.89      0.89      0.89        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.76      0.74      0.74       200

2023-06-22 15:35:56,596 - INFO - test loss 0.02113819195570631
2023-06-22 15:35:56,596 - INFO - test acc 0.7400000095367432
2023-06-22 15:35:59,401 - INFO - Distilling data from client: Client33
2023-06-22 15:35:59,401 - INFO - train loss: 0.0006492078059172913
2023-06-22 15:35:59,402 - INFO - train acc: 1.0
2023-06-22 15:35:59,457 - INFO - report:               precision    recall  f1-score   support

           3       0.71      0.46      0.56        74
           6       0.54      0.81      0.65        54
           8       0.90      0.89      0.90        72

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.70       200
weighted avg       0.73      0.71      0.70       200

2023-06-22 15:35:59,458 - INFO - test loss 0.021512658815698138
2023-06-22 15:35:59,458 - INFO - test acc 0.7099999785423279
2023-06-22 15:36:02,359 - INFO - Distilling data from client: Client33
2023-06-22 15:36:02,359 - INFO - train loss: 0.0005967161923601414
2023-06-22 15:36:02,359 - INFO - train acc: 1.0
2023-06-22 15:36:02,413 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.54      0.63        74
           6       0.58      0.83      0.69        54
           8       0.90      0.89      0.90        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.77      0.74      0.74       200

2023-06-22 15:36:02,413 - INFO - test loss 0.02183491513034106
2023-06-22 15:36:02,413 - INFO - test acc 0.7450000047683716
2023-06-22 15:36:05,322 - INFO - Distilling data from client: Client33
2023-06-22 15:36:05,322 - INFO - train loss: 0.00045968837036365655
2023-06-22 15:36:05,322 - INFO - train acc: 1.0
2023-06-22 15:36:05,375 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.49      0.61        74
           6       0.57      0.87      0.69        54
           8       0.88      0.89      0.88        72

    accuracy                           0.73       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.77      0.73      0.73       200

2023-06-22 15:36:05,376 - INFO - test loss 0.021110554939490447
2023-06-22 15:36:05,376 - INFO - test acc 0.73499995470047
2023-06-22 15:36:08,311 - INFO - Distilling data from client: Client33
2023-06-22 15:36:08,312 - INFO - train loss: 0.0004605005623948648
2023-06-22 15:36:08,312 - INFO - train acc: 1.0
2023-06-22 15:36:08,373 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.53      0.63        74
           6       0.59      0.87      0.70        54
           8       0.90      0.89      0.90        72

    accuracy                           0.75       200
   macro avg       0.76      0.76      0.74       200
weighted avg       0.78      0.75      0.75       200

2023-06-22 15:36:08,375 - INFO - test loss 0.021292183079603816
2023-06-22 15:36:08,375 - INFO - test acc 0.75
2023-06-22 15:36:11,265 - INFO - Distilling data from client: Client33
2023-06-22 15:36:11,266 - INFO - train loss: 0.0004867944827848999
2023-06-22 15:36:11,266 - INFO - train acc: 1.0
2023-06-22 15:36:11,325 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.49      0.60        74
           6       0.57      0.87      0.69        54
           8       0.89      0.88      0.88        72

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.72       200
weighted avg       0.76      0.73      0.72       200

2023-06-22 15:36:11,326 - INFO - test loss 0.02171237557396913
2023-06-22 15:36:11,327 - INFO - test acc 0.7299999594688416
2023-06-22 15:36:14,161 - INFO - Distilling data from client: Client33
2023-06-22 15:36:14,162 - INFO - train loss: 0.00038611077187358117
2023-06-22 15:36:14,162 - INFO - train acc: 1.0
2023-06-22 15:36:14,223 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.50      0.61        74
           6       0.57      0.85      0.69        54
           8       0.88      0.88      0.88        72

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.72       200
weighted avg       0.76      0.73      0.72       200

2023-06-22 15:36:14,224 - INFO - test loss 0.02099697746129608
2023-06-22 15:36:14,225 - INFO - test acc 0.7299999594688416
2023-06-22 15:36:17,194 - INFO - Distilling data from client: Client33
2023-06-22 15:36:17,195 - INFO - train loss: 0.0003445035295365641
2023-06-22 15:36:17,195 - INFO - train acc: 1.0
2023-06-22 15:36:17,385 - INFO - report:               precision    recall  f1-score   support

           3       0.78      0.54      0.64        74
           6       0.62      0.89      0.73        54
           8       0.90      0.89      0.90        72

    accuracy                           0.76       200
   macro avg       0.77      0.77      0.75       200
weighted avg       0.78      0.76      0.76       200

2023-06-22 15:36:17,385 - INFO - test loss 0.02104523673329558
2023-06-22 15:36:17,385 - INFO - test acc 0.7599999904632568
2023-06-22 15:36:20,213 - INFO - Distilling data from client: Client33
2023-06-22 15:36:20,213 - INFO - train loss: 0.0003744502725483146
2023-06-22 15:36:20,213 - INFO - train acc: 1.0
2023-06-22 15:36:20,284 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.50      0.60        74
           6       0.59      0.87      0.70        54
           8       0.89      0.88      0.88        72

    accuracy                           0.73       200
   macro avg       0.74      0.75      0.73       200
weighted avg       0.76      0.73      0.73       200

2023-06-22 15:36:20,285 - INFO - test loss 0.02207707389257549
2023-06-22 15:36:20,285 - INFO - test acc 0.73499995470047
2023-06-22 15:36:23,153 - INFO - Distilling data from client: Client33
2023-06-22 15:36:23,153 - INFO - train loss: 0.00037712457637007954
2023-06-22 15:36:23,153 - INFO - train acc: 1.0
2023-06-22 15:36:23,222 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.51      0.61        74
           6       0.59      0.87      0.71        54
           8       0.90      0.88      0.89        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.76      0.74      0.74       200

2023-06-22 15:36:23,223 - INFO - test loss 0.021781569536323042
2023-06-22 15:36:23,223 - INFO - test acc 0.7400000095367432
2023-06-22 15:36:26,134 - INFO - Distilling data from client: Client33
2023-06-22 15:36:26,134 - INFO - train loss: 0.00042875058257155267
2023-06-22 15:36:26,134 - INFO - train acc: 1.0
2023-06-22 15:36:26,191 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.53      0.63        74
           6       0.60      0.85      0.70        54
           8       0.86      0.89      0.88        72

    accuracy                           0.74       200
   macro avg       0.75      0.76      0.74       200
weighted avg       0.77      0.74      0.74       200

2023-06-22 15:36:26,191 - INFO - test loss 0.021014772552279518
2023-06-22 15:36:26,192 - INFO - test acc 0.7450000047683716
2023-06-22 15:36:29,064 - INFO - Distilling data from client: Client33
2023-06-22 15:36:29,065 - INFO - train loss: 0.0004147253934991959
2023-06-22 15:36:29,066 - INFO - train acc: 1.0
2023-06-22 15:36:29,115 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.51      0.61        74
           6       0.58      0.85      0.69        54
           8       0.89      0.88      0.88        72

    accuracy                           0.73       200
   macro avg       0.74      0.75      0.73       200
weighted avg       0.76      0.73      0.73       200

2023-06-22 15:36:29,118 - INFO - test loss 0.02153265010332265
2023-06-22 15:36:29,119 - INFO - test acc 0.73499995470047
2023-06-22 15:36:31,904 - INFO - Distilling data from client: Client33
2023-06-22 15:36:31,904 - INFO - train loss: 0.00039879180723058176
2023-06-22 15:36:31,904 - INFO - train acc: 1.0
2023-06-22 15:36:31,961 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.50      0.60        74
           6       0.57      0.87      0.69        54
           8       0.90      0.86      0.88        72

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.72       200
weighted avg       0.76      0.73      0.73       200

2023-06-22 15:36:31,962 - INFO - test loss 0.02137824993057511
2023-06-22 15:36:31,963 - INFO - test acc 0.7299999594688416
2023-06-22 15:36:35,109 - INFO - Distilling data from client: Client33
2023-06-22 15:36:35,110 - INFO - train loss: 0.0003289433019417187
2023-06-22 15:36:35,111 - INFO - train acc: 1.0
2023-06-22 15:36:35,177 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.50      0.60        74
           6       0.58      0.87      0.70        54
           8       0.91      0.89      0.90        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.77      0.74      0.74       200

2023-06-22 15:36:35,178 - INFO - test loss 0.021728117849963643
2023-06-22 15:36:35,179 - INFO - test acc 0.7400000095367432
2023-06-22 15:36:38,084 - INFO - Distilling data from client: Client33
2023-06-22 15:36:38,085 - INFO - train loss: 0.00035366726232794675
2023-06-22 15:36:38,085 - INFO - train acc: 1.0
2023-06-22 15:36:38,169 - INFO - report:               precision    recall  f1-score   support

           3       0.78      0.47      0.59        74
           6       0.58      0.89      0.70        54
           8       0.89      0.89      0.89        72

    accuracy                           0.73       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.76      0.73      0.73       200

2023-06-22 15:36:38,170 - INFO - test loss 0.02211922297373038
2023-06-22 15:36:38,171 - INFO - test acc 0.73499995470047
2023-06-22 15:36:41,141 - INFO - Distilling data from client: Client33
2023-06-22 15:36:41,141 - INFO - train loss: 0.00032492198611021826
2023-06-22 15:36:41,141 - INFO - train acc: 1.0
2023-06-22 15:36:41,211 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.46      0.57        74
           6       0.57      0.85      0.68        54
           8       0.86      0.89      0.88        72

    accuracy                           0.72       200
   macro avg       0.73      0.73      0.71       200
weighted avg       0.74      0.72      0.71       200

2023-06-22 15:36:41,212 - INFO - test loss 0.021583312836239415
2023-06-22 15:36:41,212 - INFO - test acc 0.7199999690055847
2023-06-22 15:36:44,454 - INFO - Distilling data from client: Client33
2023-06-22 15:36:44,454 - INFO - train loss: 0.0003227192539180547
2023-06-22 15:36:44,454 - INFO - train acc: 1.0
2023-06-22 15:36:44,506 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.50      0.62        74
           6       0.58      0.87      0.70        54
           8       0.88      0.89      0.88        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.77      0.74      0.73       200

2023-06-22 15:36:44,507 - INFO - test loss 0.022314126134533405
2023-06-22 15:36:44,507 - INFO - test acc 0.7400000095367432
2023-06-22 15:36:44,537 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:36:44,578 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:36:44,597 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:36:44,614 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:36:44,627 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:36:44,642 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:36:44,657 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:36:44,671 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:36:44,686 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:36:45,315 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client33//synthetic.png
2023-06-22 15:36:45,336 - INFO - c: 0.0 and total_data_in_this_class: 267
2023-06-22 15:36:45,336 - INFO - c: 2.0 and total_data_in_this_class: 270
2023-06-22 15:36:45,337 - INFO - c: 4.0 and total_data_in_this_class: 262
2023-06-22 15:36:45,337 - INFO - c: 0.0 and total_data_in_this_class: 66
2023-06-22 15:36:45,337 - INFO - c: 2.0 and total_data_in_this_class: 63
2023-06-22 15:36:45,337 - INFO - c: 4.0 and total_data_in_this_class: 71
2023-06-22 15:36:45,467 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0820457935333252 sec
2023-06-22 15:36:45,545 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07668709754943848 sec
2023-06-22 15:36:45,554 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.17098379135131836 sec
2023-06-22 15:36:45,557 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:36:45,616 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.058235883712768555 sec
2023-06-22 15:36:45,616 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:36:45,800 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1838836669921875 sec
2023-06-22 15:36:45,839 - INFO - initial test loss: 0.02796067960305155
2023-06-22 15:36:45,839 - INFO - initial test acc: 0.6399999856948853
2023-06-22 15:36:45,858 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.013188838958740234 sec
2023-06-22 15:36:46,057 - WARNING - Finished tracing + transforming update_fn for pjit in 0.21349310874938965 sec
2023-06-22 15:36:46,062 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:36:46,173 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.1101064682006836 sec
2023-06-22 15:36:46,173 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:36:46,633 - WARNING - Finished XLA compilation of jit(update_fn) in 0.45963168144226074 sec
2023-06-22 15:36:49,695 - INFO - Distilling data from client: Client34
2023-06-22 15:36:49,695 - INFO - train loss: 0.003363568399213265
2023-06-22 15:36:49,695 - INFO - train acc: 0.9904761910438538
2023-06-22 15:36:49,939 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.68      0.69        66
           2       0.49      0.49      0.49        63
           4       0.59      0.61      0.60        71

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.60      0.59      0.60       200

2023-06-22 15:36:49,940 - INFO - test loss 0.02693736065089329
2023-06-22 15:36:49,940 - INFO - test acc 0.5949999690055847
2023-06-22 15:36:52,910 - INFO - Distilling data from client: Client34
2023-06-22 15:36:52,912 - INFO - train loss: 0.001889918855789849
2023-06-22 15:36:52,913 - INFO - train acc: 1.0
2023-06-22 15:36:53,125 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.70      0.72        66
           2       0.57      0.49      0.53        63
           4       0.56      0.68      0.62        71

    accuracy                           0.62       200
   macro avg       0.63      0.62      0.62       200
weighted avg       0.63      0.62      0.62       200

2023-06-22 15:36:53,126 - INFO - test loss 0.026118630103789515
2023-06-22 15:36:53,126 - INFO - test acc 0.625
2023-06-22 15:36:56,152 - INFO - Distilling data from client: Client34
2023-06-22 15:36:56,153 - INFO - train loss: 0.0015755727736552368
2023-06-22 15:36:56,153 - INFO - train acc: 1.0
2023-06-22 15:36:56,222 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.76      0.74        66
           2       0.53      0.46      0.49        63
           4       0.55      0.59      0.57        71

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.60      0.60       200

2023-06-22 15:36:56,223 - INFO - test loss 0.02705505636272788
2023-06-22 15:36:56,224 - INFO - test acc 0.6049999594688416
2023-06-22 15:36:59,399 - INFO - Distilling data from client: Client34
2023-06-22 15:36:59,400 - INFO - train loss: 0.001557270826686019
2023-06-22 15:36:59,400 - INFO - train acc: 1.0
2023-06-22 15:36:59,456 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.79      0.75        66
           2       0.54      0.43      0.48        63
           4       0.56      0.62      0.59        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-06-22 15:36:59,457 - INFO - test loss 0.025942623769027756
2023-06-22 15:36:59,457 - INFO - test acc 0.6150000095367432
2023-06-22 15:37:02,504 - INFO - Distilling data from client: Client34
2023-06-22 15:37:02,509 - INFO - train loss: 0.0011756603125677792
2023-06-22 15:37:02,515 - INFO - train acc: 0.9980952143669128
2023-06-22 15:37:02,589 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.76      0.73        66
           2       0.49      0.44      0.47        63
           4       0.56      0.56      0.56        71

    accuracy                           0.59       200
   macro avg       0.58      0.59      0.59       200
weighted avg       0.58      0.59      0.59       200

2023-06-22 15:37:02,591 - INFO - test loss 0.027019386425249464
2023-06-22 15:37:02,591 - INFO - test acc 0.5899999737739563
2023-06-22 15:37:05,609 - INFO - Distilling data from client: Client34
2023-06-22 15:37:05,609 - INFO - train loss: 0.0010476330276157739
2023-06-22 15:37:05,609 - INFO - train acc: 1.0
2023-06-22 15:37:05,668 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.71      0.72        66
           2       0.47      0.44      0.46        63
           4       0.52      0.55      0.53        71

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-06-22 15:37:05,672 - INFO - test loss 0.02690186655136319
2023-06-22 15:37:05,672 - INFO - test acc 0.5699999928474426
2023-06-22 15:37:08,843 - INFO - Distilling data from client: Client34
2023-06-22 15:37:08,843 - INFO - train loss: 0.0010174703359194495
2023-06-22 15:37:08,844 - INFO - train acc: 1.0
2023-06-22 15:37:08,901 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.70      0.70        66
           2       0.49      0.48      0.48        63
           4       0.51      0.54      0.52        71

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-06-22 15:37:08,902 - INFO - test loss 0.027282733698540583
2023-06-22 15:37:08,902 - INFO - test acc 0.5699999928474426
2023-06-22 15:37:11,827 - INFO - Distilling data from client: Client34
2023-06-22 15:37:11,828 - INFO - train loss: 0.0010207715370135356
2023-06-22 15:37:11,828 - INFO - train acc: 1.0
2023-06-22 15:37:11,881 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.70      0.70        66
           2       0.53      0.49      0.51        63
           4       0.55      0.58      0.56        71

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.59      0.59      0.59       200

2023-06-22 15:37:11,881 - INFO - test loss 0.02726616658674871
2023-06-22 15:37:11,882 - INFO - test acc 0.5899999737739563
2023-06-22 15:37:14,734 - INFO - Distilling data from client: Client34
2023-06-22 15:37:14,735 - INFO - train loss: 0.0008960242887978351
2023-06-22 15:37:14,735 - INFO - train acc: 0.9980952143669128
2023-06-22 15:37:14,808 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.76      0.73        66
           2       0.54      0.51      0.52        63
           4       0.57      0.56      0.57        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-06-22 15:37:14,809 - INFO - test loss 0.026917706331829244
2023-06-22 15:37:14,810 - INFO - test acc 0.6100000143051147
2023-06-22 15:37:17,692 - INFO - Distilling data from client: Client34
2023-06-22 15:37:17,693 - INFO - train loss: 0.0008584853105875647
2023-06-22 15:37:17,693 - INFO - train acc: 1.0
2023-06-22 15:37:17,754 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        66
           2       0.55      0.52      0.54        63
           4       0.54      0.56      0.55        71

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.61       200
weighted avg       0.61      0.60      0.60       200

2023-06-22 15:37:17,755 - INFO - test loss 0.02705067175144476
2023-06-22 15:37:17,756 - INFO - test acc 0.6049999594688416
2023-06-22 15:37:20,599 - INFO - Distilling data from client: Client34
2023-06-22 15:37:20,600 - INFO - train loss: 0.0008909168066365552
2023-06-22 15:37:20,601 - INFO - train acc: 1.0
2023-06-22 15:37:20,661 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.74      0.73        66
           2       0.56      0.48      0.51        63
           4       0.56      0.61      0.58        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-06-22 15:37:20,662 - INFO - test loss 0.026909580651886054
2023-06-22 15:37:20,662 - INFO - test acc 0.6100000143051147
2023-06-22 15:37:23,656 - INFO - Distilling data from client: Client34
2023-06-22 15:37:23,656 - INFO - train loss: 0.0007796945910465925
2023-06-22 15:37:23,657 - INFO - train acc: 1.0
2023-06-22 15:37:23,876 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.71      0.71        66
           2       0.52      0.54      0.53        63
           4       0.59      0.56      0.58        71

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.60       200
weighted avg       0.61      0.60      0.61       200

2023-06-22 15:37:23,877 - INFO - test loss 0.02771681243306992
2023-06-22 15:37:23,877 - INFO - test acc 0.6049999594688416
2023-06-22 15:37:26,961 - INFO - Distilling data from client: Client34
2023-06-22 15:37:26,962 - INFO - train loss: 0.0007499209811385166
2023-06-22 15:37:26,963 - INFO - train acc: 0.9980952143669128
2023-06-22 15:37:27,021 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.74      0.76        66
           2       0.54      0.57      0.55        63
           4       0.57      0.56      0.57        71

    accuracy                           0.62       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.62      0.63       200

2023-06-22 15:37:27,022 - INFO - test loss 0.02689696668896006
2023-06-22 15:37:27,022 - INFO - test acc 0.625
2023-06-22 15:37:30,024 - INFO - Distilling data from client: Client34
2023-06-22 15:37:30,024 - INFO - train loss: 0.0006136149588381667
2023-06-22 15:37:30,024 - INFO - train acc: 1.0
2023-06-22 15:37:30,095 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.71      0.71        66
           2       0.56      0.54      0.55        63
           4       0.56      0.58      0.57        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-06-22 15:37:30,095 - INFO - test loss 0.02636239632277132
2023-06-22 15:37:30,095 - INFO - test acc 0.6100000143051147
2023-06-22 15:37:32,963 - INFO - Distilling data from client: Client34
2023-06-22 15:37:32,964 - INFO - train loss: 0.0007314829343102557
2023-06-22 15:37:32,964 - INFO - train acc: 1.0
2023-06-22 15:37:33,029 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.71      0.72        66
           2       0.55      0.48      0.51        63
           4       0.54      0.61      0.57        71

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.60      0.60       200

2023-06-22 15:37:33,029 - INFO - test loss 0.02605334072707441
2023-06-22 15:37:33,029 - INFO - test acc 0.5999999642372131
2023-06-22 15:37:35,854 - INFO - Distilling data from client: Client34
2023-06-22 15:37:35,855 - INFO - train loss: 0.000660372367314554
2023-06-22 15:37:35,856 - INFO - train acc: 1.0
2023-06-22 15:37:35,928 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.70      0.72        66
           2       0.56      0.54      0.55        63
           4       0.55      0.59      0.57        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-06-22 15:37:35,929 - INFO - test loss 0.027265915944575475
2023-06-22 15:37:35,929 - INFO - test acc 0.6100000143051147
2023-06-22 15:37:38,840 - INFO - Distilling data from client: Client34
2023-06-22 15:37:38,840 - INFO - train loss: 0.000615714815081582
2023-06-22 15:37:38,840 - INFO - train acc: 1.0
2023-06-22 15:37:38,882 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.71      0.72        66
           2       0.50      0.51      0.50        63
           4       0.54      0.54      0.54        71

    accuracy                           0.58       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.59      0.58      0.59       200

2023-06-22 15:37:38,883 - INFO - test loss 0.027461689444838924
2023-06-22 15:37:38,883 - INFO - test acc 0.5849999785423279
2023-06-22 15:37:41,785 - INFO - Distilling data from client: Client34
2023-06-22 15:37:41,785 - INFO - train loss: 0.0006273881660046604
2023-06-22 15:37:41,786 - INFO - train acc: 1.0
2023-06-22 15:37:41,834 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.70      0.70        66
           2       0.46      0.48      0.47        63
           4       0.52      0.51      0.51        71

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-06-22 15:37:41,834 - INFO - test loss 0.028157060361229697
2023-06-22 15:37:41,835 - INFO - test acc 0.5600000023841858
2023-06-22 15:37:44,645 - INFO - Distilling data from client: Client34
2023-06-22 15:37:44,646 - INFO - train loss: 0.0006950770419690151
2023-06-22 15:37:44,646 - INFO - train acc: 1.0
2023-06-22 15:37:44,714 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.74      0.74        66
           2       0.54      0.52      0.53        63
           4       0.52      0.54      0.53        71

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.60      0.60       200

2023-06-22 15:37:44,717 - INFO - test loss 0.027513989566584372
2023-06-22 15:37:44,718 - INFO - test acc 0.5999999642372131
2023-06-22 15:37:47,621 - INFO - Distilling data from client: Client34
2023-06-22 15:37:47,622 - INFO - train loss: 0.0005689312579981436
2023-06-22 15:37:47,622 - INFO - train acc: 1.0
2023-06-22 15:37:47,682 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        66
           2       0.54      0.52      0.53        63
           4       0.53      0.56      0.55        71

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.61       200
weighted avg       0.61      0.60      0.61       200

2023-06-22 15:37:47,683 - INFO - test loss 0.02707831439803872
2023-06-22 15:37:47,683 - INFO - test acc 0.6049999594688416
2023-06-22 15:37:50,565 - INFO - Distilling data from client: Client34
2023-06-22 15:37:50,565 - INFO - train loss: 0.0006335655679331246
2023-06-22 15:37:50,565 - INFO - train acc: 1.0
2023-06-22 15:37:50,612 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.74      0.72        66
           2       0.49      0.46      0.48        63
           4       0.52      0.52      0.52        71

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-06-22 15:37:50,612 - INFO - test loss 0.027070133114381667
2023-06-22 15:37:50,612 - INFO - test acc 0.574999988079071
2023-06-22 15:37:53,620 - INFO - Distilling data from client: Client34
2023-06-22 15:37:53,621 - INFO - train loss: 0.0005924003989811264
2023-06-22 15:37:53,621 - INFO - train acc: 1.0
2023-06-22 15:37:53,678 - INFO - report:               precision    recall  f1-score   support

           0       0.69      0.73      0.71        66
           2       0.54      0.54      0.54        63
           4       0.54      0.51      0.52        71

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.59      0.59      0.59       200

2023-06-22 15:37:53,678 - INFO - test loss 0.026413922237366726
2023-06-22 15:37:53,678 - INFO - test acc 0.5899999737739563
2023-06-22 15:37:56,525 - INFO - Distilling data from client: Client34
2023-06-22 15:37:56,526 - INFO - train loss: 0.0005439141089166473
2023-06-22 15:37:56,527 - INFO - train acc: 1.0
2023-06-22 15:37:56,581 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.73      0.72        66
           2       0.53      0.56      0.54        63
           4       0.58      0.55      0.57        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-06-22 15:37:56,582 - INFO - test loss 0.027032335771998903
2023-06-22 15:37:56,583 - INFO - test acc 0.6100000143051147
2023-06-22 15:37:59,449 - INFO - Distilling data from client: Client34
2023-06-22 15:37:59,450 - INFO - train loss: 0.00048600770626629604
2023-06-22 15:37:59,452 - INFO - train acc: 1.0
2023-06-22 15:37:59,516 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.76      0.74        66
           2       0.49      0.46      0.48        63
           4       0.51      0.52      0.52        71

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-06-22 15:37:59,517 - INFO - test loss 0.027454095907907376
2023-06-22 15:37:59,517 - INFO - test acc 0.5799999833106995
2023-06-22 15:38:02,363 - INFO - Distilling data from client: Client34
2023-06-22 15:38:02,364 - INFO - train loss: 0.0006004837281248709
2023-06-22 15:38:02,371 - INFO - train acc: 1.0
2023-06-22 15:38:02,427 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        66
           2       0.53      0.52      0.53        63
           4       0.58      0.59      0.59        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-06-22 15:38:02,428 - INFO - test loss 0.027113882026789313
2023-06-22 15:38:02,428 - INFO - test acc 0.6150000095367432
2023-06-22 15:38:02,452 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:38:02,468 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:38:02,483 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:38:02,496 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:38:02,511 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:38:02,524 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:38:02,538 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:38:02,551 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:38:02,564 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:38:03,155 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client34//synthetic.png
2023-06-22 15:38:03,177 - INFO - c: 1.0 and total_data_in_this_class: 261
2023-06-22 15:38:03,178 - INFO - c: 2.0 and total_data_in_this_class: 263
2023-06-22 15:38:03,178 - INFO - c: 5.0 and total_data_in_this_class: 275
2023-06-22 15:38:03,178 - INFO - c: 1.0 and total_data_in_this_class: 72
2023-06-22 15:38:03,178 - INFO - c: 2.0 and total_data_in_this_class: 70
2023-06-22 15:38:03,178 - INFO - c: 5.0 and total_data_in_this_class: 58
2023-06-22 15:38:03,226 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005507469177246094 sec
2023-06-22 15:38:03,226 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:38:03,229 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.002619028091430664 sec
2023-06-22 15:38:03,230 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:38:03,246 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.016111135482788086 sec
2023-06-22 15:38:03,250 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00034618377685546875 sec
2023-06-22 15:38:03,250 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:38:03,252 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0017445087432861328 sec
2023-06-22 15:38:03,253 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:38:03,264 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011346817016601562 sec
2023-06-22 15:38:03,270 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021529197692871094 sec
2023-06-22 15:38:03,272 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020432472229003906 sec
2023-06-22 15:38:03,273 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000499725341796875 sec
2023-06-22 15:38:03,276 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021386146545410156 sec
2023-06-22 15:38:03,276 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003180503845214844 sec
2023-06-22 15:38:03,277 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004267692565917969 sec
2023-06-22 15:38:03,278 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037670135498046875 sec
2023-06-22 15:38:03,279 - WARNING - Finished tracing + transforming absolute for pjit in 0.00029850006103515625 sec
2023-06-22 15:38:03,280 - WARNING - Finished tracing + transforming fn for pjit in 0.00043702125549316406 sec
2023-06-22 15:38:03,281 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005512237548828125 sec
2023-06-22 15:38:03,283 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001964569091796875 sec
2023-06-22 15:38:03,284 - WARNING - Finished tracing + transforming fn for pjit in 0.0003604888916015625 sec
2023-06-22 15:38:03,285 - WARNING - Finished tracing + transforming fn for pjit in 0.0005340576171875 sec
2023-06-22 15:38:03,286 - WARNING - Finished tracing + transforming fn for pjit in 0.00034689903259277344 sec
2023-06-22 15:38:03,287 - WARNING - Finished tracing + transforming fn for pjit in 0.00041604042053222656 sec
2023-06-22 15:38:03,290 - WARNING - Finished tracing + transforming fn for pjit in 0.00037026405334472656 sec
2023-06-22 15:38:03,293 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00045561790466308594 sec
2023-06-22 15:38:03,294 - WARNING - Finished tracing + transforming fn for pjit in 0.0003807544708251953 sec
2023-06-22 15:38:03,295 - WARNING - Finished tracing + transforming fn for pjit in 0.0004248619079589844 sec
2023-06-22 15:38:03,301 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005946159362792969 sec
2023-06-22 15:38:03,302 - WARNING - Finished tracing + transforming _mean for pjit in 0.0015380382537841797 sec
2023-06-22 15:38:03,303 - WARNING - Finished tracing + transforming fn for pjit in 0.0003917217254638672 sec
2023-06-22 15:38:03,304 - WARNING - Finished tracing + transforming fn for pjit in 0.00033926963806152344 sec
2023-06-22 15:38:03,306 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040030479431152344 sec
2023-06-22 15:38:03,307 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005953311920166016 sec
2023-06-22 15:38:03,308 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029754638671875 sec
2023-06-22 15:38:03,310 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003979206085205078 sec
2023-06-22 15:38:03,311 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040149688720703125 sec
2023-06-22 15:38:03,312 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003695487976074219 sec
2023-06-22 15:38:03,313 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004668235778808594 sec
2023-06-22 15:38:03,314 - WARNING - Finished tracing + transforming _where for pjit in 0.0014219284057617188 sec
2023-06-22 15:38:03,315 - WARNING - Finished tracing + transforming fn for pjit in 0.00042366981506347656 sec
2023-06-22 15:38:03,316 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005395412445068359 sec
2023-06-22 15:38:03,318 - WARNING - Finished tracing + transforming fn for pjit in 0.0003635883331298828 sec
2023-06-22 15:38:03,319 - WARNING - Finished tracing + transforming fn for pjit in 0.0003719329833984375 sec
2023-06-22 15:38:03,320 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003616809844970703 sec
2023-06-22 15:38:03,321 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044798851013183594 sec
2023-06-22 15:38:03,322 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003046989440917969 sec
2023-06-22 15:38:03,323 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004296302795410156 sec
2023-06-22 15:38:03,325 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037097930908203125 sec
2023-06-22 15:38:03,326 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003666877746582031 sec
2023-06-22 15:38:03,327 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00048661231994628906 sec
2023-06-22 15:38:03,328 - WARNING - Finished tracing + transforming _where for pjit in 0.0014657974243164062 sec
2023-06-22 15:38:03,329 - WARNING - Finished tracing + transforming fn for pjit in 0.0004177093505859375 sec
2023-06-22 15:38:03,330 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00043010711669921875 sec
2023-06-22 15:38:03,332 - WARNING - Finished tracing + transforming fn for pjit in 0.00037360191345214844 sec
2023-06-22 15:38:03,338 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004298686981201172 sec
2023-06-22 15:38:03,340 - WARNING - Finished tracing + transforming fn for pjit in 0.0004444122314453125 sec
2023-06-22 15:38:03,341 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00043654441833496094 sec
2023-06-22 15:38:03,342 - WARNING - Finished tracing + transforming fn for pjit in 0.0005028247833251953 sec
2023-06-22 15:38:03,348 - WARNING - Finished tracing + transforming fn for pjit in 0.0003571510314941406 sec
2023-06-22 15:38:03,351 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002884864807128906 sec
2023-06-22 15:38:03,352 - WARNING - Finished tracing + transforming fn for pjit in 0.0005016326904296875 sec
2023-06-22 15:38:03,354 - WARNING - Finished tracing + transforming fn for pjit in 0.0003819465637207031 sec
2023-06-22 15:38:03,381 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11150622367858887 sec
2023-06-22 15:38:03,386 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021982192993164062 sec
2023-06-22 15:38:03,387 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001919269561767578 sec
2023-06-22 15:38:03,388 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00045418739318847656 sec
2023-06-22 15:38:03,392 - WARNING - Finished tracing + transforming fn for pjit in 0.0003523826599121094 sec
2023-06-22 15:38:03,393 - WARNING - Finished tracing + transforming fn for pjit in 0.00041222572326660156 sec
2023-06-22 15:38:03,395 - WARNING - Finished tracing + transforming fn for pjit in 0.0003745555877685547 sec
2023-06-22 15:38:03,404 - WARNING - Finished tracing + transforming fn for pjit in 0.00036454200744628906 sec
2023-06-22 15:38:03,406 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003495216369628906 sec
2023-06-22 15:38:03,407 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004317760467529297 sec
2023-06-22 15:38:03,408 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002961158752441406 sec
2023-06-22 15:38:03,409 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005557537078857422 sec
2023-06-22 15:38:03,411 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037026405334472656 sec
2023-06-22 15:38:03,412 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037479400634765625 sec
2023-06-22 15:38:03,413 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004754066467285156 sec
2023-06-22 15:38:03,414 - WARNING - Finished tracing + transforming _where for pjit in 0.0014793872833251953 sec
2023-06-22 15:38:03,415 - WARNING - Finished tracing + transforming fn for pjit in 0.0004546642303466797 sec
2023-06-22 15:38:03,416 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004057884216308594 sec
2023-06-22 15:38:03,418 - WARNING - Finished tracing + transforming fn for pjit in 0.0003540515899658203 sec
2023-06-22 15:38:03,419 - WARNING - Finished tracing + transforming fn for pjit in 0.0004863739013671875 sec
2023-06-22 15:38:03,438 - WARNING - Finished tracing + transforming fn for pjit in 0.00032830238342285156 sec
2023-06-22 15:38:03,467 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08489060401916504 sec
2023-06-22 15:38:03,470 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021409988403320312 sec
2023-06-22 15:38:03,471 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002186298370361328 sec
2023-06-22 15:38:03,472 - WARNING - Finished tracing + transforming _where for pjit in 0.0010223388671875 sec
2023-06-22 15:38:03,473 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0004885196685791016 sec
2023-06-22 15:38:03,473 - WARNING - Finished tracing + transforming trace for pjit in 0.004200935363769531 sec
2023-06-22 15:38:03,478 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00018095970153808594 sec
2023-06-22 15:38:03,479 - WARNING - Finished tracing + transforming tril for pjit in 0.0010612010955810547 sec
2023-06-22 15:38:03,480 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.003317117691040039 sec
2023-06-22 15:38:03,481 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00017523765563964844 sec
2023-06-22 15:38:03,482 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00017786026000976562 sec
2023-06-22 15:38:03,485 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0023241043090820312 sec
2023-06-22 15:38:03,491 - WARNING - Finished tracing + transforming _solve for pjit in 0.015523433685302734 sec
2023-06-22 15:38:03,492 - WARNING - Finished tracing + transforming dot for pjit in 0.000522613525390625 sec
2023-06-22 15:38:03,496 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.2298295497894287 sec
2023-06-22 15:38:03,499 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:38:03,552 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05264544486999512 sec
2023-06-22 15:38:03,553 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:38:03,723 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17036938667297363 sec
2023-06-22 15:38:03,768 - INFO - initial test loss: 0.02304179102661807
2023-06-22 15:38:03,769 - INFO - initial test acc: 0.7149999737739563
2023-06-22 15:38:03,783 - WARNING - Finished tracing + transforming dot for pjit in 0.0008549690246582031 sec
2023-06-22 15:38:03,785 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006861686706542969 sec
2023-06-22 15:38:03,788 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008571147918701172 sec
2023-06-22 15:38:03,789 - WARNING - Finished tracing + transforming _mean for pjit in 0.0025277137756347656 sec
2023-06-22 15:38:03,791 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004534721374511719 sec
2023-06-22 15:38:03,792 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004248619079589844 sec
2023-06-22 15:38:03,794 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005850791931152344 sec
2023-06-22 15:38:03,796 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008213520050048828 sec
2023-06-22 15:38:03,798 - WARNING - Finished tracing + transforming _mean for pjit in 0.0025904178619384766 sec
2023-06-22 15:38:03,799 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.024180889129638672 sec
2023-06-22 15:38:03,819 - WARNING - Finished tracing + transforming fn for pjit in 0.0006036758422851562 sec
2023-06-22 15:38:03,820 - WARNING - Finished tracing + transforming fn for pjit in 0.0006208419799804688 sec
2023-06-22 15:38:03,822 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005137920379638672 sec
2023-06-22 15:38:03,826 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.001344919204711914 sec
2023-06-22 15:38:03,828 - WARNING - Finished tracing + transforming _where for pjit in 0.003924131393432617 sec
2023-06-22 15:38:03,848 - WARNING - Finished tracing + transforming fn for pjit in 0.0005865097045898438 sec
2023-06-22 15:38:03,851 - WARNING - Finished tracing + transforming fn for pjit in 0.0008542537689208984 sec
2023-06-22 15:38:03,852 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005295276641845703 sec
2023-06-22 15:38:03,856 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0009369850158691406 sec
2023-06-22 15:38:03,857 - WARNING - Finished tracing + transforming _where for pjit in 0.0026166439056396484 sec
2023-06-22 15:38:03,920 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042319297790527344 sec
2023-06-22 15:38:04,009 - WARNING - Finished tracing + transforming fn for pjit in 0.0005183219909667969 sec
2023-06-22 15:38:04,011 - WARNING - Finished tracing + transforming fn for pjit in 0.00043892860412597656 sec
2023-06-22 15:38:04,012 - WARNING - Finished tracing + transforming square for pjit in 0.00031566619873046875 sec
2023-06-22 15:38:04,015 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004189014434814453 sec
2023-06-22 15:38:04,018 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005025863647460938 sec
2023-06-22 15:38:04,019 - WARNING - Finished tracing + transforming fn for pjit in 0.0005052089691162109 sec
2023-06-22 15:38:04,021 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004086494445800781 sec
2023-06-22 15:38:04,022 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00041365623474121094 sec
2023-06-22 15:38:04,023 - WARNING - Finished tracing + transforming fn for pjit in 0.0004870891571044922 sec
2023-06-22 15:38:04,024 - WARNING - Finished tracing + transforming fn for pjit in 0.00042128562927246094 sec
2023-06-22 15:38:04,025 - WARNING - Finished tracing + transforming square for pjit in 0.0003104209899902344 sec
2023-06-22 15:38:04,029 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00038814544677734375 sec
2023-06-22 15:38:04,032 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00030541419982910156 sec
2023-06-22 15:38:04,033 - WARNING - Finished tracing + transforming fn for pjit in 0.000453948974609375 sec
2023-06-22 15:38:04,034 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003943443298339844 sec
2023-06-22 15:38:04,035 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004107952117919922 sec
2023-06-22 15:38:04,036 - WARNING - Finished tracing + transforming update_fn for pjit in 0.26235294342041016 sec
2023-06-22 15:38:04,042 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[522,10]), ShapedArray(float32[522,10]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:38:04,158 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.11539602279663086 sec
2023-06-22 15:38:04,159 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:38:04,587 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4280738830566406 sec
2023-06-22 15:38:07,450 - INFO - Distilling data from client: Client35
2023-06-22 15:38:07,450 - INFO - train loss: 0.0023698369845100795
2023-06-22 15:38:07,450 - INFO - train acc: 0.9961685538291931
2023-06-22 15:38:07,695 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.79      0.79        72
           2       0.65      0.67      0.66        70
           5       0.60      0.57      0.58        58

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.69      0.68       200

2023-06-22 15:38:07,696 - INFO - test loss 0.021720367770601937
2023-06-22 15:38:07,696 - INFO - test acc 0.6850000023841858
2023-06-22 15:38:10,549 - INFO - Distilling data from client: Client35
2023-06-22 15:38:10,550 - INFO - train loss: 0.0013044289079269084
2023-06-22 15:38:10,550 - INFO - train acc: 0.9980842471122742
2023-06-22 15:38:10,750 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.76      0.77        72
           2       0.67      0.73      0.70        70
           5       0.62      0.57      0.59        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:38:10,750 - INFO - test loss 0.022576002553080794
2023-06-22 15:38:10,751 - INFO - test acc 0.6949999928474426
2023-06-22 15:38:13,617 - INFO - Distilling data from client: Client35
2023-06-22 15:38:13,617 - INFO - train loss: 0.0009037115816767831
2023-06-22 15:38:13,618 - INFO - train acc: 1.0
2023-06-22 15:38:13,811 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.75      0.77        72
           2       0.68      0.73      0.70        70
           5       0.66      0.64      0.65        58

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:38:13,812 - INFO - test loss 0.022856750235046557
2023-06-22 15:38:13,812 - INFO - test acc 0.7099999785423279
2023-06-22 15:38:16,744 - INFO - Distilling data from client: Client35
2023-06-22 15:38:16,744 - INFO - train loss: 0.0008091403877635007
2023-06-22 15:38:16,745 - INFO - train acc: 1.0
2023-06-22 15:38:16,813 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.75      0.77        72
           2       0.70      0.69      0.69        70
           5       0.61      0.66      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:38:16,814 - INFO - test loss 0.022725006330660918
2023-06-22 15:38:16,815 - INFO - test acc 0.699999988079071
2023-06-22 15:38:19,771 - INFO - Distilling data from client: Client35
2023-06-22 15:38:19,771 - INFO - train loss: 0.0007437455007340897
2023-06-22 15:38:19,771 - INFO - train acc: 1.0
2023-06-22 15:38:19,818 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.72      0.75        72
           2       0.65      0.67      0.66        70
           5       0.61      0.66      0.63        58

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:38:19,818 - INFO - test loss 0.023239220907861916
2023-06-22 15:38:19,819 - INFO - test acc 0.6850000023841858
2023-06-22 15:38:22,699 - INFO - Distilling data from client: Client35
2023-06-22 15:38:22,699 - INFO - train loss: 0.0006614779045390981
2023-06-22 15:38:22,700 - INFO - train acc: 1.0
2023-06-22 15:38:22,757 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.75      0.75        72
           2       0.70      0.69      0.69        70
           5       0.63      0.64      0.63        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.70       200

2023-06-22 15:38:22,757 - INFO - test loss 0.022571097169943648
2023-06-22 15:38:22,758 - INFO - test acc 0.6949999928474426
2023-06-22 15:38:25,613 - INFO - Distilling data from client: Client35
2023-06-22 15:38:25,614 - INFO - train loss: 0.000568755990089198
2023-06-22 15:38:25,614 - INFO - train acc: 1.0
2023-06-22 15:38:25,673 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.74      0.76        72
           2       0.69      0.73      0.71        70
           5       0.63      0.64      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-06-22 15:38:25,675 - INFO - test loss 0.022578718845538404
2023-06-22 15:38:25,675 - INFO - test acc 0.7049999833106995
2023-06-22 15:38:28,609 - INFO - Distilling data from client: Client35
2023-06-22 15:38:28,610 - INFO - train loss: 0.0005290184439858973
2023-06-22 15:38:28,610 - INFO - train acc: 1.0
2023-06-22 15:38:28,666 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.75      0.77        72
           2       0.71      0.69      0.70        70
           5       0.60      0.66      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:38:28,667 - INFO - test loss 0.023298308244790514
2023-06-22 15:38:28,668 - INFO - test acc 0.699999988079071
2023-06-22 15:38:31,596 - INFO - Distilling data from client: Client35
2023-06-22 15:38:31,596 - INFO - train loss: 0.0005218740071088258
2023-06-22 15:38:31,596 - INFO - train acc: 1.0
2023-06-22 15:38:31,652 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.74      0.75        72
           2       0.67      0.70      0.69        70
           5       0.66      0.66      0.66        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:38:31,654 - INFO - test loss 0.022760395923175553
2023-06-22 15:38:31,655 - INFO - test acc 0.699999988079071
2023-06-22 15:38:34,614 - INFO - Distilling data from client: Client35
2023-06-22 15:38:34,614 - INFO - train loss: 0.0005475356351699924
2023-06-22 15:38:34,615 - INFO - train acc: 1.0
2023-06-22 15:38:34,668 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.74      0.76        72
           2       0.65      0.71      0.68        70
           5       0.61      0.59      0.60        58

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:38:34,669 - INFO - test loss 0.022765727229039547
2023-06-22 15:38:34,670 - INFO - test acc 0.6850000023841858
2023-06-22 15:38:37,797 - INFO - Distilling data from client: Client35
2023-06-22 15:38:37,797 - INFO - train loss: 0.0004152232574490778
2023-06-22 15:38:37,798 - INFO - train acc: 1.0
2023-06-22 15:38:37,856 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.74      0.75        72
           2       0.68      0.73      0.70        70
           5       0.64      0.62      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:38:37,857 - INFO - test loss 0.02281499619041616
2023-06-22 15:38:37,857 - INFO - test acc 0.699999988079071
2023-06-22 15:38:40,828 - INFO - Distilling data from client: Client35
2023-06-22 15:38:40,828 - INFO - train loss: 0.00036788848554714456
2023-06-22 15:38:40,829 - INFO - train acc: 1.0
2023-06-22 15:38:40,899 - INFO - report:               precision    recall  f1-score   support

           1       0.76      0.75      0.76        72
           2       0.71      0.74      0.73        70
           5       0.64      0.62      0.63        58

    accuracy                           0.71       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:38:40,899 - INFO - test loss 0.02211961649008174
2023-06-22 15:38:40,900 - INFO - test acc 0.7099999785423279
2023-06-22 15:38:43,801 - INFO - Distilling data from client: Client35
2023-06-22 15:38:43,801 - INFO - train loss: 0.00041327330695060255
2023-06-22 15:38:43,801 - INFO - train acc: 1.0
2023-06-22 15:38:43,859 - INFO - report:               precision    recall  f1-score   support

           1       0.81      0.75      0.78        72
           2       0.68      0.71      0.70        70
           5       0.62      0.64      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-06-22 15:38:43,859 - INFO - test loss 0.023483184416886353
2023-06-22 15:38:43,860 - INFO - test acc 0.7049999833106995
2023-06-22 15:38:46,710 - INFO - Distilling data from client: Client35
2023-06-22 15:38:46,711 - INFO - train loss: 0.0005146300925655019
2023-06-22 15:38:46,711 - INFO - train acc: 1.0
2023-06-22 15:38:46,876 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.74      0.76        72
           2       0.69      0.74      0.72        70
           5       0.67      0.67      0.67        58

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:38:46,876 - INFO - test loss 0.02279217424583595
2023-06-22 15:38:46,876 - INFO - test acc 0.7199999690055847
2023-06-22 15:38:49,685 - INFO - Distilling data from client: Client35
2023-06-22 15:38:49,685 - INFO - train loss: 0.00034811354592800966
2023-06-22 15:38:49,685 - INFO - train acc: 1.0
2023-06-22 15:38:49,755 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.74      0.77        72
           2       0.66      0.73      0.69        70
           5       0.60      0.60      0.60        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.70       200

2023-06-22 15:38:49,756 - INFO - test loss 0.02284901389950546
2023-06-22 15:38:49,756 - INFO - test acc 0.6949999928474426
2023-06-22 15:38:52,634 - INFO - Distilling data from client: Client35
2023-06-22 15:38:52,634 - INFO - train loss: 0.0003870869413817149
2023-06-22 15:38:52,635 - INFO - train acc: 1.0
2023-06-22 15:38:52,685 - INFO - report:               precision    recall  f1-score   support

           1       0.76      0.71      0.73        72
           2       0.66      0.69      0.67        70
           5       0.60      0.62      0.61        58

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:38:52,686 - INFO - test loss 0.023210256088797992
2023-06-22 15:38:52,686 - INFO - test acc 0.675000011920929
2023-06-22 15:38:55,597 - INFO - Distilling data from client: Client35
2023-06-22 15:38:55,598 - INFO - train loss: 0.0003533390379248814
2023-06-22 15:38:55,599 - INFO - train acc: 1.0
2023-06-22 15:38:55,650 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.75      0.77        72
           2       0.66      0.69      0.67        70
           5       0.61      0.62      0.62        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:38:55,651 - INFO - test loss 0.023643639136100056
2023-06-22 15:38:55,651 - INFO - test acc 0.6899999976158142
2023-06-22 15:38:58,510 - INFO - Distilling data from client: Client35
2023-06-22 15:38:58,511 - INFO - train loss: 0.0003733478495841087
2023-06-22 15:38:58,512 - INFO - train acc: 1.0
2023-06-22 15:38:58,573 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.75      0.77        72
           2       0.65      0.67      0.66        70
           5       0.60      0.62      0.61        58

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:38:58,574 - INFO - test loss 0.02289261291239496
2023-06-22 15:38:58,574 - INFO - test acc 0.6850000023841858
2023-06-22 15:39:01,479 - INFO - Distilling data from client: Client35
2023-06-22 15:39:01,479 - INFO - train loss: 0.00038917647349184174
2023-06-22 15:39:01,479 - INFO - train acc: 1.0
2023-06-22 15:39:01,552 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.75      0.77        72
           2       0.69      0.70      0.70        70
           5       0.62      0.64      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:39:01,553 - INFO - test loss 0.022968672775039226
2023-06-22 15:39:01,554 - INFO - test acc 0.699999988079071
2023-06-22 15:39:04,515 - INFO - Distilling data from client: Client35
2023-06-22 15:39:04,515 - INFO - train loss: 0.0004175773416310385
2023-06-22 15:39:04,515 - INFO - train acc: 1.0
2023-06-22 15:39:04,585 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.74      0.77        72
           2       0.64      0.67      0.66        70
           5       0.56      0.60      0.58        58

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:39:04,589 - INFO - test loss 0.023288226768594072
2023-06-22 15:39:04,590 - INFO - test acc 0.675000011920929
2023-06-22 15:39:07,524 - INFO - Distilling data from client: Client35
2023-06-22 15:39:07,525 - INFO - train loss: 0.00035331764744753945
2023-06-22 15:39:07,526 - INFO - train acc: 1.0
2023-06-22 15:39:07,587 - INFO - report:               precision    recall  f1-score   support

           1       0.81      0.75      0.78        72
           2       0.66      0.73      0.69        70
           5       0.64      0.62      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-06-22 15:39:07,589 - INFO - test loss 0.022725219986455627
2023-06-22 15:39:07,590 - INFO - test acc 0.7049999833106995
2023-06-22 15:39:10,719 - INFO - Distilling data from client: Client35
2023-06-22 15:39:10,720 - INFO - train loss: 0.0002522326888688871
2023-06-22 15:39:10,720 - INFO - train acc: 1.0
2023-06-22 15:39:10,822 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.71      0.74        72
           2       0.68      0.73      0.70        70
           5       0.62      0.64      0.63        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.70       200

2023-06-22 15:39:10,822 - INFO - test loss 0.02309909922494741
2023-06-22 15:39:10,823 - INFO - test acc 0.6949999928474426
2023-06-22 15:39:13,838 - INFO - Distilling data from client: Client35
2023-06-22 15:39:13,839 - INFO - train loss: 0.00028617552973406803
2023-06-22 15:39:13,839 - INFO - train acc: 1.0
2023-06-22 15:39:13,897 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.72      0.76        72
           2       0.68      0.73      0.70        70
           5       0.60      0.62      0.61        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.70       200

2023-06-22 15:39:13,898 - INFO - test loss 0.02310197356957049
2023-06-22 15:39:13,899 - INFO - test acc 0.6949999928474426
2023-06-22 15:39:16,868 - INFO - Distilling data from client: Client35
2023-06-22 15:39:16,868 - INFO - train loss: 0.0002523648465261308
2023-06-22 15:39:16,869 - INFO - train acc: 1.0
2023-06-22 15:39:16,938 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.74      0.76        72
           2       0.64      0.70      0.67        70
           5       0.61      0.60      0.61        58

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:39:16,938 - INFO - test loss 0.022738322377258837
2023-06-22 15:39:16,938 - INFO - test acc 0.6850000023841858
2023-06-22 15:39:19,817 - INFO - Distilling data from client: Client35
2023-06-22 15:39:19,817 - INFO - train loss: 0.00028500644405861884
2023-06-22 15:39:19,817 - INFO - train acc: 1.0
2023-06-22 15:39:19,917 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.78      0.78        72
           2       0.70      0.73      0.71        70
           5       0.64      0.62      0.63        58

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:39:19,922 - INFO - test loss 0.022963078649180912
2023-06-22 15:39:19,922 - INFO - test acc 0.7149999737739563
2023-06-22 15:39:19,931 - WARNING - Finished tracing + transforming jit(gather) in 0.0009632110595703125 sec
2023-06-22 15:39:19,932 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[522,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:39:19,937 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.004105329513549805 sec
2023-06-22 15:39:19,938 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:39:19,962 - WARNING - Finished XLA compilation of jit(gather) in 0.022974014282226562 sec
2023-06-22 15:39:19,990 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:39:20,009 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:39:20,021 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:39:20,033 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:39:20,044 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:39:20,057 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:39:20,069 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:39:20,082 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:39:20,094 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:39:20,691 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client35//synthetic.png
2023-06-22 15:39:20,721 - INFO - c: 1.0 and total_data_in_this_class: 255
2023-06-22 15:39:20,722 - INFO - c: 2.0 and total_data_in_this_class: 265
2023-06-22 15:39:20,722 - INFO - c: 9.0 and total_data_in_this_class: 279
2023-06-22 15:39:20,722 - INFO - c: 1.0 and total_data_in_this_class: 78
2023-06-22 15:39:20,722 - INFO - c: 2.0 and total_data_in_this_class: 68
2023-06-22 15:39:20,722 - INFO - c: 9.0 and total_data_in_this_class: 54
2023-06-22 15:39:20,854 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08085942268371582 sec
2023-06-22 15:39:20,930 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07391142845153809 sec
2023-06-22 15:39:20,938 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16745543479919434 sec
2023-06-22 15:39:20,942 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:39:20,999 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.057395219802856445 sec
2023-06-22 15:39:21,000 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:39:21,174 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17423534393310547 sec
2023-06-22 15:39:21,217 - INFO - initial test loss: 0.023150742722133976
2023-06-22 15:39:21,217 - INFO - initial test acc: 0.6599999666213989
2023-06-22 15:39:21,239 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.013815641403198242 sec
2023-06-22 15:39:21,430 - WARNING - Finished tracing + transforming update_fn for pjit in 0.20645666122436523 sec
2023-06-22 15:39:21,435 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[510,10]), ShapedArray(float32[510,10]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:39:21,537 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10138702392578125 sec
2023-06-22 15:39:21,538 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:39:21,965 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4270768165588379 sec
2023-06-22 15:39:24,702 - INFO - Distilling data from client: Client36
2023-06-22 15:39:24,702 - INFO - train loss: 0.002402359797691684
2023-06-22 15:39:24,702 - INFO - train acc: 0.9980392456054688
2023-06-22 15:39:24,865 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.63      0.69        78
           2       0.79      0.79      0.79        68
           9       0.54      0.67      0.60        54

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.71      0.69      0.70       200

2023-06-22 15:39:24,865 - INFO - test loss 0.022358681602539524
2023-06-22 15:39:24,866 - INFO - test acc 0.6949999928474426
2023-06-22 15:39:27,545 - INFO - Distilling data from client: Client36
2023-06-22 15:39:27,546 - INFO - train loss: 0.0013761625952744545
2023-06-22 15:39:27,547 - INFO - train acc: 1.0
2023-06-22 15:39:27,604 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.59      0.63        78
           2       0.77      0.79      0.78        68
           9       0.52      0.59      0.55        54

    accuracy                           0.66       200
   macro avg       0.65      0.66      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:39:27,608 - INFO - test loss 0.022623960926756927
2023-06-22 15:39:27,609 - INFO - test acc 0.6599999666213989
2023-06-22 15:39:30,291 - INFO - Distilling data from client: Client36
2023-06-22 15:39:30,291 - INFO - train loss: 0.0010118153011267883
2023-06-22 15:39:30,292 - INFO - train acc: 1.0
2023-06-22 15:39:30,342 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.65      0.67        78
           2       0.78      0.79      0.79        68
           9       0.54      0.57      0.56        54

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:39:30,343 - INFO - test loss 0.022479766112621914
2023-06-22 15:39:30,343 - INFO - test acc 0.6800000071525574
2023-06-22 15:39:32,981 - INFO - Distilling data from client: Client36
2023-06-22 15:39:32,982 - INFO - train loss: 0.0008568079401932556
2023-06-22 15:39:32,982 - INFO - train acc: 1.0
2023-06-22 15:39:33,045 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.62      0.66        78
           2       0.76      0.82      0.79        68
           9       0.54      0.59      0.57        54

    accuracy                           0.68       200
   macro avg       0.67      0.68      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:39:33,045 - INFO - test loss 0.02308997215383127
2023-06-22 15:39:33,045 - INFO - test acc 0.6800000071525574
2023-06-22 15:39:35,756 - INFO - Distilling data from client: Client36
2023-06-22 15:39:35,756 - INFO - train loss: 0.0007579114456489472
2023-06-22 15:39:35,757 - INFO - train acc: 1.0
2023-06-22 15:39:35,815 - INFO - report:               precision    recall  f1-score   support

           1       0.71      0.63      0.67        78
           2       0.75      0.76      0.76        68
           9       0.50      0.57      0.53        54

    accuracy                           0.66       200
   macro avg       0.65      0.66      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:39:35,815 - INFO - test loss 0.02324399809603083
2023-06-22 15:39:35,815 - INFO - test acc 0.6599999666213989
2023-06-22 15:39:38,607 - INFO - Distilling data from client: Client36
2023-06-22 15:39:38,608 - INFO - train loss: 0.0006052158067574345
2023-06-22 15:39:38,608 - INFO - train acc: 1.0
2023-06-22 15:39:38,656 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.59      0.64        78
           2       0.78      0.82      0.80        68
           9       0.52      0.59      0.55        54

    accuracy                           0.67       200
   macro avg       0.66      0.67      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:39:38,657 - INFO - test loss 0.02370352102517417
2023-06-22 15:39:38,657 - INFO - test acc 0.6699999570846558
2023-06-22 15:39:41,450 - INFO - Distilling data from client: Client36
2023-06-22 15:39:41,451 - INFO - train loss: 0.000511917702750476
2023-06-22 15:39:41,451 - INFO - train acc: 1.0
2023-06-22 15:39:41,497 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.60      0.64        78
           2       0.79      0.76      0.78        68
           9       0.48      0.59      0.53        54

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:39:41,498 - INFO - test loss 0.023338880224917508
2023-06-22 15:39:41,498 - INFO - test acc 0.6549999713897705
2023-06-22 15:39:44,328 - INFO - Distilling data from client: Client36
2023-06-22 15:39:44,329 - INFO - train loss: 0.0004675148738797377
2023-06-22 15:39:44,329 - INFO - train acc: 1.0
2023-06-22 15:39:44,383 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.62      0.65        78
           2       0.77      0.78      0.77        68
           9       0.52      0.59      0.56        54

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:39:44,385 - INFO - test loss 0.022970657415584624
2023-06-22 15:39:44,385 - INFO - test acc 0.6649999618530273
2023-06-22 15:39:47,146 - INFO - Distilling data from client: Client36
2023-06-22 15:39:47,147 - INFO - train loss: 0.0004210415263712643
2023-06-22 15:39:47,147 - INFO - train acc: 1.0
2023-06-22 15:39:47,210 - INFO - report:               precision    recall  f1-score   support

           1       0.64      0.58      0.61        78
           2       0.75      0.76      0.76        68
           9       0.49      0.56      0.52        54

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:39:47,211 - INFO - test loss 0.023115141077819146
2023-06-22 15:39:47,212 - INFO - test acc 0.6349999904632568
2023-06-22 15:39:50,087 - INFO - Distilling data from client: Client36
2023-06-22 15:39:50,087 - INFO - train loss: 0.00039843021634535696
2023-06-22 15:39:50,088 - INFO - train acc: 1.0
2023-06-22 15:39:50,150 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.59      0.65        78
           2       0.75      0.79      0.77        68
           9       0.50      0.59      0.54        54

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:39:50,151 - INFO - test loss 0.023490805127821
2023-06-22 15:39:50,151 - INFO - test acc 0.6599999666213989
2023-06-22 15:39:53,145 - INFO - Distilling data from client: Client36
2023-06-22 15:39:53,145 - INFO - train loss: 0.0005350360102004742
2023-06-22 15:39:53,145 - INFO - train acc: 1.0
2023-06-22 15:39:53,207 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.60      0.64        78
           2       0.76      0.78      0.77        68
           9       0.52      0.59      0.55        54

    accuracy                           0.66       200
   macro avg       0.65      0.66      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:39:53,208 - INFO - test loss 0.023583641458006576
2023-06-22 15:39:53,209 - INFO - test acc 0.6599999666213989
2023-06-22 15:39:56,105 - INFO - Distilling data from client: Client36
2023-06-22 15:39:56,106 - INFO - train loss: 0.0004749751441991229
2023-06-22 15:39:56,106 - INFO - train acc: 1.0
2023-06-22 15:39:56,157 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.60      0.66        78
           2       0.77      0.81      0.79        68
           9       0.53      0.63      0.58        54

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:39:56,158 - INFO - test loss 0.02342032456255354
2023-06-22 15:39:56,158 - INFO - test acc 0.6800000071525574
2023-06-22 15:39:58,976 - INFO - Distilling data from client: Client36
2023-06-22 15:39:58,976 - INFO - train loss: 0.0004209965195397741
2023-06-22 15:39:58,977 - INFO - train acc: 1.0
2023-06-22 15:39:59,034 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.58      0.62        78
           2       0.76      0.78      0.77        68
           9       0.53      0.63      0.58        54

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.66      0.66       200

2023-06-22 15:39:59,036 - INFO - test loss 0.023187509860648995
2023-06-22 15:39:59,036 - INFO - test acc 0.6599999666213989
2023-06-22 15:40:01,886 - INFO - Distilling data from client: Client36
2023-06-22 15:40:01,887 - INFO - train loss: 0.00030350386917857866
2023-06-22 15:40:01,887 - INFO - train acc: 1.0
2023-06-22 15:40:01,942 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.64      0.67        78
           2       0.77      0.78      0.77        68
           9       0.53      0.59      0.56        54

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:40:01,942 - INFO - test loss 0.022991030271061648
2023-06-22 15:40:01,943 - INFO - test acc 0.675000011920929
2023-06-22 15:40:04,739 - INFO - Distilling data from client: Client36
2023-06-22 15:40:04,740 - INFO - train loss: 0.0003434706423748598
2023-06-22 15:40:04,740 - INFO - train acc: 1.0
2023-06-22 15:40:04,809 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.58      0.63        78
           2       0.75      0.81      0.78        68
           9       0.50      0.57      0.53        54

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-06-22 15:40:04,809 - INFO - test loss 0.023535365167051125
2023-06-22 15:40:04,809 - INFO - test acc 0.6549999713897705
2023-06-22 15:40:07,520 - INFO - Distilling data from client: Client36
2023-06-22 15:40:07,520 - INFO - train loss: 0.00028630461285715107
2023-06-22 15:40:07,521 - INFO - train acc: 1.0
2023-06-22 15:40:07,574 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.63      0.66        78
           2       0.76      0.79      0.78        68
           9       0.52      0.56      0.54        54

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:40:07,575 - INFO - test loss 0.023322413933573444
2023-06-22 15:40:07,575 - INFO - test acc 0.6649999618530273
2023-06-22 15:40:10,448 - INFO - Distilling data from client: Client36
2023-06-22 15:40:10,448 - INFO - train loss: 0.00025505238199790013
2023-06-22 15:40:10,448 - INFO - train acc: 1.0
2023-06-22 15:40:10,508 - INFO - report:               precision    recall  f1-score   support

           1       0.73      0.63      0.68        78
           2       0.77      0.79      0.78        68
           9       0.51      0.59      0.55        54

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:40:10,508 - INFO - test loss 0.02363412192145632
2023-06-22 15:40:10,509 - INFO - test acc 0.675000011920929
2023-06-22 15:40:13,440 - INFO - Distilling data from client: Client36
2023-06-22 15:40:13,440 - INFO - train loss: 0.00026018554988097367
2023-06-22 15:40:13,441 - INFO - train acc: 1.0
2023-06-22 15:40:13,497 - INFO - report:               precision    recall  f1-score   support

           1       0.67      0.60      0.64        78
           2       0.74      0.78      0.76        68
           9       0.48      0.52      0.50        54

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:40:13,498 - INFO - test loss 0.0244920488529196
2023-06-22 15:40:13,499 - INFO - test acc 0.6399999856948853
2023-06-22 15:40:16,295 - INFO - Distilling data from client: Client36
2023-06-22 15:40:16,296 - INFO - train loss: 0.00033945589732037144
2023-06-22 15:40:16,296 - INFO - train acc: 1.0
2023-06-22 15:40:16,358 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.58      0.62        78
           2       0.77      0.78      0.77        68
           9       0.49      0.57      0.53        54

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:40:16,358 - INFO - test loss 0.02378911441522719
2023-06-22 15:40:16,359 - INFO - test acc 0.6449999809265137
2023-06-22 15:40:19,342 - INFO - Distilling data from client: Client36
2023-06-22 15:40:19,342 - INFO - train loss: 0.0002567739131680609
2023-06-22 15:40:19,343 - INFO - train acc: 1.0
2023-06-22 15:40:19,434 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.59      0.63        78
           2       0.78      0.79      0.79        68
           9       0.52      0.61      0.56        54

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:40:19,438 - INFO - test loss 0.023981188199961433
2023-06-22 15:40:19,440 - INFO - test acc 0.6649999618530273
2023-06-22 15:40:22,385 - INFO - Distilling data from client: Client36
2023-06-22 15:40:22,385 - INFO - train loss: 0.0002597930141085004
2023-06-22 15:40:22,386 - INFO - train acc: 1.0
2023-06-22 15:40:22,442 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.60      0.65        78
           2       0.78      0.79      0.79        68
           9       0.53      0.63      0.58        54

    accuracy                           0.68       200
   macro avg       0.67      0.68      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:40:22,442 - INFO - test loss 0.023470910194837855
2023-06-22 15:40:22,443 - INFO - test acc 0.675000011920929
2023-06-22 15:40:25,309 - INFO - Distilling data from client: Client36
2023-06-22 15:40:25,309 - INFO - train loss: 0.00028492514399342867
2023-06-22 15:40:25,309 - INFO - train acc: 1.0
2023-06-22 15:40:25,360 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.58      0.62        78
           2       0.74      0.78      0.76        68
           9       0.50      0.56      0.53        54

    accuracy                           0.64       200
   macro avg       0.63      0.64      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:40:25,360 - INFO - test loss 0.024203166567768747
2023-06-22 15:40:25,360 - INFO - test acc 0.6399999856948853
2023-06-22 15:40:28,315 - INFO - Distilling data from client: Client36
2023-06-22 15:40:28,316 - INFO - train loss: 0.00023085384723182305
2023-06-22 15:40:28,316 - INFO - train acc: 1.0
2023-06-22 15:40:28,450 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.62      0.65        78
           2       0.75      0.78      0.76        68
           9       0.47      0.52      0.49        54

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:40:28,450 - INFO - test loss 0.023893853137002725
2023-06-22 15:40:28,451 - INFO - test acc 0.6449999809265137
2023-06-22 15:40:31,532 - INFO - Distilling data from client: Client36
2023-06-22 15:40:31,533 - INFO - train loss: 0.00020370533097417893
2023-06-22 15:40:31,533 - INFO - train acc: 1.0
2023-06-22 15:40:31,587 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.63      0.65        78
           2       0.75      0.79      0.77        68
           9       0.52      0.54      0.53        54

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-06-22 15:40:31,588 - INFO - test loss 0.023732333249123556
2023-06-22 15:40:31,588 - INFO - test acc 0.6599999666213989
2023-06-22 15:40:34,682 - INFO - Distilling data from client: Client36
2023-06-22 15:40:34,682 - INFO - train loss: 0.00020256100259625698
2023-06-22 15:40:34,682 - INFO - train acc: 1.0
2023-06-22 15:40:34,747 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.60      0.64        78
           2       0.76      0.79      0.78        68
           9       0.50      0.56      0.53        54

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-06-22 15:40:34,749 - INFO - test loss 0.023834531101692525
2023-06-22 15:40:34,749 - INFO - test acc 0.6549999713897705
2023-06-22 15:40:34,782 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:34,800 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:34,818 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:34,835 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:34,848 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:34,859 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:34,871 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:34,883 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:34,895 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:35,500 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client36//synthetic.png
2023-06-22 15:40:35,527 - INFO - c: 3.0 and total_data_in_this_class: 528
2023-06-22 15:40:35,527 - INFO - c: 7.0 and total_data_in_this_class: 32
2023-06-22 15:40:35,527 - INFO - c: 8.0 and total_data_in_this_class: 239
2023-06-22 15:40:35,527 - INFO - c: 3.0 and total_data_in_this_class: 138
2023-06-22 15:40:35,527 - INFO - c: 7.0 and total_data_in_this_class: 8
2023-06-22 15:40:35,527 - INFO - c: 8.0 and total_data_in_this_class: 54
2023-06-22 15:40:35,565 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005774497985839844 sec
2023-06-22 15:40:35,566 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:40:35,569 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0025701522827148438 sec
2023-06-22 15:40:35,569 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:40:35,586 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01636981964111328 sec
2023-06-22 15:40:35,589 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003592967987060547 sec
2023-06-22 15:40:35,589 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:40:35,591 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0017614364624023438 sec
2023-06-22 15:40:35,592 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:40:35,604 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011942148208618164 sec
2023-06-22 15:40:35,609 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021719932556152344 sec
2023-06-22 15:40:35,611 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00023317337036132812 sec
2023-06-22 15:40:35,612 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0006017684936523438 sec
2023-06-22 15:40:35,615 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003800392150878906 sec
2023-06-22 15:40:35,616 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001964569091796875 sec
2023-06-22 15:40:35,617 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004630088806152344 sec
2023-06-22 15:40:35,618 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042724609375 sec
2023-06-22 15:40:35,619 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003178119659423828 sec
2023-06-22 15:40:35,620 - WARNING - Finished tracing + transforming fn for pjit in 0.00044465065002441406 sec
2023-06-22 15:40:35,622 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0006344318389892578 sec
2023-06-22 15:40:35,624 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0015575885772705078 sec
2023-06-22 15:40:35,626 - WARNING - Finished tracing + transforming fn for pjit in 0.00039696693420410156 sec
2023-06-22 15:40:35,627 - WARNING - Finished tracing + transforming fn for pjit in 0.00047969818115234375 sec
2023-06-22 15:40:35,628 - WARNING - Finished tracing + transforming fn for pjit in 0.0004012584686279297 sec
2023-06-22 15:40:35,629 - WARNING - Finished tracing + transforming fn for pjit in 0.0004620552062988281 sec
2023-06-22 15:40:35,632 - WARNING - Finished tracing + transforming fn for pjit in 0.00036716461181640625 sec
2023-06-22 15:40:35,635 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002949237823486328 sec
2023-06-22 15:40:35,635 - WARNING - Finished tracing + transforming fn for pjit in 0.0003757476806640625 sec
2023-06-22 15:40:35,637 - WARNING - Finished tracing + transforming fn for pjit in 0.0004088878631591797 sec
2023-06-22 15:40:35,643 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006320476531982422 sec
2023-06-22 15:40:35,644 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016894340515136719 sec
2023-06-22 15:40:35,646 - WARNING - Finished tracing + transforming fn for pjit in 0.0003941059112548828 sec
2023-06-22 15:40:35,647 - WARNING - Finished tracing + transforming fn for pjit in 0.0003650188446044922 sec
2023-06-22 15:40:35,648 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005576610565185547 sec
2023-06-22 15:40:35,650 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004634857177734375 sec
2023-06-22 15:40:35,651 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003218650817871094 sec
2023-06-22 15:40:35,652 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004723072052001953 sec
2023-06-22 15:40:35,654 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004143714904785156 sec
2023-06-22 15:40:35,655 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004055500030517578 sec
2023-06-22 15:40:35,656 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006647109985351562 sec
2023-06-22 15:40:35,657 - WARNING - Finished tracing + transforming _where for pjit in 0.001758575439453125 sec
2023-06-22 15:40:35,658 - WARNING - Finished tracing + transforming fn for pjit in 0.0004622936248779297 sec
2023-06-22 15:40:35,659 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004513263702392578 sec
2023-06-22 15:40:35,661 - WARNING - Finished tracing + transforming fn for pjit in 0.00040841102600097656 sec
2023-06-22 15:40:35,662 - WARNING - Finished tracing + transforming fn for pjit in 0.00039458274841308594 sec
2023-06-22 15:40:35,663 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003952980041503906 sec
2023-06-22 15:40:35,665 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004832744598388672 sec
2023-06-22 15:40:35,666 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004706382751464844 sec
2023-06-22 15:40:35,667 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046706199645996094 sec
2023-06-22 15:40:35,669 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004265308380126953 sec
2023-06-22 15:40:35,670 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040984153747558594 sec
2023-06-22 15:40:35,671 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004744529724121094 sec
2023-06-22 15:40:35,672 - WARNING - Finished tracing + transforming _where for pjit in 0.0015461444854736328 sec
2023-06-22 15:40:35,673 - WARNING - Finished tracing + transforming fn for pjit in 0.0004298686981201172 sec
2023-06-22 15:40:35,674 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004668235778808594 sec
2023-06-22 15:40:35,677 - WARNING - Finished tracing + transforming fn for pjit in 0.0003902912139892578 sec
2023-06-22 15:40:35,683 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004570484161376953 sec
2023-06-22 15:40:35,685 - WARNING - Finished tracing + transforming fn for pjit in 0.0006113052368164062 sec
2023-06-22 15:40:35,686 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004954338073730469 sec
2023-06-22 15:40:35,687 - WARNING - Finished tracing + transforming fn for pjit in 0.00040078163146972656 sec
2023-06-22 15:40:35,694 - WARNING - Finished tracing + transforming fn for pjit in 0.00036025047302246094 sec
2023-06-22 15:40:35,697 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002918243408203125 sec
2023-06-22 15:40:35,698 - WARNING - Finished tracing + transforming fn for pjit in 0.0005180835723876953 sec
2023-06-22 15:40:35,699 - WARNING - Finished tracing + transforming fn for pjit in 0.00041794776916503906 sec
2023-06-22 15:40:35,729 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.12026858329772949 sec
2023-06-22 15:40:35,735 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022268295288085938 sec
2023-06-22 15:40:35,735 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002048015594482422 sec
2023-06-22 15:40:35,736 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004801750183105469 sec
2023-06-22 15:40:35,741 - WARNING - Finished tracing + transforming fn for pjit in 0.0004165172576904297 sec
2023-06-22 15:40:35,742 - WARNING - Finished tracing + transforming fn for pjit in 0.00047707557678222656 sec
2023-06-22 15:40:35,744 - WARNING - Finished tracing + transforming fn for pjit in 0.00038933753967285156 sec
2023-06-22 15:40:35,755 - WARNING - Finished tracing + transforming fn for pjit in 0.0004241466522216797 sec
2023-06-22 15:40:35,757 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003848075866699219 sec
2023-06-22 15:40:35,758 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045871734619140625 sec
2023-06-22 15:40:35,759 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00030994415283203125 sec
2023-06-22 15:40:35,761 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006160736083984375 sec
2023-06-22 15:40:35,762 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003952980041503906 sec
2023-06-22 15:40:35,763 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039196014404296875 sec
2023-06-22 15:40:35,765 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005023479461669922 sec
2023-06-22 15:40:35,766 - WARNING - Finished tracing + transforming _where for pjit in 0.0015883445739746094 sec
2023-06-22 15:40:35,767 - WARNING - Finished tracing + transforming fn for pjit in 0.00047779083251953125 sec
2023-06-22 15:40:35,768 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004458427429199219 sec
2023-06-22 15:40:35,770 - WARNING - Finished tracing + transforming fn for pjit in 0.0003886222839355469 sec
2023-06-22 15:40:35,771 - WARNING - Finished tracing + transforming fn for pjit in 0.0005285739898681641 sec
2023-06-22 15:40:35,792 - WARNING - Finished tracing + transforming fn for pjit in 0.0003886222839355469 sec
2023-06-22 15:40:35,826 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.09564852714538574 sec
2023-06-22 15:40:35,828 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022029876708984375 sec
2023-06-22 15:40:35,830 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002467632293701172 sec
2023-06-22 15:40:35,831 - WARNING - Finished tracing + transforming _where for pjit in 0.0011408329010009766 sec
2023-06-22 15:40:35,832 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005280971527099609 sec
2023-06-22 15:40:35,833 - WARNING - Finished tracing + transforming trace for pjit in 0.004692792892456055 sec
2023-06-22 15:40:35,837 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00020956993103027344 sec
2023-06-22 15:40:35,839 - WARNING - Finished tracing + transforming tril for pjit in 0.0010950565338134766 sec
2023-06-22 15:40:35,839 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.003372669219970703 sec
2023-06-22 15:40:35,841 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019693374633789062 sec
2023-06-22 15:40:35,842 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019216537475585938 sec
2023-06-22 15:40:35,845 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.002489805221557617 sec
2023-06-22 15:40:35,852 - WARNING - Finished tracing + transforming _solve for pjit in 0.01679396629333496 sec
2023-06-22 15:40:35,853 - WARNING - Finished tracing + transforming dot for pjit in 0.0005357265472412109 sec
2023-06-22 15:40:35,858 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.25136780738830566 sec
2023-06-22 15:40:35,861 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:40:35,920 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05846524238586426 sec
2023-06-22 15:40:35,920 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:40:36,083 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.16230463981628418 sec
2023-06-22 15:40:36,090 - INFO - initial test loss: 0.028384345339379585
2023-06-22 15:40:36,090 - INFO - initial test acc: 0.5949999690055847
2023-06-22 15:40:36,098 - WARNING - Finished tracing + transforming dot for pjit in 0.0006058216094970703 sec
2023-06-22 15:40:36,100 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004978179931640625 sec
2023-06-22 15:40:36,102 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006432533264160156 sec
2023-06-22 15:40:36,102 - WARNING - Finished tracing + transforming _mean for pjit in 0.0017807483673095703 sec
2023-06-22 15:40:36,104 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0003523826599121094 sec
2023-06-22 15:40:36,105 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0003218650817871094 sec
2023-06-22 15:40:36,106 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040984153747558594 sec
2023-06-22 15:40:36,108 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006136894226074219 sec
2023-06-22 15:40:36,109 - WARNING - Finished tracing + transforming _mean for pjit in 0.0018765926361083984 sec
2023-06-22 15:40:36,110 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.017189979553222656 sec
2023-06-22 15:40:36,124 - WARNING - Finished tracing + transforming fn for pjit in 0.00045752525329589844 sec
2023-06-22 15:40:36,125 - WARNING - Finished tracing + transforming fn for pjit in 0.0004565715789794922 sec
2023-06-22 15:40:36,126 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003771781921386719 sec
2023-06-22 15:40:36,128 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00047397613525390625 sec
2023-06-22 15:40:36,128 - WARNING - Finished tracing + transforming _where for pjit in 0.0015361309051513672 sec
2023-06-22 15:40:36,142 - WARNING - Finished tracing + transforming fn for pjit in 0.0004456043243408203 sec
2023-06-22 15:40:36,143 - WARNING - Finished tracing + transforming fn for pjit in 0.0004634857177734375 sec
2023-06-22 15:40:36,144 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00038552284240722656 sec
2023-06-22 15:40:36,146 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005803108215332031 sec
2023-06-22 15:40:36,147 - WARNING - Finished tracing + transforming _where for pjit in 0.0016531944274902344 sec
2023-06-22 15:40:36,198 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00036787986755371094 sec
2023-06-22 15:40:36,278 - WARNING - Finished tracing + transforming fn for pjit in 0.0004291534423828125 sec
2023-06-22 15:40:36,280 - WARNING - Finished tracing + transforming fn for pjit in 0.0003826618194580078 sec
2023-06-22 15:40:36,281 - WARNING - Finished tracing + transforming square for pjit in 0.0003120899200439453 sec
2023-06-22 15:40:36,285 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004057884216308594 sec
2023-06-22 15:40:36,288 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0009295940399169922 sec
2023-06-22 15:40:36,289 - WARNING - Finished tracing + transforming fn for pjit in 0.0004467964172363281 sec
2023-06-22 15:40:36,290 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00041747093200683594 sec
2023-06-22 15:40:36,291 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004012584686279297 sec
2023-06-22 15:40:36,293 - WARNING - Finished tracing + transforming fn for pjit in 0.0004830360412597656 sec
2023-06-22 15:40:36,294 - WARNING - Finished tracing + transforming fn for pjit in 0.00039505958557128906 sec
2023-06-22 15:40:36,295 - WARNING - Finished tracing + transforming square for pjit in 0.00032067298889160156 sec
2023-06-22 15:40:36,298 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00037217140197753906 sec
2023-06-22 15:40:36,301 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003256797790527344 sec
2023-06-22 15:40:36,302 - WARNING - Finished tracing + transforming fn for pjit in 0.0004432201385498047 sec
2023-06-22 15:40:36,303 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00040984153747558594 sec
2023-06-22 15:40:36,304 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039958953857421875 sec
2023-06-22 15:40:36,306 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2142326831817627 sec
2023-06-22 15:40:36,312 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,10]), ShapedArray(float32[63,10]), ShapedArray(float32[63,10]), ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,10]), ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:40:36,415 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10263681411743164 sec
2023-06-22 15:40:36,415 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:40:36,780 - WARNING - Finished XLA compilation of jit(update_fn) in 0.36379528045654297 sec
2023-06-22 15:40:36,915 - INFO - Distilling data from client: Client37
2023-06-22 15:40:36,915 - INFO - train loss: 0.011848838710817572
2023-06-22 15:40:36,915 - INFO - train acc: 0.9206349849700928
2023-06-22 15:40:36,934 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.80      0.85       138
           7       0.24      0.62      0.34         8
           8       0.76      0.81      0.79        54

    accuracy                           0.80       200
   macro avg       0.64      0.75      0.66       200
weighted avg       0.84      0.80      0.81       200

2023-06-22 15:40:36,934 - INFO - test loss 0.018275942823502152
2023-06-22 15:40:36,934 - INFO - test acc 0.7949999570846558
2023-06-22 15:40:37,094 - INFO - Distilling data from client: Client37
2023-06-22 15:40:37,094 - INFO - train loss: 0.011122002702238867
2023-06-22 15:40:37,094 - INFO - train acc: 0.9365079998970032
2023-06-22 15:40:37,105 - INFO - report:               precision    recall  f1-score   support

           3       0.89      0.82      0.85       138
           7       0.31      0.50      0.38         8
           8       0.70      0.78      0.74        54

    accuracy                           0.80       200
   macro avg       0.63      0.70      0.66       200
weighted avg       0.82      0.80      0.80       200

2023-06-22 15:40:37,105 - INFO - test loss 0.017883494624213393
2023-06-22 15:40:37,105 - INFO - test acc 0.7949999570846558
2023-06-22 15:40:37,267 - INFO - Distilling data from client: Client37
2023-06-22 15:40:37,268 - INFO - train loss: 0.009136510704264274
2023-06-22 15:40:37,268 - INFO - train acc: 0.9365079998970032
2023-06-22 15:40:37,287 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.87      0.89       138
           7       0.45      0.62      0.53         8
           8       0.77      0.80      0.78        54

    accuracy                           0.84       200
   macro avg       0.71      0.76      0.73       200
weighted avg       0.85      0.84      0.84       200

2023-06-22 15:40:37,288 - INFO - test loss 0.016569563293198034
2023-06-22 15:40:37,288 - INFO - test acc 0.8399999737739563
2023-06-22 15:40:37,432 - INFO - Distilling data from client: Client37
2023-06-22 15:40:37,432 - INFO - train loss: 0.007428933661873303
2023-06-22 15:40:37,433 - INFO - train acc: 0.968254029750824
2023-06-22 15:40:37,444 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.87      0.89       138
           7       0.36      0.50      0.42         8
           8       0.77      0.81      0.79        54

    accuracy                           0.84       200
   macro avg       0.68      0.73      0.70       200
weighted avg       0.85      0.84      0.84       200

2023-06-22 15:40:37,444 - INFO - test loss 0.01588245431569291
2023-06-22 15:40:37,444 - INFO - test acc 0.8399999737739563
2023-06-22 15:40:37,607 - INFO - Distilling data from client: Client37
2023-06-22 15:40:37,607 - INFO - train loss: 0.009497751664396837
2023-06-22 15:40:37,607 - INFO - train acc: 0.9206349849700928
2023-06-22 15:40:37,618 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.86      0.88       138
           7       0.60      0.38      0.46         8
           8       0.69      0.83      0.76        54

    accuracy                           0.83       200
   macro avg       0.73      0.69      0.70       200
weighted avg       0.84      0.83      0.83       200

2023-06-22 15:40:37,619 - INFO - test loss 0.016338695632629668
2023-06-22 15:40:37,619 - INFO - test acc 0.8299999833106995
2023-06-22 15:40:37,773 - INFO - Distilling data from client: Client37
2023-06-22 15:40:37,774 - INFO - train loss: 0.007783778115010007
2023-06-22 15:40:37,774 - INFO - train acc: 0.968254029750824
2023-06-22 15:40:37,785 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.86      0.88       138
           7       0.33      0.50      0.40         8
           8       0.77      0.80      0.78        54

    accuracy                           0.83       200
   macro avg       0.67      0.72      0.69       200
weighted avg       0.84      0.83      0.84       200

2023-06-22 15:40:37,785 - INFO - test loss 0.01599334085212641
2023-06-22 15:40:37,785 - INFO - test acc 0.8299999833106995
2023-06-22 15:40:37,948 - INFO - Distilling data from client: Client37
2023-06-22 15:40:37,948 - INFO - train loss: 0.010338387674188827
2023-06-22 15:40:37,948 - INFO - train acc: 0.9047619700431824
2023-06-22 15:40:37,961 - INFO - report:               precision    recall  f1-score   support

           3       0.92      0.83      0.87       138
           7       0.27      0.50      0.35         8
           8       0.77      0.85      0.81        54

    accuracy                           0.82       200
   macro avg       0.65      0.73      0.68       200
weighted avg       0.85      0.82      0.84       200

2023-06-22 15:40:37,961 - INFO - test loss 0.01622791277216285
2023-06-22 15:40:37,961 - INFO - test acc 0.824999988079071
2023-06-22 15:40:38,132 - INFO - Distilling data from client: Client37
2023-06-22 15:40:38,132 - INFO - train loss: 0.008700524121083087
2023-06-22 15:40:38,132 - INFO - train acc: 0.9365079998970032
2023-06-22 15:40:38,142 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.88      0.89       138
           7       0.33      0.38      0.35         8
           8       0.77      0.80      0.78        54

    accuracy                           0.83       200
   macro avg       0.67      0.68      0.67       200
weighted avg       0.84      0.83      0.84       200

2023-06-22 15:40:38,143 - INFO - test loss 0.016165355356513967
2023-06-22 15:40:38,143 - INFO - test acc 0.8349999785423279
2023-06-22 15:40:38,286 - INFO - Distilling data from client: Client37
2023-06-22 15:40:38,286 - INFO - train loss: 0.006906668910539294
2023-06-22 15:40:38,286 - INFO - train acc: 0.9841270446777344
2023-06-22 15:40:38,297 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.87      0.88       138
           7       0.30      0.38      0.33         8
           8       0.79      0.81      0.80        54

    accuracy                           0.83       200
   macro avg       0.66      0.69      0.67       200
weighted avg       0.84      0.83      0.84       200

2023-06-22 15:40:38,297 - INFO - test loss 0.01543894401935809
2023-06-22 15:40:38,297 - INFO - test acc 0.8349999785423279
2023-06-22 15:40:38,455 - INFO - Distilling data from client: Client37
2023-06-22 15:40:38,456 - INFO - train loss: 0.006861779114124869
2023-06-22 15:40:38,456 - INFO - train acc: 0.9523810148239136
2023-06-22 15:40:38,476 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.91      0.91       138
           7       0.40      0.25      0.31         8
           8       0.78      0.83      0.80        54

    accuracy                           0.86       200
   macro avg       0.70      0.66      0.67       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:40:38,476 - INFO - test loss 0.014483137251759462
2023-06-22 15:40:38,476 - INFO - test acc 0.85999995470047
2023-06-22 15:40:38,630 - INFO - Distilling data from client: Client37
2023-06-22 15:40:38,630 - INFO - train loss: 0.008749701477405399
2023-06-22 15:40:38,630 - INFO - train acc: 0.888888955116272
2023-06-22 15:40:38,640 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.88      0.89       138
           7       0.44      0.50      0.47         8
           8       0.77      0.81      0.79        54

    accuracy                           0.84       200
   macro avg       0.71      0.73      0.72       200
weighted avg       0.85      0.84      0.85       200

2023-06-22 15:40:38,640 - INFO - test loss 0.015099554645927224
2023-06-22 15:40:38,640 - INFO - test acc 0.8449999690055847
2023-06-22 15:40:38,805 - INFO - Distilling data from client: Client37
2023-06-22 15:40:38,805 - INFO - train loss: 0.00755013686811061
2023-06-22 15:40:38,805 - INFO - train acc: 0.9523810148239136
2023-06-22 15:40:38,815 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.90      0.90       138
           7       0.14      0.12      0.13         8
           8       0.80      0.83      0.82        54

    accuracy                           0.85       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:40:38,816 - INFO - test loss 0.015280548761275537
2023-06-22 15:40:38,816 - INFO - test acc 0.8499999642372131
2023-06-22 15:40:38,974 - INFO - Distilling data from client: Client37
2023-06-22 15:40:38,974 - INFO - train loss: 0.007057448325737372
2023-06-22 15:40:38,974 - INFO - train acc: 0.968254029750824
2023-06-22 15:40:38,984 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.88      0.89       138
           7       0.17      0.12      0.14         8
           8       0.78      0.83      0.80        54

    accuracy                           0.84       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:40:38,984 - INFO - test loss 0.0153429965790076
2023-06-22 15:40:38,984 - INFO - test acc 0.8399999737739563
2023-06-22 15:40:39,140 - INFO - Distilling data from client: Client37
2023-06-22 15:40:39,140 - INFO - train loss: 0.008061048413928273
2023-06-22 15:40:39,140 - INFO - train acc: 0.968254029750824
2023-06-22 15:40:39,151 - INFO - report:               precision    recall  f1-score   support

           3       0.89      0.92      0.91       138
           7       0.50      0.38      0.43         8
           8       0.81      0.78      0.79        54

    accuracy                           0.86       200
   macro avg       0.73      0.69      0.71       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:40:39,151 - INFO - test loss 0.01469040276257375
2023-06-22 15:40:39,151 - INFO - test acc 0.85999995470047
2023-06-22 15:40:39,318 - INFO - Distilling data from client: Client37
2023-06-22 15:40:39,318 - INFO - train loss: 0.010185787156407292
2023-06-22 15:40:39,318 - INFO - train acc: 0.8730159401893616
2023-06-22 15:40:39,329 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.87      0.88       138
           7       0.38      0.38      0.38         8
           8       0.76      0.81      0.79        54

    accuracy                           0.83       200
   macro avg       0.68      0.69      0.68       200
weighted avg       0.84      0.83      0.84       200

2023-06-22 15:40:39,329 - INFO - test loss 0.014839603914763274
2023-06-22 15:40:39,330 - INFO - test acc 0.8349999785423279
2023-06-22 15:40:39,485 - INFO - Distilling data from client: Client37
2023-06-22 15:40:39,485 - INFO - train loss: 0.006988194562533513
2023-06-22 15:40:39,485 - INFO - train acc: 0.968254029750824
2023-06-22 15:40:39,495 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.87      0.88       138
           7       0.36      0.50      0.42         8
           8       0.78      0.80      0.79        54

    accuracy                           0.83       200
   macro avg       0.68      0.72      0.70       200
weighted avg       0.84      0.83      0.84       200

2023-06-22 15:40:39,496 - INFO - test loss 0.015617834540773829
2023-06-22 15:40:39,496 - INFO - test acc 0.8349999785423279
2023-06-22 15:40:39,664 - INFO - Distilling data from client: Client37
2023-06-22 15:40:39,665 - INFO - train loss: 0.007823920217572193
2023-06-22 15:40:39,665 - INFO - train acc: 0.9523810148239136
2023-06-22 15:40:39,676 - INFO - report:               precision    recall  f1-score   support

           3       0.89      0.89      0.89       138
           7       0.44      0.50      0.47         8
           8       0.77      0.76      0.77        54

    accuracy                           0.84       200
   macro avg       0.70      0.72      0.71       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:40:39,677 - INFO - test loss 0.015726810055168707
2023-06-22 15:40:39,677 - INFO - test acc 0.8399999737739563
2023-06-22 15:40:39,838 - INFO - Distilling data from client: Client37
2023-06-22 15:40:39,838 - INFO - train loss: 0.007401917512981855
2023-06-22 15:40:39,838 - INFO - train acc: 0.9523810148239136
2023-06-22 15:40:39,849 - INFO - report:               precision    recall  f1-score   support

           3       0.89      0.86      0.88       138
           7       0.50      0.50      0.50         8
           8       0.72      0.78      0.75        54

    accuracy                           0.82       200
   macro avg       0.70      0.71      0.71       200
weighted avg       0.83      0.82      0.83       200

2023-06-22 15:40:39,849 - INFO - test loss 0.015490949951537629
2023-06-22 15:40:39,849 - INFO - test acc 0.824999988079071
2023-06-22 15:40:40,020 - INFO - Distilling data from client: Client37
2023-06-22 15:40:40,020 - INFO - train loss: 0.00827783979131067
2023-06-22 15:40:40,020 - INFO - train acc: 0.9523810148239136
2023-06-22 15:40:40,031 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.88      0.89       138
           7       0.38      0.38      0.38         8
           8       0.75      0.80      0.77        54

    accuracy                           0.84       200
   macro avg       0.68      0.69      0.68       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:40:40,031 - INFO - test loss 0.015437332762675563
2023-06-22 15:40:40,031 - INFO - test acc 0.8399999737739563
2023-06-22 15:40:40,184 - INFO - Distilling data from client: Client37
2023-06-22 15:40:40,184 - INFO - train loss: 0.0070157306005681295
2023-06-22 15:40:40,184 - INFO - train acc: 1.0
2023-06-22 15:40:40,195 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.91      0.91       138
           7       0.38      0.38      0.38         8
           8       0.80      0.80      0.80        54

    accuracy                           0.85       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:40:40,195 - INFO - test loss 0.014347665707313486
2023-06-22 15:40:40,196 - INFO - test acc 0.8549999594688416
2023-06-22 15:40:40,350 - INFO - Distilling data from client: Client37
2023-06-22 15:40:40,350 - INFO - train loss: 0.006029273953868536
2023-06-22 15:40:40,350 - INFO - train acc: 0.9841270446777344
2023-06-22 15:40:40,361 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.88      0.89       138
           7       0.31      0.50      0.38         8
           8       0.81      0.81      0.81        54

    accuracy                           0.84       200
   macro avg       0.68      0.73      0.70       200
weighted avg       0.86      0.84      0.85       200

2023-06-22 15:40:40,361 - INFO - test loss 0.015994472175780365
2023-06-22 15:40:40,361 - INFO - test acc 0.8449999690055847
2023-06-22 15:40:40,529 - INFO - Distilling data from client: Client37
2023-06-22 15:40:40,530 - INFO - train loss: 0.006296264926126264
2023-06-22 15:40:40,530 - INFO - train acc: 0.968254029750824
2023-06-22 15:40:40,551 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.92      0.91       138
           7       0.43      0.38      0.40         8
           8       0.85      0.81      0.83        54

    accuracy                           0.87       200
   macro avg       0.73      0.70      0.71       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:40:40,551 - INFO - test loss 0.013909255066854255
2023-06-22 15:40:40,551 - INFO - test acc 0.8700000047683716
2023-06-22 15:40:40,717 - INFO - Distilling data from client: Client37
2023-06-22 15:40:40,717 - INFO - train loss: 0.008269448155992395
2023-06-22 15:40:40,717 - INFO - train acc: 0.9365079998970032
2023-06-22 15:40:40,729 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.84      0.87       138
           7       0.31      0.50      0.38         8
           8       0.75      0.81      0.78        54

    accuracy                           0.82       200
   macro avg       0.65      0.72      0.68       200
weighted avg       0.84      0.82      0.83       200

2023-06-22 15:40:40,729 - INFO - test loss 0.016410002903403595
2023-06-22 15:40:40,730 - INFO - test acc 0.8199999928474426
2023-06-22 15:40:40,888 - INFO - Distilling data from client: Client37
2023-06-22 15:40:40,888 - INFO - train loss: 0.006298264288489124
2023-06-22 15:40:40,888 - INFO - train acc: 0.9523810148239136
2023-06-22 15:40:40,899 - INFO - report:               precision    recall  f1-score   support

           3       0.89      0.91      0.90       138
           7       0.44      0.50      0.47         8
           8       0.84      0.78      0.81        54

    accuracy                           0.86       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:40:40,899 - INFO - test loss 0.015183770852635628
2023-06-22 15:40:40,900 - INFO - test acc 0.85999995470047
2023-06-22 15:40:41,064 - INFO - Distilling data from client: Client37
2023-06-22 15:40:41,064 - INFO - train loss: 0.006790915965704531
2023-06-22 15:40:41,065 - INFO - train acc: 0.9523810148239136
2023-06-22 15:40:41,075 - INFO - report:               precision    recall  f1-score   support

           3       0.88      0.88      0.88       138
           7       0.33      0.50      0.40         8
           8       0.82      0.76      0.79        54

    accuracy                           0.83       200
   macro avg       0.68      0.71      0.69       200
weighted avg       0.84      0.83      0.84       200

2023-06-22 15:40:41,075 - INFO - test loss 0.015581084747644632
2023-06-22 15:40:41,075 - INFO - test acc 0.8349999785423279
2023-06-22 15:40:41,083 - WARNING - Finished tracing + transforming jit(gather) in 0.0005097389221191406 sec
2023-06-22 15:40:41,083 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[63,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:40:41,086 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0028634071350097656 sec
2023-06-22 15:40:41,087 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:40:41,107 - WARNING - Finished XLA compilation of jit(gather) in 0.019920825958251953 sec
2023-06-22 15:40:41,126 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:41,142 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:41,159 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:41,175 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:41,191 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:41,207 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:41,223 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:41,235 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:41,246 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:40:41,939 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client37//synthetic.png
2023-06-22 15:40:41,965 - INFO - c: 0.0 and total_data_in_this_class: 268
2023-06-22 15:40:41,965 - INFO - c: 2.0 and total_data_in_this_class: 269
2023-06-22 15:40:41,966 - INFO - c: 8.0 and total_data_in_this_class: 262
2023-06-22 15:40:41,966 - INFO - c: 0.0 and total_data_in_this_class: 65
2023-06-22 15:40:41,966 - INFO - c: 2.0 and total_data_in_this_class: 64
2023-06-22 15:40:41,966 - INFO - c: 8.0 and total_data_in_this_class: 71
2023-06-22 15:40:43,129 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 1.1033337116241455 sec
2023-06-22 15:40:43,235 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.10411405563354492 sec
2023-06-22 15:40:43,246 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 1.2225990295410156 sec
2023-06-22 15:40:43,251 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:40:43,316 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.06499552726745605 sec
2023-06-22 15:40:43,316 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:40:43,551 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.23441219329833984 sec
2023-06-22 15:40:43,650 - INFO - initial test loss: 0.024532207311906216
2023-06-22 15:40:43,651 - INFO - initial test acc: 0.6349999904632568
2023-06-22 15:40:43,670 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012826204299926758 sec
2023-06-22 15:40:43,882 - WARNING - Finished tracing + transforming update_fn for pjit in 0.22591638565063477 sec
2023-06-22 15:40:43,888 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:40:44,029 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.14058375358581543 sec
2023-06-22 15:40:44,029 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:40:44,493 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4630157947540283 sec
2023-06-22 15:40:47,616 - INFO - Distilling data from client: Client38
2023-06-22 15:40:47,617 - INFO - train loss: 0.002790189894200762
2023-06-22 15:40:47,618 - INFO - train acc: 0.9961904883384705
2023-06-22 15:40:47,854 - INFO - report:               precision    recall  f1-score   support

           0       0.54      0.54      0.54        65
           2       0.69      0.66      0.67        64
           8       0.72      0.75      0.73        71

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:40:47,854 - INFO - test loss 0.025212396182037057
2023-06-22 15:40:47,854 - INFO - test acc 0.6499999761581421
2023-06-22 15:40:50,966 - INFO - Distilling data from client: Client38
2023-06-22 15:40:50,966 - INFO - train loss: 0.001728448483939683
2023-06-22 15:40:50,966 - INFO - train acc: 0.9980952143669128
2023-06-22 15:40:51,029 - INFO - report:               precision    recall  f1-score   support

           0       0.52      0.52      0.52        65
           2       0.66      0.66      0.66        64
           8       0.72      0.72      0.72        71

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:40:51,030 - INFO - test loss 0.02549634565618599
2023-06-22 15:40:51,030 - INFO - test acc 0.6349999904632568
2023-06-22 15:40:54,112 - INFO - Distilling data from client: Client38
2023-06-22 15:40:54,112 - INFO - train loss: 0.0013413708883286332
2023-06-22 15:40:54,112 - INFO - train acc: 0.9980952143669128
2023-06-22 15:40:54,212 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.55      0.55        65
           2       0.65      0.66      0.65        64
           8       0.74      0.73      0.74        71

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:40:54,213 - INFO - test loss 0.024815164300580072
2023-06-22 15:40:54,213 - INFO - test acc 0.6499999761581421
2023-06-22 15:40:57,369 - INFO - Distilling data from client: Client38
2023-06-22 15:40:57,369 - INFO - train loss: 0.0011721051827328717
2023-06-22 15:40:57,369 - INFO - train acc: 1.0
2023-06-22 15:40:57,544 - INFO - report:               precision    recall  f1-score   support

           0       0.56      0.54      0.55        65
           2       0.67      0.70      0.69        64
           8       0.73      0.72      0.72        71

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.66      0.65       200

2023-06-22 15:40:57,545 - INFO - test loss 0.025711646040474486
2023-06-22 15:40:57,545 - INFO - test acc 0.6549999713897705
2023-06-22 15:41:00,722 - INFO - Distilling data from client: Client38
2023-06-22 15:41:00,723 - INFO - train loss: 0.0008477801655286726
2023-06-22 15:41:00,723 - INFO - train acc: 1.0
2023-06-22 15:41:00,863 - INFO - report:               precision    recall  f1-score   support

           0       0.58      0.57      0.57        65
           2       0.68      0.67      0.68        64
           8       0.74      0.76      0.75        71

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.67      0.67      0.67       200

2023-06-22 15:41:00,863 - INFO - test loss 0.025122953593938006
2023-06-22 15:41:00,863 - INFO - test acc 0.6699999570846558
2023-06-22 15:41:04,068 - INFO - Distilling data from client: Client38
2023-06-22 15:41:04,068 - INFO - train loss: 0.0009557494003824943
2023-06-22 15:41:04,068 - INFO - train acc: 1.0
2023-06-22 15:41:04,130 - INFO - report:               precision    recall  f1-score   support

           0       0.56      0.54      0.55        65
           2       0.63      0.67      0.65        64
           8       0.75      0.73      0.74        71

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:41:04,131 - INFO - test loss 0.025866100737052877
2023-06-22 15:41:04,131 - INFO - test acc 0.6499999761581421
2023-06-22 15:41:07,417 - INFO - Distilling data from client: Client38
2023-06-22 15:41:07,417 - INFO - train loss: 0.0008620045997280056
2023-06-22 15:41:07,417 - INFO - train acc: 1.0
2023-06-22 15:41:07,475 - INFO - report:               precision    recall  f1-score   support

           0       0.51      0.48      0.49        65
           2       0.64      0.69      0.66        64
           8       0.69      0.68      0.68        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-06-22 15:41:07,475 - INFO - test loss 0.02625784757140678
2023-06-22 15:41:07,475 - INFO - test acc 0.6150000095367432
2023-06-22 15:41:10,710 - INFO - Distilling data from client: Client38
2023-06-22 15:41:10,711 - INFO - train loss: 0.0007427760782786123
2023-06-22 15:41:10,711 - INFO - train acc: 1.0
2023-06-22 15:41:10,784 - INFO - report:               precision    recall  f1-score   support

           0       0.52      0.51      0.52        65
           2       0.62      0.66      0.64        64
           8       0.71      0.69      0.70        71

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.62      0.62      0.62       200

2023-06-22 15:41:10,786 - INFO - test loss 0.025648851421907863
2023-06-22 15:41:10,786 - INFO - test acc 0.6200000047683716
2023-06-22 15:41:14,026 - INFO - Distilling data from client: Client38
2023-06-22 15:41:14,026 - INFO - train loss: 0.0006799276838624297
2023-06-22 15:41:14,027 - INFO - train acc: 1.0
2023-06-22 15:41:14,151 - INFO - report:               precision    recall  f1-score   support

           0       0.58      0.52      0.55        65
           2       0.64      0.69      0.66        64
           8       0.74      0.75      0.74        71

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.66      0.65       200

2023-06-22 15:41:14,154 - INFO - test loss 0.025380020704119436
2023-06-22 15:41:14,155 - INFO - test acc 0.6549999713897705
2023-06-22 15:41:17,167 - INFO - Distilling data from client: Client38
2023-06-22 15:41:17,167 - INFO - train loss: 0.0007608059736061207
2023-06-22 15:41:17,168 - INFO - train acc: 1.0
2023-06-22 15:41:17,284 - INFO - report:               precision    recall  f1-score   support

           0       0.51      0.46      0.48        65
           2       0.63      0.67      0.65        64
           8       0.71      0.73      0.72        71

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.62      0.62      0.62       200

2023-06-22 15:41:17,290 - INFO - test loss 0.025844020135329517
2023-06-22 15:41:17,291 - INFO - test acc 0.625
2023-06-22 15:41:20,498 - INFO - Distilling data from client: Client38
2023-06-22 15:41:20,498 - INFO - train loss: 0.0005484953759193857
2023-06-22 15:41:20,498 - INFO - train acc: 1.0
2023-06-22 15:41:20,636 - INFO - report:               precision    recall  f1-score   support

           0       0.54      0.54      0.54        65
           2       0.64      0.69      0.66        64
           8       0.74      0.69      0.72        71

    accuracy                           0.64       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:41:20,643 - INFO - test loss 0.025515839262059293
2023-06-22 15:41:20,644 - INFO - test acc 0.6399999856948853
2023-06-22 15:41:23,839 - INFO - Distilling data from client: Client38
2023-06-22 15:41:23,839 - INFO - train loss: 0.0005976998443195341
2023-06-22 15:41:23,839 - INFO - train acc: 1.0
2023-06-22 15:41:23,892 - INFO - report:               precision    recall  f1-score   support

           0       0.52      0.51      0.51        65
           2       0.64      0.67      0.66        64
           8       0.72      0.70      0.71        71

    accuracy                           0.63       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.63      0.63       200

2023-06-22 15:41:23,893 - INFO - test loss 0.025376586624781823
2023-06-22 15:41:23,893 - INFO - test acc 0.6299999952316284
2023-06-22 15:41:27,156 - INFO - Distilling data from client: Client38
2023-06-22 15:41:27,156 - INFO - train loss: 0.0005906125894443998
2023-06-22 15:41:27,156 - INFO - train acc: 1.0
2023-06-22 15:41:27,204 - INFO - report:               precision    recall  f1-score   support

           0       0.56      0.54      0.55        65
           2       0.66      0.64      0.65        64
           8       0.72      0.76      0.74        71

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:41:27,204 - INFO - test loss 0.02648851807610856
2023-06-22 15:41:27,204 - INFO - test acc 0.6499999761581421
2023-06-22 15:41:30,316 - INFO - Distilling data from client: Client38
2023-06-22 15:41:30,316 - INFO - train loss: 0.0004995687230067238
2023-06-22 15:41:30,317 - INFO - train acc: 1.0
2023-06-22 15:41:30,401 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.51      0.53        65
           2       0.65      0.67      0.66        64
           8       0.70      0.73      0.72        71

    accuracy                           0.64       200
   macro avg       0.63      0.64      0.64       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:41:30,404 - INFO - test loss 0.02590244111273819
2023-06-22 15:41:30,404 - INFO - test acc 0.6399999856948853
2023-06-22 15:41:33,416 - INFO - Distilling data from client: Client38
2023-06-22 15:41:33,417 - INFO - train loss: 0.0005099505940318063
2023-06-22 15:41:33,417 - INFO - train acc: 1.0
2023-06-22 15:41:33,464 - INFO - report:               precision    recall  f1-score   support

           0       0.52      0.49      0.51        65
           2       0.64      0.66      0.65        64
           8       0.73      0.75      0.74        71

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.64      0.63       200

2023-06-22 15:41:33,464 - INFO - test loss 0.025864678171030536
2023-06-22 15:41:33,465 - INFO - test acc 0.6349999904632568
2023-06-22 15:41:36,272 - INFO - Distilling data from client: Client38
2023-06-22 15:41:36,272 - INFO - train loss: 0.0004956322880737048
2023-06-22 15:41:36,272 - INFO - train acc: 1.0
2023-06-22 15:41:36,335 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.55      0.55        65
           2       0.64      0.70      0.67        64
           8       0.75      0.68      0.71        71

    accuracy                           0.65       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:41:36,337 - INFO - test loss 0.02599433138111127
2023-06-22 15:41:36,337 - INFO - test acc 0.6449999809265137
2023-06-22 15:41:39,164 - INFO - Distilling data from client: Client38
2023-06-22 15:41:39,165 - INFO - train loss: 0.0004962321442693439
2023-06-22 15:41:39,165 - INFO - train acc: 1.0
2023-06-22 15:41:39,232 - INFO - report:               precision    recall  f1-score   support

           0       0.56      0.54      0.55        65
           2       0.65      0.69      0.67        64
           8       0.76      0.75      0.75        71

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.66      0.66      0.66       200

2023-06-22 15:41:39,233 - INFO - test loss 0.026000082783875143
2023-06-22 15:41:39,234 - INFO - test acc 0.6599999666213989
2023-06-22 15:41:42,065 - INFO - Distilling data from client: Client38
2023-06-22 15:41:42,066 - INFO - train loss: 0.00046081650193336424
2023-06-22 15:41:42,066 - INFO - train acc: 1.0
2023-06-22 15:41:42,112 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.55      0.55        65
           2       0.64      0.67      0.66        64
           8       0.74      0.70      0.72        71

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:41:42,112 - INFO - test loss 0.02569944509727924
2023-06-22 15:41:42,112 - INFO - test acc 0.6449999809265137
2023-06-22 15:41:44,913 - INFO - Distilling data from client: Client38
2023-06-22 15:41:44,914 - INFO - train loss: 0.00043117733449247065
2023-06-22 15:41:44,914 - INFO - train acc: 1.0
2023-06-22 15:41:44,969 - INFO - report:               precision    recall  f1-score   support

           0       0.51      0.54      0.53        65
           2       0.62      0.67      0.65        64
           8       0.73      0.65      0.69        71

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.63      0.62      0.62       200

2023-06-22 15:41:44,970 - INFO - test loss 0.026325736108963577
2023-06-22 15:41:44,970 - INFO - test acc 0.6200000047683716
2023-06-22 15:41:47,718 - INFO - Distilling data from client: Client38
2023-06-22 15:41:47,718 - INFO - train loss: 0.0004638925629253135
2023-06-22 15:41:47,719 - INFO - train acc: 1.0
2023-06-22 15:41:47,771 - INFO - report:               precision    recall  f1-score   support

           0       0.50      0.52      0.51        65
           2       0.63      0.64      0.64        64
           8       0.72      0.68      0.70        71

    accuracy                           0.61       200
   macro avg       0.62      0.61      0.61       200
weighted avg       0.62      0.61      0.62       200

2023-06-22 15:41:47,772 - INFO - test loss 0.02569060230538983
2023-06-22 15:41:47,772 - INFO - test acc 0.6150000095367432
2023-06-22 15:41:50,573 - INFO - Distilling data from client: Client38
2023-06-22 15:41:50,573 - INFO - train loss: 0.0004107730367040301
2023-06-22 15:41:50,573 - INFO - train acc: 1.0
2023-06-22 15:41:50,623 - INFO - report:               precision    recall  f1-score   support

           0       0.52      0.51      0.52        65
           2       0.63      0.66      0.64        64
           8       0.71      0.70      0.71        71

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.62      0.62      0.62       200

2023-06-22 15:41:50,623 - INFO - test loss 0.026146927059572733
2023-06-22 15:41:50,623 - INFO - test acc 0.625
2023-06-22 15:41:53,427 - INFO - Distilling data from client: Client38
2023-06-22 15:41:53,427 - INFO - train loss: 0.00039761321658083975
2023-06-22 15:41:53,427 - INFO - train acc: 1.0
2023-06-22 15:41:53,472 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.55      0.55        65
           2       0.64      0.67      0.66        64
           8       0.72      0.68      0.70        71

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:41:53,473 - INFO - test loss 0.025489252167236857
2023-06-22 15:41:53,473 - INFO - test acc 0.6349999904632568
2023-06-22 15:41:56,305 - INFO - Distilling data from client: Client38
2023-06-22 15:41:56,306 - INFO - train loss: 0.00043634056221727655
2023-06-22 15:41:56,306 - INFO - train acc: 1.0
2023-06-22 15:41:56,359 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.52      0.54        65
           2       0.63      0.67      0.65        64
           8       0.71      0.70      0.71        71

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.64      0.63       200

2023-06-22 15:41:56,360 - INFO - test loss 0.025827628115026167
2023-06-22 15:41:56,361 - INFO - test acc 0.6349999904632568
2023-06-22 15:41:59,139 - INFO - Distilling data from client: Client38
2023-06-22 15:41:59,139 - INFO - train loss: 0.0003841914470613549
2023-06-22 15:41:59,140 - INFO - train acc: 1.0
2023-06-22 15:41:59,197 - INFO - report:               precision    recall  f1-score   support

           0       0.54      0.54      0.54        65
           2       0.65      0.66      0.65        64
           8       0.73      0.72      0.72        71

    accuracy                           0.64       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:41:59,200 - INFO - test loss 0.026481467008525515
2023-06-22 15:41:59,201 - INFO - test acc 0.6399999856948853
2023-06-22 15:42:01,989 - INFO - Distilling data from client: Client38
2023-06-22 15:42:01,989 - INFO - train loss: 0.0003868134827968997
2023-06-22 15:42:01,990 - INFO - train acc: 1.0
2023-06-22 15:42:02,046 - INFO - report:               precision    recall  f1-score   support

           0       0.51      0.54      0.53        65
           2       0.62      0.64      0.63        64
           8       0.71      0.66      0.69        71

    accuracy                           0.61       200
   macro avg       0.62      0.61      0.61       200
weighted avg       0.62      0.61      0.62       200

2023-06-22 15:42:02,047 - INFO - test loss 0.02636443144100236
2023-06-22 15:42:02,047 - INFO - test acc 0.6150000095367432
2023-06-22 15:42:02,078 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:02,098 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:02,116 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:02,135 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:02,151 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:02,163 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:02,175 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:02,187 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:02,200 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:02,743 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client38//synthetic.png
2023-06-22 15:42:02,767 - INFO - c: 5.0 and total_data_in_this_class: 529
2023-06-22 15:42:02,767 - INFO - c: 6.0 and total_data_in_this_class: 270
2023-06-22 15:42:02,767 - INFO - c: 5.0 and total_data_in_this_class: 137
2023-06-22 15:42:02,767 - INFO - c: 6.0 and total_data_in_this_class: 63
2023-06-22 15:42:02,891 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07708120346069336 sec
2023-06-22 15:42:02,966 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07371926307678223 sec
2023-06-22 15:42:02,974 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16349363327026367 sec
2023-06-22 15:42:02,977 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:42:03,033 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05539250373840332 sec
2023-06-22 15:42:03,033 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:42:03,205 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17151832580566406 sec
2023-06-22 15:42:03,236 - INFO - initial test loss: 0.018981922823387183
2023-06-22 15:42:03,237 - INFO - initial test acc: 0.7249999642372131
2023-06-22 15:42:03,255 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012760639190673828 sec
2023-06-22 15:42:03,449 - WARNING - Finished tracing + transforming update_fn for pjit in 0.20737314224243164 sec
2023-06-22 15:42:03,454 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[360,10]), ShapedArray(float32[360,10]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:42:03,557 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.1022806167602539 sec
2023-06-22 15:42:03,557 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:42:03,950 - WARNING - Finished XLA compilation of jit(update_fn) in 0.39275193214416504 sec
2023-06-22 15:42:05,961 - INFO - Distilling data from client: Client39
2023-06-22 15:42:05,961 - INFO - train loss: 0.004766389297071836
2023-06-22 15:42:05,961 - INFO - train acc: 0.9777777791023254
2023-06-22 15:42:06,075 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.85      0.85       137
           6       0.67      0.67      0.67        63

    accuracy                           0.79       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:42:06,078 - INFO - test loss 0.016251229455242194
2023-06-22 15:42:06,078 - INFO - test acc 0.7899999618530273
2023-06-22 15:42:08,053 - INFO - Distilling data from client: Client39
2023-06-22 15:42:08,054 - INFO - train loss: 0.003464111543776965
2023-06-22 15:42:08,055 - INFO - train acc: 0.9805555939674377
2023-06-22 15:42:08,114 - INFO - report:               precision    recall  f1-score   support

           5       0.86      0.82      0.84       137
           6       0.65      0.70      0.67        63

    accuracy                           0.79       200
   macro avg       0.75      0.76      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:42:08,116 - INFO - test loss 0.016094209177245217
2023-06-22 15:42:08,116 - INFO - test acc 0.7849999666213989
2023-06-22 15:42:10,114 - INFO - Distilling data from client: Client39
2023-06-22 15:42:10,114 - INFO - train loss: 0.0028870630191331313
2023-06-22 15:42:10,114 - INFO - train acc: 0.9944444894790649
2023-06-22 15:42:10,246 - INFO - report:               precision    recall  f1-score   support

           5       0.88      0.83      0.86       137
           6       0.68      0.76      0.72        63

    accuracy                           0.81       200
   macro avg       0.78      0.80      0.79       200
weighted avg       0.82      0.81      0.81       200

2023-06-22 15:42:10,247 - INFO - test loss 0.01668878722685677
2023-06-22 15:42:10,247 - INFO - test acc 0.8100000023841858
2023-06-22 15:42:12,242 - INFO - Distilling data from client: Client39
2023-06-22 15:42:12,242 - INFO - train loss: 0.0029748702790684638
2023-06-22 15:42:12,242 - INFO - train acc: 0.9916666746139526
2023-06-22 15:42:12,297 - INFO - report:               precision    recall  f1-score   support

           5       0.82      0.82      0.82       137
           6       0.61      0.62      0.61        63

    accuracy                           0.76       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:42:12,299 - INFO - test loss 0.017059458649370907
2023-06-22 15:42:12,299 - INFO - test acc 0.7549999952316284
2023-06-22 15:42:14,378 - INFO - Distilling data from client: Client39
2023-06-22 15:42:14,378 - INFO - train loss: 0.002872177253339736
2023-06-22 15:42:14,378 - INFO - train acc: 0.9888889193534851
2023-06-22 15:42:14,438 - INFO - report:               precision    recall  f1-score   support

           5       0.86      0.84      0.85       137
           6       0.67      0.71      0.69        63

    accuracy                           0.80       200
   macro avg       0.77      0.78      0.77       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:42:14,438 - INFO - test loss 0.0162506867869893
2023-06-22 15:42:14,439 - INFO - test acc 0.7999999523162842
2023-06-22 15:42:16,650 - INFO - Distilling data from client: Client39
2023-06-22 15:42:16,650 - INFO - train loss: 0.002390021628949854
2023-06-22 15:42:16,651 - INFO - train acc: 0.9972222447395325
2023-06-22 15:42:16,696 - INFO - report:               precision    recall  f1-score   support

           5       0.87      0.80      0.83       137
           6       0.63      0.73      0.68        63

    accuracy                           0.78       200
   macro avg       0.75      0.77      0.75       200
weighted avg       0.79      0.78      0.78       200

2023-06-22 15:42:16,696 - INFO - test loss 0.01724616742574525
2023-06-22 15:42:16,697 - INFO - test acc 0.7799999713897705
2023-06-22 15:42:18,744 - INFO - Distilling data from client: Client39
2023-06-22 15:42:18,745 - INFO - train loss: 0.002202365628147976
2023-06-22 15:42:18,745 - INFO - train acc: 0.9972222447395325
2023-06-22 15:42:18,783 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.84      0.85       137
           6       0.66      0.68      0.67        63

    accuracy                           0.79       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:42:18,783 - INFO - test loss 0.018064409315894148
2023-06-22 15:42:18,784 - INFO - test acc 0.7899999618530273
2023-06-22 15:42:20,887 - INFO - Distilling data from client: Client39
2023-06-22 15:42:20,888 - INFO - train loss: 0.002027006974416938
2023-06-22 15:42:20,888 - INFO - train acc: 0.9944444894790649
2023-06-22 15:42:20,934 - INFO - report:               precision    recall  f1-score   support

           5       0.84      0.82      0.83       137
           6       0.63      0.67      0.65        63

    accuracy                           0.77       200
   macro avg       0.73      0.74      0.74       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:42:20,934 - INFO - test loss 0.017023324876292725
2023-06-22 15:42:20,934 - INFO - test acc 0.7699999809265137
2023-06-22 15:42:23,239 - INFO - Distilling data from client: Client39
2023-06-22 15:42:23,239 - INFO - train loss: 0.0019776099530134173
2023-06-22 15:42:23,239 - INFO - train acc: 1.0
2023-06-22 15:42:23,278 - INFO - report:               precision    recall  f1-score   support

           5       0.87      0.85      0.86       137
           6       0.68      0.71      0.70        63

    accuracy                           0.81       200
   macro avg       0.77      0.78      0.78       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:42:23,279 - INFO - test loss 0.016453064072625707
2023-06-22 15:42:23,279 - INFO - test acc 0.8050000071525574
2023-06-22 15:42:25,440 - INFO - Distilling data from client: Client39
2023-06-22 15:42:25,440 - INFO - train loss: 0.001976852607660279
2023-06-22 15:42:25,440 - INFO - train acc: 1.0
2023-06-22 15:42:25,483 - INFO - report:               precision    recall  f1-score   support

           5       0.84      0.81      0.83       137
           6       0.62      0.67      0.64        63

    accuracy                           0.77       200
   macro avg       0.73      0.74      0.73       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:42:25,484 - INFO - test loss 0.01699113836457838
2023-06-22 15:42:25,484 - INFO - test acc 0.7649999856948853
2023-06-22 15:42:27,634 - INFO - Distilling data from client: Client39
2023-06-22 15:42:27,635 - INFO - train loss: 0.002079217980424805
2023-06-22 15:42:27,635 - INFO - train acc: 1.0
2023-06-22 15:42:27,688 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.82      0.83       137
           6       0.63      0.68      0.66        63

    accuracy                           0.78       200
   macro avg       0.74      0.75      0.74       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:42:27,689 - INFO - test loss 0.016607767838506482
2023-06-22 15:42:27,690 - INFO - test acc 0.7749999761581421
2023-06-22 15:42:29,843 - INFO - Distilling data from client: Client39
2023-06-22 15:42:29,843 - INFO - train loss: 0.0018056030632797439
2023-06-22 15:42:29,844 - INFO - train acc: 1.0
2023-06-22 15:42:29,902 - INFO - report:               precision    recall  f1-score   support

           5       0.84      0.80      0.82       137
           6       0.60      0.67      0.63        63

    accuracy                           0.76       200
   macro avg       0.72      0.73      0.72       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:42:29,903 - INFO - test loss 0.016754412474914854
2023-06-22 15:42:29,904 - INFO - test acc 0.7549999952316284
2023-06-22 15:42:32,119 - INFO - Distilling data from client: Client39
2023-06-22 15:42:32,120 - INFO - train loss: 0.001905125314066076
2023-06-22 15:42:32,120 - INFO - train acc: 0.9944444894790649
2023-06-22 15:42:32,176 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.85      0.85       137
           6       0.68      0.67      0.67        63

    accuracy                           0.80       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.79      0.80      0.79       200

2023-06-22 15:42:32,177 - INFO - test loss 0.01652863541394067
2023-06-22 15:42:32,178 - INFO - test acc 0.7949999570846558
2023-06-22 15:42:34,374 - INFO - Distilling data from client: Client39
2023-06-22 15:42:34,374 - INFO - train loss: 0.0018599021197812774
2023-06-22 15:42:34,375 - INFO - train acc: 0.9972222447395325
2023-06-22 15:42:34,419 - INFO - report:               precision    recall  f1-score   support

           5       0.86      0.82      0.84       137
           6       0.65      0.70      0.67        63

    accuracy                           0.79       200
   macro avg       0.75      0.76      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:42:34,425 - INFO - test loss 0.017298513654347945
2023-06-22 15:42:34,426 - INFO - test acc 0.7849999666213989
2023-06-22 15:42:36,581 - INFO - Distilling data from client: Client39
2023-06-22 15:42:36,582 - INFO - train loss: 0.0020243326643804133
2023-06-22 15:42:36,582 - INFO - train acc: 0.9972222447395325
2023-06-22 15:42:36,632 - INFO - report:               precision    recall  f1-score   support

           5       0.86      0.82      0.84       137
           6       0.64      0.71      0.68        63

    accuracy                           0.79       200
   macro avg       0.75      0.77      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-06-22 15:42:36,633 - INFO - test loss 0.016427563660802814
2023-06-22 15:42:36,633 - INFO - test acc 0.7849999666213989
2023-06-22 15:42:38,800 - INFO - Distilling data from client: Client39
2023-06-22 15:42:38,800 - INFO - train loss: 0.0017711452064045183
2023-06-22 15:42:38,800 - INFO - train acc: 1.0
2023-06-22 15:42:38,848 - INFO - report:               precision    recall  f1-score   support

           5       0.82      0.82      0.82       137
           6       0.62      0.62      0.62        63

    accuracy                           0.76       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:42:38,849 - INFO - test loss 0.01646316062318781
2023-06-22 15:42:38,849 - INFO - test acc 0.7599999904632568
2023-06-22 15:42:40,949 - INFO - Distilling data from client: Client39
2023-06-22 15:42:40,950 - INFO - train loss: 0.0019478584539682775
2023-06-22 15:42:40,950 - INFO - train acc: 1.0
2023-06-22 15:42:40,995 - INFO - report:               precision    recall  f1-score   support

           5       0.88      0.84      0.86       137
           6       0.68      0.75      0.71        63

    accuracy                           0.81       200
   macro avg       0.78      0.79      0.79       200
weighted avg       0.82      0.81      0.81       200

2023-06-22 15:42:41,008 - INFO - test loss 0.016481642878259414
2023-06-22 15:42:41,009 - INFO - test acc 0.8100000023841858
2023-06-22 15:42:43,159 - INFO - Distilling data from client: Client39
2023-06-22 15:42:43,159 - INFO - train loss: 0.0017541571095790493
2023-06-22 15:42:43,159 - INFO - train acc: 1.0
2023-06-22 15:42:43,222 - INFO - report:               precision    recall  f1-score   support

           5       0.83      0.80      0.81       137
           6       0.59      0.65      0.62        63

    accuracy                           0.75       200
   macro avg       0.71      0.72      0.72       200
weighted avg       0.76      0.75      0.75       200

2023-06-22 15:42:43,223 - INFO - test loss 0.01698016078977217
2023-06-22 15:42:43,223 - INFO - test acc 0.75
2023-06-22 15:42:45,456 - INFO - Distilling data from client: Client39
2023-06-22 15:42:45,457 - INFO - train loss: 0.0015254040685472696
2023-06-22 15:42:45,457 - INFO - train acc: 1.0
2023-06-22 15:42:45,508 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.85      0.85       137
           6       0.67      0.68      0.68        63

    accuracy                           0.80       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:42:45,508 - INFO - test loss 0.015998517683453395
2023-06-22 15:42:45,509 - INFO - test acc 0.7949999570846558
2023-06-22 15:42:47,640 - INFO - Distilling data from client: Client39
2023-06-22 15:42:47,641 - INFO - train loss: 0.0020284659560839287
2023-06-22 15:42:47,641 - INFO - train acc: 1.0
2023-06-22 15:42:47,699 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.81      0.83       137
           6       0.63      0.70      0.66        63

    accuracy                           0.78       200
   macro avg       0.74      0.75      0.75       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:42:47,700 - INFO - test loss 0.016638308391078077
2023-06-22 15:42:47,700 - INFO - test acc 0.7749999761581421
2023-06-22 15:42:50,024 - INFO - Distilling data from client: Client39
2023-06-22 15:42:50,024 - INFO - train loss: 0.002013919127683397
2023-06-22 15:42:50,025 - INFO - train acc: 0.9944444894790649
2023-06-22 15:42:50,092 - INFO - report:               precision    recall  f1-score   support

           5       0.84      0.85      0.84       137
           6       0.66      0.65      0.66        63

    accuracy                           0.79       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.78      0.79      0.78       200

2023-06-22 15:42:50,093 - INFO - test loss 0.017048212586711003
2023-06-22 15:42:50,094 - INFO - test acc 0.7849999666213989
2023-06-22 15:42:52,115 - INFO - Distilling data from client: Client39
2023-06-22 15:42:52,115 - INFO - train loss: 0.0018056419123950943
2023-06-22 15:42:52,116 - INFO - train acc: 0.9972222447395325
2023-06-22 15:42:52,154 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.85      0.85       137
           6       0.68      0.67      0.67        63

    accuracy                           0.80       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.79      0.80      0.79       200

2023-06-22 15:42:52,155 - INFO - test loss 0.017011761651674846
2023-06-22 15:42:52,155 - INFO - test acc 0.7949999570846558
2023-06-22 15:42:54,222 - INFO - Distilling data from client: Client39
2023-06-22 15:42:54,223 - INFO - train loss: 0.0015556264836591286
2023-06-22 15:42:54,223 - INFO - train acc: 0.9972222447395325
2023-06-22 15:42:54,289 - INFO - report:               precision    recall  f1-score   support

           5       0.88      0.82      0.85       137
           6       0.67      0.76      0.71        63

    accuracy                           0.81       200
   macro avg       0.77      0.79      0.78       200
weighted avg       0.81      0.81      0.81       200

2023-06-22 15:42:54,290 - INFO - test loss 0.016703615170960955
2023-06-22 15:42:54,290 - INFO - test acc 0.8050000071525574
2023-06-22 15:42:56,401 - INFO - Distilling data from client: Client39
2023-06-22 15:42:56,402 - INFO - train loss: 0.0017419220049739535
2023-06-22 15:42:56,402 - INFO - train acc: 0.9972222447395325
2023-06-22 15:42:56,448 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.82      0.84       137
           6       0.64      0.68      0.66        63

    accuracy                           0.78       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:42:56,449 - INFO - test loss 0.017060397108098764
2023-06-22 15:42:56,449 - INFO - test acc 0.7799999713897705
2023-06-22 15:42:58,849 - INFO - Distilling data from client: Client39
2023-06-22 15:42:58,850 - INFO - train loss: 0.0018330109638763491
2023-06-22 15:42:58,851 - INFO - train acc: 1.0
2023-06-22 15:42:58,919 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.85      0.85       137
           6       0.68      0.68      0.68        63

    accuracy                           0.80       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.80      0.80      0.80       200

2023-06-22 15:42:58,937 - INFO - test loss 0.01610556334773843
2023-06-22 15:42:58,937 - INFO - test acc 0.7999999523162842
2023-06-22 15:42:58,978 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:59,008 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:59,023 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:59,039 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:59,055 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:59,072 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:42:59,574 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client39//synthetic.png
2023-06-22 15:42:59,602 - INFO - c: 1.0 and total_data_in_this_class: 261
2023-06-22 15:42:59,602 - INFO - c: 4.0 and total_data_in_this_class: 267
2023-06-22 15:42:59,602 - INFO - c: 5.0 and total_data_in_this_class: 271
2023-06-22 15:42:59,602 - INFO - c: 1.0 and total_data_in_this_class: 72
2023-06-22 15:42:59,602 - INFO - c: 4.0 and total_data_in_this_class: 66
2023-06-22 15:42:59,602 - INFO - c: 5.0 and total_data_in_this_class: 62
2023-06-22 15:42:59,742 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08190751075744629 sec
2023-06-22 15:42:59,817 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07408905029296875 sec
2023-06-22 15:42:59,826 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16853928565979004 sec
2023-06-22 15:42:59,829 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:42:59,890 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.061011552810668945 sec
2023-06-22 15:42:59,891 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:43:00,069 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17814016342163086 sec
2023-06-22 15:43:00,121 - INFO - initial test loss: 0.0225444254498857
2023-06-22 15:43:00,121 - INFO - initial test acc: 0.6899999976158142
2023-06-22 15:43:00,141 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.01366734504699707 sec
2023-06-22 15:43:00,334 - WARNING - Finished tracing + transforming update_fn for pjit in 0.20894861221313477 sec
2023-06-22 15:43:00,340 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[522,10]), ShapedArray(float32[522,10]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:43:00,447 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10653519630432129 sec
2023-06-22 15:43:00,447 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:43:00,884 - WARNING - Finished XLA compilation of jit(update_fn) in 0.43671250343322754 sec
2023-06-22 15:43:03,688 - INFO - Distilling data from client: Client40
2023-06-22 15:43:03,688 - INFO - train loss: 0.002347437703209988
2023-06-22 15:43:03,689 - INFO - train acc: 0.9961685538291931
2023-06-22 15:43:03,927 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.88      0.88        72
           4       0.73      0.77      0.75        66
           5       0.72      0.68      0.70        62

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:43:03,927 - INFO - test loss 0.01913138816096148
2023-06-22 15:43:03,927 - INFO - test acc 0.7799999713897705
2023-06-22 15:43:07,183 - INFO - Distilling data from client: Client40
2023-06-22 15:43:07,183 - INFO - train loss: 0.0011056871216344845
2023-06-22 15:43:07,183 - INFO - train acc: 1.0
2023-06-22 15:43:07,235 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.81      0.84        72
           4       0.66      0.77      0.71        66
           5       0.70      0.65      0.67        62

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.75       200

2023-06-22 15:43:07,236 - INFO - test loss 0.019685259018499466
2023-06-22 15:43:07,236 - INFO - test acc 0.7450000047683716
2023-06-22 15:43:10,762 - INFO - Distilling data from client: Client40
2023-06-22 15:43:10,763 - INFO - train loss: 0.0007714799730639222
2023-06-22 15:43:10,763 - INFO - train acc: 1.0
2023-06-22 15:43:10,827 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.81      0.85        72
           4       0.67      0.77      0.72        66
           5       0.69      0.66      0.68        62

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-06-22 15:43:10,830 - INFO - test loss 0.019738823772677418
2023-06-22 15:43:10,830 - INFO - test acc 0.75
2023-06-22 15:43:13,697 - INFO - Distilling data from client: Client40
2023-06-22 15:43:13,698 - INFO - train loss: 0.0007877019101413324
2023-06-22 15:43:13,698 - INFO - train acc: 1.0
2023-06-22 15:43:13,761 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.83      0.86        72
           4       0.71      0.80      0.75        66
           5       0.72      0.66      0.69        62

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:43:13,763 - INFO - test loss 0.020338207876697
2023-06-22 15:43:13,764 - INFO - test acc 0.7699999809265137
2023-06-22 15:43:16,735 - INFO - Distilling data from client: Client40
2023-06-22 15:43:16,735 - INFO - train loss: 0.0005070218491577641
2023-06-22 15:43:16,736 - INFO - train acc: 1.0
2023-06-22 15:43:16,802 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.75      0.81        72
           4       0.64      0.80      0.71        66
           5       0.69      0.61      0.65        62

    accuracy                           0.73       200
   macro avg       0.73      0.72      0.72       200
weighted avg       0.74      0.72      0.73       200

2023-06-22 15:43:16,803 - INFO - test loss 0.02023049329440296
2023-06-22 15:43:16,803 - INFO - test acc 0.7249999642372131
2023-06-22 15:43:19,697 - INFO - Distilling data from client: Client40
2023-06-22 15:43:19,698 - INFO - train loss: 0.0004973926053790339
2023-06-22 15:43:19,698 - INFO - train acc: 1.0
2023-06-22 15:43:19,756 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.83      0.86        72
           4       0.69      0.77      0.73        66
           5       0.69      0.65      0.67        62

    accuracy                           0.76       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:43:19,759 - INFO - test loss 0.020080647299568672
2023-06-22 15:43:19,760 - INFO - test acc 0.7549999952316284
2023-06-22 15:43:22,836 - INFO - Distilling data from client: Client40
2023-06-22 15:43:22,836 - INFO - train loss: 0.00044600842556908796
2023-06-22 15:43:22,836 - INFO - train acc: 1.0
2023-06-22 15:43:22,893 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.83      0.86        72
           4       0.70      0.79      0.74        66
           5       0.72      0.68      0.70        62

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:43:22,894 - INFO - test loss 0.019650888485116064
2023-06-22 15:43:22,895 - INFO - test acc 0.7699999809265137
2023-06-22 15:43:25,817 - INFO - Distilling data from client: Client40
2023-06-22 15:43:25,818 - INFO - train loss: 0.0005320347913606556
2023-06-22 15:43:25,818 - INFO - train acc: 1.0
2023-06-22 15:43:25,877 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.82      0.84        72
           4       0.73      0.73      0.73        66
           5       0.68      0.73      0.70        62

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:43:25,878 - INFO - test loss 0.020083586945693812
2023-06-22 15:43:25,879 - INFO - test acc 0.7599999904632568
2023-06-22 15:43:28,871 - INFO - Distilling data from client: Client40
2023-06-22 15:43:28,871 - INFO - train loss: 0.00041363704384081543
2023-06-22 15:43:28,871 - INFO - train acc: 1.0
2023-06-22 15:43:28,932 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.83      0.85        72
           4       0.73      0.80      0.76        66
           5       0.72      0.68      0.70        62

    accuracy                           0.78       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:43:28,933 - INFO - test loss 0.019748083210193185
2023-06-22 15:43:28,933 - INFO - test acc 0.7749999761581421
2023-06-22 15:43:31,919 - INFO - Distilling data from client: Client40
2023-06-22 15:43:31,919 - INFO - train loss: 0.0004796560696664156
2023-06-22 15:43:31,919 - INFO - train acc: 1.0
2023-06-22 15:43:31,984 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.83      0.85        72
           4       0.71      0.79      0.75        66
           5       0.74      0.69      0.72        62

    accuracy                           0.78       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:43:31,985 - INFO - test loss 0.020021468581274002
2023-06-22 15:43:31,985 - INFO - test acc 0.7749999761581421
2023-06-22 15:43:34,894 - INFO - Distilling data from client: Client40
2023-06-22 15:43:34,894 - INFO - train loss: 0.0003467281280439558
2023-06-22 15:43:34,895 - INFO - train acc: 1.0
2023-06-22 15:43:34,959 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.83      0.85        72
           4       0.72      0.77      0.74        66
           5       0.72      0.69      0.70        62

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:43:34,959 - INFO - test loss 0.01968881036480099
2023-06-22 15:43:34,960 - INFO - test acc 0.7699999809265137
2023-06-22 15:43:37,992 - INFO - Distilling data from client: Client40
2023-06-22 15:43:37,992 - INFO - train loss: 0.00042115454306176323
2023-06-22 15:43:37,993 - INFO - train acc: 1.0
2023-06-22 15:43:38,047 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.85      0.87        72
           4       0.66      0.77      0.71        66
           5       0.69      0.61      0.65        62

    accuracy                           0.75       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.76      0.75      0.75       200

2023-06-22 15:43:38,048 - INFO - test loss 0.020473064875648476
2023-06-22 15:43:38,048 - INFO - test acc 0.75
2023-06-22 15:43:40,952 - INFO - Distilling data from client: Client40
2023-06-22 15:43:40,952 - INFO - train loss: 0.0003707334681382321
2023-06-22 15:43:40,953 - INFO - train acc: 1.0
2023-06-22 15:43:41,008 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.83      0.85        72
           4       0.66      0.77      0.71        66
           5       0.69      0.60      0.64        62

    accuracy                           0.74       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.74      0.74       200

2023-06-22 15:43:41,010 - INFO - test loss 0.020651439530023994
2023-06-22 15:43:41,010 - INFO - test acc 0.7400000095367432
2023-06-22 15:43:43,999 - INFO - Distilling data from client: Client40
2023-06-22 15:43:44,000 - INFO - train loss: 0.0003863833949580814
2023-06-22 15:43:44,000 - INFO - train acc: 1.0
2023-06-22 15:43:44,059 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.81      0.84        72
           4       0.67      0.77      0.72        66
           5       0.69      0.65      0.67        62

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.75       200

2023-06-22 15:43:44,060 - INFO - test loss 0.019702823374585777
2023-06-22 15:43:44,061 - INFO - test acc 0.7450000047683716
2023-06-22 15:43:47,120 - INFO - Distilling data from client: Client40
2023-06-22 15:43:47,121 - INFO - train loss: 0.0003862057089530391
2023-06-22 15:43:47,121 - INFO - train acc: 1.0
2023-06-22 15:43:47,172 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.82      0.84        72
           4       0.67      0.77      0.72        66
           5       0.70      0.63      0.66        62

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.75       200

2023-06-22 15:43:47,173 - INFO - test loss 0.02021034166398645
2023-06-22 15:43:47,173 - INFO - test acc 0.7450000047683716
2023-06-22 15:43:50,288 - INFO - Distilling data from client: Client40
2023-06-22 15:43:50,288 - INFO - train loss: 0.0003386849979536909
2023-06-22 15:43:50,288 - INFO - train acc: 1.0
2023-06-22 15:43:50,347 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.83      0.86        72
           4       0.67      0.79      0.72        66
           5       0.71      0.63      0.67        62

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:43:50,347 - INFO - test loss 0.020140939994119568
2023-06-22 15:43:50,347 - INFO - test acc 0.7549999952316284
2023-06-22 15:43:53,340 - INFO - Distilling data from client: Client40
2023-06-22 15:43:53,341 - INFO - train loss: 0.0002993551826155306
2023-06-22 15:43:53,341 - INFO - train acc: 1.0
2023-06-22 15:43:53,444 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.85      0.87        72
           4       0.71      0.79      0.75        66
           5       0.72      0.68      0.70        62

    accuracy                           0.78       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:43:53,445 - INFO - test loss 0.019750846203194217
2023-06-22 15:43:53,445 - INFO - test acc 0.7749999761581421
2023-06-22 15:43:56,470 - INFO - Distilling data from client: Client40
2023-06-22 15:43:56,471 - INFO - train loss: 0.00027442942110927107
2023-06-22 15:43:56,471 - INFO - train acc: 1.0
2023-06-22 15:43:56,687 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.86      0.87        72
           4       0.73      0.82      0.77        66
           5       0.75      0.66      0.70        62

    accuracy                           0.79       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.79      0.79      0.78       200

2023-06-22 15:43:56,687 - INFO - test loss 0.019626637374781788
2023-06-22 15:43:56,687 - INFO - test acc 0.7849999666213989
2023-06-22 15:43:59,780 - INFO - Distilling data from client: Client40
2023-06-22 15:43:59,780 - INFO - train loss: 0.00022061750730618983
2023-06-22 15:43:59,780 - INFO - train acc: 1.0
2023-06-22 15:43:59,830 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.81      0.84        72
           4       0.65      0.77      0.71        66
           5       0.70      0.63      0.66        62

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:43:59,831 - INFO - test loss 0.02035106990666821
2023-06-22 15:43:59,832 - INFO - test acc 0.7400000095367432
2023-06-22 15:44:02,915 - INFO - Distilling data from client: Client40
2023-06-22 15:44:02,916 - INFO - train loss: 0.00027162397781960905
2023-06-22 15:44:02,917 - INFO - train acc: 1.0
2023-06-22 15:44:03,089 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.85      0.86        72
           4       0.72      0.79      0.75        66
           5       0.71      0.66      0.68        62

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-06-22 15:44:03,094 - INFO - test loss 0.019585401374931085
2023-06-22 15:44:03,094 - INFO - test acc 0.7699999809265137
2023-06-22 15:44:06,147 - INFO - Distilling data from client: Client40
2023-06-22 15:44:06,148 - INFO - train loss: 0.00027175487968887657
2023-06-22 15:44:06,148 - INFO - train acc: 1.0
2023-06-22 15:44:06,204 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.82      0.85        72
           4       0.69      0.80      0.74        66
           5       0.70      0.63      0.66        62

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:44:06,205 - INFO - test loss 0.019982907110337813
2023-06-22 15:44:06,205 - INFO - test acc 0.7549999952316284
2023-06-22 15:44:09,113 - INFO - Distilling data from client: Client40
2023-06-22 15:44:09,113 - INFO - train loss: 0.00026322464183980624
2023-06-22 15:44:09,114 - INFO - train acc: 1.0
2023-06-22 15:44:09,168 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.79      0.83        72
           4       0.65      0.77      0.71        66
           5       0.70      0.63      0.66        62

    accuracy                           0.73       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.73      0.74       200

2023-06-22 15:44:09,169 - INFO - test loss 0.020083554183678787
2023-06-22 15:44:09,169 - INFO - test acc 0.73499995470047
2023-06-22 15:44:12,171 - INFO - Distilling data from client: Client40
2023-06-22 15:44:12,171 - INFO - train loss: 0.00023293971501672913
2023-06-22 15:44:12,171 - INFO - train acc: 1.0
2023-06-22 15:44:12,221 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.83      0.86        72
           4       0.68      0.76      0.71        66
           5       0.69      0.66      0.68        62

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:44:12,222 - INFO - test loss 0.02020640113350163
2023-06-22 15:44:12,222 - INFO - test acc 0.7549999952316284
2023-06-22 15:44:15,248 - INFO - Distilling data from client: Client40
2023-06-22 15:44:15,248 - INFO - train loss: 0.00024668812323331034
2023-06-22 15:44:15,248 - INFO - train acc: 1.0
2023-06-22 15:44:15,332 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.82      0.85        72
           4       0.66      0.77      0.71        66
           5       0.68      0.61      0.64        62

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:44:15,332 - INFO - test loss 0.020245493580496203
2023-06-22 15:44:15,332 - INFO - test acc 0.7400000095367432
2023-06-22 15:44:18,269 - INFO - Distilling data from client: Client40
2023-06-22 15:44:18,270 - INFO - train loss: 0.0002180810895536922
2023-06-22 15:44:18,270 - INFO - train acc: 1.0
2023-06-22 15:44:18,321 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.85      0.87        72
           4       0.72      0.80      0.76        66
           5       0.74      0.68      0.71        62

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-06-22 15:44:18,323 - INFO - test loss 0.02044651781894944
2023-06-22 15:44:18,323 - INFO - test acc 0.7799999713897705
2023-06-22 15:44:18,352 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:44:18,370 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:44:18,388 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:44:18,408 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:44:18,425 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:44:18,437 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:44:18,449 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:44:18,463 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:44:18,475 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:44:19,050 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client40//synthetic.png
2023-06-22 15:44:19,075 - INFO - c: 3.0 and total_data_in_this_class: 256
2023-06-22 15:44:19,075 - INFO - c: 7.0 and total_data_in_this_class: 272
2023-06-22 15:44:19,075 - INFO - c: 9.0 and total_data_in_this_class: 271
2023-06-22 15:44:19,075 - INFO - c: 3.0 and total_data_in_this_class: 77
2023-06-22 15:44:19,075 - INFO - c: 7.0 and total_data_in_this_class: 61
2023-06-22 15:44:19,076 - INFO - c: 9.0 and total_data_in_this_class: 62
2023-06-22 15:44:19,201 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07542538642883301 sec
2023-06-22 15:44:19,276 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07371187210083008 sec
2023-06-22 15:44:19,285 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16138720512390137 sec
2023-06-22 15:44:19,288 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:44:19,345 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.057005882263183594 sec
2023-06-22 15:44:19,345 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:44:19,522 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17594313621520996 sec
2023-06-22 15:44:19,559 - INFO - initial test loss: 0.02307693739575082
2023-06-22 15:44:19,559 - INFO - initial test acc: 0.7199999690055847
2023-06-22 15:44:19,580 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.013065814971923828 sec
2023-06-22 15:44:19,776 - WARNING - Finished tracing + transforming update_fn for pjit in 0.21103405952453613 sec
2023-06-22 15:44:19,782 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:44:19,890 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10764288902282715 sec
2023-06-22 15:44:19,890 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:44:20,329 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4384593963623047 sec
2023-06-22 15:44:23,213 - INFO - Distilling data from client: Client41
2023-06-22 15:44:23,214 - INFO - train loss: 0.0027051629593146903
2023-06-22 15:44:23,214 - INFO - train acc: 0.9941520690917969
2023-06-22 15:44:23,395 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.64      0.69        77
           7       0.67      0.70      0.69        61
           9       0.76      0.85      0.80        62

    accuracy                           0.73       200
   macro avg       0.72      0.73      0.73       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:44:23,395 - INFO - test loss 0.020918933424344765
2023-06-22 15:44:23,395 - INFO - test acc 0.7249999642372131
2023-06-22 15:44:26,222 - INFO - Distilling data from client: Client41
2023-06-22 15:44:26,222 - INFO - train loss: 0.0014691404806594912
2023-06-22 15:44:26,222 - INFO - train acc: 1.0
2023-06-22 15:44:26,399 - INFO - report:               precision    recall  f1-score   support

           3       0.71      0.65      0.68        77
           7       0.68      0.74      0.71        61
           9       0.80      0.82      0.81        62

    accuracy                           0.73       200
   macro avg       0.73      0.74      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-06-22 15:44:26,399 - INFO - test loss 0.020823700329954432
2023-06-22 15:44:26,399 - INFO - test acc 0.7299999594688416
2023-06-22 15:44:29,082 - INFO - Distilling data from client: Client41
2023-06-22 15:44:29,082 - INFO - train loss: 0.0010881610686280698
2023-06-22 15:44:29,082 - INFO - train acc: 1.0
2023-06-22 15:44:29,125 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.64      0.69        77
           7       0.65      0.77      0.71        61
           9       0.79      0.79      0.79        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:44:29,125 - INFO - test loss 0.02165738998664936
2023-06-22 15:44:29,125 - INFO - test acc 0.7249999642372131
2023-06-22 15:44:31,963 - INFO - Distilling data from client: Client41
2023-06-22 15:44:31,963 - INFO - train loss: 0.0008131764150981163
2023-06-22 15:44:31,964 - INFO - train acc: 1.0
2023-06-22 15:44:32,145 - INFO - report:               precision    recall  f1-score   support

           3       0.73      0.69      0.71        77
           7       0.71      0.75      0.73        61
           9       0.81      0.81      0.81        62

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:44:32,146 - INFO - test loss 0.020764482555261095
2023-06-22 15:44:32,146 - INFO - test acc 0.7450000047683716
2023-06-22 15:44:34,900 - INFO - Distilling data from client: Client41
2023-06-22 15:44:34,901 - INFO - train loss: 0.0007477476390855453
2023-06-22 15:44:34,901 - INFO - train acc: 1.0
2023-06-22 15:44:34,951 - INFO - report:               precision    recall  f1-score   support

           3       0.73      0.62      0.67        77
           7       0.64      0.75      0.69        61
           9       0.79      0.79      0.79        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:44:34,952 - INFO - test loss 0.02076712818679314
2023-06-22 15:44:34,952 - INFO - test acc 0.7149999737739563
2023-06-22 15:44:37,724 - INFO - Distilling data from client: Client41
2023-06-22 15:44:37,724 - INFO - train loss: 0.0006237002749097092
2023-06-22 15:44:37,724 - INFO - train acc: 1.0
2023-06-22 15:44:37,776 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.64      0.69        77
           7       0.63      0.79      0.70        61
           9       0.78      0.74      0.76        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:44:37,777 - INFO - test loss 0.021088955237073485
2023-06-22 15:44:37,778 - INFO - test acc 0.7149999737739563
2023-06-22 15:44:40,590 - INFO - Distilling data from client: Client41
2023-06-22 15:44:40,590 - INFO - train loss: 0.0005125532484497854
2023-06-22 15:44:40,590 - INFO - train acc: 1.0
2023-06-22 15:44:40,648 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.62      0.68        77
           7       0.64      0.75      0.69        61
           9       0.73      0.74      0.74        62

    accuracy                           0.70       200
   macro avg       0.70      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:44:40,648 - INFO - test loss 0.02139154075962923
2023-06-22 15:44:40,649 - INFO - test acc 0.699999988079071
2023-06-22 15:44:43,477 - INFO - Distilling data from client: Client41
2023-06-22 15:44:43,478 - INFO - train loss: 0.00046050625292623536
2023-06-22 15:44:43,478 - INFO - train acc: 1.0
2023-06-22 15:44:43,640 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.71      0.73        77
           7       0.70      0.79      0.74        61
           9       0.83      0.77      0.80        62

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-06-22 15:44:43,640 - INFO - test loss 0.020958262540283615
2023-06-22 15:44:43,640 - INFO - test acc 0.7549999952316284
2023-06-22 15:44:46,453 - INFO - Distilling data from client: Client41
2023-06-22 15:44:46,453 - INFO - train loss: 0.000392765315018827
2023-06-22 15:44:46,453 - INFO - train acc: 1.0
2023-06-22 15:44:46,550 - INFO - report:               precision    recall  f1-score   support

           3       0.71      0.64      0.67        77
           7       0.64      0.74      0.69        61
           9       0.79      0.77      0.78        62

    accuracy                           0.71       200
   macro avg       0.71      0.72      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:44:46,550 - INFO - test loss 0.021073518598539457
2023-06-22 15:44:46,550 - INFO - test acc 0.7099999785423279
2023-06-22 15:44:49,360 - INFO - Distilling data from client: Client41
2023-06-22 15:44:49,360 - INFO - train loss: 0.0003540473547268534
2023-06-22 15:44:49,361 - INFO - train acc: 1.0
2023-06-22 15:44:49,419 - INFO - report:               precision    recall  f1-score   support

           3       0.73      0.69      0.71        77
           7       0.67      0.74      0.70        61
           9       0.80      0.77      0.79        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-06-22 15:44:49,420 - INFO - test loss 0.021687349265880094
2023-06-22 15:44:49,420 - INFO - test acc 0.7299999594688416
2023-06-22 15:44:52,212 - INFO - Distilling data from client: Client41
2023-06-22 15:44:52,213 - INFO - train loss: 0.00031287703784305056
2023-06-22 15:44:52,213 - INFO - train acc: 1.0
2023-06-22 15:44:52,272 - INFO - report:               precision    recall  f1-score   support

           3       0.72      0.65      0.68        77
           7       0.62      0.70      0.66        61
           9       0.79      0.79      0.79        62

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:44:52,273 - INFO - test loss 0.021836291627245454
2023-06-22 15:44:52,273 - INFO - test acc 0.7099999785423279
2023-06-22 15:44:55,028 - INFO - Distilling data from client: Client41
2023-06-22 15:44:55,029 - INFO - train loss: 0.0003629213323818143
2023-06-22 15:44:55,029 - INFO - train acc: 1.0
2023-06-22 15:44:55,088 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.68      0.71        77
           7       0.65      0.75      0.70        61
           9       0.82      0.79      0.80        62

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.73      0.74       200

2023-06-22 15:44:55,089 - INFO - test loss 0.02138582217360525
2023-06-22 15:44:55,089 - INFO - test acc 0.73499995470047
2023-06-22 15:44:57,890 - INFO - Distilling data from client: Client41
2023-06-22 15:44:57,891 - INFO - train loss: 0.00032975186718983316
2023-06-22 15:44:57,892 - INFO - train acc: 1.0
2023-06-22 15:44:57,941 - INFO - report:               precision    recall  f1-score   support

           3       0.72      0.66      0.69        77
           7       0.65      0.77      0.71        61
           9       0.82      0.76      0.79        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.73       200

2023-06-22 15:44:57,942 - INFO - test loss 0.021759875188030228
2023-06-22 15:44:57,942 - INFO - test acc 0.7249999642372131
2023-06-22 15:45:00,635 - INFO - Distilling data from client: Client41
2023-06-22 15:45:00,636 - INFO - train loss: 0.0004053102622831897
2023-06-22 15:45:00,636 - INFO - train acc: 1.0
2023-06-22 15:45:00,696 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.69      0.73        77
           7       0.67      0.72      0.69        61
           9       0.77      0.81      0.79        62

    accuracy                           0.73       200
   macro avg       0.73      0.74      0.74       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:45:00,698 - INFO - test loss 0.02123128865005107
2023-06-22 15:45:00,698 - INFO - test acc 0.73499995470047
2023-06-22 15:45:03,409 - INFO - Distilling data from client: Client41
2023-06-22 15:45:03,409 - INFO - train loss: 0.0002759141834676349
2023-06-22 15:45:03,410 - INFO - train acc: 1.0
2023-06-22 15:45:03,459 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.62      0.68        77
           7       0.64      0.77      0.70        61
           9       0.79      0.81      0.80        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:45:03,459 - INFO - test loss 0.021634718491698898
2023-06-22 15:45:03,459 - INFO - test acc 0.7249999642372131
2023-06-22 15:45:06,254 - INFO - Distilling data from client: Client41
2023-06-22 15:45:06,254 - INFO - train loss: 0.00029773679953120155
2023-06-22 15:45:06,255 - INFO - train acc: 1.0
2023-06-22 15:45:06,310 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.64      0.70        77
           7       0.66      0.80      0.73        61
           9       0.79      0.79      0.79        62

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:45:06,310 - INFO - test loss 0.021355814570600366
2023-06-22 15:45:06,311 - INFO - test acc 0.73499995470047
2023-06-22 15:45:09,102 - INFO - Distilling data from client: Client41
2023-06-22 15:45:09,102 - INFO - train loss: 0.0002930942426341496
2023-06-22 15:45:09,102 - INFO - train acc: 1.0
2023-06-22 15:45:09,166 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.65      0.69        77
           7       0.64      0.70      0.67        61
           9       0.77      0.81      0.79        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:45:09,167 - INFO - test loss 0.021531936160703403
2023-06-22 15:45:09,167 - INFO - test acc 0.7149999737739563
2023-06-22 15:45:11,939 - INFO - Distilling data from client: Client41
2023-06-22 15:45:11,940 - INFO - train loss: 0.0002565751396583097
2023-06-22 15:45:11,940 - INFO - train acc: 1.0
2023-06-22 15:45:11,999 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.65      0.70        77
           7       0.63      0.74      0.68        61
           9       0.79      0.81      0.80        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.73       200

2023-06-22 15:45:12,000 - INFO - test loss 0.021464522083704283
2023-06-22 15:45:12,000 - INFO - test acc 0.7249999642372131
2023-06-22 15:45:14,762 - INFO - Distilling data from client: Client41
2023-06-22 15:45:14,762 - INFO - train loss: 0.00025255625876902164
2023-06-22 15:45:14,763 - INFO - train acc: 1.0
2023-06-22 15:45:14,814 - INFO - report:               precision    recall  f1-score   support

           3       0.71      0.65      0.68        77
           7       0.65      0.77      0.71        61
           9       0.83      0.77      0.80        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.73       200

2023-06-22 15:45:14,814 - INFO - test loss 0.021291552580321588
2023-06-22 15:45:14,815 - INFO - test acc 0.7249999642372131
2023-06-22 15:45:17,510 - INFO - Distilling data from client: Client41
2023-06-22 15:45:17,510 - INFO - train loss: 0.00027578559157508754
2023-06-22 15:45:17,511 - INFO - train acc: 1.0
2023-06-22 15:45:17,587 - INFO - report:               precision    recall  f1-score   support

           3       0.72      0.65      0.68        77
           7       0.63      0.74      0.68        61
           9       0.80      0.77      0.79        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:45:17,588 - INFO - test loss 0.022046212446342178
2023-06-22 15:45:17,588 - INFO - test acc 0.7149999737739563
2023-06-22 15:45:20,319 - INFO - Distilling data from client: Client41
2023-06-22 15:45:20,319 - INFO - train loss: 0.00020246631115340412
2023-06-22 15:45:20,319 - INFO - train acc: 1.0
2023-06-22 15:45:20,371 - INFO - report:               precision    recall  f1-score   support

           3       0.72      0.64      0.68        77
           7       0.66      0.75      0.70        61
           9       0.77      0.77      0.77        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:45:20,371 - INFO - test loss 0.021825343370641813
2023-06-22 15:45:20,372 - INFO - test acc 0.7149999737739563
2023-06-22 15:45:23,146 - INFO - Distilling data from client: Client41
2023-06-22 15:45:23,146 - INFO - train loss: 0.000207139717920308
2023-06-22 15:45:23,146 - INFO - train acc: 1.0
2023-06-22 15:45:23,202 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.61      0.68        77
           7       0.63      0.79      0.70        61
           9       0.81      0.81      0.81        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:45:23,202 - INFO - test loss 0.02160391285675537
2023-06-22 15:45:23,202 - INFO - test acc 0.7249999642372131
2023-06-22 15:45:26,024 - INFO - Distilling data from client: Client41
2023-06-22 15:45:26,024 - INFO - train loss: 0.00027333635641475253
2023-06-22 15:45:26,024 - INFO - train acc: 1.0
2023-06-22 15:45:26,073 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.62      0.69        77
           7       0.62      0.75      0.68        61
           9       0.78      0.79      0.78        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:45:26,074 - INFO - test loss 0.021563227383169042
2023-06-22 15:45:26,074 - INFO - test acc 0.7149999737739563
2023-06-22 15:45:28,808 - INFO - Distilling data from client: Client41
2023-06-22 15:45:28,809 - INFO - train loss: 0.00019954882499997108
2023-06-22 15:45:28,809 - INFO - train acc: 1.0
2023-06-22 15:45:28,861 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.66      0.71        77
           7       0.66      0.77      0.71        61
           9       0.82      0.82      0.82        62

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.74      0.74       200

2023-06-22 15:45:28,862 - INFO - test loss 0.021323267532034743
2023-06-22 15:45:28,862 - INFO - test acc 0.7450000047683716
2023-06-22 15:45:31,775 - INFO - Distilling data from client: Client41
2023-06-22 15:45:31,777 - INFO - train loss: 0.0001831685280131498
2023-06-22 15:45:31,779 - INFO - train acc: 1.0
2023-06-22 15:45:31,822 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.64      0.69        77
           7       0.67      0.77      0.72        61
           9       0.77      0.79      0.78        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:45:31,827 - INFO - test loss 0.021528521060244787
2023-06-22 15:45:31,827 - INFO - test acc 0.7249999642372131
2023-06-22 15:45:31,858 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:31,876 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:31,895 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:31,912 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:31,928 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:31,939 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:31,952 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:31,964 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:31,975 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:32,519 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client41//synthetic.png
2023-06-22 15:45:32,542 - INFO - c: 6.0 and total_data_in_this_class: 272
2023-06-22 15:45:32,543 - INFO - c: 7.0 and total_data_in_this_class: 267
2023-06-22 15:45:32,543 - INFO - c: 8.0 and total_data_in_this_class: 34
2023-06-22 15:45:32,543 - INFO - c: 9.0 and total_data_in_this_class: 226
2023-06-22 15:45:32,543 - INFO - c: 6.0 and total_data_in_this_class: 61
2023-06-22 15:45:32,543 - INFO - c: 7.0 and total_data_in_this_class: 66
2023-06-22 15:45:32,543 - INFO - c: 8.0 and total_data_in_this_class: 11
2023-06-22 15:45:32,543 - INFO - c: 9.0 and total_data_in_this_class: 62
2023-06-22 15:45:32,587 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005042552947998047 sec
2023-06-22 15:45:32,587 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:45:32,590 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0023136138916015625 sec
2023-06-22 15:45:32,590 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:32,607 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.015920639038085938 sec
2023-06-22 15:45:32,609 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003046989440917969 sec
2023-06-22 15:45:32,609 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:45:32,611 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0015523433685302734 sec
2023-06-22 15:45:32,611 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:32,623 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01141500473022461 sec
2023-06-22 15:45:32,628 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000232696533203125 sec
2023-06-22 15:45:32,630 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019431114196777344 sec
2023-06-22 15:45:32,631 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000568389892578125 sec
2023-06-22 15:45:32,634 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003426074981689453 sec
2023-06-22 15:45:32,634 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001919269561767578 sec
2023-06-22 15:45:32,636 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0006532669067382812 sec
2023-06-22 15:45:32,637 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004017353057861328 sec
2023-06-22 15:45:32,638 - WARNING - Finished tracing + transforming absolute for pjit in 0.00029540061950683594 sec
2023-06-22 15:45:32,639 - WARNING - Finished tracing + transforming fn for pjit in 0.00044345855712890625 sec
2023-06-22 15:45:32,640 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005805492401123047 sec
2023-06-22 15:45:32,641 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003509521484375 sec
2023-06-22 15:45:32,643 - WARNING - Finished tracing + transforming fn for pjit in 0.0004208087921142578 sec
2023-06-22 15:45:32,644 - WARNING - Finished tracing + transforming fn for pjit in 0.0004341602325439453 sec
2023-06-22 15:45:32,645 - WARNING - Finished tracing + transforming fn for pjit in 0.0003693103790283203 sec
2023-06-22 15:45:32,646 - WARNING - Finished tracing + transforming fn for pjit in 0.0004553794860839844 sec
2023-06-22 15:45:32,649 - WARNING - Finished tracing + transforming fn for pjit in 0.00036454200744628906 sec
2023-06-22 15:45:32,652 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002884864807128906 sec
2023-06-22 15:45:32,652 - WARNING - Finished tracing + transforming fn for pjit in 0.00040459632873535156 sec
2023-06-22 15:45:32,654 - WARNING - Finished tracing + transforming fn for pjit in 0.0003993511199951172 sec
2023-06-22 15:45:32,660 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006160736083984375 sec
2023-06-22 15:45:32,661 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016469955444335938 sec
2023-06-22 15:45:32,662 - WARNING - Finished tracing + transforming fn for pjit in 0.0003955364227294922 sec
2023-06-22 15:45:32,663 - WARNING - Finished tracing + transforming fn for pjit in 0.0003910064697265625 sec
2023-06-22 15:45:32,665 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004918575286865234 sec
2023-06-22 15:45:32,666 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042939186096191406 sec
2023-06-22 15:45:32,667 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003228187561035156 sec
2023-06-22 15:45:32,668 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042724609375 sec
2023-06-22 15:45:32,670 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003857612609863281 sec
2023-06-22 15:45:32,671 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040984153747558594 sec
2023-06-22 15:45:32,673 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006060600280761719 sec
2023-06-22 15:45:32,673 - WARNING - Finished tracing + transforming _where for pjit in 0.0016658306121826172 sec
2023-06-22 15:45:32,674 - WARNING - Finished tracing + transforming fn for pjit in 0.00042247772216796875 sec
2023-06-22 15:45:32,676 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004570484161376953 sec
2023-06-22 15:45:32,677 - WARNING - Finished tracing + transforming fn for pjit in 0.0003829002380371094 sec
2023-06-22 15:45:32,678 - WARNING - Finished tracing + transforming fn for pjit in 0.0003924369812011719 sec
2023-06-22 15:45:32,679 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003998279571533203 sec
2023-06-22 15:45:32,681 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004923343658447266 sec
2023-06-22 15:45:32,682 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006659030914306641 sec
2023-06-22 15:45:32,683 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046443939208984375 sec
2023-06-22 15:45:32,685 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003895759582519531 sec
2023-06-22 15:45:32,686 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003898143768310547 sec
2023-06-22 15:45:32,687 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004591941833496094 sec
2023-06-22 15:45:32,688 - WARNING - Finished tracing + transforming _where for pjit in 0.0014994144439697266 sec
2023-06-22 15:45:32,689 - WARNING - Finished tracing + transforming fn for pjit in 0.00044536590576171875 sec
2023-06-22 15:45:32,690 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004470348358154297 sec
2023-06-22 15:45:32,694 - WARNING - Finished tracing + transforming fn for pjit in 0.0003590583801269531 sec
2023-06-22 15:45:32,700 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00043392181396484375 sec
2023-06-22 15:45:32,702 - WARNING - Finished tracing + transforming fn for pjit in 0.0006036758422851562 sec
2023-06-22 15:45:32,703 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004475116729736328 sec
2023-06-22 15:45:32,704 - WARNING - Finished tracing + transforming fn for pjit in 0.0003905296325683594 sec
2023-06-22 15:45:32,710 - WARNING - Finished tracing + transforming fn for pjit in 0.000339508056640625 sec
2023-06-22 15:45:32,713 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002651214599609375 sec
2023-06-22 15:45:32,715 - WARNING - Finished tracing + transforming fn for pjit in 0.0005414485931396484 sec
2023-06-22 15:45:32,716 - WARNING - Finished tracing + transforming fn for pjit in 0.0004143714904785156 sec
2023-06-22 15:45:32,743 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11576342582702637 sec
2023-06-22 15:45:32,748 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020456314086914062 sec
2023-06-22 15:45:32,749 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019097328186035156 sec
2023-06-22 15:45:32,750 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00044536590576171875 sec
2023-06-22 15:45:32,754 - WARNING - Finished tracing + transforming fn for pjit in 0.0003540515899658203 sec
2023-06-22 15:45:32,755 - WARNING - Finished tracing + transforming fn for pjit in 0.0005166530609130859 sec
2023-06-22 15:45:32,757 - WARNING - Finished tracing + transforming fn for pjit in 0.0003619194030761719 sec
2023-06-22 15:45:32,767 - WARNING - Finished tracing + transforming fn for pjit in 0.00036525726318359375 sec
2023-06-22 15:45:32,769 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00036025047302246094 sec
2023-06-22 15:45:32,770 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004317760467529297 sec
2023-06-22 15:45:32,771 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003218650817871094 sec
2023-06-22 15:45:32,772 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005831718444824219 sec
2023-06-22 15:45:32,774 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040221214294433594 sec
2023-06-22 15:45:32,775 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039839744567871094 sec
2023-06-22 15:45:32,776 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004832744598388672 sec
2023-06-22 15:45:32,777 - WARNING - Finished tracing + transforming _where for pjit in 0.001543283462524414 sec
2023-06-22 15:45:32,778 - WARNING - Finished tracing + transforming fn for pjit in 0.0004558563232421875 sec
2023-06-22 15:45:32,779 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004699230194091797 sec
2023-06-22 15:45:32,781 - WARNING - Finished tracing + transforming fn for pjit in 0.0003879070281982422 sec
2023-06-22 15:45:32,782 - WARNING - Finished tracing + transforming fn for pjit in 0.0005121231079101562 sec
2023-06-22 15:45:32,803 - WARNING - Finished tracing + transforming fn for pjit in 0.0004189014434814453 sec
2023-06-22 15:45:32,838 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.09334707260131836 sec
2023-06-22 15:45:32,841 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020647048950195312 sec
2023-06-22 15:45:32,842 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002288818359375 sec
2023-06-22 15:45:32,843 - WARNING - Finished tracing + transforming _where for pjit in 0.0010976791381835938 sec
2023-06-22 15:45:32,844 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005216598510742188 sec
2023-06-22 15:45:32,844 - WARNING - Finished tracing + transforming trace for pjit in 0.004460334777832031 sec
2023-06-22 15:45:32,849 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00020456314086914062 sec
2023-06-22 15:45:32,850 - WARNING - Finished tracing + transforming tril for pjit in 0.0011224746704101562 sec
2023-06-22 15:45:32,851 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.003297090530395508 sec
2023-06-22 15:45:32,853 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00018310546875 sec
2023-06-22 15:45:32,853 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019598007202148438 sec
2023-06-22 15:45:32,857 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0026662349700927734 sec
2023-06-22 15:45:32,864 - WARNING - Finished tracing + transforming _solve for pjit in 0.016618728637695312 sec
2023-06-22 15:45:32,865 - WARNING - Finished tracing + transforming dot for pjit in 0.0004992485046386719 sec
2023-06-22 15:45:32,869 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.2436234951019287 sec
2023-06-22 15:45:32,872 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:45:32,927 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.054474592208862305 sec
2023-06-22 15:45:32,927 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:33,070 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1424875259399414 sec
2023-06-22 15:45:33,080 - INFO - initial test loss: 0.026968793995078906
2023-06-22 15:45:33,081 - INFO - initial test acc: 0.6449999809265137
2023-06-22 15:45:33,093 - WARNING - Finished tracing + transforming dot for pjit in 0.0007843971252441406 sec
2023-06-22 15:45:33,095 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006527900695800781 sec
2023-06-22 15:45:33,097 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008513927459716797 sec
2023-06-22 15:45:33,098 - WARNING - Finished tracing + transforming _mean for pjit in 0.0024564266204833984 sec
2023-06-22 15:45:33,101 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00042939186096191406 sec
2023-06-22 15:45:33,102 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00041675567626953125 sec
2023-06-22 15:45:33,103 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005481243133544922 sec
2023-06-22 15:45:33,106 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008482933044433594 sec
2023-06-22 15:45:33,107 - WARNING - Finished tracing + transforming _mean for pjit in 0.002599477767944336 sec
2023-06-22 15:45:33,109 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.0233001708984375 sec
2023-06-22 15:45:33,129 - WARNING - Finished tracing + transforming fn for pjit in 0.0006458759307861328 sec
2023-06-22 15:45:33,131 - WARNING - Finished tracing + transforming fn for pjit in 0.0006241798400878906 sec
2023-06-22 15:45:33,132 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005214214324951172 sec
2023-06-22 15:45:33,135 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006072521209716797 sec
2023-06-22 15:45:33,136 - WARNING - Finished tracing + transforming _where for pjit in 0.002149343490600586 sec
2023-06-22 15:45:33,154 - WARNING - Finished tracing + transforming fn for pjit in 0.0005946159362792969 sec
2023-06-22 15:45:33,156 - WARNING - Finished tracing + transforming fn for pjit in 0.00063323974609375 sec
2023-06-22 15:45:33,158 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005252361297607422 sec
2023-06-22 15:45:33,160 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0007226467132568359 sec
2023-06-22 15:45:33,162 - WARNING - Finished tracing + transforming _where for pjit in 0.0025687217712402344 sec
2023-06-22 15:45:33,224 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038695335388183594 sec
2023-06-22 15:45:33,305 - WARNING - Finished tracing + transforming fn for pjit in 0.0004608631134033203 sec
2023-06-22 15:45:33,306 - WARNING - Finished tracing + transforming fn for pjit in 0.0003972053527832031 sec
2023-06-22 15:45:33,307 - WARNING - Finished tracing + transforming square for pjit in 0.0003123283386230469 sec
2023-06-22 15:45:33,311 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00040841102600097656 sec
2023-06-22 15:45:33,314 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004391670227050781 sec
2023-06-22 15:45:33,315 - WARNING - Finished tracing + transforming fn for pjit in 0.0004870891571044922 sec
2023-06-22 15:45:33,316 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00039958953857421875 sec
2023-06-22 15:45:33,317 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003895759582519531 sec
2023-06-22 15:45:33,318 - WARNING - Finished tracing + transforming fn for pjit in 0.0004763603210449219 sec
2023-06-22 15:45:33,319 - WARNING - Finished tracing + transforming fn for pjit in 0.0004105567932128906 sec
2023-06-22 15:45:33,320 - WARNING - Finished tracing + transforming square for pjit in 0.00031185150146484375 sec
2023-06-22 15:45:33,324 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003895759582519531 sec
2023-06-22 15:45:33,327 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00030803680419921875 sec
2023-06-22 15:45:33,328 - WARNING - Finished tracing + transforming fn for pjit in 0.0004775524139404297 sec
2023-06-22 15:45:33,329 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00039005279541015625 sec
2023-06-22 15:45:33,330 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003924369812011719 sec
2023-06-22 15:45:33,331 - WARNING - Finished tracing + transforming update_fn for pjit in 0.24673032760620117 sec
2023-06-22 15:45:33,337 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,10]), ShapedArray(float32[92,10]), ShapedArray(float32[92,10]), ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,10]), ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:45:33,442 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10451579093933105 sec
2023-06-22 15:45:33,442 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:33,761 - WARNING - Finished XLA compilation of jit(update_fn) in 0.31868839263916016 sec
2023-06-22 15:45:34,096 - INFO - Distilling data from client: Client42
2023-06-22 15:45:34,096 - INFO - train loss: 0.017263703497336018
2023-06-22 15:45:34,097 - INFO - train acc: 0.8152174353599548
2023-06-22 15:45:34,134 - INFO - report:               precision    recall  f1-score   support

           6       0.68      0.74      0.71        61
           7       0.75      0.62      0.68        66
           8       0.57      0.73      0.64        11
           9       0.75      0.79      0.77        62

    accuracy                           0.71       200
   macro avg       0.69      0.72      0.70       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:45:34,135 - INFO - test loss 0.024427164033340772
2023-06-22 15:45:34,136 - INFO - test acc 0.7149999737739563
2023-06-22 15:45:34,514 - INFO - Distilling data from client: Client42
2023-06-22 15:45:34,515 - INFO - train loss: 0.014695176432215587
2023-06-22 15:45:34,515 - INFO - train acc: 0.8913043737411499
2023-06-22 15:45:34,535 - INFO - report:               precision    recall  f1-score   support

           6       0.65      0.74      0.69        61
           7       0.70      0.71      0.71        66
           8       0.50      0.45      0.48        11
           9       0.83      0.73      0.78        62

    accuracy                           0.71       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:45:34,536 - INFO - test loss 0.023917374987339434
2023-06-22 15:45:34,537 - INFO - test acc 0.7099999785423279
2023-06-22 15:45:35,027 - INFO - Distilling data from client: Client42
2023-06-22 15:45:35,028 - INFO - train loss: 0.014980536741402895
2023-06-22 15:45:35,028 - INFO - train acc: 0.8478261232376099
2023-06-22 15:45:35,052 - INFO - report:               precision    recall  f1-score   support

           6       0.62      0.77      0.69        61
           7       0.77      0.61      0.68        66
           8       0.50      0.55      0.52        11
           9       0.75      0.73      0.74        62

    accuracy                           0.69       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:45:35,058 - INFO - test loss 0.0252854096018243
2023-06-22 15:45:35,059 - INFO - test acc 0.6899999976158142
2023-06-22 15:45:35,552 - INFO - Distilling data from client: Client42
2023-06-22 15:45:35,553 - INFO - train loss: 0.010521185380555865
2023-06-22 15:45:35,553 - INFO - train acc: 0.9347826242446899
2023-06-22 15:45:35,568 - INFO - report:               precision    recall  f1-score   support

           6       0.70      0.75      0.72        61
           7       0.70      0.74      0.72        66
           8       0.46      0.55      0.50        11
           9       0.80      0.66      0.73        62

    accuracy                           0.71       200
   macro avg       0.67      0.68      0.67       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:45:35,584 - INFO - test loss 0.025007296087158865
2023-06-22 15:45:35,585 - INFO - test acc 0.7099999785423279
2023-06-22 15:45:36,019 - INFO - Distilling data from client: Client42
2023-06-22 15:45:36,020 - INFO - train loss: 0.012646496459445924
2023-06-22 15:45:36,020 - INFO - train acc: 0.9021739363670349
2023-06-22 15:45:36,040 - INFO - report:               precision    recall  f1-score   support

           6       0.68      0.72      0.70        61
           7       0.68      0.68      0.68        66
           8       0.36      0.36      0.36        11
           9       0.72      0.68      0.70        62

    accuracy                           0.68       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:45:36,054 - INFO - test loss 0.02423607451212459
2023-06-22 15:45:36,056 - INFO - test acc 0.675000011920929
2023-06-22 15:45:36,428 - INFO - Distilling data from client: Client42
2023-06-22 15:45:36,429 - INFO - train loss: 0.010486873576745711
2023-06-22 15:45:36,429 - INFO - train acc: 0.8913043737411499
2023-06-22 15:45:36,470 - INFO - report:               precision    recall  f1-score   support

           6       0.72      0.75      0.74        61
           7       0.72      0.76      0.74        66
           8       0.45      0.45      0.45        11
           9       0.86      0.77      0.81        62

    accuracy                           0.74       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.75      0.74      0.75       200

2023-06-22 15:45:36,471 - INFO - test loss 0.023836735624423
2023-06-22 15:45:36,471 - INFO - test acc 0.7450000047683716
2023-06-22 15:45:36,800 - INFO - Distilling data from client: Client42
2023-06-22 15:45:36,800 - INFO - train loss: 0.012918091412379477
2023-06-22 15:45:36,801 - INFO - train acc: 0.8804348111152649
2023-06-22 15:45:36,819 - INFO - report:               precision    recall  f1-score   support

           6       0.65      0.72      0.68        61
           7       0.72      0.64      0.68        66
           8       0.55      0.55      0.55        11
           9       0.73      0.74      0.74        62

    accuracy                           0.69       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:45:36,819 - INFO - test loss 0.024592587156695447
2023-06-22 15:45:36,820 - INFO - test acc 0.6899999976158142
2023-06-22 15:45:37,126 - INFO - Distilling data from client: Client42
2023-06-22 15:45:37,127 - INFO - train loss: 0.012318208521600016
2023-06-22 15:45:37,127 - INFO - train acc: 0.9130434989929199
2023-06-22 15:45:37,148 - INFO - report:               precision    recall  f1-score   support

           6       0.70      0.72      0.71        61
           7       0.70      0.73      0.71        66
           8       0.54      0.64      0.58        11
           9       0.84      0.74      0.79        62

    accuracy                           0.73       200
   macro avg       0.69      0.71      0.70       200
weighted avg       0.73      0.72      0.73       200

2023-06-22 15:45:37,149 - INFO - test loss 0.023580007393159682
2023-06-22 15:45:37,150 - INFO - test acc 0.7249999642372131
2023-06-22 15:45:37,480 - INFO - Distilling data from client: Client42
2023-06-22 15:45:37,481 - INFO - train loss: 0.011452638533612463
2023-06-22 15:45:37,482 - INFO - train acc: 0.9130434989929199
2023-06-22 15:45:37,503 - INFO - report:               precision    recall  f1-score   support

           6       0.68      0.84      0.75        61
           7       0.80      0.74      0.77        66
           8       0.40      0.36      0.38        11
           9       0.80      0.69      0.74        62

    accuracy                           0.73       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:45:37,504 - INFO - test loss 0.02316130866943985
2023-06-22 15:45:37,505 - INFO - test acc 0.73499995470047
2023-06-22 15:45:37,829 - INFO - Distilling data from client: Client42
2023-06-22 15:45:37,830 - INFO - train loss: 0.01202267880861867
2023-06-22 15:45:37,830 - INFO - train acc: 0.8913043737411499
2023-06-22 15:45:37,847 - INFO - report:               precision    recall  f1-score   support

           6       0.69      0.74      0.71        61
           7       0.72      0.70      0.71        66
           8       0.50      0.45      0.48        11
           9       0.75      0.74      0.75        62

    accuracy                           0.71       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:45:37,847 - INFO - test loss 0.02337569613705611
2023-06-22 15:45:37,848 - INFO - test acc 0.7099999785423279
2023-06-22 15:45:38,190 - INFO - Distilling data from client: Client42
2023-06-22 15:45:38,190 - INFO - train loss: 0.011006442258476455
2023-06-22 15:45:38,191 - INFO - train acc: 0.9239130616188049
2023-06-22 15:45:38,212 - INFO - report:               precision    recall  f1-score   support

           6       0.70      0.77      0.73        61
           7       0.75      0.70      0.72        66
           8       0.55      0.55      0.55        11
           9       0.79      0.77      0.78        62

    accuracy                           0.73       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:45:38,215 - INFO - test loss 0.02376693441411325
2023-06-22 15:45:38,216 - INFO - test acc 0.73499995470047
2023-06-22 15:45:38,572 - INFO - Distilling data from client: Client42
2023-06-22 15:45:38,572 - INFO - train loss: 0.011577363157374058
2023-06-22 15:45:38,573 - INFO - train acc: 0.8804348111152649
2023-06-22 15:45:38,591 - INFO - report:               precision    recall  f1-score   support

           6       0.69      0.72      0.70        61
           7       0.74      0.70      0.72        66
           8       0.50      0.55      0.52        11
           9       0.77      0.77      0.77        62

    accuracy                           0.72       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:45:38,592 - INFO - test loss 0.023711398126332512
2023-06-22 15:45:38,593 - INFO - test acc 0.7199999690055847
2023-06-22 15:45:38,923 - INFO - Distilling data from client: Client42
2023-06-22 15:45:38,923 - INFO - train loss: 0.011770327744720024
2023-06-22 15:45:38,923 - INFO - train acc: 0.9239130616188049
2023-06-22 15:45:38,938 - INFO - report:               precision    recall  f1-score   support

           6       0.62      0.77      0.69        61
           7       0.71      0.64      0.67        66
           8       0.55      0.55      0.55        11
           9       0.80      0.69      0.74        62

    accuracy                           0.69       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:45:38,939 - INFO - test loss 0.023867339756785896
2023-06-22 15:45:38,939 - INFO - test acc 0.6899999976158142
2023-06-22 15:45:39,256 - INFO - Distilling data from client: Client42
2023-06-22 15:45:39,256 - INFO - train loss: 0.010928674823571578
2023-06-22 15:45:39,257 - INFO - train acc: 0.9347826242446899
2023-06-22 15:45:39,291 - INFO - report:               precision    recall  f1-score   support

           6       0.69      0.90      0.78        61
           7       0.82      0.68      0.74        66
           8       0.50      0.45      0.48        11
           9       0.82      0.73      0.77        62

    accuracy                           0.75       200
   macro avg       0.71      0.69      0.69       200
weighted avg       0.76      0.75      0.75       200

2023-06-22 15:45:39,292 - INFO - test loss 0.023550074969875354
2023-06-22 15:45:39,292 - INFO - test acc 0.75
2023-06-22 15:45:39,636 - INFO - Distilling data from client: Client42
2023-06-22 15:45:39,637 - INFO - train loss: 0.010981581791793473
2023-06-22 15:45:39,638 - INFO - train acc: 0.9347826242446899
2023-06-22 15:45:39,659 - INFO - report:               precision    recall  f1-score   support

           6       0.69      0.70      0.70        61
           7       0.71      0.70      0.70        66
           8       0.45      0.45      0.45        11
           9       0.79      0.79      0.79        62

    accuracy                           0.71       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:45:39,662 - INFO - test loss 0.023606558700186915
2023-06-22 15:45:39,663 - INFO - test acc 0.7149999737739563
2023-06-22 15:45:40,053 - INFO - Distilling data from client: Client42
2023-06-22 15:45:40,054 - INFO - train loss: 0.011006878953617068
2023-06-22 15:45:40,054 - INFO - train acc: 0.9021739363670349
2023-06-22 15:45:40,077 - INFO - report:               precision    recall  f1-score   support

           6       0.67      0.74      0.70        61
           7       0.72      0.71      0.72        66
           8       0.41      0.64      0.50        11
           9       0.84      0.69      0.76        62

    accuracy                           0.71       200
   macro avg       0.66      0.69      0.67       200
weighted avg       0.73      0.71      0.71       200

2023-06-22 15:45:40,079 - INFO - test loss 0.024486214133822532
2023-06-22 15:45:40,079 - INFO - test acc 0.7099999785423279
2023-06-22 15:45:40,454 - INFO - Distilling data from client: Client42
2023-06-22 15:45:40,454 - INFO - train loss: 0.010566586384447433
2023-06-22 15:45:40,455 - INFO - train acc: 0.9021739363670349
2023-06-22 15:45:40,474 - INFO - report:               precision    recall  f1-score   support

           6       0.64      0.69      0.66        61
           7       0.67      0.68      0.68        66
           8       0.50      0.45      0.48        11
           9       0.77      0.71      0.74        62

    accuracy                           0.68       200
   macro avg       0.64      0.63      0.64       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:45:40,476 - INFO - test loss 0.02363114389960863
2023-06-22 15:45:40,477 - INFO - test acc 0.6800000071525574
2023-06-22 15:45:40,813 - INFO - Distilling data from client: Client42
2023-06-22 15:45:40,814 - INFO - train loss: 0.010679021607079495
2023-06-22 15:45:40,814 - INFO - train acc: 0.9347826242446899
2023-06-22 15:45:40,832 - INFO - report:               precision    recall  f1-score   support

           6       0.69      0.82      0.75        61
           7       0.76      0.68      0.72        66
           8       0.58      0.64      0.61        11
           9       0.79      0.73      0.76        62

    accuracy                           0.73       200
   macro avg       0.71      0.72      0.71       200
weighted avg       0.74      0.73      0.73       200

2023-06-22 15:45:40,833 - INFO - test loss 0.02477469440993364
2023-06-22 15:45:40,833 - INFO - test acc 0.73499995470047
2023-06-22 15:45:41,173 - INFO - Distilling data from client: Client42
2023-06-22 15:45:41,174 - INFO - train loss: 0.01019800384351642
2023-06-22 15:45:41,174 - INFO - train acc: 0.9347826242446899
2023-06-22 15:45:41,192 - INFO - report:               precision    recall  f1-score   support

           6       0.65      0.72      0.68        61
           7       0.75      0.73      0.74        66
           8       0.44      0.36      0.40        11
           9       0.80      0.76      0.78        62

    accuracy                           0.71       200
   macro avg       0.66      0.64      0.65       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:45:41,192 - INFO - test loss 0.023486429116827426
2023-06-22 15:45:41,192 - INFO - test acc 0.7149999737739563
2023-06-22 15:45:41,518 - INFO - Distilling data from client: Client42
2023-06-22 15:45:41,519 - INFO - train loss: 0.012338432981678669
2023-06-22 15:45:41,519 - INFO - train acc: 0.8586956858634949
2023-06-22 15:45:41,534 - INFO - report:               precision    recall  f1-score   support

           6       0.63      0.79      0.70        61
           7       0.70      0.64      0.67        66
           8       0.50      0.45      0.48        11
           9       0.76      0.66      0.71        62

    accuracy                           0.68       200
   macro avg       0.65      0.63      0.64       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:45:41,535 - INFO - test loss 0.024262584683386223
2023-06-22 15:45:41,535 - INFO - test acc 0.6800000071525574
2023-06-22 15:45:41,921 - INFO - Distilling data from client: Client42
2023-06-22 15:45:41,921 - INFO - train loss: 0.011264617122601793
2023-06-22 15:45:41,921 - INFO - train acc: 0.8804348111152649
2023-06-22 15:45:41,940 - INFO - report:               precision    recall  f1-score   support

           6       0.71      0.77      0.74        61
           7       0.70      0.76      0.73        66
           8       0.36      0.36      0.36        11
           9       0.81      0.68      0.74        62

    accuracy                           0.71       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:45:41,940 - INFO - test loss 0.02436669221341803
2023-06-22 15:45:41,941 - INFO - test acc 0.7149999737739563
2023-06-22 15:45:42,258 - INFO - Distilling data from client: Client42
2023-06-22 15:45:42,258 - INFO - train loss: 0.0111520901643125
2023-06-22 15:45:42,258 - INFO - train acc: 0.9239130616188049
2023-06-22 15:45:42,273 - INFO - report:               precision    recall  f1-score   support

           6       0.64      0.77      0.70        61
           7       0.68      0.67      0.67        66
           8       0.50      0.45      0.48        11
           9       0.81      0.68      0.74        62

    accuracy                           0.69       200
   macro avg       0.66      0.64      0.65       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:45:42,273 - INFO - test loss 0.023920620996911578
2023-06-22 15:45:42,273 - INFO - test acc 0.6899999976158142
2023-06-22 15:45:42,616 - INFO - Distilling data from client: Client42
2023-06-22 15:45:42,616 - INFO - train loss: 0.010639873388212442
2023-06-22 15:45:42,617 - INFO - train acc: 0.9130434989929199
2023-06-22 15:45:42,632 - INFO - report:               precision    recall  f1-score   support

           6       0.65      0.72      0.68        61
           7       0.70      0.67      0.68        66
           8       0.50      0.45      0.48        11
           9       0.76      0.73      0.74        62

    accuracy                           0.69       200
   macro avg       0.65      0.64      0.65       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:45:42,632 - INFO - test loss 0.023896677432570915
2023-06-22 15:45:42,632 - INFO - test acc 0.6899999976158142
2023-06-22 15:45:42,958 - INFO - Distilling data from client: Client42
2023-06-22 15:45:42,958 - INFO - train loss: 0.009193100156711976
2023-06-22 15:45:42,958 - INFO - train acc: 0.9239130616188049
2023-06-22 15:45:42,978 - INFO - report:               precision    recall  f1-score   support

           6       0.63      0.72      0.67        61
           7       0.73      0.70      0.71        66
           8       0.50      0.55      0.52        11
           9       0.78      0.69      0.74        62

    accuracy                           0.69       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.70      0.69      0.70       200

2023-06-22 15:45:42,978 - INFO - test loss 0.024200031586014605
2023-06-22 15:45:42,979 - INFO - test acc 0.6949999928474426
2023-06-22 15:45:43,327 - INFO - Distilling data from client: Client42
2023-06-22 15:45:43,327 - INFO - train loss: 0.009996626988227942
2023-06-22 15:45:43,328 - INFO - train acc: 0.9347826242446899
2023-06-22 15:45:43,346 - INFO - report:               precision    recall  f1-score   support

           6       0.64      0.74      0.69        61
           7       0.70      0.65      0.68        66
           8       0.67      0.55      0.60        11
           9       0.78      0.76      0.77        62

    accuracy                           0.70       200
   macro avg       0.70      0.67      0.68       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:45:43,347 - INFO - test loss 0.024135850486772165
2023-06-22 15:45:43,347 - INFO - test acc 0.7049999833106995
2023-06-22 15:45:43,351 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.00042629241943359375 sec
2023-06-22 15:45:43,353 - WARNING - Finished tracing + transforming fn for pjit in 0.0007534027099609375 sec
2023-06-22 15:45:43,354 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(int64[4]), ShapedArray(int64[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:45:43,358 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.003215312957763672 sec
2023-06-22 15:45:43,358 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:43,374 - WARNING - Finished XLA compilation of jit(fn) in 0.01530599594116211 sec
2023-06-22 15:45:43,376 - WARNING - Finished tracing + transforming jit(add) in 0.00041365623474121094 sec
2023-06-22 15:45:43,376 - DEBUG - Compiling add for with global shapes and types [ShapedArray(int64[4]), ShapedArray(int64[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:45:43,380 - WARNING - Finished jaxpr to MLIR module conversion jit(add) in 0.0036759376525878906 sec
2023-06-22 15:45:43,381 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:43,394 - WARNING - Finished XLA compilation of jit(add) in 0.013349056243896484 sec
2023-06-22 15:45:43,396 - WARNING - Finished tracing + transforming jit(select_n) in 0.0004286766052246094 sec
2023-06-22 15:45:43,397 - DEBUG - Compiling select_n for with global shapes and types [ShapedArray(bool[4]), ShapedArray(int64[4]), ShapedArray(int64[4])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:45:43,399 - WARNING - Finished jaxpr to MLIR module conversion jit(select_n) in 0.0022072792053222656 sec
2023-06-22 15:45:43,400 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:43,415 - WARNING - Finished XLA compilation of jit(select_n) in 0.014620542526245117 sec
2023-06-22 15:45:43,417 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003600120544433594 sec
2023-06-22 15:45:43,419 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.00033354759216308594 sec
2023-06-22 15:45:43,419 - DEBUG - Compiling convert_element_type for with global shapes and types [ShapedArray(int64[4])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:45:43,422 - WARNING - Finished jaxpr to MLIR module conversion jit(convert_element_type) in 0.0022361278533935547 sec
2023-06-22 15:45:43,422 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:43,436 - WARNING - Finished XLA compilation of jit(convert_element_type) in 0.013384342193603516 sec
2023-06-22 15:45:43,438 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0004031658172607422 sec
2023-06-22 15:45:43,438 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(int32[4])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:45:43,441 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0021109580993652344 sec
2023-06-22 15:45:43,441 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:43,453 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011260986328125 sec
2023-06-22 15:45:43,455 - WARNING - Finished tracing + transforming jit(gather) in 0.0004680156707763672 sec
2023-06-22 15:45:43,455 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[92,3,32,32]), ShapedArray(int32[4,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:45:43,457 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0017006397247314453 sec
2023-06-22 15:45:43,457 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:43,470 - WARNING - Finished XLA compilation of jit(gather) in 0.012770652770996094 sec
2023-06-22 15:45:43,472 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00033926963806152344 sec
2023-06-22 15:45:43,473 - WARNING - Finished tracing + transforming jit(copy) in 0.00017952919006347656 sec
2023-06-22 15:45:43,473 - DEBUG - Compiling copy for with global shapes and types [ShapedArray(float32[4,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:45:43,475 - WARNING - Finished jaxpr to MLIR module conversion jit(copy) in 0.001489400863647461 sec
2023-06-22 15:45:43,475 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:43,483 - WARNING - Finished XLA compilation of jit(copy) in 0.007628917694091797 sec
2023-06-22 15:45:43,495 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:43,506 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:43,516 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:43,527 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:43,538 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:43,548 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:43,558 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:43,569 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:43,572 - WARNING - Finished tracing + transforming _unstack for pjit in 0.0022110939025878906 sec
2023-06-22 15:45:43,573 - DEBUG - Compiling _unstack for with global shapes and types [ShapedArray(float32[4,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:45:43,576 - WARNING - Finished jaxpr to MLIR module conversion jit(_unstack) in 0.0028963088989257812 sec
2023-06-22 15:45:43,576 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:43,598 - WARNING - Finished XLA compilation of jit(_unstack) in 0.02193284034729004 sec
2023-06-22 15:45:43,611 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:43,623 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:43,634 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:43,647 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:45:44,326 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client42//synthetic.png
2023-06-22 15:45:44,349 - INFO - c: 0.0 and total_data_in_this_class: 260
2023-06-22 15:45:44,349 - INFO - c: 1.0 and total_data_in_this_class: 8
2023-06-22 15:45:44,349 - INFO - c: 2.0 and total_data_in_this_class: 254
2023-06-22 15:45:44,349 - INFO - c: 3.0 and total_data_in_this_class: 277
2023-06-22 15:45:44,349 - INFO - c: 0.0 and total_data_in_this_class: 73
2023-06-22 15:45:44,349 - INFO - c: 1.0 and total_data_in_this_class: 2
2023-06-22 15:45:44,349 - INFO - c: 2.0 and total_data_in_this_class: 69
2023-06-22 15:45:44,349 - INFO - c: 3.0 and total_data_in_this_class: 56
2023-06-22 15:45:44,394 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00040602684020996094 sec
2023-06-22 15:45:44,394 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:45:44,397 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0022051334381103516 sec
2023-06-22 15:45:44,397 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:44,414 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.016047000885009766 sec
2023-06-22 15:45:44,418 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003230571746826172 sec
2023-06-22 15:45:44,418 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:45:44,420 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0016560554504394531 sec
2023-06-22 15:45:44,420 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:44,432 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011137962341308594 sec
2023-06-22 15:45:44,436 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002193450927734375 sec
2023-06-22 15:45:44,438 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001976490020751953 sec
2023-06-22 15:45:44,439 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005023479461669922 sec
2023-06-22 15:45:44,441 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003407001495361328 sec
2023-06-22 15:45:44,442 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001857280731201172 sec
2023-06-22 15:45:44,443 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00045943260192871094 sec
2023-06-22 15:45:44,444 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004322528839111328 sec
2023-06-22 15:45:44,445 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003027915954589844 sec
2023-06-22 15:45:44,446 - WARNING - Finished tracing + transforming fn for pjit in 0.0004286766052246094 sec
2023-06-22 15:45:44,447 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0005953311920166016 sec
2023-06-22 15:45:44,449 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00034546852111816406 sec
2023-06-22 15:45:44,450 - WARNING - Finished tracing + transforming fn for pjit in 0.00034332275390625 sec
2023-06-22 15:45:44,451 - WARNING - Finished tracing + transforming fn for pjit in 0.00044608116149902344 sec
2023-06-22 15:45:44,452 - WARNING - Finished tracing + transforming fn for pjit in 0.0003600120544433594 sec
2023-06-22 15:45:44,453 - WARNING - Finished tracing + transforming fn for pjit in 0.00039124488830566406 sec
2023-06-22 15:45:44,455 - WARNING - Finished tracing + transforming fn for pjit in 0.0004107952117919922 sec
2023-06-22 15:45:44,458 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000274658203125 sec
2023-06-22 15:45:44,459 - WARNING - Finished tracing + transforming fn for pjit in 0.0003542900085449219 sec
2023-06-22 15:45:44,460 - WARNING - Finished tracing + transforming fn for pjit in 0.00037932395935058594 sec
2023-06-22 15:45:44,466 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005617141723632812 sec
2023-06-22 15:45:44,466 - WARNING - Finished tracing + transforming _mean for pjit in 0.001483917236328125 sec
2023-06-22 15:45:44,468 - WARNING - Finished tracing + transforming fn for pjit in 0.00036454200744628906 sec
2023-06-22 15:45:44,469 - WARNING - Finished tracing + transforming fn for pjit in 0.00033545494079589844 sec
2023-06-22 15:45:44,470 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00047135353088378906 sec
2023-06-22 15:45:44,471 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042176246643066406 sec
2023-06-22 15:45:44,472 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029015541076660156 sec
2023-06-22 15:45:44,474 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004291534423828125 sec
2023-06-22 15:45:44,475 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00034427642822265625 sec
2023-06-22 15:45:44,476 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003383159637451172 sec
2023-06-22 15:45:44,478 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0016493797302246094 sec
2023-06-22 15:45:44,479 - WARNING - Finished tracing + transforming _where for pjit in 0.0026459693908691406 sec
2023-06-22 15:45:44,480 - WARNING - Finished tracing + transforming fn for pjit in 0.00041985511779785156 sec
2023-06-22 15:45:44,481 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039839744567871094 sec
2023-06-22 15:45:44,482 - WARNING - Finished tracing + transforming fn for pjit in 0.00034499168395996094 sec
2023-06-22 15:45:44,483 - WARNING - Finished tracing + transforming fn for pjit in 0.00033211708068847656 sec
2023-06-22 15:45:44,484 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003333091735839844 sec
2023-06-22 15:45:44,486 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004596710205078125 sec
2023-06-22 15:45:44,487 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042510032653808594 sec
2023-06-22 15:45:44,488 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00043129920959472656 sec
2023-06-22 15:45:44,489 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000339508056640625 sec
2023-06-22 15:45:44,490 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003426074981689453 sec
2023-06-22 15:45:44,491 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003917217254638672 sec
2023-06-22 15:45:44,492 - WARNING - Finished tracing + transforming _where for pjit in 0.0012972354888916016 sec
2023-06-22 15:45:44,493 - WARNING - Finished tracing + transforming fn for pjit in 0.00039005279541015625 sec
2023-06-22 15:45:44,494 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038313865661621094 sec
2023-06-22 15:45:44,496 - WARNING - Finished tracing + transforming fn for pjit in 0.0003643035888671875 sec
2023-06-22 15:45:44,502 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004215240478515625 sec
2023-06-22 15:45:44,503 - WARNING - Finished tracing + transforming fn for pjit in 0.0005743503570556641 sec
2023-06-22 15:45:44,505 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004591941833496094 sec
2023-06-22 15:45:44,506 - WARNING - Finished tracing + transforming fn for pjit in 0.00035762786865234375 sec
2023-06-22 15:45:44,512 - WARNING - Finished tracing + transforming fn for pjit in 0.0003409385681152344 sec
2023-06-22 15:45:44,515 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002620220184326172 sec
2023-06-22 15:45:44,516 - WARNING - Finished tracing + transforming fn for pjit in 0.0005316734313964844 sec
2023-06-22 15:45:44,517 - WARNING - Finished tracing + transforming fn for pjit in 0.000354766845703125 sec
2023-06-22 15:45:44,543 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.1075434684753418 sec
2023-06-22 15:45:44,548 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019073486328125 sec
2023-06-22 15:45:44,549 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00017333030700683594 sec
2023-06-22 15:45:44,550 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00040268898010253906 sec
2023-06-22 15:45:44,553 - WARNING - Finished tracing + transforming fn for pjit in 0.000331878662109375 sec
2023-06-22 15:45:44,554 - WARNING - Finished tracing + transforming fn for pjit in 0.0004036426544189453 sec
2023-06-22 15:45:44,556 - WARNING - Finished tracing + transforming fn for pjit in 0.0003273487091064453 sec
2023-06-22 15:45:44,565 - WARNING - Finished tracing + transforming fn for pjit in 0.0003418922424316406 sec
2023-06-22 15:45:44,566 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032448768615722656 sec
2023-06-22 15:45:44,568 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040340423583984375 sec
2023-06-22 15:45:44,568 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002627372741699219 sec
2023-06-22 15:45:44,570 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004889965057373047 sec
2023-06-22 15:45:44,571 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037670135498046875 sec
2023-06-22 15:45:44,572 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003509521484375 sec
2023-06-22 15:45:44,573 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003921985626220703 sec
2023-06-22 15:45:44,574 - WARNING - Finished tracing + transforming _where for pjit in 0.0012869834899902344 sec
2023-06-22 15:45:44,575 - WARNING - Finished tracing + transforming fn for pjit in 0.0004146099090576172 sec
2023-06-22 15:45:44,576 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00043129920959472656 sec
2023-06-22 15:45:44,577 - WARNING - Finished tracing + transforming fn for pjit in 0.0003571510314941406 sec
2023-06-22 15:45:44,579 - WARNING - Finished tracing + transforming fn for pjit in 0.0008640289306640625 sec
2023-06-22 15:45:44,597 - WARNING - Finished tracing + transforming fn for pjit in 0.00034308433532714844 sec
2023-06-22 15:45:44,627 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08267521858215332 sec
2023-06-22 15:45:44,629 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00018405914306640625 sec
2023-06-22 15:45:44,631 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002167224884033203 sec
2023-06-22 15:45:44,631 - WARNING - Finished tracing + transforming _where for pjit in 0.0009732246398925781 sec
2023-06-22 15:45:44,632 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00046372413635253906 sec
2023-06-22 15:45:44,633 - WARNING - Finished tracing + transforming trace for pjit in 0.0039484500885009766 sec
2023-06-22 15:45:44,636 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00016260147094726562 sec
2023-06-22 15:45:44,638 - WARNING - Finished tracing + transforming tril for pjit in 0.0009784698486328125 sec
2023-06-22 15:45:44,639 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0028464794158935547 sec
2023-06-22 15:45:44,640 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001823902130126953 sec
2023-06-22 15:45:44,641 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.000171661376953125 sec
2023-06-22 15:45:44,644 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.002084970474243164 sec
2023-06-22 15:45:44,649 - WARNING - Finished tracing + transforming _solve for pjit in 0.01423954963684082 sec
2023-06-22 15:45:44,650 - WARNING - Finished tracing + transforming dot for pjit in 0.0004904270172119141 sec
2023-06-22 15:45:44,654 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.22025513648986816 sec
2023-06-22 15:45:44,657 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:45:44,708 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.0510869026184082 sec
2023-06-22 15:45:44,709 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:44,879 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1701490879058838 sec
2023-06-22 15:45:44,916 - INFO - initial test loss: 0.028775188642601535
2023-06-22 15:45:44,917 - INFO - initial test acc: 0.6800000071525574
2023-06-22 15:45:44,931 - WARNING - Finished tracing + transforming dot for pjit in 0.0008645057678222656 sec
2023-06-22 15:45:44,933 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0007081031799316406 sec
2023-06-22 15:45:44,936 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008389949798583984 sec
2023-06-22 15:45:44,937 - WARNING - Finished tracing + transforming _mean for pjit in 0.0023224353790283203 sec
2023-06-22 15:45:44,939 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00045680999755859375 sec
2023-06-22 15:45:44,940 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004229545593261719 sec
2023-06-22 15:45:44,942 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005774497985839844 sec
2023-06-22 15:45:44,944 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008289813995361328 sec
2023-06-22 15:45:44,945 - WARNING - Finished tracing + transforming _mean for pjit in 0.0024530887603759766 sec
2023-06-22 15:45:44,946 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.02260875701904297 sec
2023-06-22 15:45:44,966 - WARNING - Finished tracing + transforming fn for pjit in 0.0006079673767089844 sec
2023-06-22 15:45:44,968 - WARNING - Finished tracing + transforming fn for pjit in 0.0006227493286132812 sec
2023-06-22 15:45:44,969 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005214214324951172 sec
2023-06-22 15:45:44,971 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006268024444580078 sec
2023-06-22 15:45:44,972 - WARNING - Finished tracing + transforming _where for pjit in 0.0020494461059570312 sec
2023-06-22 15:45:44,991 - WARNING - Finished tracing + transforming fn for pjit in 0.0006122589111328125 sec
2023-06-22 15:45:44,993 - WARNING - Finished tracing + transforming fn for pjit in 0.0006058216094970703 sec
2023-06-22 15:45:44,994 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005066394805908203 sec
2023-06-22 15:45:44,996 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0007064342498779297 sec
2023-06-22 15:45:44,997 - WARNING - Finished tracing + transforming _where for pjit in 0.0020775794982910156 sec
2023-06-22 15:45:45,055 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003428459167480469 sec
2023-06-22 15:45:45,132 - WARNING - Finished tracing + transforming fn for pjit in 0.00041174888610839844 sec
2023-06-22 15:45:45,134 - WARNING - Finished tracing + transforming fn for pjit in 0.0003714561462402344 sec
2023-06-22 15:45:45,135 - WARNING - Finished tracing + transforming square for pjit in 0.00027441978454589844 sec
2023-06-22 15:45:45,138 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00034308433532714844 sec
2023-06-22 15:45:45,140 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038433074951171875 sec
2023-06-22 15:45:45,141 - WARNING - Finished tracing + transforming fn for pjit in 0.0004296302795410156 sec
2023-06-22 15:45:45,142 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003554821014404297 sec
2023-06-22 15:45:45,143 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037598609924316406 sec
2023-06-22 15:45:45,144 - WARNING - Finished tracing + transforming fn for pjit in 0.0004343986511230469 sec
2023-06-22 15:45:45,146 - WARNING - Finished tracing + transforming fn for pjit in 0.00038170814514160156 sec
2023-06-22 15:45:45,147 - WARNING - Finished tracing + transforming square for pjit in 0.00027823448181152344 sec
2023-06-22 15:45:45,150 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003380775451660156 sec
2023-06-22 15:45:45,152 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026488304138183594 sec
2023-06-22 15:45:45,153 - WARNING - Finished tracing + transforming fn for pjit in 0.0004317760467529297 sec
2023-06-22 15:45:45,154 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00036263465881347656 sec
2023-06-22 15:45:45,155 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038242340087890625 sec
2023-06-22 15:45:45,156 - WARNING - Finished tracing + transforming update_fn for pjit in 0.23362064361572266 sec
2023-06-22 15:45:45,162 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,10]), ShapedArray(float32[507,10]), ShapedArray(float32[507,10]), ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,10]), ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:45:45,259 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.09688115119934082 sec
2023-06-22 15:45:45,259 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:45:45,672 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4123721122741699 sec
2023-06-22 15:45:48,347 - INFO - Distilling data from client: Client43
2023-06-22 15:45:48,348 - INFO - train loss: 0.0032530716784636036
2023-06-22 15:45:48,348 - INFO - train acc: 0.9822485446929932
2023-06-22 15:45:48,546 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        73
           1       0.00      0.00      0.00         2
           2       0.63      0.62      0.63        69
           3       0.57      0.62      0.60        56

    accuracy                           0.66       200
   macro avg       0.49      0.49      0.49       200
weighted avg       0.65      0.66      0.65       200

2023-06-22 15:45:48,546 - INFO - test loss 0.026518904306658837
2023-06-22 15:45:48,546 - INFO - test acc 0.6549999713897705
2023-06-22 15:45:51,186 - INFO - Distilling data from client: Client43
2023-06-22 15:45:51,186 - INFO - train loss: 0.0014983460583052467
2023-06-22 15:45:51,187 - INFO - train acc: 1.0
2023-06-22 15:45:51,377 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.78      0.77        73
           1       0.00      0.00      0.00         2
           2       0.62      0.59      0.61        69
           3       0.59      0.61      0.60        56

    accuracy                           0.66       200
   macro avg       0.49      0.50      0.49       200
weighted avg       0.65      0.66      0.66       200

2023-06-22 15:45:51,377 - INFO - test loss 0.02720173395298957
2023-06-22 15:45:51,377 - INFO - test acc 0.6599999666213989
2023-06-22 15:45:53,982 - INFO - Distilling data from client: Client43
2023-06-22 15:45:53,982 - INFO - train loss: 0.001151423683619963
2023-06-22 15:45:53,983 - INFO - train acc: 1.0
2023-06-22 15:45:54,152 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.79      0.76        73
           1       0.00      0.00      0.00         2
           2       0.65      0.61      0.63        69
           3       0.62      0.62      0.62        56

    accuracy                           0.68       200
   macro avg       0.50      0.51      0.50       200
weighted avg       0.67      0.68      0.67       200

2023-06-22 15:45:54,152 - INFO - test loss 0.027396099322100607
2023-06-22 15:45:54,152 - INFO - test acc 0.675000011920929
2023-06-22 15:45:56,859 - INFO - Distilling data from client: Client43
2023-06-22 15:45:56,860 - INFO - train loss: 0.0010099381914191307
2023-06-22 15:45:56,860 - INFO - train acc: 1.0
2023-06-22 15:45:56,915 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.81      0.77        73
           1       0.00      0.00      0.00         2
           2       0.65      0.59      0.62        69
           3       0.61      0.62      0.62        56

    accuracy                           0.68       200
   macro avg       0.50      0.51      0.50       200
weighted avg       0.67      0.68      0.67       200

2023-06-22 15:45:56,917 - INFO - test loss 0.027921059667809472
2023-06-22 15:45:56,917 - INFO - test acc 0.675000011920929
2023-06-22 15:45:59,649 - INFO - Distilling data from client: Client43
2023-06-22 15:45:59,650 - INFO - train loss: 0.0008987066302059829
2023-06-22 15:45:59,650 - INFO - train acc: 1.0
2023-06-22 15:45:59,692 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.74      0.74        73
           1       0.00      0.00      0.00         2
           2       0.59      0.58      0.58        69
           3       0.58      0.61      0.59        56

    accuracy                           0.64       200
   macro avg       0.48      0.48      0.48       200
weighted avg       0.63      0.64      0.64       200

2023-06-22 15:45:59,693 - INFO - test loss 0.02802630211816335
2023-06-22 15:45:59,693 - INFO - test acc 0.6399999856948853
2023-06-22 15:46:02,394 - INFO - Distilling data from client: Client43
2023-06-22 15:46:02,395 - INFO - train loss: 0.0007417513907052287
2023-06-22 15:46:02,395 - INFO - train acc: 1.0
2023-06-22 15:46:02,445 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.73        73
           1       0.00      0.00      0.00         2
           2       0.61      0.57      0.59        69
           3       0.59      0.68      0.63        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.49       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:46:02,445 - INFO - test loss 0.027855169574378037
2023-06-22 15:46:02,445 - INFO - test acc 0.6499999761581421
2023-06-22 15:46:05,175 - INFO - Distilling data from client: Client43
2023-06-22 15:46:05,175 - INFO - train loss: 0.0009081197862073752
2023-06-22 15:46:05,175 - INFO - train acc: 1.0
2023-06-22 15:46:05,246 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.77      0.76        73
           1       0.00      0.00      0.00         2
           2       0.60      0.55      0.58        69
           3       0.60      0.66      0.63        56

    accuracy                           0.66       200
   macro avg       0.49      0.49      0.49       200
weighted avg       0.65      0.66      0.65       200

2023-06-22 15:46:05,248 - INFO - test loss 0.028066554676592914
2023-06-22 15:46:05,248 - INFO - test acc 0.6549999713897705
2023-06-22 15:46:07,983 - INFO - Distilling data from client: Client43
2023-06-22 15:46:07,983 - INFO - train loss: 0.0007054019153323198
2023-06-22 15:46:07,984 - INFO - train acc: 1.0
2023-06-22 15:46:08,037 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.73      0.74        73
           1       0.00      0.00      0.00         2
           2       0.58      0.55      0.57        69
           3       0.55      0.64      0.60        56

    accuracy                           0.64       200
   macro avg       0.47      0.48      0.48       200
weighted avg       0.63      0.64      0.63       200

2023-06-22 15:46:08,038 - INFO - test loss 0.028545846195593784
2023-06-22 15:46:08,038 - INFO - test acc 0.6349999904632568
2023-06-22 15:46:10,737 - INFO - Distilling data from client: Client43
2023-06-22 15:46:10,737 - INFO - train loss: 0.0005393669634772748
2023-06-22 15:46:10,738 - INFO - train acc: 1.0
2023-06-22 15:46:10,792 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.71      0.72        73
           1       0.00      0.00      0.00         2
           2       0.58      0.62      0.60        69
           3       0.58      0.57      0.58        56

    accuracy                           0.64       200
   macro avg       0.47      0.48      0.48       200
weighted avg       0.63      0.64      0.63       200

2023-06-22 15:46:10,792 - INFO - test loss 0.02877703359981873
2023-06-22 15:46:10,792 - INFO - test acc 0.6349999904632568
2023-06-22 15:46:13,456 - INFO - Distilling data from client: Client43
2023-06-22 15:46:13,457 - INFO - train loss: 0.0006626969394870039
2023-06-22 15:46:13,457 - INFO - train acc: 1.0
2023-06-22 15:46:13,514 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.75      0.73        73
           1       0.00      0.00      0.00         2
           2       0.60      0.55      0.58        69
           3       0.58      0.62      0.60        56

    accuracy                           0.64       200
   macro avg       0.48      0.48      0.48       200
weighted avg       0.63      0.64      0.64       200

2023-06-22 15:46:13,515 - INFO - test loss 0.027690241275592144
2023-06-22 15:46:13,515 - INFO - test acc 0.6399999856948853
2023-06-22 15:46:16,255 - INFO - Distilling data from client: Client43
2023-06-22 15:46:16,255 - INFO - train loss: 0.0005821154687062828
2023-06-22 15:46:16,256 - INFO - train acc: 1.0
2023-06-22 15:46:16,304 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.78      0.76        73
           1       0.00      0.00      0.00         2
           2       0.62      0.57      0.59        69
           3       0.60      0.64      0.62        56

    accuracy                           0.66       200
   macro avg       0.49      0.50      0.49       200
weighted avg       0.65      0.66      0.66       200

2023-06-22 15:46:16,305 - INFO - test loss 0.028166403610593884
2023-06-22 15:46:16,305 - INFO - test acc 0.6599999666213989
2023-06-22 15:46:19,223 - INFO - Distilling data from client: Client43
2023-06-22 15:46:19,224 - INFO - train loss: 0.0005197772990069108
2023-06-22 15:46:19,224 - INFO - train acc: 1.0
2023-06-22 15:46:19,277 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.75      0.75        73
           1       0.00      0.00      0.00         2
           2       0.59      0.59      0.59        69
           3       0.60      0.61      0.60        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.49       200
weighted avg       0.64      0.65      0.65       200

2023-06-22 15:46:19,277 - INFO - test loss 0.028636924372217067
2023-06-22 15:46:19,278 - INFO - test acc 0.6499999761581421
2023-06-22 15:46:22,377 - INFO - Distilling data from client: Client43
2023-06-22 15:46:22,377 - INFO - train loss: 0.0004952227719850953
2023-06-22 15:46:22,377 - INFO - train acc: 1.0
2023-06-22 15:46:22,428 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.75      0.76        73
           1       0.00      0.00      0.00         2
           2       0.60      0.55      0.58        69
           3       0.57      0.66      0.61        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.49       200
weighted avg       0.65      0.65      0.65       200

2023-06-22 15:46:22,429 - INFO - test loss 0.028484665466316866
2023-06-22 15:46:22,429 - INFO - test acc 0.6499999761581421
2023-06-22 15:46:25,286 - INFO - Distilling data from client: Client43
2023-06-22 15:46:25,287 - INFO - train loss: 0.0004640529161806738
2023-06-22 15:46:25,287 - INFO - train acc: 1.0
2023-06-22 15:46:25,332 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.77      0.76        73
           1       0.00      0.00      0.00         2
           2       0.60      0.55      0.58        69
           3       0.58      0.64      0.61        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.49       200
weighted avg       0.64      0.65      0.65       200

2023-06-22 15:46:25,333 - INFO - test loss 0.02802004035479156
2023-06-22 15:46:25,333 - INFO - test acc 0.6499999761581421
2023-06-22 15:46:28,121 - INFO - Distilling data from client: Client43
2023-06-22 15:46:28,122 - INFO - train loss: 0.0004972208671722408
2023-06-22 15:46:28,123 - INFO - train acc: 1.0
2023-06-22 15:46:28,182 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.77      0.75        73
           1       0.00      0.00      0.00         2
           2       0.61      0.55      0.58        69
           3       0.57      0.62      0.60        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.48       200
weighted avg       0.64      0.65      0.64       200

2023-06-22 15:46:28,184 - INFO - test loss 0.028509387999326664
2023-06-22 15:46:28,184 - INFO - test acc 0.6449999809265137
2023-06-22 15:46:31,046 - INFO - Distilling data from client: Client43
2023-06-22 15:46:31,047 - INFO - train loss: 0.0004789923420556074
2023-06-22 15:46:31,047 - INFO - train acc: 1.0
2023-06-22 15:46:31,100 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.73      0.72        73
           1       0.00      0.00      0.00         2
           2       0.62      0.57      0.59        69
           3       0.59      0.66      0.62        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.48       200
weighted avg       0.64      0.65      0.64       200

2023-06-22 15:46:31,101 - INFO - test loss 0.027674643847774558
2023-06-22 15:46:31,101 - INFO - test acc 0.6449999809265137
2023-06-22 15:46:33,889 - INFO - Distilling data from client: Client43
2023-06-22 15:46:33,889 - INFO - train loss: 0.0003604899691569455
2023-06-22 15:46:33,889 - INFO - train acc: 1.0
2023-06-22 15:46:33,965 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        73
           1       0.00      0.00      0.00         2
           2       0.60      0.55      0.58        69
           3       0.58      0.68      0.62        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.48       200
weighted avg       0.64      0.65      0.64       200

2023-06-22 15:46:33,965 - INFO - test loss 0.028556544517692916
2023-06-22 15:46:33,965 - INFO - test acc 0.6449999809265137
2023-06-22 15:46:36,788 - INFO - Distilling data from client: Client43
2023-06-22 15:46:36,790 - INFO - train loss: 0.00040037826470239884
2023-06-22 15:46:36,790 - INFO - train acc: 1.0
2023-06-22 15:46:36,854 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.68      0.70        73
           1       0.00      0.00      0.00         2
           2       0.59      0.59      0.59        69
           3       0.60      0.66      0.63        56

    accuracy                           0.64       200
   macro avg       0.48      0.48      0.48       200
weighted avg       0.64      0.64      0.64       200

2023-06-22 15:46:36,857 - INFO - test loss 0.02820381520698287
2023-06-22 15:46:36,858 - INFO - test acc 0.6399999856948853
2023-06-22 15:46:39,609 - INFO - Distilling data from client: Client43
2023-06-22 15:46:39,609 - INFO - train loss: 0.00041253824017085694
2023-06-22 15:46:39,609 - INFO - train acc: 1.0
2023-06-22 15:46:39,659 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.75      0.75        73
           1       0.00      0.00      0.00         2
           2       0.62      0.59      0.61        69
           3       0.58      0.62      0.60        56

    accuracy                           0.66       200
   macro avg       0.49      0.49      0.49       200
weighted avg       0.65      0.66      0.65       200

2023-06-22 15:46:39,659 - INFO - test loss 0.028111590033948183
2023-06-22 15:46:39,660 - INFO - test acc 0.6549999713897705
2023-06-22 15:46:42,508 - INFO - Distilling data from client: Client43
2023-06-22 15:46:42,508 - INFO - train loss: 0.0004450091367254339
2023-06-22 15:46:42,509 - INFO - train acc: 1.0
2023-06-22 15:46:42,571 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.74      0.73        73
           1       0.00      0.00      0.00         2
           2       0.60      0.61      0.60        69
           3       0.61      0.61      0.61        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.49       200
weighted avg       0.64      0.65      0.65       200

2023-06-22 15:46:42,573 - INFO - test loss 0.02816962699541547
2023-06-22 15:46:42,573 - INFO - test acc 0.6499999761581421
2023-06-22 15:46:45,346 - INFO - Distilling data from client: Client43
2023-06-22 15:46:45,347 - INFO - train loss: 0.0005110762985923471
2023-06-22 15:46:45,348 - INFO - train acc: 1.0
2023-06-22 15:46:45,400 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.74      0.72        73
           1       0.00      0.00      0.00         2
           2       0.63      0.55      0.59        69
           3       0.58      0.66      0.62        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.48       200
weighted avg       0.64      0.65      0.64       200

2023-06-22 15:46:45,401 - INFO - test loss 0.02825564320534079
2023-06-22 15:46:45,401 - INFO - test acc 0.6449999809265137
2023-06-22 15:46:48,182 - INFO - Distilling data from client: Client43
2023-06-22 15:46:48,182 - INFO - train loss: 0.0004322145151549819
2023-06-22 15:46:48,183 - INFO - train acc: 1.0
2023-06-22 15:46:48,242 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.75      0.74        73
           1       0.00      0.00      0.00         2
           2       0.62      0.58      0.60        69
           3       0.59      0.64      0.62        56

    accuracy                           0.66       200
   macro avg       0.49      0.49      0.49       200
weighted avg       0.65      0.66      0.65       200

2023-06-22 15:46:48,243 - INFO - test loss 0.02816652131198561
2023-06-22 15:46:48,244 - INFO - test acc 0.6549999713897705
2023-06-22 15:46:51,212 - INFO - Distilling data from client: Client43
2023-06-22 15:46:51,212 - INFO - train loss: 0.0004222380267176236
2023-06-22 15:46:51,212 - INFO - train acc: 1.0
2023-06-22 15:46:51,265 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.74      0.74        73
           1       0.00      0.00      0.00         2
           2       0.64      0.61      0.62        69
           3       0.62      0.68      0.65        56

    accuracy                           0.67       200
   macro avg       0.50      0.51      0.50       200
weighted avg       0.66      0.67      0.67       200

2023-06-22 15:46:51,266 - INFO - test loss 0.028368658771639823
2023-06-22 15:46:51,267 - INFO - test acc 0.6699999570846558
2023-06-22 15:46:54,036 - INFO - Distilling data from client: Client43
2023-06-22 15:46:54,036 - INFO - train loss: 0.0004609723649107331
2023-06-22 15:46:54,037 - INFO - train acc: 1.0
2023-06-22 15:46:54,104 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.75      0.75        73
           1       0.00      0.00      0.00         2
           2       0.63      0.58      0.61        69
           3       0.57      0.64      0.61        56

    accuracy                           0.66       200
   macro avg       0.49      0.49      0.49       200
weighted avg       0.65      0.66      0.65       200

2023-06-22 15:46:54,105 - INFO - test loss 0.02857409118178
2023-06-22 15:46:54,105 - INFO - test acc 0.6549999713897705
2023-06-22 15:46:56,914 - INFO - Distilling data from client: Client43
2023-06-22 15:46:56,914 - INFO - train loss: 0.0003541372929117307
2023-06-22 15:46:56,915 - INFO - train acc: 1.0
2023-06-22 15:46:56,975 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.70      0.71        73
           1       0.00      0.00      0.00         2
           2       0.57      0.58      0.58        69
           3       0.59      0.62      0.61        56

    accuracy                           0.63       200
   macro avg       0.47      0.48      0.47       200
weighted avg       0.63      0.63      0.63       200

2023-06-22 15:46:56,976 - INFO - test loss 0.02854810295520868
2023-06-22 15:46:56,976 - INFO - test acc 0.6299999952316284
2023-06-22 15:46:56,986 - WARNING - Finished tracing + transforming jit(gather) in 0.00103759765625 sec
2023-06-22 15:46:56,987 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[507,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:46:56,991 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.004252910614013672 sec
2023-06-22 15:46:56,992 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:46:57,020 - WARNING - Finished XLA compilation of jit(gather) in 0.02678537368774414 sec
2023-06-22 15:46:57,045 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:46:57,063 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:46:57,076 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:46:57,089 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:46:57,100 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:46:57,111 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:46:57,124 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:46:57,136 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:46:57,148 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:46:57,818 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client43//synthetic.png
2023-06-22 15:46:57,854 - INFO - c: 0.0 and total_data_in_this_class: 263
2023-06-22 15:46:57,854 - INFO - c: 5.0 and total_data_in_this_class: 270
2023-06-22 15:46:57,855 - INFO - c: 9.0 and total_data_in_this_class: 266
2023-06-22 15:46:57,855 - INFO - c: 0.0 and total_data_in_this_class: 70
2023-06-22 15:46:57,855 - INFO - c: 5.0 and total_data_in_this_class: 63
2023-06-22 15:46:57,855 - INFO - c: 9.0 and total_data_in_this_class: 67
2023-06-22 15:46:58,005 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07630586624145508 sec
2023-06-22 15:46:58,083 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07680320739746094 sec
2023-06-22 15:46:58,091 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16464018821716309 sec
2023-06-22 15:46:58,094 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:46:58,150 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05553627014160156 sec
2023-06-22 15:46:58,150 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:46:58,322 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17135334014892578 sec
2023-06-22 15:46:58,363 - INFO - initial test loss: 0.021516209376683956
2023-06-22 15:46:58,364 - INFO - initial test acc: 0.7299999594688416
2023-06-22 15:46:58,382 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.012814521789550781 sec
2023-06-22 15:46:58,572 - WARNING - Finished tracing + transforming update_fn for pjit in 0.20386099815368652 sec
2023-06-22 15:46:58,577 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:46:58,681 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10318779945373535 sec
2023-06-22 15:46:58,681 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:46:59,116 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4344055652618408 sec
2023-06-22 15:47:01,945 - INFO - Distilling data from client: Client44
2023-06-22 15:47:01,945 - INFO - train loss: 0.0017533485658490567
2023-06-22 15:47:01,945 - INFO - train acc: 1.0
2023-06-22 15:47:02,127 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.71      0.75        70
           5       0.72      0.81      0.76        63
           9       0.68      0.66      0.67        67

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:47:02,127 - INFO - test loss 0.021638721703655557
2023-06-22 15:47:02,127 - INFO - test acc 0.7249999642372131
2023-06-22 15:47:04,977 - INFO - Distilling data from client: Client44
2023-06-22 15:47:04,978 - INFO - train loss: 0.0008228041115841648
2023-06-22 15:47:04,978 - INFO - train acc: 1.0
2023-06-22 15:47:05,051 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.71      0.74        70
           5       0.69      0.79      0.74        63
           9       0.69      0.64      0.67        67

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:47:05,053 - INFO - test loss 0.021030490880125628
2023-06-22 15:47:05,054 - INFO - test acc 0.7149999737739563
2023-06-22 15:47:08,090 - INFO - Distilling data from client: Client44
2023-06-22 15:47:08,090 - INFO - train loss: 0.000702020146935355
2023-06-22 15:47:08,090 - INFO - train acc: 1.0
2023-06-22 15:47:08,148 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.69      0.72        70
           5       0.69      0.79      0.74        63
           9       0.69      0.67      0.68        67

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:47:08,148 - INFO - test loss 0.02145602839360074
2023-06-22 15:47:08,149 - INFO - test acc 0.7149999737739563
2023-06-22 15:47:11,370 - INFO - Distilling data from client: Client44
2023-06-22 15:47:11,370 - INFO - train loss: 0.0004841851705774171
2023-06-22 15:47:11,370 - INFO - train acc: 1.0
2023-06-22 15:47:11,434 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.67      0.71        70
           5       0.68      0.78      0.73        63
           9       0.68      0.66      0.67        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:47:11,434 - INFO - test loss 0.021862380937444876
2023-06-22 15:47:11,435 - INFO - test acc 0.699999988079071
2023-06-22 15:47:14,673 - INFO - Distilling data from client: Client44
2023-06-22 15:47:14,674 - INFO - train loss: 0.0004730436016741842
2023-06-22 15:47:14,674 - INFO - train acc: 1.0
2023-06-22 15:47:14,722 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.66      0.69        70
           5       0.72      0.79      0.76        63
           9       0.63      0.64      0.64        67

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:47:14,723 - INFO - test loss 0.022357052388093303
2023-06-22 15:47:14,723 - INFO - test acc 0.6949999928474426
2023-06-22 15:47:17,672 - INFO - Distilling data from client: Client44
2023-06-22 15:47:17,673 - INFO - train loss: 0.0004320946179882153
2023-06-22 15:47:17,673 - INFO - train acc: 1.0
2023-06-22 15:47:17,734 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.67      0.71        70
           5       0.70      0.81      0.75        63
           9       0.68      0.66      0.67        67

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:47:17,735 - INFO - test loss 0.0214527623346102
2023-06-22 15:47:17,735 - INFO - test acc 0.7099999785423279
2023-06-22 15:47:20,691 - INFO - Distilling data from client: Client44
2023-06-22 15:47:20,692 - INFO - train loss: 0.00026372529950662726
2023-06-22 15:47:20,692 - INFO - train acc: 1.0
2023-06-22 15:47:20,747 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.67      0.71        70
           5       0.71      0.81      0.76        63
           9       0.69      0.67      0.68        67

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:47:20,748 - INFO - test loss 0.022019545379243374
2023-06-22 15:47:20,748 - INFO - test acc 0.7149999737739563
2023-06-22 15:47:23,606 - INFO - Distilling data from client: Client44
2023-06-22 15:47:23,606 - INFO - train loss: 0.0003231840013818789
2023-06-22 15:47:23,606 - INFO - train acc: 1.0
2023-06-22 15:47:23,664 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.66      0.69        70
           5       0.69      0.78      0.73        63
           9       0.68      0.67      0.68        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:47:23,664 - INFO - test loss 0.022380272375378315
2023-06-22 15:47:23,664 - INFO - test acc 0.699999988079071
2023-06-22 15:47:26,512 - INFO - Distilling data from client: Client44
2023-06-22 15:47:26,512 - INFO - train loss: 0.000287995523185562
2023-06-22 15:47:26,513 - INFO - train acc: 1.0
2023-06-22 15:47:26,588 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.66      0.69        70
           5       0.70      0.79      0.75        63
           9       0.68      0.67      0.68        67

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:47:26,589 - INFO - test loss 0.02181873786097624
2023-06-22 15:47:26,590 - INFO - test acc 0.7049999833106995
2023-06-22 15:47:29,605 - INFO - Distilling data from client: Client44
2023-06-22 15:47:29,606 - INFO - train loss: 0.0002471675880932682
2023-06-22 15:47:29,606 - INFO - train acc: 1.0
2023-06-22 15:47:29,692 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.69      0.72        70
           5       0.71      0.79      0.75        63
           9       0.68      0.67      0.68        67

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:47:29,693 - INFO - test loss 0.02183030092328264
2023-06-22 15:47:29,693 - INFO - test acc 0.7149999737739563
2023-06-22 15:47:32,802 - INFO - Distilling data from client: Client44
2023-06-22 15:47:32,803 - INFO - train loss: 0.000253790058148161
2023-06-22 15:47:32,803 - INFO - train acc: 1.0
2023-06-22 15:47:32,861 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.67      0.70        70
           5       0.72      0.81      0.76        63
           9       0.71      0.69      0.70        67

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:47:32,861 - INFO - test loss 0.022015298227060668
2023-06-22 15:47:32,862 - INFO - test acc 0.7199999690055847
2023-06-22 15:47:36,016 - INFO - Distilling data from client: Client44
2023-06-22 15:47:36,016 - INFO - train loss: 0.00022493811259055693
2023-06-22 15:47:36,017 - INFO - train acc: 1.0
2023-06-22 15:47:36,075 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.69      0.70        70
           5       0.70      0.78      0.74        63
           9       0.69      0.64      0.67        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:47:36,077 - INFO - test loss 0.022153719947812518
2023-06-22 15:47:36,077 - INFO - test acc 0.699999988079071
2023-06-22 15:47:39,002 - INFO - Distilling data from client: Client44
2023-06-22 15:47:39,002 - INFO - train loss: 0.00019088268304116765
2023-06-22 15:47:39,002 - INFO - train acc: 1.0
2023-06-22 15:47:39,059 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.67      0.71        70
           5       0.68      0.79      0.73        63
           9       0.68      0.64      0.66        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:47:39,060 - INFO - test loss 0.022441318109142426
2023-06-22 15:47:39,061 - INFO - test acc 0.699999988079071
2023-06-22 15:47:42,016 - INFO - Distilling data from client: Client44
2023-06-22 15:47:42,017 - INFO - train loss: 0.000144333242593744
2023-06-22 15:47:42,017 - INFO - train acc: 1.0
2023-06-22 15:47:42,082 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.67      0.70        70
           5       0.74      0.79      0.76        63
           9       0.69      0.70      0.70        67

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:47:42,082 - INFO - test loss 0.0220438747809883
2023-06-22 15:47:42,083 - INFO - test acc 0.7199999690055847
2023-06-22 15:47:44,933 - INFO - Distilling data from client: Client44
2023-06-22 15:47:44,933 - INFO - train loss: 0.00015754772806191562
2023-06-22 15:47:44,933 - INFO - train acc: 1.0
2023-06-22 15:47:44,988 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.61      0.67        70
           5       0.68      0.81      0.74        63
           9       0.67      0.66      0.66        67

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:47:44,989 - INFO - test loss 0.022230391973939077
2023-06-22 15:47:44,989 - INFO - test acc 0.6899999976158142
2023-06-22 15:47:47,818 - INFO - Distilling data from client: Client44
2023-06-22 15:47:47,818 - INFO - train loss: 0.0001557042669528182
2023-06-22 15:47:47,818 - INFO - train acc: 1.0
2023-06-22 15:47:47,873 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.67      0.70        70
           5       0.69      0.78      0.73        63
           9       0.68      0.66      0.67        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:47:47,873 - INFO - test loss 0.022102566181448645
2023-06-22 15:47:47,874 - INFO - test acc 0.699999988079071
2023-06-22 15:47:50,901 - INFO - Distilling data from client: Client44
2023-06-22 15:47:50,901 - INFO - train loss: 0.00016115442767466876
2023-06-22 15:47:50,901 - INFO - train acc: 1.0
2023-06-22 15:47:50,949 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.67      0.70        70
           5       0.70      0.78      0.74        63
           9       0.68      0.67      0.68        67

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:47:50,950 - INFO - test loss 0.021720694037713904
2023-06-22 15:47:50,950 - INFO - test acc 0.7049999833106995
2023-06-22 15:47:53,902 - INFO - Distilling data from client: Client44
2023-06-22 15:47:53,902 - INFO - train loss: 0.00016864138633736087
2023-06-22 15:47:53,902 - INFO - train acc: 1.0
2023-06-22 15:47:53,948 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.67      0.69        70
           5       0.68      0.75      0.71        63
           9       0.66      0.64      0.65        67

    accuracy                           0.69       200
   macro avg       0.68      0.69      0.68       200
weighted avg       0.69      0.69      0.68       200

2023-06-22 15:47:53,949 - INFO - test loss 0.022388364949919436
2023-06-22 15:47:53,949 - INFO - test acc 0.6850000023841858
2023-06-22 15:47:57,040 - INFO - Distilling data from client: Client44
2023-06-22 15:47:57,040 - INFO - train loss: 0.00014966897111971743
2023-06-22 15:47:57,041 - INFO - train acc: 1.0
2023-06-22 15:47:57,093 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.67      0.70        70
           5       0.67      0.76      0.71        63
           9       0.68      0.64      0.66        67

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:47:57,094 - INFO - test loss 0.022249358320021395
2023-06-22 15:47:57,094 - INFO - test acc 0.6899999976158142
2023-06-22 15:48:00,370 - INFO - Distilling data from client: Client44
2023-06-22 15:48:00,371 - INFO - train loss: 0.00020546061100766143
2023-06-22 15:48:00,371 - INFO - train acc: 1.0
2023-06-22 15:48:00,423 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.67      0.71        70
           5       0.70      0.78      0.74        63
           9       0.69      0.69      0.69        67

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:48:00,424 - INFO - test loss 0.02226291262216305
2023-06-22 15:48:00,424 - INFO - test acc 0.7099999785423279
2023-06-22 15:48:03,646 - INFO - Distilling data from client: Client44
2023-06-22 15:48:03,647 - INFO - train loss: 0.0001756775572531031
2023-06-22 15:48:03,647 - INFO - train acc: 1.0
2023-06-22 15:48:03,710 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.63      0.66        70
           5       0.70      0.79      0.75        63
           9       0.70      0.69      0.69        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:48:03,712 - INFO - test loss 0.021912471246790063
2023-06-22 15:48:03,713 - INFO - test acc 0.699999988079071
2023-06-22 15:48:06,560 - INFO - Distilling data from client: Client44
2023-06-22 15:48:06,560 - INFO - train loss: 0.00015698594052290145
2023-06-22 15:48:06,560 - INFO - train acc: 1.0
2023-06-22 15:48:06,637 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.66      0.70        70
           5       0.71      0.79      0.75        63
           9       0.69      0.70      0.70        67

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:48:06,638 - INFO - test loss 0.021982240270374057
2023-06-22 15:48:06,638 - INFO - test acc 0.7149999737739563
2023-06-22 15:48:09,604 - INFO - Distilling data from client: Client44
2023-06-22 15:48:09,605 - INFO - train loss: 0.0001481815272139766
2023-06-22 15:48:09,605 - INFO - train acc: 1.0
2023-06-22 15:48:09,664 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.67      0.71        70
           5       0.69      0.79      0.74        63
           9       0.67      0.66      0.66        67

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:48:09,665 - INFO - test loss 0.02212318467805203
2023-06-22 15:48:09,665 - INFO - test acc 0.7049999833106995
2023-06-22 15:48:12,664 - INFO - Distilling data from client: Client44
2023-06-22 15:48:12,664 - INFO - train loss: 0.00015887589579171516
2023-06-22 15:48:12,664 - INFO - train acc: 1.0
2023-06-22 15:48:12,713 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.66      0.70        70
           5       0.69      0.81      0.74        63
           9       0.68      0.66      0.67        67

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:48:12,715 - INFO - test loss 0.022230189732874054
2023-06-22 15:48:12,715 - INFO - test acc 0.7049999833106995
2023-06-22 15:48:15,715 - INFO - Distilling data from client: Client44
2023-06-22 15:48:15,715 - INFO - train loss: 0.00014412864787284908
2023-06-22 15:48:15,716 - INFO - train acc: 1.0
2023-06-22 15:48:15,770 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.66      0.70        70
           5       0.68      0.79      0.74        63
           9       0.65      0.64      0.65        67

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.70      0.69      0.69       200

2023-06-22 15:48:15,771 - INFO - test loss 0.02238038463342695
2023-06-22 15:48:15,771 - INFO - test acc 0.6949999928474426
2023-06-22 15:48:15,803 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:48:15,821 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:48:15,841 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:48:15,860 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:48:15,874 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:48:15,885 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:48:15,899 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:48:15,911 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:48:15,924 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:48:16,493 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client44//synthetic.png
2023-06-22 15:48:16,518 - INFO - c: 3.0 and total_data_in_this_class: 15
2023-06-22 15:48:16,519 - INFO - c: 4.0 and total_data_in_this_class: 243
2023-06-22 15:48:16,519 - INFO - c: 5.0 and total_data_in_this_class: 264
2023-06-22 15:48:16,519 - INFO - c: 7.0 and total_data_in_this_class: 277
2023-06-22 15:48:16,519 - INFO - c: 3.0 and total_data_in_this_class: 5
2023-06-22 15:48:16,519 - INFO - c: 4.0 and total_data_in_this_class: 70
2023-06-22 15:48:16,519 - INFO - c: 5.0 and total_data_in_this_class: 69
2023-06-22 15:48:16,519 - INFO - c: 7.0 and total_data_in_this_class: 56
2023-06-22 15:48:16,566 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005578994750976562 sec
2023-06-22 15:48:16,567 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:48:16,570 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.002543210983276367 sec
2023-06-22 15:48:16,570 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:48:16,587 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01660609245300293 sec
2023-06-22 15:48:16,591 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00034999847412109375 sec
2023-06-22 15:48:16,592 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:48:16,594 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001819610595703125 sec
2023-06-22 15:48:16,594 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:48:16,608 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01373910903930664 sec
2023-06-22 15:48:16,614 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021409988403320312 sec
2023-06-22 15:48:16,616 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000209808349609375 sec
2023-06-22 15:48:16,617 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005364418029785156 sec
2023-06-22 15:48:16,619 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003924369812011719 sec
2023-06-22 15:48:16,620 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001964569091796875 sec
2023-06-22 15:48:16,621 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00046133995056152344 sec
2023-06-22 15:48:16,622 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004315376281738281 sec
2023-06-22 15:48:16,623 - WARNING - Finished tracing + transforming absolute for pjit in 0.00029850006103515625 sec
2023-06-22 15:48:16,624 - WARNING - Finished tracing + transforming fn for pjit in 0.00047469139099121094 sec
2023-06-22 15:48:16,626 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0006229877471923828 sec
2023-06-22 15:48:16,627 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00035834312438964844 sec
2023-06-22 15:48:16,629 - WARNING - Finished tracing + transforming fn for pjit in 0.00037169456481933594 sec
2023-06-22 15:48:16,630 - WARNING - Finished tracing + transforming fn for pjit in 0.00046825408935546875 sec
2023-06-22 15:48:16,631 - WARNING - Finished tracing + transforming fn for pjit in 0.0003991127014160156 sec
2023-06-22 15:48:16,632 - WARNING - Finished tracing + transforming fn for pjit in 0.0004355907440185547 sec
2023-06-22 15:48:16,635 - WARNING - Finished tracing + transforming fn for pjit in 0.0003368854522705078 sec
2023-06-22 15:48:16,638 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00028967857360839844 sec
2023-06-22 15:48:16,639 - WARNING - Finished tracing + transforming fn for pjit in 0.0003771781921386719 sec
2023-06-22 15:48:16,640 - WARNING - Finished tracing + transforming fn for pjit in 0.0004131793975830078 sec
2023-06-22 15:48:16,646 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006411075592041016 sec
2023-06-22 15:48:16,647 - WARNING - Finished tracing + transforming _mean for pjit in 0.0016739368438720703 sec
2023-06-22 15:48:16,648 - WARNING - Finished tracing + transforming fn for pjit in 0.0003790855407714844 sec
2023-06-22 15:48:16,649 - WARNING - Finished tracing + transforming fn for pjit in 0.0003962516784667969 sec
2023-06-22 15:48:16,651 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005383491516113281 sec
2023-06-22 15:48:16,653 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004596710205078125 sec
2023-06-22 15:48:16,654 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031948089599609375 sec
2023-06-22 15:48:16,655 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00048232078552246094 sec
2023-06-22 15:48:16,657 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040078163146972656 sec
2023-06-22 15:48:16,657 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00037407875061035156 sec
2023-06-22 15:48:16,659 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006699562072753906 sec
2023-06-22 15:48:16,660 - WARNING - Finished tracing + transforming _where for pjit in 0.001726388931274414 sec
2023-06-22 15:48:16,661 - WARNING - Finished tracing + transforming fn for pjit in 0.000457763671875 sec
2023-06-22 15:48:16,662 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044465065002441406 sec
2023-06-22 15:48:16,664 - WARNING - Finished tracing + transforming fn for pjit in 0.0004012584686279297 sec
2023-06-22 15:48:16,665 - WARNING - Finished tracing + transforming fn for pjit in 0.00037407875061035156 sec
2023-06-22 15:48:16,666 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040602684020996094 sec
2023-06-22 15:48:16,667 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046634674072265625 sec
2023-06-22 15:48:16,669 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004558563232421875 sec
2023-06-22 15:48:16,670 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045609474182128906 sec
2023-06-22 15:48:16,672 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004012584686279297 sec
2023-06-22 15:48:16,673 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040340423583984375 sec
2023-06-22 15:48:16,674 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00047397613525390625 sec
2023-06-22 15:48:16,675 - WARNING - Finished tracing + transforming _where for pjit in 0.0015583038330078125 sec
2023-06-22 15:48:16,676 - WARNING - Finished tracing + transforming fn for pjit in 0.0004673004150390625 sec
2023-06-22 15:48:16,677 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046181678771972656 sec
2023-06-22 15:48:16,681 - WARNING - Finished tracing + transforming fn for pjit in 0.0003757476806640625 sec
2023-06-22 15:48:16,688 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004494190216064453 sec
2023-06-22 15:48:16,689 - WARNING - Finished tracing + transforming fn for pjit in 0.0005826950073242188 sec
2023-06-22 15:48:16,690 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00047779083251953125 sec
2023-06-22 15:48:16,692 - WARNING - Finished tracing + transforming fn for pjit in 0.0003972053527832031 sec
2023-06-22 15:48:16,698 - WARNING - Finished tracing + transforming fn for pjit in 0.000370025634765625 sec
2023-06-22 15:48:16,701 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00028204917907714844 sec
2023-06-22 15:48:16,702 - WARNING - Finished tracing + transforming fn for pjit in 0.0005218982696533203 sec
2023-06-22 15:48:16,704 - WARNING - Finished tracing + transforming fn for pjit in 0.00040602684020996094 sec
2023-06-22 15:48:16,732 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.11908149719238281 sec
2023-06-22 15:48:16,738 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001990795135498047 sec
2023-06-22 15:48:16,738 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019311904907226562 sec
2023-06-22 15:48:16,739 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0004947185516357422 sec
2023-06-22 15:48:16,743 - WARNING - Finished tracing + transforming fn for pjit in 0.00039005279541015625 sec
2023-06-22 15:48:16,744 - WARNING - Finished tracing + transforming fn for pjit in 0.0004277229309082031 sec
2023-06-22 15:48:16,747 - WARNING - Finished tracing + transforming fn for pjit in 0.0006895065307617188 sec
2023-06-22 15:48:16,756 - WARNING - Finished tracing + transforming fn for pjit in 0.00036025047302246094 sec
2023-06-22 15:48:16,758 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00035691261291503906 sec
2023-06-22 15:48:16,759 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004467964172363281 sec
2023-06-22 15:48:16,760 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031876564025878906 sec
2023-06-22 15:48:16,762 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005862712860107422 sec
2023-06-22 15:48:16,763 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00036907196044921875 sec
2023-06-22 15:48:16,764 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039696693420410156 sec
2023-06-22 15:48:16,766 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005218982696533203 sec
2023-06-22 15:48:16,767 - WARNING - Finished tracing + transforming _where for pjit in 0.0015869140625 sec
2023-06-22 15:48:16,768 - WARNING - Finished tracing + transforming fn for pjit in 0.00045680999755859375 sec
2023-06-22 15:48:16,769 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044798851013183594 sec
2023-06-22 15:48:16,771 - WARNING - Finished tracing + transforming fn for pjit in 0.000396728515625 sec
2023-06-22 15:48:16,772 - WARNING - Finished tracing + transforming fn for pjit in 0.0005133152008056641 sec
2023-06-22 15:48:16,791 - WARNING - Finished tracing + transforming fn for pjit in 0.00034165382385253906 sec
2023-06-22 15:48:16,822 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08835721015930176 sec
2023-06-22 15:48:16,824 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002002716064453125 sec
2023-06-22 15:48:16,826 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00021982192993164062 sec
2023-06-22 15:48:16,826 - WARNING - Finished tracing + transforming _where for pjit in 0.0010762214660644531 sec
2023-06-22 15:48:16,828 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005207061767578125 sec
2023-06-22 15:48:16,829 - WARNING - Finished tracing + transforming trace for pjit in 0.00557255744934082 sec
2023-06-22 15:48:16,837 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0002262592315673828 sec
2023-06-22 15:48:16,840 - WARNING - Finished tracing + transforming tril for pjit in 0.0022296905517578125 sec
2023-06-22 15:48:16,840 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.005700111389160156 sec
2023-06-22 15:48:16,843 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00021195411682128906 sec
2023-06-22 15:48:16,843 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0002002716064453125 sec
2023-06-22 15:48:16,847 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.002568960189819336 sec
2023-06-22 15:48:16,854 - WARNING - Finished tracing + transforming _solve for pjit in 0.02035212516784668 sec
2023-06-22 15:48:16,855 - WARNING - Finished tracing + transforming dot for pjit in 0.0005128383636474609 sec
2023-06-22 15:48:16,859 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.2484748363494873 sec
2023-06-22 15:48:16,862 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:48:16,919 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05675458908081055 sec
2023-06-22 15:48:16,920 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:48:17,090 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.16975879669189453 sec
2023-06-22 15:48:17,147 - INFO - initial test loss: 0.03239411183600721
2023-06-22 15:48:17,147 - INFO - initial test acc: 0.5399999618530273
2023-06-22 15:48:17,162 - WARNING - Finished tracing + transforming dot for pjit in 0.0008528232574462891 sec
2023-06-22 15:48:17,164 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006937980651855469 sec
2023-06-22 15:48:17,167 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008378028869628906 sec
2023-06-22 15:48:17,168 - WARNING - Finished tracing + transforming _mean for pjit in 0.0026159286499023438 sec
2023-06-22 15:48:17,170 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00044536590576171875 sec
2023-06-22 15:48:17,172 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004820823669433594 sec
2023-06-22 15:48:17,174 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0007126331329345703 sec
2023-06-22 15:48:17,176 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.000888824462890625 sec
2023-06-22 15:48:17,177 - WARNING - Finished tracing + transforming _mean for pjit in 0.002710103988647461 sec
2023-06-22 15:48:17,179 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.025428056716918945 sec
2023-06-22 15:48:17,200 - WARNING - Finished tracing + transforming fn for pjit in 0.0006134510040283203 sec
2023-06-22 15:48:17,202 - WARNING - Finished tracing + transforming fn for pjit in 0.0006601810455322266 sec
2023-06-22 15:48:17,204 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005214214324951172 sec
2023-06-22 15:48:17,206 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006406307220458984 sec
2023-06-22 15:48:17,207 - WARNING - Finished tracing + transforming _where for pjit in 0.0024268627166748047 sec
2023-06-22 15:48:17,226 - WARNING - Finished tracing + transforming fn for pjit in 0.0005927085876464844 sec
2023-06-22 15:48:17,228 - WARNING - Finished tracing + transforming fn for pjit in 0.0006153583526611328 sec
2023-06-22 15:48:17,230 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005259513854980469 sec
2023-06-22 15:48:17,232 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0007121562957763672 sec
2023-06-22 15:48:17,234 - WARNING - Finished tracing + transforming _where for pjit in 0.002498626708984375 sec
2023-06-22 15:48:17,288 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003387928009033203 sec
2023-06-22 15:48:17,369 - WARNING - Finished tracing + transforming fn for pjit in 0.0004620552062988281 sec
2023-06-22 15:48:17,371 - WARNING - Finished tracing + transforming fn for pjit in 0.0004017353057861328 sec
2023-06-22 15:48:17,372 - WARNING - Finished tracing + transforming square for pjit in 0.0002949237823486328 sec
2023-06-22 15:48:17,375 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00037741661071777344 sec
2023-06-22 15:48:17,378 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004172325134277344 sec
2023-06-22 15:48:17,379 - WARNING - Finished tracing + transforming fn for pjit in 0.00045228004455566406 sec
2023-06-22 15:48:17,380 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003676414489746094 sec
2023-06-22 15:48:17,381 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003819465637207031 sec
2023-06-22 15:48:17,382 - WARNING - Finished tracing + transforming fn for pjit in 0.0004444122314453125 sec
2023-06-22 15:48:17,383 - WARNING - Finished tracing + transforming fn for pjit in 0.00040149688720703125 sec
2023-06-22 15:48:17,384 - WARNING - Finished tracing + transforming square for pjit in 0.0002880096435546875 sec
2023-06-22 15:48:17,387 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00039386749267578125 sec
2023-06-22 15:48:17,390 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002694129943847656 sec
2023-06-22 15:48:17,391 - WARNING - Finished tracing + transforming fn for pjit in 0.0004544258117675781 sec
2023-06-22 15:48:17,392 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00037097930908203125 sec
2023-06-22 15:48:17,393 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003833770751953125 sec
2023-06-22 15:48:17,394 - WARNING - Finished tracing + transforming update_fn for pjit in 0.24143052101135254 sec
2023-06-22 15:48:17,400 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,10]), ShapedArray(float32[486,10]), ShapedArray(float32[486,10]), ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,10]), ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:48:17,501 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.1010284423828125 sec
2023-06-22 15:48:17,501 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:48:17,934 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4321401119232178 sec
2023-06-22 15:48:20,673 - INFO - Distilling data from client: Client45
2023-06-22 15:48:20,674 - INFO - train loss: 0.0033602687011347154
2023-06-22 15:48:20,674 - INFO - train acc: 0.9958847165107727
2023-06-22 15:48:20,824 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.59      0.71      0.65        70
           5       0.58      0.54      0.56        69
           7       0.49      0.45      0.47        56

    accuracy                           0.56       200
   macro avg       0.41      0.42      0.42       200
weighted avg       0.54      0.56      0.55       200

2023-06-22 15:48:20,825 - INFO - test loss 0.03069412131502908
2023-06-22 15:48:20,825 - INFO - test acc 0.5600000023841858
2023-06-22 15:48:23,521 - INFO - Distilling data from client: Client45
2023-06-22 15:48:23,522 - INFO - train loss: 0.0019647025937434928
2023-06-22 15:48:23,522 - INFO - train acc: 0.9958847165107727
2023-06-22 15:48:23,570 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.56      0.70      0.62        70
           5       0.53      0.51      0.52        69
           7       0.51      0.43      0.47        56

    accuracy                           0.54       200
   macro avg       0.40      0.41      0.40       200
weighted avg       0.52      0.54      0.53       200

2023-06-22 15:48:23,572 - INFO - test loss 0.031192534316630865
2023-06-22 15:48:23,572 - INFO - test acc 0.5399999618530273
2023-06-22 15:48:26,465 - INFO - Distilling data from client: Client45
2023-06-22 15:48:26,465 - INFO - train loss: 0.0013487989048117403
2023-06-22 15:48:26,466 - INFO - train acc: 0.9999999403953552
2023-06-22 15:48:26,526 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.66      0.60        70
           5       0.56      0.55      0.55        69
           7       0.51      0.45      0.48        56

    accuracy                           0.55       200
   macro avg       0.41      0.41      0.41       200
weighted avg       0.53      0.55      0.54       200

2023-06-22 15:48:26,527 - INFO - test loss 0.030981017836619173
2023-06-22 15:48:26,528 - INFO - test acc 0.5450000166893005
2023-06-22 15:48:29,367 - INFO - Distilling data from client: Client45
2023-06-22 15:48:29,367 - INFO - train loss: 0.001185692516935356
2023-06-22 15:48:29,368 - INFO - train acc: 0.997942328453064
2023-06-22 15:48:29,426 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.67      0.60        70
           5       0.54      0.54      0.54        69
           7       0.51      0.41      0.46        56

    accuracy                           0.54       200
   macro avg       0.40      0.40      0.40       200
weighted avg       0.52      0.54      0.52       200

2023-06-22 15:48:29,426 - INFO - test loss 0.03127755632028135
2023-06-22 15:48:29,426 - INFO - test acc 0.5349999666213989
2023-06-22 15:48:32,127 - INFO - Distilling data from client: Client45
2023-06-22 15:48:32,127 - INFO - train loss: 0.0011324139019320686
2023-06-22 15:48:32,128 - INFO - train acc: 0.997942328453064
2023-06-22 15:48:32,288 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.59      0.73      0.65        70
           5       0.56      0.57      0.56        69
           7       0.52      0.41      0.46        56

    accuracy                           0.56       200
   macro avg       0.42      0.43      0.42       200
weighted avg       0.55      0.56      0.55       200

2023-06-22 15:48:32,288 - INFO - test loss 0.030498429990375404
2023-06-22 15:48:32,288 - INFO - test acc 0.5649999976158142
2023-06-22 15:48:35,086 - INFO - Distilling data from client: Client45
2023-06-22 15:48:35,086 - INFO - train loss: 0.001047035676769347
2023-06-22 15:48:35,086 - INFO - train acc: 0.997942328453064
2023-06-22 15:48:35,140 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.69      0.61        70
           5       0.53      0.51      0.52        69
           7       0.49      0.41      0.45        56

    accuracy                           0.53       200
   macro avg       0.39      0.40      0.39       200
weighted avg       0.51      0.53      0.52       200

2023-06-22 15:48:35,141 - INFO - test loss 0.032014233516557584
2023-06-22 15:48:35,141 - INFO - test acc 0.5299999713897705
2023-06-22 15:48:37,944 - INFO - Distilling data from client: Client45
2023-06-22 15:48:37,944 - INFO - train loss: 0.0009481858963635896
2023-06-22 15:48:37,944 - INFO - train acc: 0.9999999403953552
2023-06-22 15:48:38,059 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.53      0.64      0.58        70
           5       0.54      0.52      0.53        69
           7       0.46      0.39      0.42        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.38       200
weighted avg       0.50      0.52      0.50       200

2023-06-22 15:48:38,061 - INFO - test loss 0.03231032235607678
2023-06-22 15:48:38,062 - INFO - test acc 0.5149999856948853
2023-06-22 15:48:40,765 - INFO - Distilling data from client: Client45
2023-06-22 15:48:40,766 - INFO - train loss: 0.0008982462984365033
2023-06-22 15:48:40,767 - INFO - train acc: 0.9999999403953552
2023-06-22 15:48:40,820 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.54      0.64      0.58        70
           5       0.54      0.54      0.54        69
           7       0.46      0.39      0.42        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-06-22 15:48:40,822 - INFO - test loss 0.03184741705467702
2023-06-22 15:48:40,822 - INFO - test acc 0.5199999809265137
2023-06-22 15:48:43,499 - INFO - Distilling data from client: Client45
2023-06-22 15:48:43,499 - INFO - train loss: 0.000745119969233867
2023-06-22 15:48:43,500 - INFO - train acc: 0.9999999403953552
2023-06-22 15:48:43,557 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.53      0.66      0.59        70
           5       0.53      0.51      0.52        69
           7       0.46      0.39      0.42        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.38       200
weighted avg       0.50      0.52      0.50       200

2023-06-22 15:48:43,558 - INFO - test loss 0.03210941852847741
2023-06-22 15:48:43,558 - INFO - test acc 0.5149999856948853
2023-06-22 15:48:46,327 - INFO - Distilling data from client: Client45
2023-06-22 15:48:46,327 - INFO - train loss: 0.0006899299912253747
2023-06-22 15:48:46,327 - INFO - train acc: 0.9999999403953552
2023-06-22 15:48:46,396 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.59      0.74      0.66        70
           5       0.55      0.51      0.53        69
           7       0.48      0.41      0.44        56

    accuracy                           0.55       200
   macro avg       0.40      0.42      0.41       200
weighted avg       0.53      0.55      0.54       200

2023-06-22 15:48:46,397 - INFO - test loss 0.0314980019215143
2023-06-22 15:48:46,397 - INFO - test acc 0.550000011920929
2023-06-22 15:48:49,280 - INFO - Distilling data from client: Client45
2023-06-22 15:48:49,280 - INFO - train loss: 0.0007294034007103024
2023-06-22 15:48:49,281 - INFO - train acc: 0.9999999403953552
2023-06-22 15:48:49,336 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.54      0.63      0.58        70
           5       0.52      0.52      0.52        69
           7       0.49      0.43      0.46        56

    accuracy                           0.52       200
   macro avg       0.39      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-06-22 15:48:49,337 - INFO - test loss 0.03214475978524421
2023-06-22 15:48:49,338 - INFO - test acc 0.5199999809265137
2023-06-22 15:48:52,045 - INFO - Distilling data from client: Client45
2023-06-22 15:48:52,045 - INFO - train loss: 0.0006736530092736364
2023-06-22 15:48:52,045 - INFO - train acc: 0.9999999403953552
2023-06-22 15:48:52,095 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.56      0.67      0.61        70
           5       0.52      0.54      0.53        69
           7       0.53      0.43      0.48        56

    accuracy                           0.54       200
   macro avg       0.40      0.41      0.40       200
weighted avg       0.52      0.54      0.53       200

2023-06-22 15:48:52,096 - INFO - test loss 0.03184772996482445
2023-06-22 15:48:52,096 - INFO - test acc 0.5399999618530273
2023-06-22 15:48:54,664 - INFO - Distilling data from client: Client45
2023-06-22 15:48:54,665 - INFO - train loss: 0.0007240128414728416
2023-06-22 15:48:54,665 - INFO - train acc: 0.9999999403953552
2023-06-22 15:48:54,718 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.53      0.66      0.59        70
           5       0.55      0.55      0.55        69
           7       0.47      0.38      0.42        56

    accuracy                           0.53       200
   macro avg       0.39      0.40      0.39       200
weighted avg       0.51      0.53      0.51       200

2023-06-22 15:48:54,719 - INFO - test loss 0.03163009434832648
2023-06-22 15:48:54,720 - INFO - test acc 0.5249999761581421
2023-06-22 15:48:57,344 - INFO - Distilling data from client: Client45
2023-06-22 15:48:57,344 - INFO - train loss: 0.000576280091245172
2023-06-22 15:48:57,345 - INFO - train acc: 0.9999999403953552
2023-06-22 15:48:57,402 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.70      0.62        70
           5       0.55      0.52      0.53        69
           7       0.47      0.38      0.42        56

    accuracy                           0.53       200
   macro avg       0.39      0.40      0.39       200
weighted avg       0.51      0.53      0.52       200

2023-06-22 15:48:57,404 - INFO - test loss 0.03214840188789977
2023-06-22 15:48:57,404 - INFO - test acc 0.5299999713897705
2023-06-22 15:49:00,075 - INFO - Distilling data from client: Client45
2023-06-22 15:49:00,076 - INFO - train loss: 0.0006338942685889423
2023-06-22 15:49:00,077 - INFO - train acc: 0.9999999403953552
2023-06-22 15:49:00,126 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.66      0.60        70
           5       0.55      0.52      0.54        69
           7       0.45      0.41      0.43        56

    accuracy                           0.53       200
   macro avg       0.39      0.40      0.39       200
weighted avg       0.51      0.53      0.51       200

2023-06-22 15:49:00,126 - INFO - test loss 0.03133492457541306
2023-06-22 15:49:00,126 - INFO - test acc 0.5249999761581421
2023-06-22 15:49:02,707 - INFO - Distilling data from client: Client45
2023-06-22 15:49:02,708 - INFO - train loss: 0.0004995200170355861
2023-06-22 15:49:02,708 - INFO - train acc: 0.9999999403953552
2023-06-22 15:49:02,763 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.54      0.63      0.58        70
           5       0.51      0.51      0.51        69
           7       0.48      0.43      0.45        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-06-22 15:49:02,764 - INFO - test loss 0.0330029376375773
2023-06-22 15:49:02,764 - INFO - test acc 0.5149999856948853
2023-06-22 15:49:05,389 - INFO - Distilling data from client: Client45
2023-06-22 15:49:05,389 - INFO - train loss: 0.0005569944792847502
2023-06-22 15:49:05,389 - INFO - train acc: 0.9999999403953552
2023-06-22 15:49:05,441 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.69      0.61        70
           5       0.52      0.49      0.50        69
           7       0.48      0.39      0.43        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-06-22 15:49:05,443 - INFO - test loss 0.03248630096713451
2023-06-22 15:49:05,444 - INFO - test acc 0.5199999809265137
2023-06-22 15:49:08,032 - INFO - Distilling data from client: Client45
2023-06-22 15:49:08,032 - INFO - train loss: 0.000539005206599443
2023-06-22 15:49:08,033 - INFO - train acc: 0.9999999403953552
2023-06-22 15:49:08,098 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.66      0.60        70
           5       0.52      0.51      0.51        69
           7       0.47      0.41      0.44        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-06-22 15:49:08,100 - INFO - test loss 0.03169676270801816
2023-06-22 15:49:08,100 - INFO - test acc 0.5199999809265137
2023-06-22 15:49:10,906 - INFO - Distilling data from client: Client45
2023-06-22 15:49:10,907 - INFO - train loss: 0.00051321482768074
2023-06-22 15:49:10,907 - INFO - train acc: 0.9999999403953552
2023-06-22 15:49:10,955 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.52      0.61      0.57        70
           5       0.49      0.51      0.50        69
           7       0.43      0.36      0.39        56

    accuracy                           0.49       200
   macro avg       0.36      0.37      0.36       200
weighted avg       0.47      0.49      0.48       200

2023-06-22 15:49:10,957 - INFO - test loss 0.032788089159705595
2023-06-22 15:49:10,957 - INFO - test acc 0.4899999797344208
2023-06-22 15:49:13,576 - INFO - Distilling data from client: Client45
2023-06-22 15:49:13,576 - INFO - train loss: 0.0004998825782976039
2023-06-22 15:49:13,577 - INFO - train acc: 0.9999999403953552
2023-06-22 15:49:13,636 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.53      0.67      0.59        70
           5       0.53      0.51      0.52        69
           7       0.46      0.38      0.41        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.38       200
weighted avg       0.50      0.52      0.50       200

2023-06-22 15:49:13,637 - INFO - test loss 0.032042687454948315
2023-06-22 15:49:13,637 - INFO - test acc 0.5149999856948853
2023-06-22 15:49:16,289 - INFO - Distilling data from client: Client45
2023-06-22 15:49:16,290 - INFO - train loss: 0.00043975892089844404
2023-06-22 15:49:16,290 - INFO - train acc: 0.9999999403953552
2023-06-22 15:49:16,344 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.53      0.66      0.59        70
           5       0.54      0.51      0.52        69
           7       0.46      0.39      0.42        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.38       200
weighted avg       0.50      0.52      0.50       200

2023-06-22 15:49:16,345 - INFO - test loss 0.032515538198514465
2023-06-22 15:49:16,345 - INFO - test acc 0.5149999856948853
2023-06-22 15:49:18,992 - INFO - Distilling data from client: Client45
2023-06-22 15:49:18,992 - INFO - train loss: 0.00046367049406988116
2023-06-22 15:49:18,992 - INFO - train acc: 0.9999999403953552
2023-06-22 15:49:19,037 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.52      0.66      0.58        70
           5       0.52      0.51      0.51        69
           7       0.43      0.34      0.38        56

    accuracy                           0.50       200
   macro avg       0.37      0.38      0.37       200
weighted avg       0.48      0.50      0.49       200

2023-06-22 15:49:19,038 - INFO - test loss 0.032496019810237746
2023-06-22 15:49:19,038 - INFO - test acc 0.5
2023-06-22 15:49:21,681 - INFO - Distilling data from client: Client45
2023-06-22 15:49:21,682 - INFO - train loss: 0.0004982960257792703
2023-06-22 15:49:21,682 - INFO - train acc: 0.9999999403953552
2023-06-22 15:49:21,747 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.66      0.60        70
           5       0.51      0.51      0.51        69
           7       0.48      0.41      0.44        56

    accuracy                           0.52       200
   macro avg       0.39      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-06-22 15:49:21,750 - INFO - test loss 0.03183181647488517
2023-06-22 15:49:21,751 - INFO - test acc 0.5199999809265137
2023-06-22 15:49:24,400 - INFO - Distilling data from client: Client45
2023-06-22 15:49:24,400 - INFO - train loss: 0.00045077382659029254
2023-06-22 15:49:24,400 - INFO - train acc: 0.9999999403953552
2023-06-22 15:49:24,463 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.54      0.61      0.57        70
           5       0.51      0.51      0.51        69
           7       0.49      0.45      0.47        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-06-22 15:49:24,464 - INFO - test loss 0.032606560195931264
2023-06-22 15:49:24,464 - INFO - test acc 0.5149999856948853
2023-06-22 15:49:27,148 - INFO - Distilling data from client: Client45
2023-06-22 15:49:27,149 - INFO - train loss: 0.0004443870530029551
2023-06-22 15:49:27,149 - INFO - train acc: 0.9999999403953552
2023-06-22 15:49:27,194 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.60      0.57        70
           5       0.51      0.54      0.52        69
           7       0.45      0.41      0.43        56

    accuracy                           0.51       200
   macro avg       0.38      0.39      0.38       200
weighted avg       0.49      0.51      0.50       200

2023-06-22 15:49:27,196 - INFO - test loss 0.03198769484029931
2023-06-22 15:49:27,197 - INFO - test acc 0.5099999904632568
2023-06-22 15:49:27,205 - WARNING - Finished tracing + transforming jit(gather) in 0.0009365081787109375 sec
2023-06-22 15:49:27,206 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[486,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:49:27,210 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.003844022750854492 sec
2023-06-22 15:49:27,211 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:49:27,234 - WARNING - Finished XLA compilation of jit(gather) in 0.02185845375061035 sec
2023-06-22 15:49:27,262 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:49:27,280 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:49:27,297 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:49:27,308 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:49:27,319 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:49:27,330 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:49:27,342 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:49:27,353 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:49:27,365 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:49:27,913 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client45//synthetic.png
2023-06-22 15:49:27,945 - INFO - c: 0.0 and total_data_in_this_class: 273
2023-06-22 15:49:27,945 - INFO - c: 1.0 and total_data_in_this_class: 526
2023-06-22 15:49:27,945 - INFO - c: 0.0 and total_data_in_this_class: 60
2023-06-22 15:49:27,945 - INFO - c: 1.0 and total_data_in_this_class: 140
2023-06-22 15:49:27,988 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0005345344543457031 sec
2023-06-22 15:49:27,989 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:49:27,991 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.002412557601928711 sec
2023-06-22 15:49:27,992 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:49:28,009 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.016470909118652344 sec
2023-06-22 15:49:28,014 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0007114410400390625 sec
2023-06-22 15:49:28,014 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:49:28,018 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.002832174301147461 sec
2023-06-22 15:49:28,018 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:49:28,034 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.015418529510498047 sec
2023-06-22 15:49:28,041 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002512931823730469 sec
2023-06-22 15:49:28,043 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022864341735839844 sec
2023-06-22 15:49:28,044 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0006818771362304688 sec
2023-06-22 15:49:28,047 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00024819374084472656 sec
2023-06-22 15:49:29,312 - WARNING - Finished tracing + transforming _moveaxis for pjit in 1.2645587921142578 sec
2023-06-22 15:49:29,314 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0006968975067138672 sec
2023-06-22 15:49:29,315 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004825592041015625 sec
2023-06-22 15:49:29,316 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003256797790527344 sec
2023-06-22 15:49:29,317 - WARNING - Finished tracing + transforming fn for pjit in 0.00047588348388671875 sec
2023-06-22 15:49:29,319 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0007088184356689453 sec
2023-06-22 15:49:29,320 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002346038818359375 sec
2023-06-22 15:49:29,322 - WARNING - Finished tracing + transforming fn for pjit in 0.0004315376281738281 sec
2023-06-22 15:49:29,323 - WARNING - Finished tracing + transforming fn for pjit in 0.0004878044128417969 sec
2023-06-22 15:49:29,324 - WARNING - Finished tracing + transforming fn for pjit in 0.00040602684020996094 sec
2023-06-22 15:49:29,325 - WARNING - Finished tracing + transforming fn for pjit in 0.0004699230194091797 sec
2023-06-22 15:49:29,327 - WARNING - Finished tracing + transforming fn for pjit in 0.0004913806915283203 sec
2023-06-22 15:49:29,330 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003147125244140625 sec
2023-06-22 15:49:29,331 - WARNING - Finished tracing + transforming fn for pjit in 0.00038743019104003906 sec
2023-06-22 15:49:29,333 - WARNING - Finished tracing + transforming fn for pjit in 0.00042057037353515625 sec
2023-06-22 15:49:29,339 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005576610565185547 sec
2023-06-22 15:49:29,339 - WARNING - Finished tracing + transforming _mean for pjit in 0.0015718936920166016 sec
2023-06-22 15:49:29,341 - WARNING - Finished tracing + transforming fn for pjit in 0.0004107952117919922 sec
2023-06-22 15:49:29,342 - WARNING - Finished tracing + transforming fn for pjit in 0.00036025047302246094 sec
2023-06-22 15:49:29,343 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003597736358642578 sec
2023-06-22 15:49:29,345 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006158351898193359 sec
2023-06-22 15:49:29,346 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000316619873046875 sec
2023-06-22 15:49:29,347 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00048279762268066406 sec
2023-06-22 15:49:29,349 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004031658172607422 sec
2023-06-22 15:49:29,350 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039696693420410156 sec
2023-06-22 15:49:29,352 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006563663482666016 sec
2023-06-22 15:49:29,352 - WARNING - Finished tracing + transforming _where for pjit in 0.0017364025115966797 sec
2023-06-22 15:49:29,354 - WARNING - Finished tracing + transforming fn for pjit in 0.0004646778106689453 sec
2023-06-22 15:49:29,355 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046515464782714844 sec
2023-06-22 15:49:29,356 - WARNING - Finished tracing + transforming fn for pjit in 0.0004069805145263672 sec
2023-06-22 15:49:29,357 - WARNING - Finished tracing + transforming fn for pjit in 0.0004010200500488281 sec
2023-06-22 15:49:29,359 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003726482391357422 sec
2023-06-22 15:49:29,360 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00047397613525390625 sec
2023-06-22 15:49:29,361 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00046372413635253906 sec
2023-06-22 15:49:29,363 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004668235778808594 sec
2023-06-22 15:49:29,364 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00039386749267578125 sec
2023-06-22 15:49:29,365 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003981590270996094 sec
2023-06-22 15:49:29,366 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0004794597625732422 sec
2023-06-22 15:49:29,367 - WARNING - Finished tracing + transforming _where for pjit in 0.0016734600067138672 sec
2023-06-22 15:49:29,368 - WARNING - Finished tracing + transforming fn for pjit in 0.00042176246643066406 sec
2023-06-22 15:49:29,370 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004591941833496094 sec
2023-06-22 15:49:29,372 - WARNING - Finished tracing + transforming fn for pjit in 0.0003972053527832031 sec
2023-06-22 15:49:29,378 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00043129920959472656 sec
2023-06-22 15:49:29,380 - WARNING - Finished tracing + transforming fn for pjit in 0.0004525184631347656 sec
2023-06-22 15:49:29,381 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004734992980957031 sec
2023-06-22 15:49:29,382 - WARNING - Finished tracing + transforming fn for pjit in 0.00039577484130859375 sec
2023-06-22 15:49:29,388 - WARNING - Finished tracing + transforming fn for pjit in 0.00036334991455078125 sec
2023-06-22 15:49:29,392 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002770423889160156 sec
2023-06-22 15:49:29,392 - WARNING - Finished tracing + transforming fn for pjit in 0.00037741661071777344 sec
2023-06-22 15:49:29,394 - WARNING - Finished tracing + transforming fn for pjit in 0.0004069805145263672 sec
2023-06-22 15:49:29,422 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 1.3821382522583008 sec
2023-06-22 15:49:29,427 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022339820861816406 sec
2023-06-22 15:49:29,428 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020170211791992188 sec
2023-06-22 15:49:29,429 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00044989585876464844 sec
2023-06-22 15:49:29,433 - WARNING - Finished tracing + transforming fn for pjit in 0.00035262107849121094 sec
2023-06-22 15:49:29,434 - WARNING - Finished tracing + transforming fn for pjit in 0.0004210472106933594 sec
2023-06-22 15:49:29,436 - WARNING - Finished tracing + transforming fn for pjit in 0.00040411949157714844 sec
2023-06-22 15:49:29,446 - WARNING - Finished tracing + transforming fn for pjit in 0.00037384033203125 sec
2023-06-22 15:49:29,448 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038170814514160156 sec
2023-06-22 15:49:29,449 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004563331604003906 sec
2023-06-22 15:49:29,450 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003154277801513672 sec
2023-06-22 15:49:29,452 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006096363067626953 sec
2023-06-22 15:49:29,453 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003952980041503906 sec
2023-06-22 15:49:29,454 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040030479431152344 sec
2023-06-22 15:49:29,456 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0005033016204833984 sec
2023-06-22 15:49:29,456 - WARNING - Finished tracing + transforming _where for pjit in 0.0015857219696044922 sec
2023-06-22 15:49:29,458 - WARNING - Finished tracing + transforming fn for pjit in 0.00047016143798828125 sec
2023-06-22 15:49:29,459 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004305839538574219 sec
2023-06-22 15:49:29,461 - WARNING - Finished tracing + transforming fn for pjit in 0.0005738735198974609 sec
2023-06-22 15:49:29,461 - WARNING - Finished tracing + transforming fn for pjit in 0.0003924369812011719 sec
2023-06-22 15:49:29,482 - WARNING - Finished tracing + transforming fn for pjit in 0.0003514289855957031 sec
2023-06-22 15:49:29,513 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.09000205993652344 sec
2023-06-22 15:49:29,516 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021529197692871094 sec
2023-06-22 15:49:29,517 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00021719932556152344 sec
2023-06-22 15:49:29,518 - WARNING - Finished tracing + transforming _where for pjit in 0.0010759830474853516 sec
2023-06-22 15:49:29,519 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005328655242919922 sec
2023-06-22 15:49:29,520 - WARNING - Finished tracing + transforming trace for pjit in 0.004408836364746094 sec
2023-06-22 15:49:29,524 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0001888275146484375 sec
2023-06-22 15:49:29,526 - WARNING - Finished tracing + transforming tril for pjit in 0.0010602474212646484 sec
2023-06-22 15:49:29,526 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.002880096435546875 sec
2023-06-22 15:49:29,528 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019478797912597656 sec
2023-06-22 15:49:29,528 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019049644470214844 sec
2023-06-22 15:49:29,532 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.002374410629272461 sec
2023-06-22 15:49:29,538 - WARNING - Finished tracing + transforming _solve for pjit in 0.015752553939819336 sec
2023-06-22 15:49:29,539 - WARNING - Finished tracing + transforming dot for pjit in 0.0005533695220947266 sec
2023-06-22 15:49:29,543 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 1.5067143440246582 sec
2023-06-22 15:49:29,546 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:49:29,599 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05254340171813965 sec
2023-06-22 15:49:29,599 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:49:29,769 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.16872739791870117 sec
2023-06-22 15:49:29,812 - INFO - initial test loss: 0.01792233714506402
2023-06-22 15:49:29,812 - INFO - initial test acc: 0.7649999856948853
2023-06-22 15:49:29,826 - WARNING - Finished tracing + transforming dot for pjit in 0.000843048095703125 sec
2023-06-22 15:49:29,827 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0007023811340332031 sec
2023-06-22 15:49:29,830 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008289813995361328 sec
2023-06-22 15:49:29,831 - WARNING - Finished tracing + transforming _mean for pjit in 0.0022842884063720703 sec
2023-06-22 15:49:29,833 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004477500915527344 sec
2023-06-22 15:49:29,834 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00042319297790527344 sec
2023-06-22 15:49:29,835 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005731582641601562 sec
2023-06-22 15:49:29,838 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.000865936279296875 sec
2023-06-22 15:49:29,839 - WARNING - Finished tracing + transforming _mean for pjit in 0.0024411678314208984 sec
2023-06-22 15:49:29,840 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.022381305694580078 sec
2023-06-22 15:49:29,860 - WARNING - Finished tracing + transforming fn for pjit in 0.0006008148193359375 sec
2023-06-22 15:49:29,862 - WARNING - Finished tracing + transforming fn for pjit in 0.0006256103515625 sec
2023-06-22 15:49:29,863 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005297660827636719 sec
2023-06-22 15:49:29,865 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006091594696044922 sec
2023-06-22 15:49:29,866 - WARNING - Finished tracing + transforming _where for pjit in 0.0020987987518310547 sec
2023-06-22 15:49:29,885 - WARNING - Finished tracing + transforming fn for pjit in 0.0005915164947509766 sec
2023-06-22 15:49:29,887 - WARNING - Finished tracing + transforming fn for pjit in 0.0006122589111328125 sec
2023-06-22 15:49:29,888 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005109310150146484 sec
2023-06-22 15:49:29,890 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0007212162017822266 sec
2023-06-22 15:49:29,891 - WARNING - Finished tracing + transforming _where for pjit in 0.0022521018981933594 sec
2023-06-22 15:49:29,951 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003693103790283203 sec
2023-06-22 15:49:30,031 - WARNING - Finished tracing + transforming fn for pjit in 0.0004417896270751953 sec
2023-06-22 15:49:30,032 - WARNING - Finished tracing + transforming fn for pjit in 0.0004189014434814453 sec
2023-06-22 15:49:30,033 - WARNING - Finished tracing + transforming square for pjit in 0.00030231475830078125 sec
2023-06-22 15:49:30,037 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00036215782165527344 sec
2023-06-22 15:49:30,040 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004417896270751953 sec
2023-06-22 15:49:30,041 - WARNING - Finished tracing + transforming fn for pjit in 0.0004749298095703125 sec
2023-06-22 15:49:30,042 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00040149688720703125 sec
2023-06-22 15:49:30,043 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00042057037353515625 sec
2023-06-22 15:49:30,044 - WARNING - Finished tracing + transforming fn for pjit in 0.00047278404235839844 sec
2023-06-22 15:49:30,045 - WARNING - Finished tracing + transforming fn for pjit in 0.00041985511779785156 sec
2023-06-22 15:49:30,046 - WARNING - Finished tracing + transforming square for pjit in 0.0002980232238769531 sec
2023-06-22 15:49:30,050 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003380775451660156 sec
2023-06-22 15:49:30,053 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002884864807128906 sec
2023-06-22 15:49:30,054 - WARNING - Finished tracing + transforming fn for pjit in 0.0004608631134033203 sec
2023-06-22 15:49:30,055 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00039076805114746094 sec
2023-06-22 15:49:30,056 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004093647003173828 sec
2023-06-22 15:49:30,057 - WARNING - Finished tracing + transforming update_fn for pjit in 0.24040555953979492 sec
2023-06-22 15:49:30,063 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,10]), ShapedArray(float32[364,10]), ShapedArray(float32[364,10]), ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,10]), ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:49:30,162 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.09886622428894043 sec
2023-06-22 15:49:30,162 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:49:30,549 - WARNING - Finished XLA compilation of jit(update_fn) in 0.38661909103393555 sec
2023-06-22 15:49:32,793 - INFO - Distilling data from client: Client46
2023-06-22 15:49:32,793 - INFO - train loss: 0.0030305057607989874
2023-06-22 15:49:32,793 - INFO - train acc: 0.9917582869529724
2023-06-22 15:49:32,922 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.72      0.73        60
           1       0.88      0.89      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:49:32,923 - INFO - test loss 0.012686659534831897
2023-06-22 15:49:32,923 - INFO - test acc 0.8399999737739563
2023-06-22 15:49:34,946 - INFO - Distilling data from client: Client46
2023-06-22 15:49:34,946 - INFO - train loss: 0.0024882853163783027
2023-06-22 15:49:34,946 - INFO - train acc: 0.9917582869529724
2023-06-22 15:49:35,057 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.73      0.76        60
           1       0.89      0.91      0.90       140

    accuracy                           0.86       200
   macro avg       0.84      0.82      0.83       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:49:35,057 - INFO - test loss 0.01377887361163604
2023-06-22 15:49:35,057 - INFO - test acc 0.85999995470047
2023-06-22 15:49:37,080 - INFO - Distilling data from client: Client46
2023-06-22 15:49:37,080 - INFO - train loss: 0.00204631909606679
2023-06-22 15:49:37,080 - INFO - train acc: 0.9945055246353149
2023-06-22 15:49:37,126 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.70      0.74        60
           1       0.88      0.91      0.90       140

    accuracy                           0.85       200
   macro avg       0.83      0.81      0.82       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:49:37,127 - INFO - test loss 0.01340421973941331
2023-06-22 15:49:37,127 - INFO - test acc 0.8499999642372131
2023-06-22 15:49:39,282 - INFO - Distilling data from client: Client46
2023-06-22 15:49:39,283 - INFO - train loss: 0.0016884062425400195
2023-06-22 15:49:39,283 - INFO - train acc: 0.9945055246353149
2023-06-22 15:49:39,326 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.72      0.71        60
           1       0.88      0.87      0.87       140

    accuracy                           0.82       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.83      0.82      0.83       200

2023-06-22 15:49:39,327 - INFO - test loss 0.014170569550817065
2023-06-22 15:49:39,327 - INFO - test acc 0.824999988079071
2023-06-22 15:49:41,439 - INFO - Distilling data from client: Client46
2023-06-22 15:49:41,440 - INFO - train loss: 0.0014085715411758415
2023-06-22 15:49:41,440 - INFO - train acc: 1.0
2023-06-22 15:49:41,480 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.70      0.72        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:49:41,481 - INFO - test loss 0.013897355191903485
2023-06-22 15:49:41,481 - INFO - test acc 0.8399999737739563
2023-06-22 15:49:43,604 - INFO - Distilling data from client: Client46
2023-06-22 15:49:43,605 - INFO - train loss: 0.001501616168491285
2023-06-22 15:49:43,605 - INFO - train acc: 0.9972527623176575
2023-06-22 15:49:43,710 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.80      0.79        60
           1       0.91      0.91      0.91       140

    accuracy                           0.88       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:49:43,711 - INFO - test loss 0.012208917038003411
2023-06-22 15:49:43,711 - INFO - test acc 0.875
2023-06-22 15:49:45,883 - INFO - Distilling data from client: Client46
2023-06-22 15:49:45,883 - INFO - train loss: 0.0016151286019359388
2023-06-22 15:49:45,884 - INFO - train acc: 0.9972527623176575
2023-06-22 15:49:45,941 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.72      0.72        60
           1       0.88      0.89      0.88       140

    accuracy                           0.83       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.83      0.83      0.83       200

2023-06-22 15:49:45,942 - INFO - test loss 0.013911480140114908
2023-06-22 15:49:45,942 - INFO - test acc 0.8349999785423279
2023-06-22 15:49:48,085 - INFO - Distilling data from client: Client46
2023-06-22 15:49:48,086 - INFO - train loss: 0.0014684972784630573
2023-06-22 15:49:48,087 - INFO - train acc: 0.9972527623176575
2023-06-22 15:49:48,131 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        60
           1       0.89      0.89      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:49:48,132 - INFO - test loss 0.014583938202089228
2023-06-22 15:49:48,133 - INFO - test acc 0.8449999690055847
2023-06-22 15:49:50,303 - INFO - Distilling data from client: Client46
2023-06-22 15:49:50,303 - INFO - train loss: 0.001805336175050468
2023-06-22 15:49:50,303 - INFO - train acc: 0.9972527623176575
2023-06-22 15:49:50,361 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.73      0.76        60
           1       0.89      0.91      0.90       140

    accuracy                           0.86       200
   macro avg       0.84      0.82      0.83       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:49:50,362 - INFO - test loss 0.013334063734299623
2023-06-22 15:49:50,362 - INFO - test acc 0.85999995470047
2023-06-22 15:49:52,510 - INFO - Distilling data from client: Client46
2023-06-22 15:49:52,510 - INFO - train loss: 0.0013222352742809898
2023-06-22 15:49:52,511 - INFO - train acc: 1.0
2023-06-22 15:49:52,558 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.68      0.74        60
           1       0.87      0.93      0.90       140

    accuracy                           0.85       200
   macro avg       0.84      0.81      0.82       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:49:52,559 - INFO - test loss 0.013672154393591448
2023-06-22 15:49:52,559 - INFO - test acc 0.8549999594688416
2023-06-22 15:49:54,718 - INFO - Distilling data from client: Client46
2023-06-22 15:49:54,718 - INFO - train loss: 0.0012029185215902448
2023-06-22 15:49:54,719 - INFO - train acc: 1.0
2023-06-22 15:49:54,762 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.68      0.71        60
           1       0.87      0.90      0.88       140

    accuracy                           0.83       200
   macro avg       0.81      0.79      0.80       200
weighted avg       0.83      0.83      0.83       200

2023-06-22 15:49:54,763 - INFO - test loss 0.013233715752061255
2023-06-22 15:49:54,763 - INFO - test acc 0.8349999785423279
2023-06-22 15:49:56,983 - INFO - Distilling data from client: Client46
2023-06-22 15:49:56,984 - INFO - train loss: 0.0014506393568990306
2023-06-22 15:49:56,985 - INFO - train acc: 0.9972527623176575
2023-06-22 15:49:57,026 - INFO - report:               precision    recall  f1-score   support

           0       0.82      0.67      0.73        60
           1       0.87      0.94      0.90       140

    accuracy                           0.85       200
   macro avg       0.84      0.80      0.82       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:49:57,029 - INFO - test loss 0.013634459144352174
2023-06-22 15:49:57,030 - INFO - test acc 0.8549999594688416
2023-06-22 15:49:59,224 - INFO - Distilling data from client: Client46
2023-06-22 15:49:59,225 - INFO - train loss: 0.0013401352927854668
2023-06-22 15:49:59,225 - INFO - train acc: 1.0
2023-06-22 15:49:59,273 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.70      0.72        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:49:59,275 - INFO - test loss 0.013204577636011907
2023-06-22 15:49:59,275 - INFO - test acc 0.8399999737739563
2023-06-22 15:50:01,457 - INFO - Distilling data from client: Client46
2023-06-22 15:50:01,457 - INFO - train loss: 0.0012370907265488462
2023-06-22 15:50:01,458 - INFO - train acc: 1.0
2023-06-22 15:50:01,510 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        60
           1       0.89      0.89      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:50:01,511 - INFO - test loss 0.014047138628164493
2023-06-22 15:50:01,511 - INFO - test acc 0.8449999690055847
2023-06-22 15:50:03,682 - INFO - Distilling data from client: Client46
2023-06-22 15:50:03,683 - INFO - train loss: 0.0012074591453603064
2023-06-22 15:50:03,683 - INFO - train acc: 1.0
2023-06-22 15:50:03,734 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.72      0.74        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:50:03,734 - INFO - test loss 0.01451806699802292
2023-06-22 15:50:03,735 - INFO - test acc 0.8449999690055847
2023-06-22 15:50:05,930 - INFO - Distilling data from client: Client46
2023-06-22 15:50:05,930 - INFO - train loss: 0.0011775216665288175
2023-06-22 15:50:05,931 - INFO - train acc: 1.0
2023-06-22 15:50:05,977 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.70      0.72        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:50:05,977 - INFO - test loss 0.013318785395144996
2023-06-22 15:50:05,977 - INFO - test acc 0.8399999737739563
2023-06-22 15:50:08,116 - INFO - Distilling data from client: Client46
2023-06-22 15:50:08,116 - INFO - train loss: 0.0010583477878569607
2023-06-22 15:50:08,116 - INFO - train acc: 0.9972527623176575
2023-06-22 15:50:08,170 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.70      0.73        60
           1       0.88      0.91      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:50:08,170 - INFO - test loss 0.013507121596749442
2023-06-22 15:50:08,170 - INFO - test acc 0.8449999690055847
2023-06-22 15:50:10,425 - INFO - Distilling data from client: Client46
2023-06-22 15:50:10,426 - INFO - train loss: 0.0010989144306094499
2023-06-22 15:50:10,426 - INFO - train acc: 0.9972527623176575
2023-06-22 15:50:10,471 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        60
           1       0.89      0.89      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:50:10,472 - INFO - test loss 0.013810911073655018
2023-06-22 15:50:10,472 - INFO - test acc 0.8399999737739563
2023-06-22 15:50:12,585 - INFO - Distilling data from client: Client46
2023-06-22 15:50:12,585 - INFO - train loss: 0.0010753209548060724
2023-06-22 15:50:12,586 - INFO - train acc: 1.0
2023-06-22 15:50:12,629 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.65      0.71        60
           1       0.86      0.92      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.79      0.80       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:50:12,629 - INFO - test loss 0.013680712374976076
2023-06-22 15:50:12,629 - INFO - test acc 0.8399999737739563
2023-06-22 15:50:14,859 - INFO - Distilling data from client: Client46
2023-06-22 15:50:14,860 - INFO - train loss: 0.0010296810239151177
2023-06-22 15:50:14,861 - INFO - train acc: 1.0
2023-06-22 15:50:14,915 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.70      0.72        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:50:14,916 - INFO - test loss 0.013642441389138863
2023-06-22 15:50:14,916 - INFO - test acc 0.8399999737739563
2023-06-22 15:50:17,099 - INFO - Distilling data from client: Client46
2023-06-22 15:50:17,100 - INFO - train loss: 0.001047041697996378
2023-06-22 15:50:17,101 - INFO - train acc: 1.0
2023-06-22 15:50:17,154 - INFO - report:               precision    recall  f1-score   support

           0       0.81      0.73      0.77        60
           1       0.89      0.93      0.91       140

    accuracy                           0.87       200
   macro avg       0.85      0.83      0.84       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:50:17,155 - INFO - test loss 0.014136224269439666
2023-06-22 15:50:17,155 - INFO - test acc 0.8700000047683716
2023-06-22 15:50:19,352 - INFO - Distilling data from client: Client46
2023-06-22 15:50:19,353 - INFO - train loss: 0.0010791779589144645
2023-06-22 15:50:19,353 - INFO - train acc: 1.0
2023-06-22 15:50:19,398 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.73      0.77        60
           1       0.89      0.92      0.91       140

    accuracy                           0.86       200
   macro avg       0.84      0.83      0.84       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:50:19,399 - INFO - test loss 0.013573071111001452
2023-06-22 15:50:19,399 - INFO - test acc 0.8650000095367432
2023-06-22 15:50:21,619 - INFO - Distilling data from client: Client46
2023-06-22 15:50:21,620 - INFO - train loss: 0.0008732238976446535
2023-06-22 15:50:21,620 - INFO - train acc: 1.0
2023-06-22 15:50:21,665 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.70      0.73        60
           1       0.88      0.91      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:50:21,668 - INFO - test loss 0.01322793151564441
2023-06-22 15:50:21,668 - INFO - test acc 0.8449999690055847
2023-06-22 15:50:23,866 - INFO - Distilling data from client: Client46
2023-06-22 15:50:23,866 - INFO - train loss: 0.0010564097032749284
2023-06-22 15:50:23,866 - INFO - train acc: 1.0
2023-06-22 15:50:23,922 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.72      0.74        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:50:23,923 - INFO - test loss 0.013941681417370856
2023-06-22 15:50:23,923 - INFO - test acc 0.8449999690055847
2023-06-22 15:50:26,176 - INFO - Distilling data from client: Client46
2023-06-22 15:50:26,177 - INFO - train loss: 0.0008911734125226547
2023-06-22 15:50:26,177 - INFO - train acc: 1.0
2023-06-22 15:50:26,226 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.72      0.75        60
           1       0.88      0.91      0.90       140

    accuracy                           0.85       200
   macro avg       0.83      0.82      0.82       200
weighted avg       0.85      0.85      0.85       200

2023-06-22 15:50:26,226 - INFO - test loss 0.013114417504218523
2023-06-22 15:50:26,226 - INFO - test acc 0.8549999594688416
2023-06-22 15:50:26,233 - WARNING - Finished tracing + transforming jit(gather) in 0.0007646083831787109 sec
2023-06-22 15:50:26,234 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[364,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:50:26,237 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0032224655151367188 sec
2023-06-22 15:50:26,238 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:50:26,255 - WARNING - Finished XLA compilation of jit(gather) in 0.01666426658630371 sec
2023-06-22 15:50:26,280 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:50:26,302 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:50:26,321 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:50:26,336 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:50:26,350 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:50:26,361 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:50:26,782 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client46//synthetic.png
2023-06-22 15:50:26,805 - INFO - c: 0.0 and total_data_in_this_class: 258
2023-06-22 15:50:26,805 - INFO - c: 3.0 and total_data_in_this_class: 273
2023-06-22 15:50:26,805 - INFO - c: 6.0 and total_data_in_this_class: 268
2023-06-22 15:50:26,805 - INFO - c: 0.0 and total_data_in_this_class: 75
2023-06-22 15:50:26,806 - INFO - c: 3.0 and total_data_in_this_class: 60
2023-06-22 15:50:26,806 - INFO - c: 6.0 and total_data_in_this_class: 65
2023-06-22 15:50:26,937 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07684087753295898 sec
2023-06-22 15:50:27,011 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0731348991394043 sec
2023-06-22 15:50:27,020 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16237354278564453 sec
2023-06-22 15:50:27,023 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:50:27,078 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.0544736385345459 sec
2023-06-22 15:50:27,078 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:50:27,251 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1727142333984375 sec
2023-06-22 15:50:27,292 - INFO - initial test loss: 0.02198435581061443
2023-06-22 15:50:27,292 - INFO - initial test acc: 0.7149999737739563
2023-06-22 15:50:27,312 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.013078451156616211 sec
2023-06-22 15:50:27,506 - WARNING - Finished tracing + transforming update_fn for pjit in 0.20852875709533691 sec
2023-06-22 15:50:27,511 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:50:27,613 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10139918327331543 sec
2023-06-22 15:50:27,613 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:50:28,015 - WARNING - Finished XLA compilation of jit(update_fn) in 0.40094757080078125 sec
2023-06-22 15:50:30,782 - INFO - Distilling data from client: Client47
2023-06-22 15:50:30,782 - INFO - train loss: 0.002685142598797741
2023-06-22 15:50:30,782 - INFO - train acc: 0.9961240291595459
2023-06-22 15:50:30,951 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.79      0.86        75
           3       0.71      0.70      0.71        60
           6       0.69      0.83      0.76        65

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.79      0.78      0.78       200

2023-06-22 15:50:30,952 - INFO - test loss 0.020075554871272057
2023-06-22 15:50:30,952 - INFO - test acc 0.7749999761581421
2023-06-22 15:50:33,725 - INFO - Distilling data from client: Client47
2023-06-22 15:50:33,725 - INFO - train loss: 0.0016654561977906999
2023-06-22 15:50:33,725 - INFO - train acc: 1.0
2023-06-22 15:50:33,781 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.83      0.89        75
           3       0.66      0.72      0.69        60
           6       0.70      0.77      0.74        65

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.79      0.78      0.78       200

2023-06-22 15:50:33,782 - INFO - test loss 0.01989363986009914
2023-06-22 15:50:33,782 - INFO - test acc 0.7749999761581421
2023-06-22 15:50:36,528 - INFO - Distilling data from client: Client47
2023-06-22 15:50:36,529 - INFO - train loss: 0.0010203158204719152
2023-06-22 15:50:36,529 - INFO - train acc: 1.0
2023-06-22 15:50:36,584 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.80      0.88        75
           3       0.67      0.62      0.64        60
           6       0.64      0.82      0.72        65

    accuracy                           0.75       200
   macro avg       0.76      0.74      0.75       200
weighted avg       0.77      0.75      0.75       200

2023-06-22 15:50:36,585 - INFO - test loss 0.02019941429526225
2023-06-22 15:50:36,585 - INFO - test acc 0.75
2023-06-22 15:50:39,270 - INFO - Distilling data from client: Client47
2023-06-22 15:50:39,271 - INFO - train loss: 0.0008200310336051778
2023-06-22 15:50:39,272 - INFO - train acc: 1.0
2023-06-22 15:50:39,335 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.83      0.88        75
           3       0.67      0.67      0.67        60
           6       0.69      0.78      0.73        65

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.78      0.77      0.77       200

2023-06-22 15:50:39,336 - INFO - test loss 0.020728802267377302
2023-06-22 15:50:39,336 - INFO - test acc 0.7649999856948853
2023-06-22 15:50:42,077 - INFO - Distilling data from client: Client47
2023-06-22 15:50:42,077 - INFO - train loss: 0.0006822577293107332
2023-06-22 15:50:42,077 - INFO - train acc: 1.0
2023-06-22 15:50:42,128 - INFO - report:               precision    recall  f1-score   support

           0       0.95      0.81      0.88        75
           3       0.67      0.65      0.66        60
           6       0.65      0.78      0.71        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:50:42,129 - INFO - test loss 0.020553761415488613
2023-06-22 15:50:42,129 - INFO - test acc 0.7549999952316284
2023-06-22 15:50:44,923 - INFO - Distilling data from client: Client47
2023-06-22 15:50:44,924 - INFO - train loss: 0.0007016836520737719
2023-06-22 15:50:44,924 - INFO - train acc: 1.0
2023-06-22 15:50:44,978 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.81      0.88        75
           3       0.63      0.65      0.64        60
           6       0.68      0.78      0.73        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:50:44,978 - INFO - test loss 0.02090679026130441
2023-06-22 15:50:44,978 - INFO - test acc 0.7549999952316284
2023-06-22 15:50:47,709 - INFO - Distilling data from client: Client47
2023-06-22 15:50:47,709 - INFO - train loss: 0.0006560497544422164
2023-06-22 15:50:47,710 - INFO - train acc: 1.0
2023-06-22 15:50:47,767 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.77      0.86        75
           3       0.68      0.72      0.70        60
           6       0.70      0.83      0.76        65

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.80      0.78      0.78       200

2023-06-22 15:50:47,768 - INFO - test loss 0.020757009119003408
2023-06-22 15:50:47,768 - INFO - test acc 0.7749999761581421
2023-06-22 15:50:50,651 - INFO - Distilling data from client: Client47
2023-06-22 15:50:50,652 - INFO - train loss: 0.0007158091144514527
2023-06-22 15:50:50,652 - INFO - train acc: 1.0
2023-06-22 15:50:50,705 - INFO - report:               precision    recall  f1-score   support

           0       0.95      0.84      0.89        75
           3       0.71      0.67      0.69        60
           6       0.67      0.80      0.73        65

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.79      0.78      0.78       200

2023-06-22 15:50:50,705 - INFO - test loss 0.020892171883540145
2023-06-22 15:50:50,706 - INFO - test acc 0.7749999761581421
2023-06-22 15:50:53,444 - INFO - Distilling data from client: Client47
2023-06-22 15:50:53,445 - INFO - train loss: 0.00045347035685098277
2023-06-22 15:50:53,445 - INFO - train acc: 1.0
2023-06-22 15:50:53,507 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.83      0.88        75
           3       0.63      0.63      0.63        60
           6       0.69      0.78      0.73        65

    accuracy                           0.76       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:50:53,508 - INFO - test loss 0.02068158093204669
2023-06-22 15:50:53,509 - INFO - test acc 0.7549999952316284
2023-06-22 15:50:56,310 - INFO - Distilling data from client: Client47
2023-06-22 15:50:56,310 - INFO - train loss: 0.00039229782935380796
2023-06-22 15:50:56,311 - INFO - train acc: 1.0
2023-06-22 15:50:56,491 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.83      0.89        75
           3       0.70      0.72      0.71        60
           6       0.71      0.82      0.76        65

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.80      0.79      0.79       200

2023-06-22 15:50:56,492 - INFO - test loss 0.020613468261363288
2023-06-22 15:50:56,492 - INFO - test acc 0.7899999618530273
2023-06-22 15:50:59,246 - INFO - Distilling data from client: Client47
2023-06-22 15:50:59,246 - INFO - train loss: 0.00039589636937014874
2023-06-22 15:50:59,247 - INFO - train acc: 1.0
2023-06-22 15:50:59,300 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.80      0.86        75
           3       0.66      0.67      0.66        60
           6       0.68      0.78      0.73        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:50:59,301 - INFO - test loss 0.02097950554056562
2023-06-22 15:50:59,301 - INFO - test acc 0.7549999952316284
2023-06-22 15:51:02,045 - INFO - Distilling data from client: Client47
2023-06-22 15:51:02,047 - INFO - train loss: 0.00048025884550746006
2023-06-22 15:51:02,047 - INFO - train acc: 1.0
2023-06-22 15:51:02,108 - INFO - report:               precision    recall  f1-score   support

           0       0.95      0.81      0.88        75
           3       0.68      0.72      0.70        60
           6       0.68      0.77      0.72        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-06-22 15:51:02,109 - INFO - test loss 0.020750988350760564
2023-06-22 15:51:02,109 - INFO - test acc 0.7699999809265137
2023-06-22 15:51:04,820 - INFO - Distilling data from client: Client47
2023-06-22 15:51:04,820 - INFO - train loss: 0.0004284425425421418
2023-06-22 15:51:04,820 - INFO - train acc: 1.0
2023-06-22 15:51:04,890 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.81      0.88        75
           3       0.70      0.72      0.71        60
           6       0.70      0.82      0.75        65

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.80      0.79      0.79       200

2023-06-22 15:51:04,890 - INFO - test loss 0.020655948768044118
2023-06-22 15:51:04,890 - INFO - test acc 0.7849999666213989
2023-06-22 15:51:07,653 - INFO - Distilling data from client: Client47
2023-06-22 15:51:07,654 - INFO - train loss: 0.0004615229816471538
2023-06-22 15:51:07,654 - INFO - train acc: 1.0
2023-06-22 15:51:07,712 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.77      0.85        75
           3       0.69      0.68      0.69        60
           6       0.68      0.83      0.75        65

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.78      0.77      0.77       200

2023-06-22 15:51:07,712 - INFO - test loss 0.020664746263403727
2023-06-22 15:51:07,712 - INFO - test acc 0.7649999856948853
2023-06-22 15:51:10,585 - INFO - Distilling data from client: Client47
2023-06-22 15:51:10,586 - INFO - train loss: 0.00042112586331409063
2023-06-22 15:51:10,586 - INFO - train acc: 1.0
2023-06-22 15:51:10,630 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.79      0.86        75
           3       0.67      0.67      0.67        60
           6       0.68      0.80      0.73        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:51:10,630 - INFO - test loss 0.020593856995966348
2023-06-22 15:51:10,630 - INFO - test acc 0.7549999952316284
2023-06-22 15:51:13,415 - INFO - Distilling data from client: Client47
2023-06-22 15:51:13,415 - INFO - train loss: 0.0004220244559108777
2023-06-22 15:51:13,416 - INFO - train acc: 1.0
2023-06-22 15:51:13,476 - INFO - report:               precision    recall  f1-score   support

           0       0.95      0.83      0.89        75
           3       0.69      0.70      0.69        60
           6       0.72      0.82      0.76        65

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.80      0.79      0.79       200

2023-06-22 15:51:13,476 - INFO - test loss 0.02046893224937427
2023-06-22 15:51:13,476 - INFO - test acc 0.7849999666213989
2023-06-22 15:51:16,170 - INFO - Distilling data from client: Client47
2023-06-22 15:51:16,170 - INFO - train loss: 0.00032471600029049144
2023-06-22 15:51:16,170 - INFO - train acc: 1.0
2023-06-22 15:51:16,219 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.80      0.86        75
           3       0.64      0.65      0.64        60
           6       0.68      0.78      0.73        65

    accuracy                           0.75       200
   macro avg       0.75      0.74      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-06-22 15:51:16,219 - INFO - test loss 0.02145758390530571
2023-06-22 15:51:16,220 - INFO - test acc 0.75
2023-06-22 15:51:18,966 - INFO - Distilling data from client: Client47
2023-06-22 15:51:18,966 - INFO - train loss: 0.00032630732730081256
2023-06-22 15:51:18,966 - INFO - train acc: 1.0
2023-06-22 15:51:19,041 - INFO - report:               precision    recall  f1-score   support

           0       0.92      0.81      0.87        75
           3       0.63      0.60      0.62        60
           6       0.68      0.80      0.73        65

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.76      0.74      0.75       200

2023-06-22 15:51:19,042 - INFO - test loss 0.02086425840191631
2023-06-22 15:51:19,042 - INFO - test acc 0.7450000047683716
2023-06-22 15:51:21,825 - INFO - Distilling data from client: Client47
2023-06-22 15:51:21,826 - INFO - train loss: 0.00032157870692993017
2023-06-22 15:51:21,826 - INFO - train acc: 1.0
2023-06-22 15:51:21,889 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.84      0.90        75
           3       0.67      0.72      0.69        60
           6       0.72      0.78      0.75        65

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.80      0.79      0.79       200

2023-06-22 15:51:21,889 - INFO - test loss 0.02076480926199303
2023-06-22 15:51:21,890 - INFO - test acc 0.7849999666213989
2023-06-22 15:51:24,814 - INFO - Distilling data from client: Client47
2023-06-22 15:51:24,815 - INFO - train loss: 0.0003971247656179396
2023-06-22 15:51:24,816 - INFO - train acc: 1.0
2023-06-22 15:51:24,873 - INFO - report:               precision    recall  f1-score   support

           0       0.92      0.81      0.87        75
           3       0.67      0.67      0.67        60
           6       0.68      0.77      0.72        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:51:24,874 - INFO - test loss 0.020559918378555215
2023-06-22 15:51:24,874 - INFO - test acc 0.7549999952316284
2023-06-22 15:51:27,789 - INFO - Distilling data from client: Client47
2023-06-22 15:51:27,790 - INFO - train loss: 0.00030443385555823174
2023-06-22 15:51:27,790 - INFO - train acc: 1.0
2023-06-22 15:51:27,887 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.81      0.88        75
           3       0.70      0.73      0.72        60
           6       0.70      0.80      0.75        65

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.80      0.79      0.79       200

2023-06-22 15:51:27,893 - INFO - test loss 0.021568962449618652
2023-06-22 15:51:27,894 - INFO - test acc 0.7849999666213989
2023-06-22 15:51:31,525 - INFO - Distilling data from client: Client47
2023-06-22 15:51:31,525 - INFO - train loss: 0.0002649459180991196
2023-06-22 15:51:31,525 - INFO - train acc: 1.0
2023-06-22 15:51:31,585 - INFO - report:               precision    recall  f1-score   support

           0       0.95      0.83      0.89        75
           3       0.68      0.68      0.68        60
           6       0.68      0.78      0.73        65

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-06-22 15:51:31,585 - INFO - test loss 0.020371372315228838
2023-06-22 15:51:31,585 - INFO - test acc 0.7699999809265137
2023-06-22 15:51:34,249 - INFO - Distilling data from client: Client47
2023-06-22 15:51:34,251 - INFO - train loss: 0.0003047936537302189
2023-06-22 15:51:34,251 - INFO - train acc: 1.0
2023-06-22 15:51:34,309 - INFO - report:               precision    recall  f1-score   support

           0       0.93      0.83      0.87        75
           3       0.67      0.63      0.65        60
           6       0.68      0.80      0.74        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-06-22 15:51:34,309 - INFO - test loss 0.020920945634079702
2023-06-22 15:51:34,310 - INFO - test acc 0.7599999904632568
2023-06-22 15:51:37,043 - INFO - Distilling data from client: Client47
2023-06-22 15:51:37,043 - INFO - train loss: 0.00034334705969426416
2023-06-22 15:51:37,044 - INFO - train acc: 1.0
2023-06-22 15:51:37,094 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.83      0.88        75
           3       0.67      0.70      0.68        60
           6       0.70      0.77      0.74        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-06-22 15:51:37,095 - INFO - test loss 0.020957986150516385
2023-06-22 15:51:37,095 - INFO - test acc 0.7699999809265137
2023-06-22 15:51:39,811 - INFO - Distilling data from client: Client47
2023-06-22 15:51:39,812 - INFO - train loss: 0.00029407244166942476
2023-06-22 15:51:39,812 - INFO - train acc: 1.0
2023-06-22 15:51:39,875 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.83      0.88        75
           3       0.69      0.72      0.70        60
           6       0.69      0.77      0.73        65

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.79      0.78      0.78       200

2023-06-22 15:51:39,876 - INFO - test loss 0.020568960337629515
2023-06-22 15:51:39,877 - INFO - test acc 0.7749999761581421
2023-06-22 15:51:39,909 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:51:39,932 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:51:39,952 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:51:39,969 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:51:39,981 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:51:39,994 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:51:40,008 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:51:40,021 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:51:40,034 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:51:40,622 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client47//synthetic.png
2023-06-22 15:51:40,655 - INFO - c: 1.0 and total_data_in_this_class: 272
2023-06-22 15:51:40,656 - INFO - c: 5.0 and total_data_in_this_class: 527
2023-06-22 15:51:40,656 - INFO - c: 1.0 and total_data_in_this_class: 61
2023-06-22 15:51:40,656 - INFO - c: 5.0 and total_data_in_this_class: 139
2023-06-22 15:51:40,705 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0006015300750732422 sec
2023-06-22 15:51:40,706 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:51:40,709 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0025725364685058594 sec
2023-06-22 15:51:40,709 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:51:40,726 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01650714874267578 sec
2023-06-22 15:51:40,730 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0003151893615722656 sec
2023-06-22 15:51:40,730 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-06-22 15:51:40,732 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0017690658569335938 sec
2023-06-22 15:51:40,733 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:51:40,744 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011411666870117188 sec
2023-06-22 15:51:40,750 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00023555755615234375 sec
2023-06-22 15:51:40,752 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021386146545410156 sec
2023-06-22 15:51:40,754 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0007033348083496094 sec
2023-06-22 15:51:40,756 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003304481506347656 sec
2023-06-22 15:51:40,757 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020885467529296875 sec
2023-06-22 15:51:40,758 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00047707557678222656 sec
2023-06-22 15:51:40,759 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004398822784423828 sec
2023-06-22 15:51:40,760 - WARNING - Finished tracing + transforming absolute for pjit in 0.0003674030303955078 sec
2023-06-22 15:51:40,761 - WARNING - Finished tracing + transforming fn for pjit in 0.00047016143798828125 sec
2023-06-22 15:51:40,763 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.000728607177734375 sec
2023-06-22 15:51:40,765 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0003833770751953125 sec
2023-06-22 15:51:40,767 - WARNING - Finished tracing + transforming fn for pjit in 0.00041675567626953125 sec
2023-06-22 15:51:40,768 - WARNING - Finished tracing + transforming fn for pjit in 0.00048089027404785156 sec
2023-06-22 15:51:40,769 - WARNING - Finished tracing + transforming fn for pjit in 0.00037789344787597656 sec
2023-06-22 15:51:40,770 - WARNING - Finished tracing + transforming fn for pjit in 0.0004322528839111328 sec
2023-06-22 15:51:40,773 - WARNING - Finished tracing + transforming fn for pjit in 0.00035858154296875 sec
2023-06-22 15:51:40,776 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000301361083984375 sec
2023-06-22 15:51:40,778 - WARNING - Finished tracing + transforming fn for pjit in 0.0005288124084472656 sec
2023-06-22 15:51:40,779 - WARNING - Finished tracing + transforming fn for pjit in 0.00041604042053222656 sec
2023-06-22 15:51:40,786 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0006716251373291016 sec
2023-06-22 15:51:40,787 - WARNING - Finished tracing + transforming _mean for pjit in 0.0017638206481933594 sec
2023-06-22 15:51:40,788 - WARNING - Finished tracing + transforming fn for pjit in 0.0004019737243652344 sec
2023-06-22 15:51:40,790 - WARNING - Finished tracing + transforming fn for pjit in 0.0003757476806640625 sec
2023-06-22 15:51:40,792 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0016682147979736328 sec
2023-06-22 15:51:40,794 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005028247833251953 sec
2023-06-22 15:51:40,795 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00033783912658691406 sec
2023-06-22 15:51:40,796 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000453948974609375 sec
2023-06-22 15:51:40,798 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000446319580078125 sec
2023-06-22 15:51:40,799 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004515647888183594 sec
2023-06-22 15:51:40,801 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006313323974609375 sec
2023-06-22 15:51:40,801 - WARNING - Finished tracing + transforming _where for pjit in 0.0017058849334716797 sec
2023-06-22 15:51:40,803 - WARNING - Finished tracing + transforming fn for pjit in 0.0004673004150390625 sec
2023-06-22 15:51:40,804 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004305839538574219 sec
2023-06-22 15:51:40,805 - WARNING - Finished tracing + transforming fn for pjit in 0.0004010200500488281 sec
2023-06-22 15:51:40,807 - WARNING - Finished tracing + transforming fn for pjit in 0.0004050731658935547 sec
2023-06-22 15:51:40,808 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003826618194580078 sec
2023-06-22 15:51:40,809 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00044083595275878906 sec
2023-06-22 15:51:40,810 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004572868347167969 sec
2023-06-22 15:51:40,812 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004944801330566406 sec
2023-06-22 15:51:40,813 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004074573516845703 sec
2023-06-22 15:51:40,814 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003986358642578125 sec
2023-06-22 15:51:40,816 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.000469207763671875 sec
2023-06-22 15:51:40,817 - WARNING - Finished tracing + transforming _where for pjit in 0.0015072822570800781 sec
2023-06-22 15:51:40,818 - WARNING - Finished tracing + transforming fn for pjit in 0.0004374980926513672 sec
2023-06-22 15:51:40,819 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00045800209045410156 sec
2023-06-22 15:51:40,821 - WARNING - Finished tracing + transforming fn for pjit in 0.0003745555877685547 sec
2023-06-22 15:51:40,828 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004673004150390625 sec
2023-06-22 15:51:40,830 - WARNING - Finished tracing + transforming fn for pjit in 0.0006155967712402344 sec
2023-06-22 15:51:40,831 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004818439483642578 sec
2023-06-22 15:51:40,833 - WARNING - Finished tracing + transforming fn for pjit in 0.0003960132598876953 sec
2023-06-22 15:51:40,839 - WARNING - Finished tracing + transforming fn for pjit in 0.0003821849822998047 sec
2023-06-22 15:51:40,842 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002856254577636719 sec
2023-06-22 15:51:40,843 - WARNING - Finished tracing + transforming fn for pjit in 0.00054931640625 sec
2023-06-22 15:51:40,845 - WARNING - Finished tracing + transforming fn for pjit in 0.00039315223693847656 sec
2023-06-22 15:51:40,873 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.12426185607910156 sec
2023-06-22 15:51:40,879 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021648406982421875 sec
2023-06-22 15:51:40,880 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019788742065429688 sec
2023-06-22 15:51:40,881 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00046706199645996094 sec
2023-06-22 15:51:40,884 - WARNING - Finished tracing + transforming fn for pjit in 0.00034165382385253906 sec
2023-06-22 15:51:40,885 - WARNING - Finished tracing + transforming fn for pjit in 0.00043964385986328125 sec
2023-06-22 15:51:40,888 - WARNING - Finished tracing + transforming fn for pjit in 0.00037288665771484375 sec
2023-06-22 15:51:40,897 - WARNING - Finished tracing + transforming fn for pjit in 0.00038242340087890625 sec
2023-06-22 15:51:40,899 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003514289855957031 sec
2023-06-22 15:51:40,900 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00043129920959472656 sec
2023-06-22 15:51:40,901 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029540061950683594 sec
2023-06-22 15:51:40,903 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.001003265380859375 sec
2023-06-22 15:51:40,905 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00038242340087890625 sec
2023-06-22 15:51:40,906 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004115104675292969 sec
2023-06-22 15:51:40,908 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00044989585876464844 sec
2023-06-22 15:51:40,908 - WARNING - Finished tracing + transforming _where for pjit in 0.0015664100646972656 sec
2023-06-22 15:51:40,910 - WARNING - Finished tracing + transforming fn for pjit in 0.00047850608825683594 sec
2023-06-22 15:51:40,911 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004391670227050781 sec
2023-06-22 15:51:40,912 - WARNING - Finished tracing + transforming fn for pjit in 0.00039386749267578125 sec
2023-06-22 15:51:40,913 - WARNING - Finished tracing + transforming fn for pjit in 0.0005216598510742188 sec
2023-06-22 15:51:40,934 - WARNING - Finished tracing + transforming fn for pjit in 0.00036072731018066406 sec
2023-06-22 15:51:40,966 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.09067845344543457 sec
2023-06-22 15:51:40,968 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001926422119140625 sec
2023-06-22 15:51:40,970 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00021576881408691406 sec
2023-06-22 15:51:40,970 - WARNING - Finished tracing + transforming _where for pjit in 0.0010709762573242188 sec
2023-06-22 15:51:40,971 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0005092620849609375 sec
2023-06-22 15:51:40,972 - WARNING - Finished tracing + transforming trace for pjit in 0.004515886306762695 sec
2023-06-22 15:51:40,976 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0001723766326904297 sec
2023-06-22 15:51:40,978 - WARNING - Finished tracing + transforming tril for pjit in 0.0010685920715332031 sec
2023-06-22 15:51:40,979 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0032525062561035156 sec
2023-06-22 15:51:40,980 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001804828643798828 sec
2023-06-22 15:51:40,981 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001862049102783203 sec
2023-06-22 15:51:40,985 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0023653507232666016 sec
2023-06-22 15:51:40,991 - WARNING - Finished tracing + transforming _solve for pjit in 0.01613163948059082 sec
2023-06-22 15:51:40,992 - WARNING - Finished tracing + transforming dot for pjit in 0.0005319118499755859 sec
2023-06-22 15:51:40,996 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.24983978271484375 sec
2023-06-22 15:51:41,000 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:51:41,058 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.058309316635131836 sec
2023-06-22 15:51:41,058 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:51:41,238 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17912936210632324 sec
2023-06-22 15:51:41,272 - INFO - initial test loss: 0.012909192357437656
2023-06-22 15:51:41,273 - INFO - initial test acc: 0.8449999690055847
2023-06-22 15:51:41,287 - WARNING - Finished tracing + transforming dot for pjit in 0.0009233951568603516 sec
2023-06-22 15:51:41,289 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0007348060607910156 sec
2023-06-22 15:51:41,293 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0020139217376708984 sec
2023-06-22 15:51:41,294 - WARNING - Finished tracing + transforming _mean for pjit in 0.0036287307739257812 sec
2023-06-22 15:51:41,296 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004754066467285156 sec
2023-06-22 15:51:41,297 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0004773139953613281 sec
2023-06-22 15:51:41,299 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006237030029296875 sec
2023-06-22 15:51:41,301 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0008978843688964844 sec
2023-06-22 15:51:41,302 - WARNING - Finished tracing + transforming _mean for pjit in 0.0026695728302001953 sec
2023-06-22 15:51:41,304 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.026065826416015625 sec
2023-06-22 15:51:41,327 - WARNING - Finished tracing + transforming fn for pjit in 0.0006697177886962891 sec
2023-06-22 15:51:41,329 - WARNING - Finished tracing + transforming fn for pjit in 0.0006692409515380859 sec
2023-06-22 15:51:41,331 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005724430084228516 sec
2023-06-22 15:51:41,333 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0006613731384277344 sec
2023-06-22 15:51:41,335 - WARNING - Finished tracing + transforming _where for pjit in 0.002581357955932617 sec
2023-06-22 15:51:41,355 - WARNING - Finished tracing + transforming fn for pjit in 0.0006420612335205078 sec
2023-06-22 15:51:41,357 - WARNING - Finished tracing + transforming fn for pjit in 0.0006945133209228516 sec
2023-06-22 15:51:41,359 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0005769729614257812 sec
2023-06-22 15:51:41,362 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0008070468902587891 sec
2023-06-22 15:51:41,363 - WARNING - Finished tracing + transforming _where for pjit in 0.0025823116302490234 sec
2023-06-22 15:51:41,420 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003457069396972656 sec
2023-06-22 15:51:41,503 - WARNING - Finished tracing + transforming fn for pjit in 0.0004878044128417969 sec
2023-06-22 15:51:41,505 - WARNING - Finished tracing + transforming fn for pjit in 0.0004093647003173828 sec
2023-06-22 15:51:41,506 - WARNING - Finished tracing + transforming square for pjit in 0.00031375885009765625 sec
2023-06-22 15:51:41,510 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003769397735595703 sec
2023-06-22 15:51:41,513 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004138946533203125 sec
2023-06-22 15:51:41,514 - WARNING - Finished tracing + transforming fn for pjit in 0.00045037269592285156 sec
2023-06-22 15:51:41,515 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003821849822998047 sec
2023-06-22 15:51:41,516 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004165172576904297 sec
2023-06-22 15:51:41,517 - WARNING - Finished tracing + transforming fn for pjit in 0.00047326087951660156 sec
2023-06-22 15:51:41,518 - WARNING - Finished tracing + transforming fn for pjit in 0.0004069805145263672 sec
2023-06-22 15:51:41,519 - WARNING - Finished tracing + transforming square for pjit in 0.00029659271240234375 sec
2023-06-22 15:51:41,523 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00036215782165527344 sec
2023-06-22 15:51:41,525 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002980232238769531 sec
2023-06-22 15:51:41,526 - WARNING - Finished tracing + transforming fn for pjit in 0.00046753883361816406 sec
2023-06-22 15:51:41,527 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003933906555175781 sec
2023-06-22 15:51:41,528 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003859996795654297 sec
2023-06-22 15:51:41,530 - WARNING - Finished tracing + transforming update_fn for pjit in 0.2539989948272705 sec
2023-06-22 15:51:41,536 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,10]), ShapedArray(float32[362,10]), ShapedArray(float32[362,10]), ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,10]), ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:51:41,643 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.10724735260009766 sec
2023-06-22 15:51:41,644 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:51:42,086 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4418652057647705 sec
2023-06-22 15:51:44,129 - INFO - Distilling data from client: Client48
2023-06-22 15:51:44,129 - INFO - train loss: 0.0023374003280108736
2023-06-22 15:51:44,129 - INFO - train acc: 0.9944751858711243
2023-06-22 15:51:44,247 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.80      0.78        61
           5       0.91      0.89      0.90       139

    accuracy                           0.86       200
   macro avg       0.84      0.85      0.84       200
weighted avg       0.87      0.86      0.87       200

2023-06-22 15:51:44,248 - INFO - test loss 0.01107310257010721
2023-06-22 15:51:44,248 - INFO - test acc 0.8650000095367432
2023-06-22 15:51:46,466 - INFO - Distilling data from client: Client48
2023-06-22 15:51:46,466 - INFO - train loss: 0.001564073065910774
2023-06-22 15:51:46,466 - INFO - train acc: 0.9972376227378845
2023-06-22 15:51:46,631 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.79      0.79        61
           5       0.91      0.91      0.91       139

    accuracy                           0.88       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.88      0.87       200

2023-06-22 15:51:46,632 - INFO - test loss 0.010737075021625358
2023-06-22 15:51:46,632 - INFO - test acc 0.875
2023-06-22 15:51:48,732 - INFO - Distilling data from client: Client48
2023-06-22 15:51:48,732 - INFO - train loss: 0.0012726623055805558
2023-06-22 15:51:48,732 - INFO - train acc: 1.0
2023-06-22 15:51:48,790 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.79      0.79        61
           5       0.91      0.91      0.91       139

    accuracy                           0.88       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.88      0.87       200

2023-06-22 15:51:48,795 - INFO - test loss 0.010710910309457745
2023-06-22 15:51:48,796 - INFO - test acc 0.875
2023-06-22 15:51:50,876 - INFO - Distilling data from client: Client48
2023-06-22 15:51:50,877 - INFO - train loss: 0.0010895165621687044
2023-06-22 15:51:50,877 - INFO - train acc: 1.0
2023-06-22 15:51:50,973 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.82      0.83        61
           5       0.92      0.93      0.92       139

    accuracy                           0.90       200
   macro avg       0.88      0.87      0.88       200
weighted avg       0.89      0.90      0.89       200

2023-06-22 15:51:50,974 - INFO - test loss 0.009992694534632491
2023-06-22 15:51:50,975 - INFO - test acc 0.8949999809265137
2023-06-22 15:51:53,115 - INFO - Distilling data from client: Client48
2023-06-22 15:51:53,116 - INFO - train loss: 0.0009512853479144908
2023-06-22 15:51:53,116 - INFO - train acc: 1.0
2023-06-22 15:51:53,177 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.77      0.78        61
           5       0.90      0.91      0.91       139

    accuracy                           0.87       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:51:53,178 - INFO - test loss 0.011160662073619753
2023-06-22 15:51:53,178 - INFO - test acc 0.8700000047683716
2023-06-22 15:51:55,444 - INFO - Distilling data from client: Client48
2023-06-22 15:51:55,444 - INFO - train loss: 0.00095381438778797
2023-06-22 15:51:55,444 - INFO - train acc: 1.0
2023-06-22 15:51:55,492 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.80      0.80        61
           5       0.91      0.91      0.91       139

    accuracy                           0.88       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:51:55,493 - INFO - test loss 0.010833163393160986
2023-06-22 15:51:55,493 - INFO - test acc 0.875
2023-06-22 15:51:57,837 - INFO - Distilling data from client: Client48
2023-06-22 15:51:57,838 - INFO - train loss: 0.0008819489818355221
2023-06-22 15:51:57,840 - INFO - train acc: 1.0
2023-06-22 15:51:57,898 - INFO - report:               precision    recall  f1-score   support

           1       0.81      0.79      0.80        61
           5       0.91      0.92      0.91       139

    accuracy                           0.88       200
   macro avg       0.86      0.85      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:51:57,898 - INFO - test loss 0.011132712254572168
2023-06-22 15:51:57,898 - INFO - test acc 0.8799999952316284
2023-06-22 15:52:00,241 - INFO - Distilling data from client: Client48
2023-06-22 15:52:00,242 - INFO - train loss: 0.0009313691446416608
2023-06-22 15:52:00,242 - INFO - train acc: 0.9972376227378845
2023-06-22 15:52:00,282 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.74      0.76        61
           5       0.89      0.91      0.90       139

    accuracy                           0.86       200
   macro avg       0.84      0.83      0.83       200
weighted avg       0.86      0.86      0.86       200

2023-06-22 15:52:00,282 - INFO - test loss 0.011369338451360619
2023-06-22 15:52:00,282 - INFO - test acc 0.85999995470047
2023-06-22 15:52:02,680 - INFO - Distilling data from client: Client48
2023-06-22 15:52:02,681 - INFO - train loss: 0.0008670027347747859
2023-06-22 15:52:02,681 - INFO - train acc: 1.0
2023-06-22 15:52:02,724 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.80      0.81        61
           5       0.91      0.92      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-06-22 15:52:02,724 - INFO - test loss 0.010482192568091638
2023-06-22 15:52:02,725 - INFO - test acc 0.8849999904632568
2023-06-22 15:52:05,066 - INFO - Distilling data from client: Client48
2023-06-22 15:52:05,066 - INFO - train loss: 0.0007166746327196096
2023-06-22 15:52:05,067 - INFO - train acc: 0.9972376227378845
2023-06-22 15:52:05,107 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.79      0.81        61
           5       0.91      0.93      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-06-22 15:52:05,107 - INFO - test loss 0.010752405512825636
2023-06-22 15:52:05,108 - INFO - test acc 0.8849999904632568
2023-06-22 15:52:07,068 - INFO - Distilling data from client: Client48
2023-06-22 15:52:07,069 - INFO - train loss: 0.0007491767984735028
2023-06-22 15:52:07,069 - INFO - train acc: 1.0
2023-06-22 15:52:07,123 - INFO - report:               precision    recall  f1-score   support

           1       0.81      0.79      0.80        61
           5       0.91      0.92      0.91       139

    accuracy                           0.88       200
   macro avg       0.86      0.85      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:52:07,124 - INFO - test loss 0.010145315354570816
2023-06-22 15:52:07,124 - INFO - test acc 0.8799999952316284
2023-06-22 15:52:09,276 - INFO - Distilling data from client: Client48
2023-06-22 15:52:09,277 - INFO - train loss: 0.0007469068477159706
2023-06-22 15:52:09,277 - INFO - train acc: 1.0
2023-06-22 15:52:09,324 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.74      0.74        61
           5       0.89      0.89      0.89       139

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.82       200
weighted avg       0.84      0.84      0.84       200

2023-06-22 15:52:09,324 - INFO - test loss 0.011195125814832223
2023-06-22 15:52:09,324 - INFO - test acc 0.8449999690055847
2023-06-22 15:52:11,441 - INFO - Distilling data from client: Client48
2023-06-22 15:52:11,442 - INFO - train loss: 0.0007022674940785973
2023-06-22 15:52:11,442 - INFO - train acc: 1.0
2023-06-22 15:52:11,487 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.80      0.80        61
           5       0.91      0.91      0.91       139

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-06-22 15:52:11,487 - INFO - test loss 0.010357561149456225
2023-06-22 15:52:11,488 - INFO - test acc 0.8799999952316284
2023-06-22 15:52:13,980 - INFO - Distilling data from client: Client48
2023-06-22 15:52:13,981 - INFO - train loss: 0.0009108224730392686
2023-06-22 15:52:13,981 - INFO - train acc: 1.0
2023-06-22 15:52:14,029 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.80      0.81        61
           5       0.91      0.92      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-06-22 15:52:14,030 - INFO - test loss 0.010145431979198271
2023-06-22 15:52:14,030 - INFO - test acc 0.8849999904632568
2023-06-22 15:52:16,364 - INFO - Distilling data from client: Client48
2023-06-22 15:52:16,366 - INFO - train loss: 0.0005420574263042362
2023-06-22 15:52:16,367 - INFO - train acc: 1.0
2023-06-22 15:52:16,444 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.77      0.78        61
           5       0.90      0.91      0.91       139

    accuracy                           0.87       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:52:16,448 - INFO - test loss 0.011101429558165201
2023-06-22 15:52:16,449 - INFO - test acc 0.8700000047683716
2023-06-22 15:52:19,649 - INFO - Distilling data from client: Client48
2023-06-22 15:52:19,650 - INFO - train loss: 0.0006324655583596406
2023-06-22 15:52:19,650 - INFO - train acc: 1.0
2023-06-22 15:52:19,699 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.80      0.82        61
           5       0.91      0.93      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.87      0.87       200
weighted avg       0.89      0.89      0.89       200

2023-06-22 15:52:19,700 - INFO - test loss 0.010405820269107177
2023-06-22 15:52:19,701 - INFO - test acc 0.8899999856948853
2023-06-22 15:52:22,317 - INFO - Distilling data from client: Client48
2023-06-22 15:52:22,318 - INFO - train loss: 0.0005522779708850518
2023-06-22 15:52:22,318 - INFO - train acc: 1.0
2023-06-22 15:52:22,362 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.79      0.79        61
           5       0.91      0.91      0.91       139

    accuracy                           0.87       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:52:22,363 - INFO - test loss 0.011429868745829776
2023-06-22 15:52:22,363 - INFO - test acc 0.8700000047683716
2023-06-22 15:52:24,639 - INFO - Distilling data from client: Client48
2023-06-22 15:52:24,640 - INFO - train loss: 0.0005363255054466907
2023-06-22 15:52:24,640 - INFO - train acc: 1.0
2023-06-22 15:52:24,706 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.80      0.81        61
           5       0.91      0.92      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-06-22 15:52:24,708 - INFO - test loss 0.010728864712131471
2023-06-22 15:52:24,708 - INFO - test acc 0.8849999904632568
2023-06-22 15:52:27,166 - INFO - Distilling data from client: Client48
2023-06-22 15:52:27,166 - INFO - train loss: 0.0004595553379696012
2023-06-22 15:52:27,167 - INFO - train acc: 1.0
2023-06-22 15:52:27,211 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.79      0.81        61
           5       0.91      0.94      0.92       139

    accuracy                           0.89       200
   macro avg       0.88      0.86      0.87       200
weighted avg       0.89      0.89      0.89       200

2023-06-22 15:52:27,212 - INFO - test loss 0.010514012719872777
2023-06-22 15:52:27,212 - INFO - test acc 0.8899999856948853
2023-06-22 15:52:29,492 - INFO - Distilling data from client: Client48
2023-06-22 15:52:29,492 - INFO - train loss: 0.0005481394748331183
2023-06-22 15:52:29,493 - INFO - train acc: 1.0
2023-06-22 15:52:29,535 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.79      0.81        61
           5       0.91      0.93      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-06-22 15:52:29,536 - INFO - test loss 0.010500656992364925
2023-06-22 15:52:29,537 - INFO - test acc 0.8849999904632568
2023-06-22 15:52:31,894 - INFO - Distilling data from client: Client48
2023-06-22 15:52:31,894 - INFO - train loss: 0.0004794279428098966
2023-06-22 15:52:31,894 - INFO - train acc: 1.0
2023-06-22 15:52:31,970 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.79      0.81        61
           5       0.91      0.93      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-06-22 15:52:31,970 - INFO - test loss 0.010694602056339304
2023-06-22 15:52:31,971 - INFO - test acc 0.8849999904632568
2023-06-22 15:52:34,286 - INFO - Distilling data from client: Client48
2023-06-22 15:52:34,288 - INFO - train loss: 0.0004845552610089526
2023-06-22 15:52:34,288 - INFO - train acc: 1.0
2023-06-22 15:52:34,348 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.75      0.79        61
           5       0.90      0.93      0.91       139

    accuracy                           0.88       200
   macro avg       0.86      0.84      0.85       200
weighted avg       0.87      0.88      0.87       200

2023-06-22 15:52:34,350 - INFO - test loss 0.010494079931438062
2023-06-22 15:52:34,350 - INFO - test acc 0.875
2023-06-22 15:52:36,661 - INFO - Distilling data from client: Client48
2023-06-22 15:52:36,661 - INFO - train loss: 0.0004966475026342225
2023-06-22 15:52:36,661 - INFO - train acc: 1.0
2023-06-22 15:52:36,701 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.79      0.79        61
           5       0.91      0.91      0.91       139

    accuracy                           0.88       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.88      0.87       200

2023-06-22 15:52:36,701 - INFO - test loss 0.011235613271036695
2023-06-22 15:52:36,701 - INFO - test acc 0.875
2023-06-22 15:52:38,871 - INFO - Distilling data from client: Client48
2023-06-22 15:52:38,871 - INFO - train loss: 0.0006114274332163634
2023-06-22 15:52:38,872 - INFO - train acc: 1.0
2023-06-22 15:52:38,923 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.80      0.78        61
           5       0.91      0.89      0.90       139

    accuracy                           0.86       200
   macro avg       0.84      0.85      0.84       200
weighted avg       0.87      0.86      0.87       200

2023-06-22 15:52:38,923 - INFO - test loss 0.010874894294013707
2023-06-22 15:52:38,924 - INFO - test acc 0.8650000095367432
2023-06-22 15:52:41,374 - INFO - Distilling data from client: Client48
2023-06-22 15:52:41,374 - INFO - train loss: 0.00048679897652180795
2023-06-22 15:52:41,375 - INFO - train acc: 1.0
2023-06-22 15:52:41,431 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.77      0.78        61
           5       0.90      0.91      0.91       139

    accuracy                           0.87       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-06-22 15:52:41,434 - INFO - test loss 0.010447407517977046
2023-06-22 15:52:41,434 - INFO - test acc 0.8700000047683716
2023-06-22 15:52:41,444 - WARNING - Finished tracing + transforming jit(gather) in 0.0008990764617919922 sec
2023-06-22 15:52:41,445 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[362,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:52:41,450 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.00433802604675293 sec
2023-06-22 15:52:41,450 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:52:41,474 - WARNING - Finished XLA compilation of jit(gather) in 0.023244142532348633 sec
2023-06-22 15:52:41,498 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:52:41,515 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:52:41,532 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:52:41,547 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:52:41,560 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:52:41,573 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:52:42,004 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client48//synthetic.png
2023-06-22 15:52:42,028 - INFO - c: 0.0 and total_data_in_this_class: 268
2023-06-22 15:52:42,028 - INFO - c: 8.0 and total_data_in_this_class: 266
2023-06-22 15:52:42,028 - INFO - c: 9.0 and total_data_in_this_class: 265
2023-06-22 15:52:42,028 - INFO - c: 0.0 and total_data_in_this_class: 65
2023-06-22 15:52:42,028 - INFO - c: 8.0 and total_data_in_this_class: 67
2023-06-22 15:52:42,028 - INFO - c: 9.0 and total_data_in_this_class: 68
2023-06-22 15:52:42,158 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07990717887878418 sec
2023-06-22 15:52:42,231 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07183504104614258 sec
2023-06-22 15:52:42,240 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.16440725326538086 sec
2023-06-22 15:52:42,243 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:52:42,297 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.05340766906738281 sec
2023-06-22 15:52:42,297 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:52:42,469 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.17115187644958496 sec
2023-06-22 15:52:42,520 - INFO - initial test loss: 0.025331383698125633
2023-06-22 15:52:42,521 - INFO - initial test acc: 0.6800000071525574
2023-06-22 15:52:42,542 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.014639854431152344 sec
2023-06-22 15:52:42,736 - WARNING - Finished tracing + transforming update_fn for pjit in 0.209458589553833 sec
2023-06-22 15:52:42,741 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[531,10]), ShapedArray(float32[531,10]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-06-22 15:52:42,854 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.11276578903198242 sec
2023-06-22 15:52:42,855 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-06-22 15:52:43,284 - WARNING - Finished XLA compilation of jit(update_fn) in 0.4290335178375244 sec
2023-06-22 15:52:46,409 - INFO - Distilling data from client: Client49
2023-06-22 15:52:46,410 - INFO - train loss: 0.0025124905916378585
2023-06-22 15:52:46,410 - INFO - train acc: 0.994350254535675
2023-06-22 15:52:46,614 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.72      0.72        65
           8       0.66      0.73      0.70        67
           9       0.80      0.71      0.75        68

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:52:46,614 - INFO - test loss 0.022170773683104748
2023-06-22 15:52:46,614 - INFO - test acc 0.7199999690055847
2023-06-22 15:52:49,581 - INFO - Distilling data from client: Client49
2023-06-22 15:52:49,581 - INFO - train loss: 0.001501776624883326
2023-06-22 15:52:49,581 - INFO - train acc: 0.9981167316436768
2023-06-22 15:52:49,632 - INFO - report:               precision    recall  f1-score   support

           0       0.62      0.71      0.66        65
           8       0.64      0.66      0.65        67
           9       0.79      0.66      0.72        68

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-06-22 15:52:49,633 - INFO - test loss 0.02336974320296036
2023-06-22 15:52:49,633 - INFO - test acc 0.675000011920929
2023-06-22 15:52:52,472 - INFO - Distilling data from client: Client49
2023-06-22 15:52:52,472 - INFO - train loss: 0.001209486117883802
2023-06-22 15:52:52,472 - INFO - train acc: 1.0
2023-06-22 15:52:52,527 - INFO - report:               precision    recall  f1-score   support

           0       0.69      0.71      0.70        65
           8       0.67      0.72      0.69        67
           9       0.80      0.72      0.76        68

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.72       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:52:52,527 - INFO - test loss 0.022278305954045317
2023-06-22 15:52:52,527 - INFO - test acc 0.7149999737739563
2023-06-22 15:52:55,320 - INFO - Distilling data from client: Client49
2023-06-22 15:52:55,320 - INFO - train loss: 0.0009582177415516567
2023-06-22 15:52:55,320 - INFO - train acc: 1.0
2023-06-22 15:52:55,369 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.74      0.72        65
           8       0.68      0.75      0.71        67
           9       0.74      0.63      0.68        68

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:52:55,369 - INFO - test loss 0.02271651501523522
2023-06-22 15:52:55,369 - INFO - test acc 0.7049999833106995
2023-06-22 15:52:58,184 - INFO - Distilling data from client: Client49
2023-06-22 15:52:58,184 - INFO - train loss: 0.0008823862891588149
2023-06-22 15:52:58,185 - INFO - train acc: 1.0
2023-06-22 15:52:58,233 - INFO - report:               precision    recall  f1-score   support

           0       0.68      0.72      0.70        65
           8       0.66      0.73      0.70        67
           9       0.81      0.68      0.74        68

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:52:58,234 - INFO - test loss 0.02296563997504266
2023-06-22 15:52:58,234 - INFO - test acc 0.7099999785423279
2023-06-22 15:53:01,073 - INFO - Distilling data from client: Client49
2023-06-22 15:53:01,074 - INFO - train loss: 0.0009525194981959011
2023-06-22 15:53:01,074 - INFO - train acc: 1.0
2023-06-22 15:53:01,133 - INFO - report:               precision    recall  f1-score   support

           0       0.65      0.71      0.68        65
           8       0.62      0.67      0.65        67
           9       0.79      0.66      0.72        68

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-06-22 15:53:01,134 - INFO - test loss 0.023108862638212952
2023-06-22 15:53:01,135 - INFO - test acc 0.6800000071525574
2023-06-22 15:53:03,915 - INFO - Distilling data from client: Client49
2023-06-22 15:53:03,915 - INFO - train loss: 0.0007360235027832112
2023-06-22 15:53:03,916 - INFO - train acc: 1.0
2023-06-22 15:53:03,969 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.72      0.70        65
           8       0.68      0.72      0.70        67
           9       0.81      0.71      0.76        68

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:53:03,971 - INFO - test loss 0.023450932177961934
2023-06-22 15:53:03,972 - INFO - test acc 0.7149999737739563
2023-06-22 15:53:06,812 - INFO - Distilling data from client: Client49
2023-06-22 15:53:06,812 - INFO - train loss: 0.0008956292835635626
2023-06-22 15:53:06,812 - INFO - train acc: 1.0
2023-06-22 15:53:06,872 - INFO - report:               precision    recall  f1-score   support

           0       0.63      0.69      0.66        65
           8       0.63      0.67      0.65        67
           9       0.76      0.65      0.70        68

    accuracy                           0.67       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-06-22 15:53:06,873 - INFO - test loss 0.024347211507129346
2023-06-22 15:53:06,873 - INFO - test acc 0.6699999570846558
2023-06-22 15:53:09,803 - INFO - Distilling data from client: Client49
2023-06-22 15:53:09,804 - INFO - train loss: 0.0006238227861684491
2023-06-22 15:53:09,804 - INFO - train acc: 1.0
2023-06-22 15:53:09,853 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.69      0.68        65
           8       0.63      0.69      0.66        67
           9       0.77      0.68      0.72        68

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:53:09,854 - INFO - test loss 0.02339766966591901
2023-06-22 15:53:09,855 - INFO - test acc 0.6850000023841858
2023-06-22 15:53:12,737 - INFO - Distilling data from client: Client49
2023-06-22 15:53:12,738 - INFO - train loss: 0.0007535854850370322
2023-06-22 15:53:12,738 - INFO - train acc: 1.0
2023-06-22 15:53:12,794 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.68      0.67        65
           8       0.64      0.73      0.69        67
           9       0.76      0.65      0.70        68

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:53:12,795 - INFO - test loss 0.023503866795322893
2023-06-22 15:53:12,795 - INFO - test acc 0.6850000023841858
2023-06-22 15:53:15,657 - INFO - Distilling data from client: Client49
2023-06-22 15:53:15,658 - INFO - train loss: 0.0006949764156708025
2023-06-22 15:53:15,658 - INFO - train acc: 1.0
2023-06-22 15:53:15,709 - INFO - report:               precision    recall  f1-score   support

           0       0.69      0.74      0.71        65
           8       0.67      0.70      0.69        67
           9       0.78      0.69      0.73        68

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:53:15,710 - INFO - test loss 0.023143232517386987
2023-06-22 15:53:15,710 - INFO - test acc 0.7099999785423279
2023-06-22 15:53:18,570 - INFO - Distilling data from client: Client49
2023-06-22 15:53:18,571 - INFO - train loss: 0.000680707768660793
2023-06-22 15:53:18,571 - INFO - train acc: 1.0
2023-06-22 15:53:18,627 - INFO - report:               precision    recall  f1-score   support

           0       0.68      0.69      0.69        65
           8       0.65      0.69      0.67        67
           9       0.76      0.71      0.73        68

    accuracy                           0.69       200
   macro avg       0.70      0.69      0.70       200
weighted avg       0.70      0.69      0.70       200

2023-06-22 15:53:18,629 - INFO - test loss 0.02385845558826506
2023-06-22 15:53:18,629 - INFO - test acc 0.6949999928474426
2023-06-22 15:53:21,603 - INFO - Distilling data from client: Client49
2023-06-22 15:53:21,604 - INFO - train loss: 0.0005098981593281803
2023-06-22 15:53:21,604 - INFO - train acc: 1.0
2023-06-22 15:53:21,661 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.69      0.68        65
           8       0.65      0.72      0.68        67
           9       0.80      0.69      0.74        68

    accuracy                           0.70       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-06-22 15:53:21,661 - INFO - test loss 0.02330119501562529
2023-06-22 15:53:21,662 - INFO - test acc 0.699999988079071
2023-06-22 15:53:24,648 - INFO - Distilling data from client: Client49
2023-06-22 15:53:24,649 - INFO - train loss: 0.0005317172491509478
2023-06-22 15:53:24,649 - INFO - train acc: 1.0
2023-06-22 15:53:24,703 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.68      0.67        65
           8       0.64      0.70      0.67        67
           9       0.77      0.69      0.73        68

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:53:24,703 - INFO - test loss 0.023891988690482736
2023-06-22 15:53:24,703 - INFO - test acc 0.6899999976158142
2023-06-22 15:53:28,141 - INFO - Distilling data from client: Client49
2023-06-22 15:53:28,142 - INFO - train loss: 0.0004641060955841843
2023-06-22 15:53:28,142 - INFO - train acc: 1.0
2023-06-22 15:53:28,206 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.71      0.71        65
           8       0.66      0.72      0.69        67
           9       0.77      0.71      0.74        68

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-06-22 15:53:28,208 - INFO - test loss 0.023973709507413087
2023-06-22 15:53:28,208 - INFO - test acc 0.7099999785423279
2023-06-22 15:53:31,225 - INFO - Distilling data from client: Client49
2023-06-22 15:53:31,226 - INFO - train loss: 0.00047938841838598116
2023-06-22 15:53:31,226 - INFO - train acc: 1.0
2023-06-22 15:53:31,294 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.72      0.70        65
           8       0.67      0.73      0.70        67
           9       0.79      0.66      0.72        68

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.70      0.71       200

2023-06-22 15:53:31,295 - INFO - test loss 0.023938322755878278
2023-06-22 15:53:31,296 - INFO - test acc 0.7049999833106995
2023-06-22 15:53:34,273 - INFO - Distilling data from client: Client49
2023-06-22 15:53:34,274 - INFO - train loss: 0.0004926769186718159
2023-06-22 15:53:34,275 - INFO - train acc: 1.0
2023-06-22 15:53:34,329 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.71      0.70        65
           8       0.67      0.73      0.70        67
           9       0.80      0.72      0.76        68

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:53:34,334 - INFO - test loss 0.023572621737580568
2023-06-22 15:53:34,334 - INFO - test acc 0.7199999690055847
2023-06-22 15:53:37,523 - INFO - Distilling data from client: Client49
2023-06-22 15:53:37,524 - INFO - train loss: 0.00036328528829625127
2023-06-22 15:53:37,524 - INFO - train acc: 1.0
2023-06-22 15:53:37,579 - INFO - report:               precision    recall  f1-score   support

           0       0.65      0.71      0.68        65
           8       0.66      0.70      0.68        67
           9       0.79      0.68      0.73        68

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.69      0.70       200

2023-06-22 15:53:37,579 - INFO - test loss 0.02444505544229976
2023-06-22 15:53:37,580 - INFO - test acc 0.6949999928474426
2023-06-22 15:53:40,612 - INFO - Distilling data from client: Client49
2023-06-22 15:53:40,612 - INFO - train loss: 0.00040317369343432705
2023-06-22 15:53:40,612 - INFO - train acc: 1.0
2023-06-22 15:53:40,666 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.74      0.72        65
           8       0.68      0.73      0.71        67
           9       0.78      0.69      0.73        68

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-06-22 15:53:40,666 - INFO - test loss 0.023309543886259783
2023-06-22 15:53:40,666 - INFO - test acc 0.7199999690055847
2023-06-22 15:53:43,811 - INFO - Distilling data from client: Client49
2023-06-22 15:53:43,811 - INFO - train loss: 0.0004637675570614643
2023-06-22 15:53:43,812 - INFO - train acc: 1.0
2023-06-22 15:53:43,865 - INFO - report:               precision    recall  f1-score   support

           0       0.66      0.71      0.68        65
           8       0.70      0.72      0.71        67
           9       0.80      0.72      0.76        68

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.72       200
weighted avg       0.72      0.71      0.72       200

2023-06-22 15:53:43,866 - INFO - test loss 0.02376597136926121
2023-06-22 15:53:43,866 - INFO - test acc 0.7149999737739563
2023-06-22 15:53:46,687 - INFO - Distilling data from client: Client49
2023-06-22 15:53:46,687 - INFO - train loss: 0.00036958883296470883
2023-06-22 15:53:46,687 - INFO - train acc: 1.0
2023-06-22 15:53:46,749 - INFO - report:               precision    recall  f1-score   support

           0       0.63      0.66      0.65        65
           8       0.66      0.70      0.68        67
           9       0.77      0.69      0.73        68

    accuracy                           0.69       200
   macro avg       0.69      0.68      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-06-22 15:53:46,750 - INFO - test loss 0.023488146527034615
2023-06-22 15:53:46,750 - INFO - test acc 0.6850000023841858
2023-06-22 15:53:49,575 - INFO - Distilling data from client: Client49
2023-06-22 15:53:49,576 - INFO - train loss: 0.00033954047632873627
2023-06-22 15:53:49,576 - INFO - train acc: 1.0
2023-06-22 15:53:49,621 - INFO - report:               precision    recall  f1-score   support

           0       0.65      0.71      0.68        65
           8       0.70      0.72      0.71        67
           9       0.77      0.68      0.72        68

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-06-22 15:53:49,622 - INFO - test loss 0.02392481230529922
2023-06-22 15:53:49,622 - INFO - test acc 0.699999988079071
2023-06-22 15:53:52,411 - INFO - Distilling data from client: Client49
2023-06-22 15:53:52,412 - INFO - train loss: 0.00041129060409403616
2023-06-22 15:53:52,413 - INFO - train acc: 1.0
2023-06-22 15:53:52,464 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.74      0.72        65
           8       0.67      0.73      0.70        67
           9       0.80      0.69      0.74        68

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-06-22 15:53:52,464 - INFO - test loss 0.023818676081198392
2023-06-22 15:53:52,464 - INFO - test acc 0.7199999690055847
2023-06-22 15:53:55,269 - INFO - Distilling data from client: Client49
2023-06-22 15:53:55,270 - INFO - train loss: 0.0004176463525784006
2023-06-22 15:53:55,270 - INFO - train acc: 1.0
2023-06-22 15:53:55,323 - INFO - report:               precision    recall  f1-score   support

           0       0.66      0.71      0.68        65
           8       0.69      0.73      0.71        67
           9       0.80      0.69      0.74        68

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-06-22 15:53:55,324 - INFO - test loss 0.023696799674968037
2023-06-22 15:53:55,325 - INFO - test acc 0.7099999785423279
2023-06-22 15:53:58,203 - INFO - Distilling data from client: Client49
2023-06-22 15:53:58,204 - INFO - train loss: 0.00042755417541357816
2023-06-22 15:53:58,204 - INFO - train acc: 1.0
2023-06-22 15:53:58,260 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.71      0.69        65
           8       0.68      0.72      0.70        67
           9       0.75      0.66      0.70        68

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.69      0.70       200

2023-06-22 15:53:58,260 - INFO - test loss 0.02338257626411661
2023-06-22 15:53:58,261 - INFO - test acc 0.6949999928474426
2023-06-22 15:53:58,293 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:53:58,312 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:53:58,330 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:53:58,350 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:53:58,365 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:53:58,377 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:53:58,391 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:53:58,403 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:53:58,417 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-06-22 15:53:59,017 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client49//synthetic.png
2023-07-01 00:18:49,945 - INFO - c: 1.0 and total_data_in_this_class: 258
2023-07-01 00:18:49,945 - INFO - c: 3.0 and total_data_in_this_class: 258
2023-07-01 00:18:49,945 - INFO - c: 4.0 and total_data_in_this_class: 283
2023-07-01 00:18:49,945 - INFO - c: 1.0 and total_data_in_this_class: 75
2023-07-01 00:18:49,945 - INFO - c: 3.0 and total_data_in_this_class: 75
2023-07-01 00:18:49,945 - INFO - c: 4.0 and total_data_in_this_class: 50
2023-07-01 00:18:49,967 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.0003001689910888672 sec
2023-07-01 00:18:49,969 - DEBUG - Initializing backend 'cpu'
2023-07-01 00:18:49,971 - DEBUG - Backend 'cpu' initialized
2023-07-01 00:18:49,971 - DEBUG - Initializing backend 'cuda'
2023-07-01 00:18:49,971 - INFO - Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
2023-07-01 00:18:49,971 - DEBUG - Initializing backend 'rocm'
2023-07-01 00:18:49,971 - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
2023-07-01 00:18:49,972 - DEBUG - Initializing backend 'tpu'
2023-07-01 00:18:49,972 - INFO - Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
2023-07-01 00:18:49,972 - WARNING - No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
2023-07-01 00:18:49,973 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002129077911376953 sec
2023-07-01 00:18:49,973 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:18:49,974 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012521743774414062 sec
2023-07-01 00:18:49,975 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:18:49,986 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.011392593383789062 sec
2023-07-01 00:18:49,988 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00023818016052246094 sec
2023-07-01 00:18:49,988 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:18:49,990 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009777545928955078 sec
2023-07-01 00:18:49,990 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:18:49,998 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008228302001953125 sec
2023-07-01 00:18:50,001 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013709068298339844 sec
2023-07-01 00:18:50,002 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012540817260742188 sec
2023-07-01 00:18:50,003 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0008542537689208984 sec
2023-07-01 00:18:50,005 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00014328956604003906 sec
2023-07-01 00:18:50,005 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000118255615234375 sec
2023-07-01 00:18:50,006 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002837181091308594 sec
2023-07-01 00:18:50,006 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002582073211669922 sec
2023-07-01 00:18:50,007 - WARNING - Finished tracing + transforming absolute for pjit in 0.0001685619354248047 sec
2023-07-01 00:18:50,008 - WARNING - Finished tracing + transforming fn for pjit in 0.00033855438232421875 sec
2023-07-01 00:18:50,008 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.000335693359375 sec
2023-07-01 00:18:50,009 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000133514404296875 sec
2023-07-01 00:18:50,010 - WARNING - Finished tracing + transforming fn for pjit in 0.00024628639221191406 sec
2023-07-01 00:18:50,011 - WARNING - Finished tracing + transforming fn for pjit in 0.00027441978454589844 sec
2023-07-01 00:18:50,012 - WARNING - Finished tracing + transforming fn for pjit in 0.0002903938293457031 sec
2023-07-01 00:18:50,012 - WARNING - Finished tracing + transforming fn for pjit in 0.0002872943878173828 sec
2023-07-01 00:18:50,013 - WARNING - Finished tracing + transforming fn for pjit in 0.0002307891845703125 sec
2023-07-01 00:18:50,014 - WARNING - Finished tracing + transforming fn for pjit in 0.00024008750915527344 sec
2023-07-01 00:18:50,016 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023984909057617188 sec
2023-07-01 00:18:50,017 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00028586387634277344 sec
2023-07-01 00:18:50,017 - WARNING - Finished tracing + transforming fn for pjit in 0.00026226043701171875 sec
2023-07-01 00:18:50,018 - WARNING - Finished tracing + transforming fn for pjit in 0.00024199485778808594 sec
2023-07-01 00:18:50,018 - WARNING - Finished tracing + transforming _uniform for pjit in 0.003028392791748047 sec
2023-07-01 00:18:50,019 - WARNING - Finished tracing + transforming _normal_real for pjit in 0.003696441650390625 sec
2023-07-01 00:18:50,019 - WARNING - Finished tracing + transforming _normal for pjit in 0.00414729118347168 sec
2023-07-01 00:18:50,020 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003287792205810547 sec
2023-07-01 00:18:50,021 - WARNING - Finished tracing + transforming fn for pjit in 0.0003142356872558594 sec
2023-07-01 00:18:50,022 - WARNING - Finished tracing + transforming fn for pjit in 0.0002446174621582031 sec
2023-07-01 00:18:50,022 - WARNING - Finished tracing + transforming _uniform for pjit in 0.002727985382080078 sec
2023-07-01 00:18:50,023 - WARNING - Finished tracing + transforming _normal_real for pjit in 0.0033617019653320312 sec
2023-07-01 00:18:50,023 - WARNING - Finished tracing + transforming _normal for pjit in 0.003741741180419922 sec
2023-07-01 00:18:50,024 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001647472381591797 sec
2023-07-01 00:18:50,024 - WARNING - Finished tracing + transforming fn for pjit in 0.00024199485778808594 sec
2023-07-01 00:18:50,025 - WARNING - Finished tracing + transforming fn for pjit in 0.0002777576446533203 sec
2023-07-01 00:18:50,025 - WARNING - Finished tracing + transforming fn for pjit in 0.0002536773681640625 sec
2023-07-01 00:18:50,029 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003802776336669922 sec
2023-07-01 00:18:50,029 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009779930114746094 sec
2023-07-01 00:18:50,030 - WARNING - Finished tracing + transforming fn for pjit in 0.0002396106719970703 sec
2023-07-01 00:18:50,031 - WARNING - Finished tracing + transforming fn for pjit in 0.00024175643920898438 sec
2023-07-01 00:18:50,032 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003120899200439453 sec
2023-07-01 00:18:50,033 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002834796905517578 sec
2023-07-01 00:18:50,033 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00019502639770507812 sec
2023-07-01 00:18:50,034 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002715587615966797 sec
2023-07-01 00:18:50,035 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023865699768066406 sec
2023-07-01 00:18:50,036 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023436546325683594 sec
2023-07-01 00:18:50,036 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002849102020263672 sec
2023-07-01 00:18:50,037 - WARNING - Finished tracing + transforming _where for pjit in 0.000982046127319336 sec
2023-07-01 00:18:50,038 - WARNING - Finished tracing + transforming fn for pjit in 0.00026726722717285156 sec
2023-07-01 00:18:50,038 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002741813659667969 sec
2023-07-01 00:18:50,039 - WARNING - Finished tracing + transforming fn for pjit in 0.00023484230041503906 sec
2023-07-01 00:18:50,040 - WARNING - Finished tracing + transforming fn for pjit in 0.0002353191375732422 sec
2023-07-01 00:18:50,041 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002257823944091797 sec
2023-07-01 00:18:50,041 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002734661102294922 sec
2023-07-01 00:18:50,042 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001773834228515625 sec
2023-07-01 00:18:50,043 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002682209014892578 sec
2023-07-01 00:18:50,043 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023317337036132812 sec
2023-07-01 00:18:50,044 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023031234741210938 sec
2023-07-01 00:18:50,045 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00032591819763183594 sec
2023-07-01 00:18:50,045 - WARNING - Finished tracing + transforming _where for pjit in 0.0009353160858154297 sec
2023-07-01 00:18:50,046 - WARNING - Finished tracing + transforming fn for pjit in 0.0002636909484863281 sec
2023-07-01 00:18:50,047 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027108192443847656 sec
2023-07-01 00:18:50,048 - WARNING - Finished tracing + transforming fn for pjit in 0.00022649765014648438 sec
2023-07-01 00:18:50,052 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002834796905517578 sec
2023-07-01 00:18:50,053 - WARNING - Finished tracing + transforming fn for pjit in 0.0002841949462890625 sec
2023-07-01 00:18:50,053 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002703666687011719 sec
2023-07-01 00:18:50,054 - WARNING - Finished tracing + transforming fn for pjit in 0.00022983551025390625 sec
2023-07-01 00:18:50,058 - WARNING - Finished tracing + transforming fn for pjit in 0.00022840499877929688 sec
2023-07-01 00:18:50,060 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002307891845703125 sec
2023-07-01 00:18:50,061 - WARNING - Finished tracing + transforming fn for pjit in 0.0006253719329833984 sec
2023-07-01 00:18:50,062 - WARNING - Finished tracing + transforming fn for pjit in 0.000244140625 sec
2023-07-01 00:18:50,062 - WARNING - Finished tracing + transforming _uniform for pjit in 0.0028951168060302734 sec
2023-07-01 00:18:50,063 - WARNING - Finished tracing + transforming _normal_real for pjit in 0.003540515899658203 sec
2023-07-01 00:18:50,063 - WARNING - Finished tracing + transforming _normal for pjit in 0.0039250850677490234 sec
2023-07-01 00:18:50,064 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002276897430419922 sec
2023-07-01 00:18:50,065 - WARNING - Finished tracing + transforming fn for pjit in 0.00024199485778808594 sec
2023-07-01 00:18:50,066 - WARNING - Finished tracing + transforming fn for pjit in 0.0003142356872558594 sec
2023-07-01 00:18:50,066 - WARNING - Finished tracing + transforming _uniform for pjit in 0.0025415420532226562 sec
2023-07-01 00:18:50,066 - WARNING - Finished tracing + transforming _normal_real for pjit in 0.0031671524047851562 sec
2023-07-01 00:18:50,067 - WARNING - Finished tracing + transforming _normal for pjit in 0.0035467147827148438 sec
2023-07-01 00:18:50,067 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00017070770263671875 sec
2023-07-01 00:18:50,068 - WARNING - Finished tracing + transforming fn for pjit in 0.0002384185791015625 sec
2023-07-01 00:18:50,068 - WARNING - Finished tracing + transforming fn for pjit in 0.00027251243591308594 sec
2023-07-01 00:18:50,069 - WARNING - Finished tracing + transforming fn for pjit in 0.00023245811462402344 sec
2023-07-01 00:18:50,087 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.08645033836364746 sec
2023-07-01 00:18:50,088 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011920928955078125 sec
2023-07-01 00:18:50,089 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011610984802246094 sec
2023-07-01 00:18:50,090 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00027871131896972656 sec
2023-07-01 00:18:50,091 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020503997802734375 sec
2023-07-01 00:18:50,092 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011491775512695312 sec
2023-07-01 00:18:50,092 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002651214599609375 sec
2023-07-01 00:18:50,093 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012254714965820312 sec
2023-07-01 00:18:50,094 - WARNING - Finished tracing + transforming fn for pjit in 0.0002224445343017578 sec
2023-07-01 00:18:50,094 - WARNING - Finished tracing + transforming fn for pjit in 0.0002560615539550781 sec
2023-07-01 00:18:50,095 - WARNING - Finished tracing + transforming fn for pjit in 0.00021910667419433594 sec
2023-07-01 00:18:50,096 - WARNING - Finished tracing + transforming fn for pjit in 0.0003192424774169922 sec
2023-07-01 00:18:50,097 - WARNING - Finished tracing + transforming fn for pjit in 0.00022077560424804688 sec
2023-07-01 00:18:50,099 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001723766326904297 sec
2023-07-01 00:18:50,099 - WARNING - Finished tracing + transforming fn for pjit in 0.00022101402282714844 sec
2023-07-01 00:18:50,100 - WARNING - Finished tracing + transforming fn for pjit in 0.000286102294921875 sec
2023-07-01 00:18:50,103 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003612041473388672 sec
2023-07-01 00:18:50,104 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009546279907226562 sec
2023-07-01 00:18:50,105 - WARNING - Finished tracing + transforming fn for pjit in 0.0002491474151611328 sec
2023-07-01 00:18:50,106 - WARNING - Finished tracing + transforming fn for pjit in 0.00022554397583007812 sec
2023-07-01 00:18:50,106 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022935867309570312 sec
2023-07-01 00:18:50,107 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026607513427734375 sec
2023-07-01 00:18:50,108 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017571449279785156 sec
2023-07-01 00:18:50,108 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027179718017578125 sec
2023-07-01 00:18:50,109 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002315044403076172 sec
2023-07-01 00:18:50,110 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002319812774658203 sec
2023-07-01 00:18:50,111 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003254413604736328 sec
2023-07-01 00:18:50,111 - WARNING - Finished tracing + transforming _where for pjit in 0.0009222030639648438 sec
2023-07-01 00:18:50,112 - WARNING - Finished tracing + transforming fn for pjit in 0.0002624988555908203 sec
2023-07-01 00:18:50,112 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027823448181152344 sec
2023-07-01 00:18:50,113 - WARNING - Finished tracing + transforming fn for pjit in 0.0002231597900390625 sec
2023-07-01 00:18:50,114 - WARNING - Finished tracing + transforming fn for pjit in 0.000225067138671875 sec
2023-07-01 00:18:50,115 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002181529998779297 sec
2023-07-01 00:18:50,115 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002613067626953125 sec
2023-07-01 00:18:50,116 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000240325927734375 sec
2023-07-01 00:18:50,117 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027179718017578125 sec
2023-07-01 00:18:50,117 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023555755615234375 sec
2023-07-01 00:18:50,118 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002307891845703125 sec
2023-07-01 00:18:50,119 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002682209014892578 sec
2023-07-01 00:18:50,119 - WARNING - Finished tracing + transforming _where for pjit in 0.0008797645568847656 sec
2023-07-01 00:18:50,120 - WARNING - Finished tracing + transforming fn for pjit in 0.0002598762512207031 sec
2023-07-01 00:18:50,121 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005867481231689453 sec
2023-07-01 00:18:50,122 - WARNING - Finished tracing + transforming fn for pjit in 0.00022649765014648438 sec
2023-07-01 00:18:50,126 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032591819763183594 sec
2023-07-01 00:18:50,127 - WARNING - Finished tracing + transforming fn for pjit in 0.00027370452880859375 sec
2023-07-01 00:18:50,127 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027298927307128906 sec
2023-07-01 00:18:50,128 - WARNING - Finished tracing + transforming fn for pjit in 0.0002219676971435547 sec
2023-07-01 00:18:50,132 - WARNING - Finished tracing + transforming fn for pjit in 0.00022530555725097656 sec
2023-07-01 00:18:50,134 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00023412704467773438 sec
2023-07-01 00:18:50,134 - WARNING - Finished tracing + transforming fn for pjit in 0.0002262592315673828 sec
2023-07-01 00:18:50,135 - WARNING - Finished tracing + transforming fn for pjit in 0.000225067138671875 sec
2023-07-01 00:18:50,153 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06477165222167969 sec
2023-07-01 00:18:50,154 - WARNING - Finished tracing + transforming absolute for pjit in 0.00016021728515625 sec
2023-07-01 00:18:50,154 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012445449829101562 sec
2023-07-01 00:18:50,155 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013589859008789062 sec
2023-07-01 00:18:50,156 - WARNING - Finished tracing + transforming _where for pjit in 0.0006103515625 sec
2023-07-01 00:18:50,156 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003638267517089844 sec
2023-07-01 00:18:50,157 - WARNING - Finished tracing + transforming trace for pjit in 0.0025892257690429688 sec
2023-07-01 00:18:50,157 - WARNING - Finished tracing + transforming fn for pjit in 0.0002601146697998047 sec
2023-07-01 00:18:50,168 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0001499652862548828 sec
2023-07-01 00:18:50,169 - WARNING - Finished tracing + transforming tril for pjit in 0.0009195804595947266 sec
2023-07-01 00:18:50,170 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0022509098052978516 sec
2023-07-01 00:18:50,171 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019121170043945312 sec
2023-07-01 00:18:50,171 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00011301040649414062 sec
2023-07-01 00:18:50,174 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014591217041015625 sec
2023-07-01 00:18:50,178 - WARNING - Finished tracing + transforming _solve for pjit in 0.010586738586425781 sec
2023-07-01 00:18:50,178 - WARNING - Finished tracing + transforming dot for pjit in 0.0003151893615722656 sec
2023-07-01 00:18:50,179 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002644062042236328 sec
2023-07-01 00:18:50,180 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00028133392333984375 sec
2023-07-01 00:18:50,180 - WARNING - Finished tracing + transforming _mean for pjit in 0.0008666515350341797 sec
2023-07-01 00:18:50,181 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00018358230590820312 sec
2023-07-01 00:18:50,182 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00017309188842773438 sec
2023-07-01 00:18:50,182 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002231597900390625 sec
2023-07-01 00:18:50,183 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003559589385986328 sec
2023-07-01 00:18:50,184 - WARNING - Finished tracing + transforming _mean for pjit in 0.0013852119445800781 sec
2023-07-01 00:18:50,185 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.18501543998718262 sec
2023-07-01 00:18:50,187 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:18:50,219 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03262448310852051 sec
2023-07-01 00:18:50,220 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:18:50,343 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12377405166625977 sec
2023-07-01 00:18:50,375 - INFO - initial test loss: 0.027373878899937403
2023-07-01 00:18:50,375 - INFO - initial test acc: 0.6449999809265137
2023-07-01 00:18:50,383 - WARNING - Finished tracing + transforming dot for pjit in 0.00037217140197753906 sec
2023-07-01 00:18:50,384 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002894401550292969 sec
2023-07-01 00:18:50,385 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003483295440673828 sec
2023-07-01 00:18:50,385 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010309219360351562 sec
2023-07-01 00:18:50,386 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00020742416381835938 sec
2023-07-01 00:18:50,387 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00018310546875 sec
2023-07-01 00:18:50,387 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002512931823730469 sec
2023-07-01 00:18:50,388 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00036525726318359375 sec
2023-07-01 00:18:50,389 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010678768157958984 sec
2023-07-01 00:18:50,389 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.010982751846313477 sec
2023-07-01 00:18:50,398 - WARNING - Finished tracing + transforming fn for pjit in 0.000255584716796875 sec
2023-07-01 00:18:50,398 - WARNING - Finished tracing + transforming fn for pjit in 0.0002620220184326172 sec
2023-07-01 00:18:50,399 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00027680397033691406 sec
2023-07-01 00:18:50,400 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002601146697998047 sec
2023-07-01 00:18:50,400 - WARNING - Finished tracing + transforming _where for pjit in 0.0008418560028076172 sec
2023-07-01 00:18:50,408 - WARNING - Finished tracing + transforming fn for pjit in 0.0002510547637939453 sec
2023-07-01 00:18:50,409 - WARNING - Finished tracing + transforming fn for pjit in 0.0002601146697998047 sec
2023-07-01 00:18:50,409 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021600723266601562 sec
2023-07-01 00:18:50,410 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025272369384765625 sec
2023-07-01 00:18:50,411 - WARNING - Finished tracing + transforming _where for pjit in 0.0008625984191894531 sec
2023-07-01 00:18:50,445 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021314620971679688 sec
2023-07-01 00:18:50,501 - WARNING - Finished tracing + transforming fn for pjit in 0.0002830028533935547 sec
2023-07-01 00:18:50,502 - WARNING - Finished tracing + transforming fn for pjit in 0.0002391338348388672 sec
2023-07-01 00:18:50,502 - WARNING - Finished tracing + transforming square for pjit in 0.00019288063049316406 sec
2023-07-01 00:18:50,503 - WARNING - Finished tracing + transforming fn for pjit in 0.00024819374084472656 sec
2023-07-01 00:18:50,504 - WARNING - Finished tracing + transforming _power for pjit in 0.0002796649932861328 sec
2023-07-01 00:18:50,505 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003502368927001953 sec
2023-07-01 00:18:50,505 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00023221969604492188 sec
2023-07-01 00:18:50,507 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001838207244873047 sec
2023-07-01 00:18:50,508 - WARNING - Finished tracing + transforming fn for pjit in 0.0002682209014892578 sec
2023-07-01 00:18:50,508 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002243518829345703 sec
2023-07-01 00:18:50,509 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024366378784179688 sec
2023-07-01 00:18:50,509 - WARNING - Finished tracing + transforming fn for pjit in 0.00026488304138183594 sec
2023-07-01 00:18:50,510 - WARNING - Finished tracing + transforming fn for pjit in 0.00023126602172851562 sec
2023-07-01 00:18:50,511 - WARNING - Finished tracing + transforming square for pjit in 0.00017023086547851562 sec
2023-07-01 00:18:50,513 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00023555755615234375 sec
2023-07-01 00:18:50,514 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017499923706054688 sec
2023-07-01 00:18:50,515 - WARNING - Finished tracing + transforming fn for pjit in 0.00030803680419921875 sec
2023-07-01 00:18:50,516 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002148151397705078 sec
2023-07-01 00:18:50,516 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024580955505371094 sec
2023-07-01 00:18:50,517 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1394939422607422 sec
2023-07-01 00:18:50,521 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:18:50,584 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06287360191345215 sec
2023-07-01 00:18:50,584 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:18:50,891 - WARNING - Finished XLA compilation of jit(update_fn) in 0.30652356147766113 sec
2023-07-01 00:18:52,122 - INFO - Distilling data from client: Client00
2023-07-01 00:18:52,123 - INFO - train loss: 0.0019305313463606098
2023-07-01 00:18:52,123 - INFO - train acc: 1.0
2023-07-01 00:18:52,144 - WARNING - Finished tracing + transforming fn for pjit in 0.0004398822784423828 sec
2023-07-01 00:18:52,145 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(float32[]), ShapedArray(int64[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:18:52,146 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.001230478286743164 sec
2023-07-01 00:18:52,147 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:18:52,152 - WARNING - Finished XLA compilation of jit(fn) in 0.0057866573333740234 sec
2023-07-01 00:18:52,193 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.77      0.80        75
           3       0.71      0.61      0.66        75
           4       0.54      0.70      0.61        50

    accuracy                           0.69       200
   macro avg       0.69      0.70      0.69       200
weighted avg       0.71      0.69      0.70       200

2023-07-01 00:18:52,193 - INFO - test loss 0.024331815478736463
2023-07-01 00:18:52,193 - INFO - test acc 0.6949999928474426
2023-07-01 00:18:53,408 - INFO - Distilling data from client: Client00
2023-07-01 00:18:53,408 - INFO - train loss: 0.0010560636984034597
2023-07-01 00:18:53,408 - INFO - train acc: 1.0
2023-07-01 00:18:53,430 - WARNING - Finished tracing + transforming fn for pjit in 0.0005552768707275391 sec
2023-07-01 00:18:53,430 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(float32[]), ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:18:53,432 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.0010857582092285156 sec
2023-07-01 00:18:53,432 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:18:53,438 - WARNING - Finished XLA compilation of jit(fn) in 0.005552530288696289 sec
2023-07-01 00:18:53,440 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.72      0.78        75
           3       0.70      0.60      0.65        75
           4       0.46      0.66      0.54        50

    accuracy                           0.66       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.69      0.66      0.67       200

2023-07-01 00:18:53,440 - INFO - test loss 0.025278067353869903
2023-07-01 00:18:53,440 - INFO - test acc 0.6599999666213989
2023-07-01 00:18:54,652 - INFO - Distilling data from client: Client00
2023-07-01 00:18:54,652 - INFO - train loss: 0.0007134751070744179
2023-07-01 00:18:54,653 - INFO - train acc: 1.0
2023-07-01 00:18:54,712 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.80      0.83        75
           3       0.77      0.63      0.69        75
           4       0.52      0.72      0.61        50

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.74      0.71      0.72       200

2023-07-01 00:18:54,712 - INFO - test loss 0.02404176617444412
2023-07-01 00:18:54,712 - INFO - test acc 0.7149999737739563
2023-07-01 00:18:55,923 - INFO - Distilling data from client: Client00
2023-07-01 00:18:55,923 - INFO - train loss: 0.000516324560702221
2023-07-01 00:18:55,923 - INFO - train acc: 1.0
2023-07-01 00:18:55,945 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.79      0.81        75
           3       0.78      0.63      0.70        75
           4       0.54      0.74      0.62        50

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.74      0.71      0.72       200

2023-07-01 00:18:55,945 - INFO - test loss 0.0243767714945259
2023-07-01 00:18:55,945 - INFO - test acc 0.7149999737739563
2023-07-01 00:18:57,161 - INFO - Distilling data from client: Client00
2023-07-01 00:18:57,161 - INFO - train loss: 0.0005808997317697795
2023-07-01 00:18:57,161 - INFO - train acc: 1.0
2023-07-01 00:18:57,184 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.79      0.82        75
           3       0.75      0.63      0.68        75
           4       0.53      0.72      0.61        50

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.73      0.71      0.72       200

2023-07-01 00:18:57,184 - INFO - test loss 0.024754712492635556
2023-07-01 00:18:57,184 - INFO - test acc 0.7099999785423279
2023-07-01 00:18:58,401 - INFO - Distilling data from client: Client00
2023-07-01 00:18:58,401 - INFO - train loss: 0.00055899417421811
2023-07-01 00:18:58,401 - INFO - train acc: 1.0
2023-07-01 00:18:58,426 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.80      0.83        75
           3       0.75      0.64      0.69        75
           4       0.52      0.70      0.60        50

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.74      0.71      0.72       200

2023-07-01 00:18:58,426 - INFO - test loss 0.024789689432357178
2023-07-01 00:18:58,426 - INFO - test acc 0.7149999737739563
2023-07-01 00:18:59,647 - INFO - Distilling data from client: Client00
2023-07-01 00:18:59,647 - INFO - train loss: 0.0004093346409810705
2023-07-01 00:18:59,647 - INFO - train acc: 1.0
2023-07-01 00:18:59,669 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.76      0.81        75
           3       0.75      0.64      0.69        75
           4       0.51      0.72      0.60        50

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.74      0.70      0.71       200

2023-07-01 00:18:59,670 - INFO - test loss 0.02473846900160119
2023-07-01 00:18:59,670 - INFO - test acc 0.7049999833106995
2023-07-01 00:19:00,888 - INFO - Distilling data from client: Client00
2023-07-01 00:19:00,888 - INFO - train loss: 0.00037653618761818744
2023-07-01 00:19:00,888 - INFO - train acc: 1.0
2023-07-01 00:19:00,912 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.77      0.81        75
           3       0.76      0.63      0.69        75
           4       0.53      0.74      0.62        50

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.74      0.71      0.72       200

2023-07-01 00:19:00,912 - INFO - test loss 0.02466133502907594
2023-07-01 00:19:00,912 - INFO - test acc 0.7099999785423279
2023-07-01 00:19:02,138 - INFO - Distilling data from client: Client00
2023-07-01 00:19:02,138 - INFO - train loss: 0.0004766920682703327
2023-07-01 00:19:02,138 - INFO - train acc: 1.0
2023-07-01 00:19:02,161 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.76      0.80        75
           3       0.77      0.64      0.70        75
           4       0.54      0.76      0.63        50

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.74      0.71      0.72       200

2023-07-01 00:19:02,161 - INFO - test loss 0.025009660155006257
2023-07-01 00:19:02,161 - INFO - test acc 0.7149999737739563
2023-07-01 00:19:03,384 - INFO - Distilling data from client: Client00
2023-07-01 00:19:03,384 - INFO - train loss: 0.0004449541579421393
2023-07-01 00:19:03,384 - INFO - train acc: 1.0
2023-07-01 00:19:03,407 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.77      0.82        75
           3       0.77      0.64      0.70        75
           4       0.51      0.72      0.60        50

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.70       200
weighted avg       0.74      0.71      0.72       200

2023-07-01 00:19:03,407 - INFO - test loss 0.024750236079656908
2023-07-01 00:19:03,407 - INFO - test acc 0.7099999785423279
2023-07-01 00:19:04,630 - INFO - Distilling data from client: Client00
2023-07-01 00:19:04,630 - INFO - train loss: 0.0003428892142521491
2023-07-01 00:19:04,630 - INFO - train acc: 1.0
2023-07-01 00:19:04,655 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.76      0.80        75
           3       0.74      0.60      0.66        75
           4       0.48      0.68      0.56        50

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.71      0.68      0.69       200

2023-07-01 00:19:04,655 - INFO - test loss 0.024810398566028385
2023-07-01 00:19:04,655 - INFO - test acc 0.6800000071525574
2023-07-01 00:19:05,883 - INFO - Distilling data from client: Client00
2023-07-01 00:19:05,884 - INFO - train loss: 0.00038178157488322196
2023-07-01 00:19:05,884 - INFO - train acc: 1.0
2023-07-01 00:19:05,910 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.76      0.80        75
           3       0.72      0.63      0.67        75
           4       0.51      0.70      0.59        50

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.72      0.69      0.70       200

2023-07-01 00:19:05,910 - INFO - test loss 0.0250797455100698
2023-07-01 00:19:05,910 - INFO - test acc 0.6949999928474426
2023-07-01 00:19:07,130 - INFO - Distilling data from client: Client00
2023-07-01 00:19:07,130 - INFO - train loss: 0.00029795551621882835
2023-07-01 00:19:07,130 - INFO - train acc: 1.0
2023-07-01 00:19:07,153 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.76      0.81        75
           3       0.77      0.63      0.69        75
           4       0.51      0.74      0.60        50

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.74      0.70      0.71       200

2023-07-01 00:19:07,153 - INFO - test loss 0.02476529740373414
2023-07-01 00:19:07,154 - INFO - test acc 0.7049999833106995
2023-07-01 00:19:08,361 - INFO - Distilling data from client: Client00
2023-07-01 00:19:08,361 - INFO - train loss: 0.00029750781905382105
2023-07-01 00:19:08,361 - INFO - train acc: 1.0
2023-07-01 00:19:08,424 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.81      0.84        75
           3       0.77      0.67      0.71        75
           4       0.51      0.66      0.57        50

    accuracy                           0.72       200
   macro avg       0.72      0.71      0.71       200
weighted avg       0.74      0.72      0.73       200

2023-07-01 00:19:08,424 - INFO - test loss 0.024705024383138163
2023-07-01 00:19:08,424 - INFO - test acc 0.7199999690055847
2023-07-01 00:19:09,641 - INFO - Distilling data from client: Client00
2023-07-01 00:19:09,641 - INFO - train loss: 0.00037271346432324895
2023-07-01 00:19:09,641 - INFO - train acc: 1.0
2023-07-01 00:19:09,665 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.79      0.81        75
           3       0.75      0.60      0.67        75
           4       0.52      0.72      0.61        50

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.72      0.70      0.70       200

2023-07-01 00:19:09,665 - INFO - test loss 0.02498467757737385
2023-07-01 00:19:09,665 - INFO - test acc 0.699999988079071
2023-07-01 00:19:10,884 - INFO - Distilling data from client: Client00
2023-07-01 00:19:10,884 - INFO - train loss: 0.00035677595357487416
2023-07-01 00:19:10,884 - INFO - train acc: 1.0
2023-07-01 00:19:10,907 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.79      0.81        75
           3       0.77      0.61      0.68        75
           4       0.51      0.72      0.60        50

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.73      0.70      0.71       200

2023-07-01 00:19:10,907 - INFO - test loss 0.025359237613029884
2023-07-01 00:19:10,907 - INFO - test acc 0.7049999833106995
2023-07-01 00:19:12,122 - INFO - Distilling data from client: Client00
2023-07-01 00:19:12,122 - INFO - train loss: 0.00030268340635130627
2023-07-01 00:19:12,122 - INFO - train acc: 1.0
2023-07-01 00:19:12,144 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.76      0.80        75
           3       0.76      0.64      0.70        75
           4       0.50      0.70      0.58        50

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.73      0.70      0.71       200

2023-07-01 00:19:12,145 - INFO - test loss 0.025014041586974612
2023-07-01 00:19:12,145 - INFO - test acc 0.699999988079071
2023-07-01 00:19:13,372 - INFO - Distilling data from client: Client00
2023-07-01 00:19:13,372 - INFO - train loss: 0.00032623669269159423
2023-07-01 00:19:13,372 - INFO - train acc: 1.0
2023-07-01 00:19:13,397 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.79      0.81        75
           3       0.78      0.61      0.69        75
           4       0.52      0.74      0.61        50

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.74      0.71      0.72       200

2023-07-01 00:19:13,397 - INFO - test loss 0.025247657005344714
2023-07-01 00:19:13,397 - INFO - test acc 0.7099999785423279
2023-07-01 00:19:14,619 - INFO - Distilling data from client: Client00
2023-07-01 00:19:14,619 - INFO - train loss: 0.0002518049767953235
2023-07-01 00:19:14,619 - INFO - train acc: 1.0
2023-07-01 00:19:14,644 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.77      0.81        75
           3       0.75      0.63      0.68        75
           4       0.51      0.70      0.59        50

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.72      0.70      0.71       200

2023-07-01 00:19:14,645 - INFO - test loss 0.025228594334413858
2023-07-01 00:19:14,645 - INFO - test acc 0.699999988079071
2023-07-01 00:19:15,854 - INFO - Distilling data from client: Client00
2023-07-01 00:19:15,854 - INFO - train loss: 0.00030988315722569617
2023-07-01 00:19:15,854 - INFO - train acc: 1.0
2023-07-01 00:19:15,878 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.77      0.82        75
           3       0.75      0.64      0.69        75
           4       0.52      0.72      0.61        50

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.74      0.71      0.72       200

2023-07-01 00:19:15,878 - INFO - test loss 0.025027645364043394
2023-07-01 00:19:15,878 - INFO - test acc 0.7099999785423279
2023-07-01 00:19:17,092 - INFO - Distilling data from client: Client00
2023-07-01 00:19:17,092 - INFO - train loss: 0.00024687493874689934
2023-07-01 00:19:17,092 - INFO - train acc: 1.0
2023-07-01 00:19:17,115 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.79      0.82        75
           3       0.75      0.60      0.67        75
           4       0.51      0.72      0.60        50

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.73      0.70      0.71       200

2023-07-01 00:19:17,115 - INFO - test loss 0.025135580434979102
2023-07-01 00:19:17,116 - INFO - test acc 0.699999988079071
2023-07-01 00:19:18,332 - INFO - Distilling data from client: Client00
2023-07-01 00:19:18,332 - INFO - train loss: 0.00019960668658963087
2023-07-01 00:19:18,333 - INFO - train acc: 1.0
2023-07-01 00:19:18,355 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.76      0.80        75
           3       0.73      0.63      0.68        75
           4       0.51      0.70      0.59        50

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.72      0.69      0.70       200

2023-07-01 00:19:18,356 - INFO - test loss 0.02506766502990993
2023-07-01 00:19:18,356 - INFO - test acc 0.6949999928474426
2023-07-01 00:19:19,571 - INFO - Distilling data from client: Client00
2023-07-01 00:19:19,571 - INFO - train loss: 0.00026061832156823624
2023-07-01 00:19:19,571 - INFO - train acc: 1.0
2023-07-01 00:19:19,595 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.77      0.81        75
           3       0.74      0.61      0.67        75
           4       0.48      0.66      0.55        50

    accuracy                           0.69       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.71      0.69      0.69       200

2023-07-01 00:19:19,595 - INFO - test loss 0.024657638897909572
2023-07-01 00:19:19,595 - INFO - test acc 0.6850000023841858
2023-07-01 00:19:20,810 - INFO - Distilling data from client: Client00
2023-07-01 00:19:20,810 - INFO - train loss: 0.00020624884357522256
2023-07-01 00:19:20,810 - INFO - train acc: 1.0
2023-07-01 00:19:20,839 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.73      0.78        75
           3       0.74      0.57      0.65        75
           4       0.49      0.74      0.59        50

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.67       200
weighted avg       0.71      0.68      0.68       200

2023-07-01 00:19:20,839 - INFO - test loss 0.025429439883865777
2023-07-01 00:19:20,839 - INFO - test acc 0.675000011920929
2023-07-01 00:19:22,052 - INFO - Distilling data from client: Client00
2023-07-01 00:19:22,052 - INFO - train loss: 0.00027846634334541324
2023-07-01 00:19:22,052 - INFO - train acc: 1.0
2023-07-01 00:19:22,075 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.76      0.79        75
           3       0.72      0.61      0.66        75
           4       0.54      0.72      0.62        50

    accuracy                           0.69       200
   macro avg       0.69      0.70      0.69       200
weighted avg       0.71      0.69      0.70       200

2023-07-01 00:19:22,075 - INFO - test loss 0.025011346885702865
2023-07-01 00:19:22,075 - INFO - test acc 0.6949999928474426
2023-07-01 00:19:22,077 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.00019741058349609375 sec
2023-07-01 00:19:22,078 - WARNING - Finished tracing + transforming fn for pjit in 0.0003170967102050781 sec
2023-07-01 00:19:22,078 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(int64[3]), ShapedArray(int64[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:19:22,080 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.001922607421875 sec
2023-07-01 00:19:22,080 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,088 - WARNING - Finished XLA compilation of jit(fn) in 0.007286787033081055 sec
2023-07-01 00:19:22,089 - WARNING - Finished tracing + transforming jit(add) in 0.00024056434631347656 sec
2023-07-01 00:19:22,089 - DEBUG - Compiling add for with global shapes and types [ShapedArray(int64[3]), ShapedArray(int64[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:19:22,090 - WARNING - Finished jaxpr to MLIR module conversion jit(add) in 0.001069784164428711 sec
2023-07-01 00:19:22,090 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,097 - WARNING - Finished XLA compilation of jit(add) in 0.0067102909088134766 sec
2023-07-01 00:19:22,098 - WARNING - Finished tracing + transforming jit(select_n) in 0.00023436546325683594 sec
2023-07-01 00:19:22,098 - DEBUG - Compiling select_n for with global shapes and types [ShapedArray(bool[3]), ShapedArray(int64[3]), ShapedArray(int64[3])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:19:22,099 - WARNING - Finished jaxpr to MLIR module conversion jit(select_n) in 0.0010287761688232422 sec
2023-07-01 00:19:22,100 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,107 - WARNING - Finished XLA compilation of jit(select_n) in 0.007207155227661133 sec
2023-07-01 00:19:22,108 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0001964569091796875 sec
2023-07-01 00:19:22,109 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.0001499652862548828 sec
2023-07-01 00:19:22,109 - DEBUG - Compiling convert_element_type for with global shapes and types [ShapedArray(int64[3])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:19:22,110 - WARNING - Finished jaxpr to MLIR module conversion jit(convert_element_type) in 0.0009546279907226562 sec
2023-07-01 00:19:22,110 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,117 - WARNING - Finished XLA compilation of jit(convert_element_type) in 0.006706953048706055 sec
2023-07-01 00:19:22,118 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002338886260986328 sec
2023-07-01 00:19:22,118 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(int32[3])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:19:22,119 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009624958038330078 sec
2023-07-01 00:19:22,120 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,125 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.00528717041015625 sec
2023-07-01 00:19:22,126 - WARNING - Finished tracing + transforming jit(gather) in 0.0002484321594238281 sec
2023-07-01 00:19:22,126 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[516,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:19:22,127 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0010805130004882812 sec
2023-07-01 00:19:22,128 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,137 - WARNING - Finished XLA compilation of jit(gather) in 0.009376287460327148 sec
2023-07-01 00:19:22,138 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00024890899658203125 sec
2023-07-01 00:19:22,139 - WARNING - Finished tracing + transforming jit(copy) in 0.00010991096496582031 sec
2023-07-01 00:19:22,139 - DEBUG - Compiling copy for with global shapes and types [ShapedArray(float32[3,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:19:22,140 - WARNING - Finished jaxpr to MLIR module conversion jit(copy) in 0.0009100437164306641 sec
2023-07-01 00:19:22,140 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,146 - WARNING - Finished XLA compilation of jit(copy) in 0.005303621292114258 sec
2023-07-01 00:19:22,147 - DEBUG - Loaded backend agg version v2.2.
2023-07-01 00:19:22,149 - DEBUG - findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0.
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBolIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmsy10.ttf', name='cmsy10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymBol.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmss10.ttf', name='cmss10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneral.ttf', name='STIXGeneral', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBol.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf', name='cmtt10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUni.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,150 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymReg.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFiveSymReg.ttf', name='STIXSizeFiveSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymReg.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBolIta.ttf', name='STIXGeneral', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymReg.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmb10.ttf', name='cmb10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymBol.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymBol.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymBol.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymReg.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmr10.ttf', name='cmr10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmmi10.ttf', name='cmmi10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralItalic.ttf', name='STIXGeneral', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmex10.ttf', name='cmex10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansDisplay.ttf', name='DejaVu Sans Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerifDisplay.ttf', name='DejaVu Serif Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBol.ttf', name='STIXGeneral', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996
2023-07-01 00:19:22,151 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='condensed', size='scalable')) = 1.25
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuMathTeXGyre.ttf', name='DejaVu Math TeX Gyre', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-ExtraLight.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=200, stretch='normal', size='scalable')) = 0.24
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 10.25
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 0.25
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='condensed', size='scalable')) = 1.535
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 0.5349999999999999
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335
2023-07-01 00:19:22,152 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,153 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05
2023-07-01 00:19:22,153 - DEBUG - findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.
2023-07-01 00:19:22,161 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:22,170 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:22,179 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:22,188 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:22,197 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:22,206 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:22,207 - WARNING - Finished tracing + transforming _unstack for pjit in 0.0007474422454833984 sec
2023-07-01 00:19:22,208 - DEBUG - Compiling _unstack for with global shapes and types [ShapedArray(float32[3,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:19:22,209 - WARNING - Finished jaxpr to MLIR module conversion jit(_unstack) in 0.0016863346099853516 sec
2023-07-01 00:19:22,210 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,223 - WARNING - Finished XLA compilation of jit(_unstack) in 0.013127803802490234 sec
2023-07-01 00:19:22,233 - WARNING - Finished tracing + transforming jit(transpose) in 0.00020456314086914062 sec
2023-07-01 00:19:22,233 - DEBUG - Compiling transpose for with global shapes and types [ShapedArray(float32[3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:19:22,234 - WARNING - Finished jaxpr to MLIR module conversion jit(transpose) in 0.0010194778442382812 sec
2023-07-01 00:19:22,234 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,242 - WARNING - Finished XLA compilation of jit(transpose) in 0.007993936538696289 sec
2023-07-01 00:19:22,243 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00019884109497070312 sec
2023-07-01 00:19:22,244 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:22,255 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:22,264 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:22,442 - DEBUG - findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=16.0.
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBolIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmsy10.ttf', name='cmsy10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymBol.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmss10.ttf', name='cmss10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneral.ttf', name='STIXGeneral', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBol.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf', name='cmtt10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUni.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,443 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymReg.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFiveSymReg.ttf', name='STIXSizeFiveSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymReg.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBolIta.ttf', name='STIXGeneral', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymReg.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmb10.ttf', name='cmb10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymBol.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymBol.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymBol.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymReg.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmr10.ttf', name='cmr10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmmi10.ttf', name='cmmi10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralItalic.ttf', name='STIXGeneral', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/cmex10.ttf', name='cmex10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansDisplay.ttf', name='DejaVu Sans Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerifDisplay.ttf', name='DejaVu Serif Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBol.ttf', name='STIXGeneral', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996
2023-07-01 00:19:22,444 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='condensed', size='scalable')) = 1.25
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuMathTeXGyre.ttf', name='DejaVu Math TeX Gyre', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-ExtraLight.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=200, stretch='normal', size='scalable')) = 0.24
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 10.25
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 0.25
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='condensed', size='scalable')) = 1.535
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 0.5349999999999999
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2023-07-01 00:19:22,445 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05
2023-07-01 00:19:22,446 - DEBUG - findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=16.0 to DejaVu Sans ('/home/jun/anaconda3/envs/anhtn_dung_fl/lib/python3.9/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.
2023-07-01 00:19:22,641 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client00//synthetic.png
2023-07-01 00:19:22,652 - INFO - c: 3.0 and total_data_in_this_class: 265
2023-07-01 00:19:22,652 - INFO - c: 4.0 and total_data_in_this_class: 270
2023-07-01 00:19:22,652 - INFO - c: 7.0 and total_data_in_this_class: 264
2023-07-01 00:19:22,652 - INFO - c: 3.0 and total_data_in_this_class: 68
2023-07-01 00:19:22,652 - INFO - c: 4.0 and total_data_in_this_class: 63
2023-07-01 00:19:22,652 - INFO - c: 7.0 and total_data_in_this_class: 69
2023-07-01 00:19:22,673 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025081634521484375 sec
2023-07-01 00:19:22,673 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:19:22,674 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0011751651763916016 sec
2023-07-01 00:19:22,675 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,685 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010317325592041016 sec
2023-07-01 00:19:22,687 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002465248107910156 sec
2023-07-01 00:19:22,687 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:19:22,688 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009627342224121094 sec
2023-07-01 00:19:22,689 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,697 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008416414260864258 sec
2023-07-01 00:19:22,700 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013637542724609375 sec
2023-07-01 00:19:22,701 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012493133544921875 sec
2023-07-01 00:19:22,702 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00030303001403808594 sec
2023-07-01 00:19:22,704 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013399124145507812 sec
2023-07-01 00:19:22,704 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001232624053955078 sec
2023-07-01 00:19:22,705 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00041747093200683594 sec
2023-07-01 00:19:22,706 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002486705780029297 sec
2023-07-01 00:19:22,706 - WARNING - Finished tracing + transforming absolute for pjit in 0.0001709461212158203 sec
2023-07-01 00:19:22,707 - WARNING - Finished tracing + transforming fn for pjit in 0.0002791881561279297 sec
2023-07-01 00:19:22,708 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0003294944763183594 sec
2023-07-01 00:19:22,708 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020051002502441406 sec
2023-07-01 00:19:22,709 - WARNING - Finished tracing + transforming fn for pjit in 0.0002269744873046875 sec
2023-07-01 00:19:22,710 - WARNING - Finished tracing + transforming fn for pjit in 0.0002658367156982422 sec
2023-07-01 00:19:22,711 - WARNING - Finished tracing + transforming fn for pjit in 0.00022602081298828125 sec
2023-07-01 00:19:22,711 - WARNING - Finished tracing + transforming fn for pjit in 0.0002598762512207031 sec
2023-07-01 00:19:22,713 - WARNING - Finished tracing + transforming fn for pjit in 0.00024056434631347656 sec
2023-07-01 00:19:22,714 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001697540283203125 sec
2023-07-01 00:19:22,715 - WARNING - Finished tracing + transforming fn for pjit in 0.00022411346435546875 sec
2023-07-01 00:19:22,716 - WARNING - Finished tracing + transforming fn for pjit in 0.0002300739288330078 sec
2023-07-01 00:19:22,720 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003790855407714844 sec
2023-07-01 00:19:22,720 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010170936584472656 sec
2023-07-01 00:19:22,721 - WARNING - Finished tracing + transforming fn for pjit in 0.000232696533203125 sec
2023-07-01 00:19:22,722 - WARNING - Finished tracing + transforming fn for pjit in 0.00021910667419433594 sec
2023-07-01 00:19:22,723 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00028204917907714844 sec
2023-07-01 00:19:22,723 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025916099548339844 sec
2023-07-01 00:19:22,724 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001780986785888672 sec
2023-07-01 00:19:22,725 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027680397033691406 sec
2023-07-01 00:19:22,726 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023412704467773438 sec
2023-07-01 00:19:22,726 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002315044403076172 sec
2023-07-01 00:19:22,727 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00034546852111816406 sec
2023-07-01 00:19:22,727 - WARNING - Finished tracing + transforming _where for pjit in 0.0009543895721435547 sec
2023-07-01 00:19:22,728 - WARNING - Finished tracing + transforming fn for pjit in 0.0002651214599609375 sec
2023-07-01 00:19:22,729 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026702880859375 sec
2023-07-01 00:19:22,730 - WARNING - Finished tracing + transforming fn for pjit in 0.0002257823944091797 sec
2023-07-01 00:19:22,730 - WARNING - Finished tracing + transforming fn for pjit in 0.00022411346435546875 sec
2023-07-01 00:19:22,731 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002224445343017578 sec
2023-07-01 00:19:22,732 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026154518127441406 sec
2023-07-01 00:19:22,733 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0009481906890869141 sec
2023-07-01 00:19:22,734 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002639293670654297 sec
2023-07-01 00:19:22,735 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023365020751953125 sec
2023-07-01 00:19:22,735 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002307891845703125 sec
2023-07-01 00:19:22,736 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002689361572265625 sec
2023-07-01 00:19:22,736 - WARNING - Finished tracing + transforming _where for pjit in 0.0008594989776611328 sec
2023-07-01 00:19:22,737 - WARNING - Finished tracing + transforming fn for pjit in 0.00025844573974609375 sec
2023-07-01 00:19:22,738 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002536773681640625 sec
2023-07-01 00:19:22,739 - WARNING - Finished tracing + transforming fn for pjit in 0.0002200603485107422 sec
2023-07-01 00:19:22,743 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002651214599609375 sec
2023-07-01 00:19:22,743 - WARNING - Finished tracing + transforming fn for pjit in 0.00032782554626464844 sec
2023-07-01 00:19:22,744 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026297569274902344 sec
2023-07-01 00:19:22,745 - WARNING - Finished tracing + transforming fn for pjit in 0.0002315044403076172 sec
2023-07-01 00:19:22,749 - WARNING - Finished tracing + transforming fn for pjit in 0.00022149085998535156 sec
2023-07-01 00:19:22,750 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016617774963378906 sec
2023-07-01 00:19:22,751 - WARNING - Finished tracing + transforming fn for pjit in 0.0002930164337158203 sec
2023-07-01 00:19:22,752 - WARNING - Finished tracing + transforming fn for pjit in 0.0002315044403076172 sec
2023-07-01 00:19:22,769 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06949496269226074 sec
2023-07-01 00:19:22,773 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013017654418945312 sec
2023-07-01 00:19:22,773 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011134147644042969 sec
2023-07-01 00:19:22,773 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00026535987854003906 sec
2023-07-01 00:19:22,776 - WARNING - Finished tracing + transforming fn for pjit in 0.0002262592315673828 sec
2023-07-01 00:19:22,776 - WARNING - Finished tracing + transforming fn for pjit in 0.0002593994140625 sec
2023-07-01 00:19:22,777 - WARNING - Finished tracing + transforming fn for pjit in 0.0002143383026123047 sec
2023-07-01 00:19:22,783 - WARNING - Finished tracing + transforming fn for pjit in 0.00022292137145996094 sec
2023-07-01 00:19:22,784 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021409988403320312 sec
2023-07-01 00:19:22,785 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002741813659667969 sec
2023-07-01 00:19:22,786 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017213821411132812 sec
2023-07-01 00:19:22,786 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003342628479003906 sec
2023-07-01 00:19:22,787 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022101402282714844 sec
2023-07-01 00:19:22,788 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002193450927734375 sec
2023-07-01 00:19:22,789 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.000263214111328125 sec
2023-07-01 00:19:22,789 - WARNING - Finished tracing + transforming _where for pjit in 0.0008406639099121094 sec
2023-07-01 00:19:22,790 - WARNING - Finished tracing + transforming fn for pjit in 0.00026345252990722656 sec
2023-07-01 00:19:22,790 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002605915069580078 sec
2023-07-01 00:19:22,791 - WARNING - Finished tracing + transforming fn for pjit in 0.00021409988403320312 sec
2023-07-01 00:19:22,792 - WARNING - Finished tracing + transforming fn for pjit in 0.0002701282501220703 sec
2023-07-01 00:19:22,803 - WARNING - Finished tracing + transforming fn for pjit in 0.0002143383026123047 sec
2023-07-01 00:19:22,822 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05217337608337402 sec
2023-07-01 00:19:22,824 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011873245239257812 sec
2023-07-01 00:19:22,825 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013113021850585938 sec
2023-07-01 00:19:22,825 - WARNING - Finished tracing + transforming _where for pjit in 0.0006086826324462891 sec
2023-07-01 00:19:22,825 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002856254577636719 sec
2023-07-01 00:19:22,826 - WARNING - Finished tracing + transforming trace for pjit in 0.0024213790893554688 sec
2023-07-01 00:19:22,828 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010013580322265625 sec
2023-07-01 00:19:22,829 - WARNING - Finished tracing + transforming tril for pjit in 0.0006411075592041016 sec
2023-07-01 00:19:22,829 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0016207695007324219 sec
2023-07-01 00:19:22,830 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010418891906738281 sec
2023-07-01 00:19:22,830 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010156631469726562 sec
2023-07-01 00:19:22,832 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0013468265533447266 sec
2023-07-01 00:19:22,836 - WARNING - Finished tracing + transforming _solve for pjit in 0.008736848831176758 sec
2023-07-01 00:19:22,837 - WARNING - Finished tracing + transforming dot for pjit in 0.00030922889709472656 sec
2023-07-01 00:19:22,839 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.14025306701660156 sec
2023-07-01 00:19:22,841 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:19:22,872 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.031142473220825195 sec
2023-07-01 00:19:22,872 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:22,985 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.11241865158081055 sec
2023-07-01 00:19:23,013 - INFO - initial test loss: 0.029367785421095277
2023-07-01 00:19:23,013 - INFO - initial test acc: 0.5550000071525574
2023-07-01 00:19:23,020 - WARNING - Finished tracing + transforming dot for pjit in 0.0005848407745361328 sec
2023-07-01 00:19:23,022 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0004146099090576172 sec
2023-07-01 00:19:23,023 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00029206275939941406 sec
2023-07-01 00:19:23,023 - WARNING - Finished tracing + transforming _mean for pjit in 0.0008893013000488281 sec
2023-07-01 00:19:23,024 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001900196075439453 sec
2023-07-01 00:19:23,025 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00023555755615234375 sec
2023-07-01 00:19:23,025 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002429485321044922 sec
2023-07-01 00:19:23,026 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003561973571777344 sec
2023-07-01 00:19:23,026 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010285377502441406 sec
2023-07-01 00:19:23,027 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.011315345764160156 sec
2023-07-01 00:19:23,036 - WARNING - Finished tracing + transforming fn for pjit in 0.00025653839111328125 sec
2023-07-01 00:19:23,036 - WARNING - Finished tracing + transforming fn for pjit in 0.0003108978271484375 sec
2023-07-01 00:19:23,037 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021982192993164062 sec
2023-07-01 00:19:23,038 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002579689025878906 sec
2023-07-01 00:19:23,038 - WARNING - Finished tracing + transforming _where for pjit in 0.0008308887481689453 sec
2023-07-01 00:19:23,046 - WARNING - Finished tracing + transforming fn for pjit in 0.0002601146697998047 sec
2023-07-01 00:19:23,047 - WARNING - Finished tracing + transforming fn for pjit in 0.000255584716796875 sec
2023-07-01 00:19:23,047 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002124309539794922 sec
2023-07-01 00:19:23,048 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00024175643920898438 sec
2023-07-01 00:19:23,048 - WARNING - Finished tracing + transforming _where for pjit in 0.0007953643798828125 sec
2023-07-01 00:19:23,082 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00020813941955566406 sec
2023-07-01 00:19:23,135 - WARNING - Finished tracing + transforming fn for pjit in 0.0002608299255371094 sec
2023-07-01 00:19:23,136 - WARNING - Finished tracing + transforming fn for pjit in 0.0002319812774658203 sec
2023-07-01 00:19:23,137 - WARNING - Finished tracing + transforming square for pjit in 0.0001709461212158203 sec
2023-07-01 00:19:23,139 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022220611572265625 sec
2023-07-01 00:19:23,140 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000240325927734375 sec
2023-07-01 00:19:23,141 - WARNING - Finished tracing + transforming fn for pjit in 0.0002639293670654297 sec
2023-07-01 00:19:23,142 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002200603485107422 sec
2023-07-01 00:19:23,142 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022840499877929688 sec
2023-07-01 00:19:23,143 - WARNING - Finished tracing + transforming fn for pjit in 0.0002627372741699219 sec
2023-07-01 00:19:23,144 - WARNING - Finished tracing + transforming fn for pjit in 0.00023412704467773438 sec
2023-07-01 00:19:23,144 - WARNING - Finished tracing + transforming square for pjit in 0.00016427040100097656 sec
2023-07-01 00:19:23,146 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021982192993164062 sec
2023-07-01 00:19:23,148 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017404556274414062 sec
2023-07-01 00:19:23,148 - WARNING - Finished tracing + transforming fn for pjit in 0.0002613067626953125 sec
2023-07-01 00:19:23,149 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021791458129882812 sec
2023-07-01 00:19:23,149 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022411346435546875 sec
2023-07-01 00:19:23,150 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1351916790008545 sec
2023-07-01 00:19:23,154 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[528,10]), ShapedArray(float32[528,10]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:19:23,214 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.05965137481689453 sec
2023-07-01 00:19:23,214 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:23,492 - WARNING - Finished XLA compilation of jit(update_fn) in 0.2779719829559326 sec
2023-07-01 00:19:24,743 - INFO - Distilling data from client: Client01
2023-07-01 00:19:24,743 - INFO - train loss: 0.002613988945029598
2023-07-01 00:19:24,743 - INFO - train acc: 0.9943181872367859
2023-07-01 00:19:24,809 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.59      0.61        68
           4       0.53      0.65      0.59        63
           7       0.67      0.58      0.62        69

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.60      0.61       200

2023-07-01 00:19:24,809 - INFO - test loss 0.02580014569613346
2023-07-01 00:19:24,809 - INFO - test acc 0.6049999594688416
2023-07-01 00:19:26,054 - INFO - Distilling data from client: Client01
2023-07-01 00:19:26,054 - INFO - train loss: 0.0014928603716758914
2023-07-01 00:19:26,054 - INFO - train acc: 1.0
2023-07-01 00:19:26,078 - INFO - report:               precision    recall  f1-score   support

           3       0.64      0.60      0.62        68
           4       0.53      0.63      0.58        63
           7       0.61      0.54      0.57        69

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-07-01 00:19:26,079 - INFO - test loss 0.025944434431215885
2023-07-01 00:19:26,079 - INFO - test acc 0.5899999737739563
2023-07-01 00:19:27,336 - INFO - Distilling data from client: Client01
2023-07-01 00:19:27,336 - INFO - train loss: 0.0010346933346162567
2023-07-01 00:19:27,336 - INFO - train acc: 1.0
2023-07-01 00:19:27,359 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.57      0.60        68
           4       0.50      0.62      0.55        63
           7       0.61      0.52      0.56        69

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-07-01 00:19:27,359 - INFO - test loss 0.026808643800099872
2023-07-01 00:19:27,359 - INFO - test acc 0.5699999928474426
2023-07-01 00:19:28,612 - INFO - Distilling data from client: Client01
2023-07-01 00:19:28,612 - INFO - train loss: 0.0007527850897174967
2023-07-01 00:19:28,612 - INFO - train acc: 1.0
2023-07-01 00:19:28,635 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.56      0.59        68
           4       0.54      0.68      0.60        63
           7       0.67      0.58      0.62        69

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.60      0.61       200

2023-07-01 00:19:28,635 - INFO - test loss 0.026275804256676364
2023-07-01 00:19:28,635 - INFO - test acc 0.6049999594688416
2023-07-01 00:19:29,895 - INFO - Distilling data from client: Client01
2023-07-01 00:19:29,896 - INFO - train loss: 0.0007788853219412761
2023-07-01 00:19:29,896 - INFO - train acc: 1.0
2023-07-01 00:19:29,919 - INFO - report:               precision    recall  f1-score   support

           3       0.66      0.60      0.63        68
           4       0.54      0.63      0.58        63
           7       0.62      0.58      0.60        69

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.60      0.61       200

2023-07-01 00:19:29,919 - INFO - test loss 0.026267350122898974
2023-07-01 00:19:29,919 - INFO - test acc 0.6049999594688416
2023-07-01 00:19:31,173 - INFO - Distilling data from client: Client01
2023-07-01 00:19:31,174 - INFO - train loss: 0.0007619907921253484
2023-07-01 00:19:31,174 - INFO - train acc: 1.0
2023-07-01 00:19:31,197 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.62      0.62        68
           4       0.53      0.63      0.58        63
           7       0.65      0.54      0.59        69

    accuracy                           0.59       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.59      0.60       200

2023-07-01 00:19:31,198 - INFO - test loss 0.0267806686844408
2023-07-01 00:19:31,198 - INFO - test acc 0.5949999690055847
2023-07-01 00:19:32,440 - INFO - Distilling data from client: Client01
2023-07-01 00:19:32,440 - INFO - train loss: 0.0006045999448762571
2023-07-01 00:19:32,440 - INFO - train acc: 1.0
2023-07-01 00:19:32,465 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.62      0.62        68
           4       0.54      0.63      0.58        63
           7       0.66      0.55      0.60        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:19:32,465 - INFO - test loss 0.026773551284631176
2023-07-01 00:19:32,465 - INFO - test acc 0.5999999642372131
2023-07-01 00:19:33,711 - INFO - Distilling data from client: Client01
2023-07-01 00:19:33,711 - INFO - train loss: 0.0004563867992612447
2023-07-01 00:19:33,711 - INFO - train acc: 1.0
2023-07-01 00:19:33,736 - INFO - report:               precision    recall  f1-score   support

           3       0.58      0.53      0.55        68
           4       0.50      0.62      0.55        63
           7       0.62      0.54      0.57        69

    accuracy                           0.56       200
   macro avg       0.57      0.56      0.56       200
weighted avg       0.57      0.56      0.56       200

2023-07-01 00:19:33,736 - INFO - test loss 0.02724159638134287
2023-07-01 00:19:33,736 - INFO - test acc 0.5600000023841858
2023-07-01 00:19:34,971 - INFO - Distilling data from client: Client01
2023-07-01 00:19:34,971 - INFO - train loss: 0.0006246013463468542
2023-07-01 00:19:34,971 - INFO - train acc: 1.0
2023-07-01 00:19:34,995 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.57      0.60        68
           4       0.54      0.63      0.58        63
           7       0.62      0.58      0.60        69

    accuracy                           0.59       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.59      0.60       200

2023-07-01 00:19:34,995 - INFO - test loss 0.027530172489622657
2023-07-01 00:19:34,995 - INFO - test acc 0.5949999690055847
2023-07-01 00:19:36,235 - INFO - Distilling data from client: Client01
2023-07-01 00:19:36,235 - INFO - train loss: 0.00040902201803189985
2023-07-01 00:19:36,235 - INFO - train acc: 1.0
2023-07-01 00:19:36,259 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.59      0.60        68
           4       0.57      0.67      0.61        63
           7       0.64      0.57      0.60        69

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:19:36,259 - INFO - test loss 0.02662929887093941
2023-07-01 00:19:36,259 - INFO - test acc 0.6049999594688416
2023-07-01 00:19:37,512 - INFO - Distilling data from client: Client01
2023-07-01 00:19:37,512 - INFO - train loss: 0.0005335323915643215
2023-07-01 00:19:37,512 - INFO - train acc: 1.0
2023-07-01 00:19:37,579 - INFO - report:               precision    recall  f1-score   support

           3       0.64      0.62      0.63        68
           4       0.53      0.63      0.58        63
           7       0.68      0.58      0.63        69

    accuracy                           0.61       200
   macro avg       0.62      0.61      0.61       200
weighted avg       0.62      0.61      0.61       200

2023-07-01 00:19:37,579 - INFO - test loss 0.026455217676058462
2023-07-01 00:19:37,579 - INFO - test acc 0.6100000143051147
2023-07-01 00:19:38,823 - INFO - Distilling data from client: Client01
2023-07-01 00:19:38,823 - INFO - train loss: 0.0003871980591012092
2023-07-01 00:19:38,823 - INFO - train acc: 1.0
2023-07-01 00:19:38,846 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.56      0.59        68
           4       0.49      0.60      0.54        63
           7       0.62      0.55      0.58        69

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-07-01 00:19:38,847 - INFO - test loss 0.026829528813460443
2023-07-01 00:19:38,847 - INFO - test acc 0.5699999928474426
2023-07-01 00:19:40,084 - INFO - Distilling data from client: Client01
2023-07-01 00:19:40,084 - INFO - train loss: 0.0004468278648904562
2023-07-01 00:19:40,084 - INFO - train acc: 1.0
2023-07-01 00:19:40,107 - INFO - report:               precision    recall  f1-score   support

           3       0.65      0.62      0.63        68
           4       0.48      0.57      0.52        63
           7       0.63      0.55      0.59        69

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-07-01 00:19:40,107 - INFO - test loss 0.026905706030915008
2023-07-01 00:19:40,108 - INFO - test acc 0.5799999833106995
2023-07-01 00:19:41,334 - INFO - Distilling data from client: Client01
2023-07-01 00:19:41,334 - INFO - train loss: 0.0004249786991281917
2023-07-01 00:19:41,334 - INFO - train acc: 1.0
2023-07-01 00:19:41,358 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.59      0.60        68
           4       0.54      0.65      0.59        63
           7       0.66      0.57      0.61        69

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:19:41,358 - INFO - test loss 0.026723139597879978
2023-07-01 00:19:41,358 - INFO - test acc 0.5999999642372131
2023-07-01 00:19:42,603 - INFO - Distilling data from client: Client01
2023-07-01 00:19:42,603 - INFO - train loss: 0.0003313553249029946
2023-07-01 00:19:42,603 - INFO - train acc: 1.0
2023-07-01 00:19:42,627 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.59      0.61        68
           4       0.55      0.65      0.60        63
           7       0.61      0.55      0.58        69

    accuracy                           0.59       200
   macro avg       0.60      0.60      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-07-01 00:19:42,627 - INFO - test loss 0.027025803335894646
2023-07-01 00:19:42,627 - INFO - test acc 0.5949999690055847
2023-07-01 00:19:43,868 - INFO - Distilling data from client: Client01
2023-07-01 00:19:43,868 - INFO - train loss: 0.00030271662671100175
2023-07-01 00:19:43,868 - INFO - train acc: 1.0
2023-07-01 00:19:43,893 - INFO - report:               precision    recall  f1-score   support

           3       0.64      0.60      0.62        68
           4       0.53      0.65      0.59        63
           7       0.68      0.58      0.63        69

    accuracy                           0.61       200
   macro avg       0.62      0.61      0.61       200
weighted avg       0.62      0.61      0.61       200

2023-07-01 00:19:43,893 - INFO - test loss 0.02734083005468459
2023-07-01 00:19:43,893 - INFO - test acc 0.6100000143051147
2023-07-01 00:19:45,134 - INFO - Distilling data from client: Client01
2023-07-01 00:19:45,134 - INFO - train loss: 0.00036632680519271577
2023-07-01 00:19:45,134 - INFO - train acc: 1.0
2023-07-01 00:19:45,158 - INFO - report:               precision    recall  f1-score   support

           3       0.61      0.60      0.61        68
           4       0.55      0.68      0.61        63
           7       0.67      0.54      0.60        69

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:19:45,158 - INFO - test loss 0.026959074128100318
2023-07-01 00:19:45,158 - INFO - test acc 0.6049999594688416
2023-07-01 00:19:46,405 - INFO - Distilling data from client: Client01
2023-07-01 00:19:46,405 - INFO - train loss: 0.00036666735390028125
2023-07-01 00:19:46,405 - INFO - train acc: 1.0
2023-07-01 00:19:46,429 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.59      0.61        68
           4       0.50      0.62      0.55        63
           7       0.63      0.54      0.58        69

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-07-01 00:19:46,429 - INFO - test loss 0.0268030515848413
2023-07-01 00:19:46,429 - INFO - test acc 0.5799999833106995
2023-07-01 00:19:47,675 - INFO - Distilling data from client: Client01
2023-07-01 00:19:47,675 - INFO - train loss: 0.00034931229277853184
2023-07-01 00:19:47,675 - INFO - train acc: 1.0
2023-07-01 00:19:47,698 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.63      0.63        68
           4       0.53      0.62      0.57        63
           7       0.64      0.55      0.59        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:19:47,698 - INFO - test loss 0.027307640668767275
2023-07-01 00:19:47,698 - INFO - test acc 0.5999999642372131
2023-07-01 00:19:48,937 - INFO - Distilling data from client: Client01
2023-07-01 00:19:48,938 - INFO - train loss: 0.00035184797749520627
2023-07-01 00:19:48,938 - INFO - train acc: 1.0
2023-07-01 00:19:48,962 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.59      0.61        68
           4       0.54      0.65      0.59        63
           7       0.61      0.54      0.57        69

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-07-01 00:19:48,962 - INFO - test loss 0.026962140161763008
2023-07-01 00:19:48,963 - INFO - test acc 0.5899999737739563
2023-07-01 00:19:50,206 - INFO - Distilling data from client: Client01
2023-07-01 00:19:50,206 - INFO - train loss: 0.0002779726906437135
2023-07-01 00:19:50,206 - INFO - train acc: 1.0
2023-07-01 00:19:50,230 - INFO - report:               precision    recall  f1-score   support

           3       0.63      0.57      0.60        68
           4       0.50      0.60      0.55        63
           7       0.61      0.55      0.58        69

    accuracy                           0.57       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.57      0.58       200

2023-07-01 00:19:50,230 - INFO - test loss 0.02680401636273895
2023-07-01 00:19:50,230 - INFO - test acc 0.574999988079071
2023-07-01 00:19:51,477 - INFO - Distilling data from client: Client01
2023-07-01 00:19:51,477 - INFO - train loss: 0.0002228154673384419
2023-07-01 00:19:51,477 - INFO - train acc: 1.0
2023-07-01 00:19:51,501 - INFO - report:               precision    recall  f1-score   support

           3       0.62      0.59      0.61        68
           4       0.53      0.63      0.58        63
           7       0.62      0.55      0.58        69

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-07-01 00:19:51,501 - INFO - test loss 0.026762276136645554
2023-07-01 00:19:51,501 - INFO - test acc 0.5899999737739563
2023-07-01 00:19:52,747 - INFO - Distilling data from client: Client01
2023-07-01 00:19:52,748 - INFO - train loss: 0.00026002880543650556
2023-07-01 00:19:52,748 - INFO - train acc: 1.0
2023-07-01 00:19:52,771 - INFO - report:               precision    recall  f1-score   support

           3       0.61      0.54      0.57        68
           4       0.51      0.63      0.57        63
           7       0.62      0.55      0.58        69

    accuracy                           0.57       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.57      0.58       200

2023-07-01 00:19:52,771 - INFO - test loss 0.02702699047580622
2023-07-01 00:19:52,771 - INFO - test acc 0.574999988079071
2023-07-01 00:19:54,008 - INFO - Distilling data from client: Client01
2023-07-01 00:19:54,008 - INFO - train loss: 0.00018310377566220937
2023-07-01 00:19:54,008 - INFO - train acc: 1.0
2023-07-01 00:19:54,032 - INFO - report:               precision    recall  f1-score   support

           3       0.61      0.57      0.59        68
           4       0.53      0.62      0.57        63
           7       0.63      0.58      0.61        69

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.59      0.59      0.59       200

2023-07-01 00:19:54,032 - INFO - test loss 0.026932106987280022
2023-07-01 00:19:54,032 - INFO - test acc 0.5899999737739563
2023-07-01 00:19:55,283 - INFO - Distilling data from client: Client01
2023-07-01 00:19:55,283 - INFO - train loss: 0.000238445023534614
2023-07-01 00:19:55,283 - INFO - train acc: 1.0
2023-07-01 00:19:55,307 - INFO - report:               precision    recall  f1-score   support

           3       0.58      0.56      0.57        68
           4       0.51      0.62      0.56        63
           7       0.66      0.57      0.61        69

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-07-01 00:19:55,307 - INFO - test loss 0.026825942998295566
2023-07-01 00:19:55,307 - INFO - test acc 0.5799999833106995
2023-07-01 00:19:55,309 - WARNING - Finished tracing + transforming jit(gather) in 0.0002346038818359375 sec
2023-07-01 00:19:55,309 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[528,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:19:55,311 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0011546611785888672 sec
2023-07-01 00:19:55,311 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:55,321 - WARNING - Finished XLA compilation of jit(gather) in 0.009669303894042969 sec
2023-07-01 00:19:55,331 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:55,339 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:55,348 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:55,357 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:55,365 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:55,374 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:55,383 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:55,392 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:55,401 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:55,776 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client01//synthetic.png
2023-07-01 00:19:55,788 - INFO - c: 4.0 and total_data_in_this_class: 19
2023-07-01 00:19:55,788 - INFO - c: 5.0 and total_data_in_this_class: 248
2023-07-01 00:19:55,788 - INFO - c: 6.0 and total_data_in_this_class: 29
2023-07-01 00:19:55,789 - INFO - c: 7.0 and total_data_in_this_class: 237
2023-07-01 00:19:55,789 - INFO - c: 9.0 and total_data_in_this_class: 266
2023-07-01 00:19:55,789 - INFO - c: 4.0 and total_data_in_this_class: 6
2023-07-01 00:19:55,789 - INFO - c: 5.0 and total_data_in_this_class: 60
2023-07-01 00:19:55,789 - INFO - c: 6.0 and total_data_in_this_class: 6
2023-07-01 00:19:55,789 - INFO - c: 7.0 and total_data_in_this_class: 61
2023-07-01 00:19:55,789 - INFO - c: 9.0 and total_data_in_this_class: 67
2023-07-01 00:19:55,807 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002446174621582031 sec
2023-07-01 00:19:55,807 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:19:55,808 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001138925552368164 sec
2023-07-01 00:19:55,809 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:55,819 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010345935821533203 sec
2023-07-01 00:19:55,821 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00022339820861816406 sec
2023-07-01 00:19:55,821 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:19:55,822 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009136199951171875 sec
2023-07-01 00:19:55,822 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:55,830 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.00812673568725586 sec
2023-07-01 00:19:55,833 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013589859008789062 sec
2023-07-01 00:19:55,834 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001251697540283203 sec
2023-07-01 00:19:55,835 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003006458282470703 sec
2023-07-01 00:19:55,836 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002002716064453125 sec
2023-07-01 00:19:55,837 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001270771026611328 sec
2023-07-01 00:19:55,837 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00028061866760253906 sec
2023-07-01 00:19:55,838 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000244140625 sec
2023-07-01 00:19:55,839 - WARNING - Finished tracing + transforming absolute for pjit in 0.00016999244689941406 sec
2023-07-01 00:19:55,839 - WARNING - Finished tracing + transforming fn for pjit in 0.00027823448181152344 sec
2023-07-01 00:19:55,840 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00031948089599609375 sec
2023-07-01 00:19:55,841 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019788742065429688 sec
2023-07-01 00:19:55,842 - WARNING - Finished tracing + transforming fn for pjit in 0.00023055076599121094 sec
2023-07-01 00:19:55,842 - WARNING - Finished tracing + transforming fn for pjit in 0.00026726722717285156 sec
2023-07-01 00:19:55,843 - WARNING - Finished tracing + transforming fn for pjit in 0.00023055076599121094 sec
2023-07-01 00:19:55,844 - WARNING - Finished tracing + transforming fn for pjit in 0.00026607513427734375 sec
2023-07-01 00:19:55,845 - WARNING - Finished tracing + transforming fn for pjit in 0.0002357959747314453 sec
2023-07-01 00:19:55,846 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001678466796875 sec
2023-07-01 00:19:55,847 - WARNING - Finished tracing + transforming fn for pjit in 0.00022745132446289062 sec
2023-07-01 00:19:55,848 - WARNING - Finished tracing + transforming fn for pjit in 0.00023174285888671875 sec
2023-07-01 00:19:55,851 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.000362396240234375 sec
2023-07-01 00:19:55,852 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009577274322509766 sec
2023-07-01 00:19:55,853 - WARNING - Finished tracing + transforming fn for pjit in 0.0002391338348388672 sec
2023-07-01 00:19:55,853 - WARNING - Finished tracing + transforming fn for pjit in 0.00021982192993164062 sec
2023-07-01 00:19:55,854 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002834796905517578 sec
2023-07-01 00:19:55,855 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002574920654296875 sec
2023-07-01 00:19:55,855 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017976760864257812 sec
2023-07-01 00:19:55,856 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002627372741699219 sec
2023-07-01 00:19:55,857 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002262592315673828 sec
2023-07-01 00:19:55,858 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002262592315673828 sec
2023-07-01 00:19:55,859 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00034165382385253906 sec
2023-07-01 00:19:55,859 - WARNING - Finished tracing + transforming _where for pjit in 0.0009286403656005859 sec
2023-07-01 00:19:55,860 - WARNING - Finished tracing + transforming fn for pjit in 0.0002613067626953125 sec
2023-07-01 00:19:55,860 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002567768096923828 sec
2023-07-01 00:19:55,861 - WARNING - Finished tracing + transforming fn for pjit in 0.00022220611572265625 sec
2023-07-01 00:19:55,862 - WARNING - Finished tracing + transforming fn for pjit in 0.00021910667419433594 sec
2023-07-01 00:19:55,862 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021600723266601562 sec
2023-07-01 00:19:55,863 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025534629821777344 sec
2023-07-01 00:19:55,864 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024008750915527344 sec
2023-07-01 00:19:55,864 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002560615539550781 sec
2023-07-01 00:19:55,865 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022149085998535156 sec
2023-07-01 00:19:55,866 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021910667419433594 sec
2023-07-01 00:19:55,867 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025343894958496094 sec
2023-07-01 00:19:55,867 - WARNING - Finished tracing + transforming _where for pjit in 0.0008270740509033203 sec
2023-07-01 00:19:55,868 - WARNING - Finished tracing + transforming fn for pjit in 0.00025773048400878906 sec
2023-07-01 00:19:55,868 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025534629821777344 sec
2023-07-01 00:19:55,869 - WARNING - Finished tracing + transforming fn for pjit in 0.00022101402282714844 sec
2023-07-01 00:19:55,873 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027251243591308594 sec
2023-07-01 00:19:55,874 - WARNING - Finished tracing + transforming fn for pjit in 0.0003368854522705078 sec
2023-07-01 00:19:55,875 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026726722717285156 sec
2023-07-01 00:19:55,875 - WARNING - Finished tracing + transforming fn for pjit in 0.00022554397583007812 sec
2023-07-01 00:19:55,879 - WARNING - Finished tracing + transforming fn for pjit in 0.0002219676971435547 sec
2023-07-01 00:19:55,881 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00017595291137695312 sec
2023-07-01 00:19:55,882 - WARNING - Finished tracing + transforming fn for pjit in 0.0009241104125976562 sec
2023-07-01 00:19:55,883 - WARNING - Finished tracing + transforming fn for pjit in 0.00022983551025390625 sec
2023-07-01 00:19:55,900 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06759452819824219 sec
2023-07-01 00:19:55,904 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001277923583984375 sec
2023-07-01 00:19:55,904 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012063980102539062 sec
2023-07-01 00:19:55,905 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00026917457580566406 sec
2023-07-01 00:19:55,907 - WARNING - Finished tracing + transforming fn for pjit in 0.00022077560424804688 sec
2023-07-01 00:19:55,907 - WARNING - Finished tracing + transforming fn for pjit in 0.0002532005310058594 sec
2023-07-01 00:19:55,908 - WARNING - Finished tracing + transforming fn for pjit in 0.0002193450927734375 sec
2023-07-01 00:19:55,914 - WARNING - Finished tracing + transforming fn for pjit in 0.00022792816162109375 sec
2023-07-01 00:19:55,915 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022125244140625 sec
2023-07-01 00:19:55,916 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002536773681640625 sec
2023-07-01 00:19:55,916 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017261505126953125 sec
2023-07-01 00:19:55,917 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032138824462890625 sec
2023-07-01 00:19:55,918 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002243518829345703 sec
2023-07-01 00:19:55,919 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022220611572265625 sec
2023-07-01 00:19:55,919 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002586841583251953 sec
2023-07-01 00:19:55,920 - WARNING - Finished tracing + transforming _where for pjit in 0.00083160400390625 sec
2023-07-01 00:19:55,920 - WARNING - Finished tracing + transforming fn for pjit in 0.0002582073211669922 sec
2023-07-01 00:19:55,921 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025391578674316406 sec
2023-07-01 00:19:55,922 - WARNING - Finished tracing + transforming fn for pjit in 0.0002181529998779297 sec
2023-07-01 00:19:55,922 - WARNING - Finished tracing + transforming fn for pjit in 0.0002751350402832031 sec
2023-07-01 00:19:55,934 - WARNING - Finished tracing + transforming fn for pjit in 0.00021791458129882812 sec
2023-07-01 00:19:55,953 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05207657814025879 sec
2023-07-01 00:19:55,955 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012731552124023438 sec
2023-07-01 00:19:55,956 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0001316070556640625 sec
2023-07-01 00:19:55,956 - WARNING - Finished tracing + transforming _where for pjit in 0.0005993843078613281 sec
2023-07-01 00:19:55,957 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002837181091308594 sec
2023-07-01 00:19:55,957 - WARNING - Finished tracing + transforming trace for pjit in 0.002437591552734375 sec
2023-07-01 00:19:55,959 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010371208190917969 sec
2023-07-01 00:19:55,960 - WARNING - Finished tracing + transforming tril for pjit in 0.0006532669067382812 sec
2023-07-01 00:19:55,960 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0016620159149169922 sec
2023-07-01 00:19:55,961 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010633468627929688 sec
2023-07-01 00:19:55,961 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010514259338378906 sec
2023-07-01 00:19:55,963 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.001386880874633789 sec
2023-07-01 00:19:55,967 - WARNING - Finished tracing + transforming _solve for pjit in 0.008971452713012695 sec
2023-07-01 00:19:55,968 - WARNING - Finished tracing + transforming dot for pjit in 0.00030303001403808594 sec
2023-07-01 00:19:55,970 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.13866662979125977 sec
2023-07-01 00:19:55,972 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:19:56,004 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03209948539733887 sec
2023-07-01 00:19:56,005 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:56,110 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.10481858253479004 sec
2023-07-01 00:19:56,114 - INFO - initial test loss: 0.03583813926064023
2023-07-01 00:19:56,114 - INFO - initial test acc: 0.4949999749660492
2023-07-01 00:19:56,119 - WARNING - Finished tracing + transforming dot for pjit in 0.0003514289855957031 sec
2023-07-01 00:19:56,120 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00028443336486816406 sec
2023-07-01 00:19:56,121 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003192424774169922 sec
2023-07-01 00:19:56,121 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009484291076660156 sec
2023-07-01 00:19:56,122 - WARNING - Finished tracing + transforming _argmax for pjit in 0.000194549560546875 sec
2023-07-01 00:19:56,123 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00024318695068359375 sec
2023-07-01 00:19:56,123 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024127960205078125 sec
2023-07-01 00:19:56,124 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00036907196044921875 sec
2023-07-01 00:19:56,125 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010709762573242188 sec
2023-07-01 00:19:56,125 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.00972127914428711 sec
2023-07-01 00:19:56,134 - WARNING - Finished tracing + transforming fn for pjit in 0.0002613067626953125 sec
2023-07-01 00:19:56,135 - WARNING - Finished tracing + transforming fn for pjit in 0.00027060508728027344 sec
2023-07-01 00:19:56,135 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002734661102294922 sec
2023-07-01 00:19:56,136 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002617835998535156 sec
2023-07-01 00:19:56,136 - WARNING - Finished tracing + transforming _where for pjit in 0.0008533000946044922 sec
2023-07-01 00:19:56,145 - WARNING - Finished tracing + transforming fn for pjit in 0.0002598762512207031 sec
2023-07-01 00:19:56,146 - WARNING - Finished tracing + transforming fn for pjit in 0.00026035308837890625 sec
2023-07-01 00:19:56,146 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002181529998779297 sec
2023-07-01 00:19:56,147 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025010108947753906 sec
2023-07-01 00:19:56,147 - WARNING - Finished tracing + transforming _where for pjit in 0.0008265972137451172 sec
2023-07-01 00:19:56,182 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021767616271972656 sec
2023-07-01 00:19:56,238 - WARNING - Finished tracing + transforming fn for pjit in 0.00028586387634277344 sec
2023-07-01 00:19:56,239 - WARNING - Finished tracing + transforming fn for pjit in 0.0002281665802001953 sec
2023-07-01 00:19:56,240 - WARNING - Finished tracing + transforming square for pjit in 0.0001857280731201172 sec
2023-07-01 00:19:56,242 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002281665802001953 sec
2023-07-01 00:19:56,243 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024890899658203125 sec
2023-07-01 00:19:56,244 - WARNING - Finished tracing + transforming fn for pjit in 0.0002720355987548828 sec
2023-07-01 00:19:56,245 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00023126602172851562 sec
2023-07-01 00:19:56,245 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022482872009277344 sec
2023-07-01 00:19:56,246 - WARNING - Finished tracing + transforming fn for pjit in 0.00027251243591308594 sec
2023-07-01 00:19:56,247 - WARNING - Finished tracing + transforming fn for pjit in 0.00022220611572265625 sec
2023-07-01 00:19:56,247 - WARNING - Finished tracing + transforming square for pjit in 0.00017452239990234375 sec
2023-07-01 00:19:56,249 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002338886260986328 sec
2023-07-01 00:19:56,251 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00018525123596191406 sec
2023-07-01 00:19:56,251 - WARNING - Finished tracing + transforming fn for pjit in 0.0002701282501220703 sec
2023-07-01 00:19:56,252 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002238750457763672 sec
2023-07-01 00:19:56,252 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022554397583007812 sec
2023-07-01 00:19:56,253 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1379992961883545 sec
2023-07-01 00:19:56,257 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,10]), ShapedArray(float32[76,10]), ShapedArray(float32[76,10]), ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,10]), ShapedArray(float32[76,3,32,32]), ShapedArray(float32[76,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:19:56,320 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06304407119750977 sec
2023-07-01 00:19:56,320 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:56,573 - WARNING - Finished XLA compilation of jit(update_fn) in 0.2522256374359131 sec
2023-07-01 00:19:56,681 - INFO - Distilling data from client: Client02
2023-07-01 00:19:56,681 - INFO - train loss: 0.017491416529338225
2023-07-01 00:19:56,681 - INFO - train acc: 0.8026315569877625
2023-07-01 00:19:56,694 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.61      0.72      0.66        60
           6       0.18      0.50      0.26         6
           7       0.64      0.49      0.56        61
           9       0.71      0.70      0.71        67

    accuracy                           0.61       200
   macro avg       0.43      0.48      0.44       200
weighted avg       0.62      0.61      0.61       200

2023-07-01 00:19:56,694 - INFO - test loss 0.02801425452357516
2023-07-01 00:19:56,694 - INFO - test acc 0.6150000095367432
2023-07-01 00:19:56,804 - INFO - Distilling data from client: Client02
2023-07-01 00:19:56,804 - INFO - train loss: 0.017919014800681768
2023-07-01 00:19:56,804 - INFO - train acc: 0.7763158082962036
2023-07-01 00:19:56,810 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.67      0.64        60
           6       0.19      0.50      0.27         6
           7       0.64      0.49      0.56        61
           9       0.67      0.72      0.69        67

    accuracy                           0.60       200
   macro avg       0.42      0.47      0.43       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:19:56,810 - INFO - test loss 0.027458187693608915
2023-07-01 00:19:56,810 - INFO - test acc 0.6049999594688416
2023-07-01 00:19:56,916 - INFO - Distilling data from client: Client02
2023-07-01 00:19:56,916 - INFO - train loss: 0.016875570277388535
2023-07-01 00:19:56,916 - INFO - train acc: 0.8289473652839661
2023-07-01 00:19:56,928 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.63      0.70      0.66        60
           6       0.20      0.33      0.25         6
           7       0.64      0.57      0.60        61
           9       0.68      0.69      0.68        67

    accuracy                           0.62       200
   macro avg       0.43      0.46      0.44       200
weighted avg       0.61      0.62      0.62       200

2023-07-01 00:19:56,928 - INFO - test loss 0.027348360785617316
2023-07-01 00:19:56,928 - INFO - test acc 0.625
2023-07-01 00:19:57,034 - INFO - Distilling data from client: Client02
2023-07-01 00:19:57,034 - INFO - train loss: 0.015657944012887556
2023-07-01 00:19:57,034 - INFO - train acc: 0.8552631735801697
2023-07-01 00:19:57,045 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.61      0.72      0.66        60
           6       0.15      0.33      0.21         6
           7       0.70      0.57      0.63        61
           9       0.73      0.73      0.73        67

    accuracy                           0.65       200
   macro avg       0.44      0.47      0.45       200
weighted avg       0.65      0.65      0.64       200

2023-07-01 00:19:57,045 - INFO - test loss 0.02649375397080936
2023-07-01 00:19:57,045 - INFO - test acc 0.6449999809265137
2023-07-01 00:19:57,152 - INFO - Distilling data from client: Client02
2023-07-01 00:19:57,152 - INFO - train loss: 0.016293740019599833
2023-07-01 00:19:57,152 - INFO - train acc: 0.7894737124443054
2023-07-01 00:19:57,159 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.59      0.75      0.66        60
           6       0.22      0.33      0.27         6
           7       0.70      0.54      0.61        61
           9       0.72      0.73      0.73        67

    accuracy                           0.65       200
   macro avg       0.45      0.47      0.45       200
weighted avg       0.64      0.65      0.64       200

2023-07-01 00:19:57,159 - INFO - test loss 0.02716736832248126
2023-07-01 00:19:57,159 - INFO - test acc 0.6449999809265137
2023-07-01 00:19:57,264 - INFO - Distilling data from client: Client02
2023-07-01 00:19:57,264 - INFO - train loss: 0.013396652850861794
2023-07-01 00:19:57,264 - INFO - train acc: 0.8815789818763733
2023-07-01 00:19:57,276 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.66      0.68      0.67        60
           6       0.23      0.50      0.32         6
           7       0.75      0.66      0.70        61
           9       0.69      0.75      0.72        67

    accuracy                           0.67       200
   macro avg       0.47      0.52      0.48       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:19:57,276 - INFO - test loss 0.026560535040237077
2023-07-01 00:19:57,276 - INFO - test acc 0.6699999570846558
2023-07-01 00:19:57,385 - INFO - Distilling data from client: Client02
2023-07-01 00:19:57,385 - INFO - train loss: 0.01564966541470933
2023-07-01 00:19:57,385 - INFO - train acc: 0.8684210777282715
2023-07-01 00:19:57,396 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.65      0.77      0.70        60
           6       0.30      0.50      0.37         6
           7       0.69      0.61      0.64        61
           9       0.77      0.75      0.76        67

    accuracy                           0.68       200
   macro avg       0.48      0.52      0.50       200
weighted avg       0.67      0.68      0.67       200

2023-07-01 00:19:57,396 - INFO - test loss 0.02607123835860681
2023-07-01 00:19:57,396 - INFO - test acc 0.6800000071525574
2023-07-01 00:19:57,501 - INFO - Distilling data from client: Client02
2023-07-01 00:19:57,501 - INFO - train loss: 0.014652456444020797
2023-07-01 00:19:57,501 - INFO - train acc: 0.8947368264198303
2023-07-01 00:19:57,508 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.59      0.72      0.65        60
           6       0.15      0.33      0.21         6
           7       0.63      0.52      0.57        61
           9       0.73      0.69      0.71        67

    accuracy                           0.61       200
   macro avg       0.42      0.45      0.43       200
weighted avg       0.62      0.61      0.61       200

2023-07-01 00:19:57,508 - INFO - test loss 0.02715712079358447
2023-07-01 00:19:57,508 - INFO - test acc 0.6150000095367432
2023-07-01 00:19:57,614 - INFO - Distilling data from client: Client02
2023-07-01 00:19:57,614 - INFO - train loss: 0.016171180561887103
2023-07-01 00:19:57,614 - INFO - train acc: 0.8157894611358643
2023-07-01 00:19:57,620 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.65      0.63        60
           6       0.17      0.33      0.22         6
           7       0.65      0.59      0.62        61
           9       0.70      0.73      0.72        67

    accuracy                           0.63       200
   macro avg       0.43      0.46      0.44       200
weighted avg       0.62      0.63      0.63       200

2023-07-01 00:19:57,620 - INFO - test loss 0.026941018030874234
2023-07-01 00:19:57,620 - INFO - test acc 0.6299999952316284
2023-07-01 00:19:57,725 - INFO - Distilling data from client: Client02
2023-07-01 00:19:57,725 - INFO - train loss: 0.014002661436796
2023-07-01 00:19:57,725 - INFO - train acc: 0.8684210777282715
2023-07-01 00:19:57,732 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.59      0.68      0.64        60
           6       0.18      0.33      0.24         6
           7       0.61      0.51      0.55        61
           9       0.68      0.70      0.69        67

    accuracy                           0.60       200
   macro avg       0.41      0.45      0.42       200
weighted avg       0.60      0.60      0.60       200

2023-07-01 00:19:57,732 - INFO - test loss 0.027335255833865632
2023-07-01 00:19:57,732 - INFO - test acc 0.6049999594688416
2023-07-01 00:19:57,834 - INFO - Distilling data from client: Client02
2023-07-01 00:19:57,835 - INFO - train loss: 0.014667552066513465
2023-07-01 00:19:57,835 - INFO - train acc: 0.8421052694320679
2023-07-01 00:19:57,841 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.63      0.70      0.66        60
           6       0.29      0.33      0.31         6
           7       0.66      0.67      0.67        61
           9       0.75      0.72      0.73        67

    accuracy                           0.67       200
   macro avg       0.46      0.48      0.47       200
weighted avg       0.65      0.67      0.66       200

2023-07-01 00:19:57,841 - INFO - test loss 0.026949224671038823
2023-07-01 00:19:57,841 - INFO - test acc 0.6649999618530273
2023-07-01 00:19:57,947 - INFO - Distilling data from client: Client02
2023-07-01 00:19:57,947 - INFO - train loss: 0.012600835490759382
2023-07-01 00:19:57,947 - INFO - train acc: 0.8947368264198303
2023-07-01 00:19:57,953 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.64      0.78      0.70        60
           6       0.30      0.50      0.37         6
           7       0.70      0.54      0.61        61
           9       0.68      0.70      0.69        67

    accuracy                           0.65       200
   macro avg       0.46      0.51      0.48       200
weighted avg       0.64      0.65      0.64       200

2023-07-01 00:19:57,953 - INFO - test loss 0.026661852427194124
2023-07-01 00:19:57,953 - INFO - test acc 0.6499999761581421
2023-07-01 00:19:58,056 - INFO - Distilling data from client: Client02
2023-07-01 00:19:58,056 - INFO - train loss: 0.01410150521618504
2023-07-01 00:19:58,056 - INFO - train acc: 0.9078947305679321
2023-07-01 00:19:58,068 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.64      0.85      0.73        60
           6       0.25      0.33      0.29         6
           7       0.76      0.57      0.65        61
           9       0.74      0.73      0.74        67

    accuracy                           0.69       200
   macro avg       0.48      0.50      0.48       200
weighted avg       0.68      0.69      0.67       200

2023-07-01 00:19:58,068 - INFO - test loss 0.02637506709349744
2023-07-01 00:19:58,068 - INFO - test acc 0.6850000023841858
2023-07-01 00:19:58,176 - INFO - Distilling data from client: Client02
2023-07-01 00:19:58,176 - INFO - train loss: 0.014554489025065806
2023-07-01 00:19:58,176 - INFO - train acc: 0.8421052694320679
2023-07-01 00:19:58,183 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.65      0.65      0.65        60
           6       0.30      0.50      0.37         6
           7       0.62      0.66      0.63        61
           9       0.71      0.69      0.70        67

    accuracy                           0.64       200
   macro avg       0.45      0.50      0.47       200
weighted avg       0.63      0.64      0.63       200

2023-07-01 00:19:58,183 - INFO - test loss 0.027246301791453247
2023-07-01 00:19:58,183 - INFO - test acc 0.6399999856948853
2023-07-01 00:19:58,288 - INFO - Distilling data from client: Client02
2023-07-01 00:19:58,288 - INFO - train loss: 0.013253608713901749
2023-07-01 00:19:58,288 - INFO - train acc: 0.8947368264198303
2023-07-01 00:19:58,294 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.75      0.68        60
           6       0.11      0.17      0.13         6
           7       0.67      0.59      0.63        61
           9       0.72      0.69      0.70        67

    accuracy                           0.64       200
   macro avg       0.42      0.44      0.43       200
weighted avg       0.63      0.64      0.63       200

2023-07-01 00:19:58,294 - INFO - test loss 0.02696055603645233
2023-07-01 00:19:58,294 - INFO - test acc 0.6399999856948853
2023-07-01 00:19:58,399 - INFO - Distilling data from client: Client02
2023-07-01 00:19:58,400 - INFO - train loss: 0.014644230223101923
2023-07-01 00:19:58,400 - INFO - train acc: 0.8552631735801697
2023-07-01 00:19:58,406 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.72      0.67        60
           6       0.23      0.50      0.32         6
           7       0.64      0.52      0.58        61
           9       0.68      0.69      0.68        67

    accuracy                           0.62       200
   macro avg       0.43      0.49      0.45       200
weighted avg       0.62      0.62      0.61       200

2023-07-01 00:19:58,406 - INFO - test loss 0.02655456911221747
2023-07-01 00:19:58,406 - INFO - test acc 0.6200000047683716
2023-07-01 00:19:58,510 - INFO - Distilling data from client: Client02
2023-07-01 00:19:58,511 - INFO - train loss: 0.011984718029149624
2023-07-01 00:19:58,511 - INFO - train acc: 0.8947368264198303
2023-07-01 00:19:58,522 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.68      0.80      0.73        60
           6       0.33      0.50      0.40         6
           7       0.72      0.67      0.69        61
           9       0.76      0.72      0.74        67

    accuracy                           0.70       200
   macro avg       0.50      0.54      0.51       200
weighted avg       0.69      0.70      0.69       200

2023-07-01 00:19:58,522 - INFO - test loss 0.026533517499481626
2023-07-01 00:19:58,522 - INFO - test acc 0.699999988079071
2023-07-01 00:19:58,627 - INFO - Distilling data from client: Client02
2023-07-01 00:19:58,627 - INFO - train loss: 0.012384497494424941
2023-07-01 00:19:58,627 - INFO - train acc: 0.9473684430122375
2023-07-01 00:19:58,633 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.67      0.73      0.70        60
           6       0.33      0.50      0.40         6
           7       0.66      0.66      0.66        61
           9       0.73      0.70      0.72        67

    accuracy                           0.67       200
   macro avg       0.48      0.52      0.49       200
weighted avg       0.66      0.67      0.66       200

2023-07-01 00:19:58,633 - INFO - test loss 0.02570509042342008
2023-07-01 00:19:58,633 - INFO - test acc 0.6699999570846558
2023-07-01 00:19:58,738 - INFO - Distilling data from client: Client02
2023-07-01 00:19:58,738 - INFO - train loss: 0.013774576432539046
2023-07-01 00:19:58,738 - INFO - train acc: 0.8421052694320679
2023-07-01 00:19:58,744 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.70      0.66        60
           6       0.23      0.50      0.32         6
           7       0.70      0.57      0.63        61
           9       0.67      0.69      0.68        67

    accuracy                           0.63       200
   macro avg       0.44      0.49      0.46       200
weighted avg       0.63      0.63      0.63       200

2023-07-01 00:19:58,744 - INFO - test loss 0.026908266751574993
2023-07-01 00:19:58,744 - INFO - test acc 0.6299999952316284
2023-07-01 00:19:58,847 - INFO - Distilling data from client: Client02
2023-07-01 00:19:58,847 - INFO - train loss: 0.012697027907919912
2023-07-01 00:19:58,847 - INFO - train acc: 0.8947368264198303
2023-07-01 00:19:58,853 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.63      0.72      0.67        60
           6       0.33      0.50      0.40         6
           7       0.66      0.64      0.65        61
           9       0.72      0.69      0.70        67

    accuracy                           0.66       200
   macro avg       0.47      0.51      0.48       200
weighted avg       0.64      0.66      0.65       200

2023-07-01 00:19:58,853 - INFO - test loss 0.026036246869468507
2023-07-01 00:19:58,853 - INFO - test acc 0.6549999713897705
2023-07-01 00:19:58,959 - INFO - Distilling data from client: Client02
2023-07-01 00:19:58,959 - INFO - train loss: 0.01047392297333171
2023-07-01 00:19:58,959 - INFO - train acc: 0.9605263471603394
2023-07-01 00:19:58,965 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.61      0.73      0.67        60
           6       0.14      0.17      0.15         6
           7       0.64      0.59      0.62        61
           9       0.74      0.72      0.73        67

    accuracy                           0.65       200
   macro avg       0.43      0.44      0.43       200
weighted avg       0.63      0.65      0.64       200

2023-07-01 00:19:58,965 - INFO - test loss 0.026522781779408484
2023-07-01 00:19:58,965 - INFO - test acc 0.6449999809265137
2023-07-01 00:19:59,072 - INFO - Distilling data from client: Client02
2023-07-01 00:19:59,072 - INFO - train loss: 0.012999151124331634
2023-07-01 00:19:59,072 - INFO - train acc: 0.9210526347160339
2023-07-01 00:19:59,079 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.55      0.68      0.61        60
           6       0.30      0.50      0.37         6
           7       0.62      0.52      0.57        61
           9       0.70      0.67      0.69        67

    accuracy                           0.60       200
   macro avg       0.43      0.48      0.45       200
weighted avg       0.60      0.60      0.60       200

2023-07-01 00:19:59,080 - INFO - test loss 0.027322067806552768
2023-07-01 00:19:59,080 - INFO - test acc 0.6049999594688416
2023-07-01 00:19:59,187 - INFO - Distilling data from client: Client02
2023-07-01 00:19:59,187 - INFO - train loss: 0.012210771775394364
2023-07-01 00:19:59,187 - INFO - train acc: 0.9342105388641357
2023-07-01 00:19:59,193 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.66      0.75      0.70        60
           6       0.25      0.50      0.33         6
           7       0.67      0.59      0.63        61
           9       0.73      0.72      0.72        67

    accuracy                           0.66       200
   macro avg       0.46      0.51      0.48       200
weighted avg       0.65      0.66      0.65       200

2023-07-01 00:19:59,193 - INFO - test loss 0.026064803822366218
2023-07-01 00:19:59,193 - INFO - test acc 0.6599999666213989
2023-07-01 00:19:59,298 - INFO - Distilling data from client: Client02
2023-07-01 00:19:59,298 - INFO - train loss: 0.01353872921858163
2023-07-01 00:19:59,299 - INFO - train acc: 0.8815789818763733
2023-07-01 00:19:59,305 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.66      0.72      0.69        60
           6       0.38      0.50      0.43         6
           7       0.67      0.62      0.64        61
           9       0.74      0.78      0.76        67

    accuracy                           0.68       200
   macro avg       0.49      0.52      0.50       200
weighted avg       0.66      0.68      0.67       200

2023-07-01 00:19:59,305 - INFO - test loss 0.02620081066497573
2023-07-01 00:19:59,305 - INFO - test acc 0.6800000071525574
2023-07-01 00:19:59,409 - INFO - Distilling data from client: Client02
2023-07-01 00:19:59,409 - INFO - train loss: 0.01177338788125519
2023-07-01 00:19:59,410 - INFO - train acc: 0.9342105388641357
2023-07-01 00:19:59,416 - INFO - report:               precision    recall  f1-score   support

           4       0.00      0.00      0.00         6
           5       0.62      0.72      0.67        60
           6       0.22      0.33      0.27         6
           7       0.66      0.61      0.63        61
           9       0.73      0.72      0.72        67

    accuracy                           0.65       200
   macro avg       0.45      0.47      0.46       200
weighted avg       0.64      0.65      0.64       200

2023-07-01 00:19:59,416 - INFO - test loss 0.026557448040459192
2023-07-01 00:19:59,416 - INFO - test acc 0.6499999761581421
2023-07-01 00:19:59,418 - WARNING - Finished tracing + transforming jit(gather) in 0.0002257823944091797 sec
2023-07-01 00:19:59,418 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[76,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:19:59,420 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0011322498321533203 sec
2023-07-01 00:19:59,420 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:59,430 - WARNING - Finished XLA compilation of jit(gather) in 0.009747505187988281 sec
2023-07-01 00:19:59,441 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:59,449 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:59,458 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:59,466 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:59,476 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:59,484 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:59,494 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:59,503 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:59,512 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:19:59,886 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client02//synthetic.png
2023-07-01 00:19:59,899 - INFO - c: 2.0 and total_data_in_this_class: 260
2023-07-01 00:19:59,899 - INFO - c: 4.0 and total_data_in_this_class: 267
2023-07-01 00:19:59,899 - INFO - c: 6.0 and total_data_in_this_class: 272
2023-07-01 00:19:59,899 - INFO - c: 2.0 and total_data_in_this_class: 73
2023-07-01 00:19:59,899 - INFO - c: 4.0 and total_data_in_this_class: 66
2023-07-01 00:19:59,899 - INFO - c: 6.0 and total_data_in_this_class: 61
2023-07-01 00:19:59,920 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002512931823730469 sec
2023-07-01 00:19:59,920 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:19:59,921 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0011529922485351562 sec
2023-07-01 00:19:59,922 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:59,932 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.01043844223022461 sec
2023-07-01 00:19:59,934 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002448558807373047 sec
2023-07-01 00:19:59,934 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:19:59,935 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009558200836181641 sec
2023-07-01 00:19:59,935 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:19:59,944 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008246421813964844 sec
2023-07-01 00:19:59,947 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013709068298339844 sec
2023-07-01 00:19:59,948 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013113021850585938 sec
2023-07-01 00:19:59,949 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003056526184082031 sec
2023-07-01 00:19:59,950 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013065338134765625 sec
2023-07-01 00:19:59,951 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019884109497070312 sec
2023-07-01 00:19:59,951 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002815723419189453 sec
2023-07-01 00:19:59,952 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002434253692626953 sec
2023-07-01 00:19:59,953 - WARNING - Finished tracing + transforming absolute for pjit in 0.0001671314239501953 sec
2023-07-01 00:19:59,953 - WARNING - Finished tracing + transforming fn for pjit in 0.0002741813659667969 sec
2023-07-01 00:19:59,954 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00031495094299316406 sec
2023-07-01 00:19:59,955 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001285076141357422 sec
2023-07-01 00:19:59,956 - WARNING - Finished tracing + transforming fn for pjit in 0.0002949237823486328 sec
2023-07-01 00:19:59,956 - WARNING - Finished tracing + transforming fn for pjit in 0.0002665519714355469 sec
2023-07-01 00:19:59,957 - WARNING - Finished tracing + transforming fn for pjit in 0.0002357959747314453 sec
2023-07-01 00:19:59,958 - WARNING - Finished tracing + transforming fn for pjit in 0.0002627372741699219 sec
2023-07-01 00:19:59,959 - WARNING - Finished tracing + transforming fn for pjit in 0.00022149085998535156 sec
2023-07-01 00:19:59,960 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00023627281188964844 sec
2023-07-01 00:19:59,961 - WARNING - Finished tracing + transforming fn for pjit in 0.00022745132446289062 sec
2023-07-01 00:19:59,962 - WARNING - Finished tracing + transforming fn for pjit in 0.0002300739288330078 sec
2023-07-01 00:19:59,965 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003619194030761719 sec
2023-07-01 00:19:59,966 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009636878967285156 sec
2023-07-01 00:19:59,967 - WARNING - Finished tracing + transforming fn for pjit in 0.00023221969604492188 sec
2023-07-01 00:19:59,967 - WARNING - Finished tracing + transforming fn for pjit in 0.00021982192993164062 sec
2023-07-01 00:19:59,968 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021982192993164062 sec
2023-07-01 00:19:59,969 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00034165382385253906 sec
2023-07-01 00:19:59,969 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001766681671142578 sec
2023-07-01 00:19:59,970 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025844573974609375 sec
2023-07-01 00:19:59,971 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022840499877929688 sec
2023-07-01 00:19:59,972 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022673606872558594 sec
2023-07-01 00:19:59,972 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00026917457580566406 sec
2023-07-01 00:19:59,973 - WARNING - Finished tracing + transforming _where for pjit in 0.0008580684661865234 sec
2023-07-01 00:19:59,973 - WARNING - Finished tracing + transforming fn for pjit in 0.000263214111328125 sec
2023-07-01 00:19:59,974 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003218650817871094 sec
2023-07-01 00:19:59,975 - WARNING - Finished tracing + transforming fn for pjit in 0.0002219676971435547 sec
2023-07-01 00:19:59,975 - WARNING - Finished tracing + transforming fn for pjit in 0.00022220611572265625 sec
2023-07-01 00:19:59,976 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021767616271972656 sec
2023-07-01 00:19:59,977 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002665519714355469 sec
2023-07-01 00:19:59,977 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017380714416503906 sec
2023-07-01 00:19:59,978 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025844573974609375 sec
2023-07-01 00:19:59,979 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022482872009277344 sec
2023-07-01 00:19:59,980 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002224445343017578 sec
2023-07-01 00:19:59,980 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025534629821777344 sec
2023-07-01 00:19:59,981 - WARNING - Finished tracing + transforming _where for pjit in 0.0008311271667480469 sec
2023-07-01 00:19:59,981 - WARNING - Finished tracing + transforming fn for pjit in 0.0002644062042236328 sec
2023-07-01 00:19:59,982 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025653839111328125 sec
2023-07-01 00:19:59,983 - WARNING - Finished tracing + transforming fn for pjit in 0.00021982192993164062 sec
2023-07-01 00:19:59,987 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027108192443847656 sec
2023-07-01 00:19:59,988 - WARNING - Finished tracing + transforming fn for pjit in 0.00026416778564453125 sec
2023-07-01 00:19:59,988 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002703666687011719 sec
2023-07-01 00:19:59,989 - WARNING - Finished tracing + transforming fn for pjit in 0.00029087066650390625 sec
2023-07-01 00:19:59,993 - WARNING - Finished tracing + transforming fn for pjit in 0.00021648406982421875 sec
2023-07-01 00:19:59,995 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016880035400390625 sec
2023-07-01 00:19:59,995 - WARNING - Finished tracing + transforming fn for pjit in 0.0003001689910888672 sec
2023-07-01 00:19:59,996 - WARNING - Finished tracing + transforming fn for pjit in 0.00024127960205078125 sec
2023-07-01 00:20:00,014 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06715893745422363 sec
2023-07-01 00:20:00,017 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012755393981933594 sec
2023-07-01 00:20:00,018 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001163482666015625 sec
2023-07-01 00:20:00,018 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002713203430175781 sec
2023-07-01 00:20:00,020 - WARNING - Finished tracing + transforming fn for pjit in 0.0002219676971435547 sec
2023-07-01 00:20:00,021 - WARNING - Finished tracing + transforming fn for pjit in 0.0002694129943847656 sec
2023-07-01 00:20:00,022 - WARNING - Finished tracing + transforming fn for pjit in 0.00021958351135253906 sec
2023-07-01 00:20:00,028 - WARNING - Finished tracing + transforming fn for pjit in 0.00022912025451660156 sec
2023-07-01 00:20:00,029 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022077560424804688 sec
2023-07-01 00:20:00,030 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002512931823730469 sec
2023-07-01 00:20:00,030 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017118453979492188 sec
2023-07-01 00:20:00,031 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003237724304199219 sec
2023-07-01 00:20:00,032 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022292137145996094 sec
2023-07-01 00:20:00,032 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022125244140625 sec
2023-07-01 00:20:00,033 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002636909484863281 sec
2023-07-01 00:20:00,034 - WARNING - Finished tracing + transforming _where for pjit in 0.0008525848388671875 sec
2023-07-01 00:20:00,034 - WARNING - Finished tracing + transforming fn for pjit in 0.0002601146697998047 sec
2023-07-01 00:20:00,035 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002532005310058594 sec
2023-07-01 00:20:00,036 - WARNING - Finished tracing + transforming fn for pjit in 0.0002205371856689453 sec
2023-07-01 00:20:00,036 - WARNING - Finished tracing + transforming fn for pjit in 0.000274658203125 sec
2023-07-01 00:20:00,048 - WARNING - Finished tracing + transforming fn for pjit in 0.00021457672119140625 sec
2023-07-01 00:20:00,067 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05205082893371582 sec
2023-07-01 00:20:00,068 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012230873107910156 sec
2023-07-01 00:20:00,069 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013136863708496094 sec
2023-07-01 00:20:00,070 - WARNING - Finished tracing + transforming _where for pjit in 0.0005962848663330078 sec
2023-07-01 00:20:00,070 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00028443336486816406 sec
2023-07-01 00:20:00,070 - WARNING - Finished tracing + transforming trace for pjit in 0.0024442672729492188 sec
2023-07-01 00:20:00,073 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010824203491210938 sec
2023-07-01 00:20:00,074 - WARNING - Finished tracing + transforming tril for pjit in 0.0006744861602783203 sec
2023-07-01 00:20:00,074 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0016939640045166016 sec
2023-07-01 00:20:00,075 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010776519775390625 sec
2023-07-01 00:20:00,075 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010585784912109375 sec
2023-07-01 00:20:00,077 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.001416921615600586 sec
2023-07-01 00:20:00,081 - WARNING - Finished tracing + transforming _solve for pjit in 0.00927424430847168 sec
2023-07-01 00:20:00,082 - WARNING - Finished tracing + transforming dot for pjit in 0.0003108978271484375 sec
2023-07-01 00:20:00,084 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.13895702362060547 sec
2023-07-01 00:20:00,086 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:20:00,119 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03231167793273926 sec
2023-07-01 00:20:00,119 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:00,242 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12268352508544922 sec
2023-07-01 00:20:00,266 - INFO - initial test loss: 0.0316075554966347
2023-07-01 00:20:00,266 - INFO - initial test acc: 0.5450000166893005
2023-07-01 00:20:00,272 - WARNING - Finished tracing + transforming dot for pjit in 0.0003561973571777344 sec
2023-07-01 00:20:00,273 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002906322479248047 sec
2023-07-01 00:20:00,274 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002956390380859375 sec
2023-07-01 00:20:00,274 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009052753448486328 sec
2023-07-01 00:20:00,275 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00018715858459472656 sec
2023-07-01 00:20:00,276 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00023055076599121094 sec
2023-07-01 00:20:00,276 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024390220642089844 sec
2023-07-01 00:20:00,277 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00035119056701660156 sec
2023-07-01 00:20:00,278 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010273456573486328 sec
2023-07-01 00:20:00,279 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009510278701782227 sec
2023-07-01 00:20:00,287 - WARNING - Finished tracing + transforming fn for pjit in 0.00026106834411621094 sec
2023-07-01 00:20:00,288 - WARNING - Finished tracing + transforming fn for pjit in 0.0002620220184326172 sec
2023-07-01 00:20:00,288 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002696514129638672 sec
2023-07-01 00:20:00,289 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00027060508728027344 sec
2023-07-01 00:20:00,290 - WARNING - Finished tracing + transforming _where for pjit in 0.0008833408355712891 sec
2023-07-01 00:20:00,298 - WARNING - Finished tracing + transforming fn for pjit in 0.00025916099548339844 sec
2023-07-01 00:20:00,298 - WARNING - Finished tracing + transforming fn for pjit in 0.00025463104248046875 sec
2023-07-01 00:20:00,299 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021076202392578125 sec
2023-07-01 00:20:00,300 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00024771690368652344 sec
2023-07-01 00:20:00,300 - WARNING - Finished tracing + transforming _where for pjit in 0.0008184909820556641 sec
2023-07-01 00:20:00,334 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00020956993103027344 sec
2023-07-01 00:20:00,387 - WARNING - Finished tracing + transforming fn for pjit in 0.00027060508728027344 sec
2023-07-01 00:20:00,388 - WARNING - Finished tracing + transforming fn for pjit in 0.0002300739288330078 sec
2023-07-01 00:20:00,389 - WARNING - Finished tracing + transforming square for pjit in 0.0001838207244873047 sec
2023-07-01 00:20:00,391 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021791458129882812 sec
2023-07-01 00:20:00,393 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023293495178222656 sec
2023-07-01 00:20:00,393 - WARNING - Finished tracing + transforming fn for pjit in 0.0002593994140625 sec
2023-07-01 00:20:00,394 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021648406982421875 sec
2023-07-01 00:20:00,394 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022602081298828125 sec
2023-07-01 00:20:00,395 - WARNING - Finished tracing + transforming fn for pjit in 0.00025463104248046875 sec
2023-07-01 00:20:00,396 - WARNING - Finished tracing + transforming fn for pjit in 0.0002186298370361328 sec
2023-07-01 00:20:00,396 - WARNING - Finished tracing + transforming square for pjit in 0.00016260147094726562 sec
2023-07-01 00:20:00,398 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021076202392578125 sec
2023-07-01 00:20:00,400 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017499923706054688 sec
2023-07-01 00:20:00,400 - WARNING - Finished tracing + transforming fn for pjit in 0.0002579689025878906 sec
2023-07-01 00:20:00,401 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021219253540039062 sec
2023-07-01 00:20:00,401 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002205371856689453 sec
2023-07-01 00:20:00,402 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13352131843566895 sec
2023-07-01 00:20:00,406 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:20:00,466 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.0604395866394043 sec
2023-07-01 00:20:00,467 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:00,790 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3227062225341797 sec
2023-07-01 00:20:02,033 - INFO - Distilling data from client: Client03
2023-07-01 00:20:02,033 - INFO - train loss: 0.0034801701293480937
2023-07-01 00:20:02,034 - INFO - train acc: 0.9922928810119629
2023-07-01 00:20:02,095 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.56      0.57        73
           4       0.48      0.48      0.48        66
           6       0.60      0.62      0.61        61

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-07-01 00:20:02,095 - INFO - test loss 0.031646240833285745
2023-07-01 00:20:02,096 - INFO - test acc 0.5550000071525574
2023-07-01 00:20:03,338 - INFO - Distilling data from client: Client03
2023-07-01 00:20:03,338 - INFO - train loss: 0.0019129840903875461
2023-07-01 00:20:03,338 - INFO - train acc: 1.0
2023-07-01 00:20:03,404 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.59      0.61        73
           4       0.49      0.52      0.50        66
           6       0.59      0.59      0.59        61

    accuracy                           0.56       200
   macro avg       0.57      0.56      0.57       200
weighted avg       0.57      0.56      0.57       200

2023-07-01 00:20:03,404 - INFO - test loss 0.0314048902775534
2023-07-01 00:20:03,404 - INFO - test acc 0.5649999976158142
2023-07-01 00:20:04,636 - INFO - Distilling data from client: Client03
2023-07-01 00:20:04,636 - INFO - train loss: 0.001953643418089303
2023-07-01 00:20:04,636 - INFO - train acc: 1.0
2023-07-01 00:20:04,660 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.58      0.59        73
           4       0.48      0.48      0.48        66
           6       0.59      0.62      0.61        61

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-07-01 00:20:04,660 - INFO - test loss 0.03132165194839196
2023-07-01 00:20:04,660 - INFO - test acc 0.5600000023841858
2023-07-01 00:20:05,892 - INFO - Distilling data from client: Client03
2023-07-01 00:20:05,892 - INFO - train loss: 0.0014463495806488456
2023-07-01 00:20:05,892 - INFO - train acc: 0.9980732202529907
2023-07-01 00:20:05,955 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.63      0.63        73
           4       0.52      0.52      0.52        66
           6       0.60      0.61      0.60        61

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.59      0.58      0.59       200

2023-07-01 00:20:05,955 - INFO - test loss 0.03226773671089126
2023-07-01 00:20:05,955 - INFO - test acc 0.5849999785423279
2023-07-01 00:20:07,194 - INFO - Distilling data from client: Client03
2023-07-01 00:20:07,194 - INFO - train loss: 0.0010108486076189152
2023-07-01 00:20:07,194 - INFO - train acc: 1.0
2023-07-01 00:20:07,218 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.60      0.62        73
           4       0.47      0.50      0.49        66
           6       0.60      0.59      0.60        61

    accuracy                           0.56       200
   macro avg       0.57      0.56      0.57       200
weighted avg       0.57      0.56      0.57       200

2023-07-01 00:20:07,218 - INFO - test loss 0.03208674465353463
2023-07-01 00:20:07,218 - INFO - test acc 0.5649999976158142
2023-07-01 00:20:08,458 - INFO - Distilling data from client: Client03
2023-07-01 00:20:08,458 - INFO - train loss: 0.0010964282013867358
2023-07-01 00:20:08,458 - INFO - train acc: 1.0
2023-07-01 00:20:08,481 - INFO - report:               precision    recall  f1-score   support

           2       0.65      0.62      0.63        73
           4       0.48      0.50      0.49        66
           6       0.61      0.62      0.62        61

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-07-01 00:20:08,481 - INFO - test loss 0.03153769710230856
2023-07-01 00:20:08,482 - INFO - test acc 0.5799999833106995
2023-07-01 00:20:09,729 - INFO - Distilling data from client: Client03
2023-07-01 00:20:09,729 - INFO - train loss: 0.00120624071310378
2023-07-01 00:20:09,729 - INFO - train acc: 1.0
2023-07-01 00:20:09,753 - INFO - report:               precision    recall  f1-score   support

           2       0.60      0.58      0.59        73
           4       0.47      0.48      0.48        66
           6       0.58      0.59      0.59        61

    accuracy                           0.55       200
   macro avg       0.55      0.55      0.55       200
weighted avg       0.55      0.55      0.55       200

2023-07-01 00:20:09,753 - INFO - test loss 0.03297975171240907
2023-07-01 00:20:09,753 - INFO - test acc 0.550000011920929
2023-07-01 00:20:10,995 - INFO - Distilling data from client: Client03
2023-07-01 00:20:10,995 - INFO - train loss: 0.0009280263749546413
2023-07-01 00:20:10,995 - INFO - train acc: 1.0
2023-07-01 00:20:11,019 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.56      0.58        73
           4       0.44      0.52      0.48        66
           6       0.59      0.52      0.56        61

    accuracy                           0.54       200
   macro avg       0.54      0.53      0.54       200
weighted avg       0.54      0.54      0.54       200

2023-07-01 00:20:11,019 - INFO - test loss 0.03288634106087788
2023-07-01 00:20:11,019 - INFO - test acc 0.5349999666213989
2023-07-01 00:20:12,262 - INFO - Distilling data from client: Client03
2023-07-01 00:20:12,262 - INFO - train loss: 0.0007933870322432675
2023-07-01 00:20:12,262 - INFO - train acc: 1.0
2023-07-01 00:20:12,287 - INFO - report:               precision    recall  f1-score   support

           2       0.60      0.58      0.59        73
           4       0.47      0.55      0.51        66
           6       0.59      0.52      0.56        61

    accuracy                           0.55       200
   macro avg       0.56      0.55      0.55       200
weighted avg       0.56      0.55      0.55       200

2023-07-01 00:20:12,287 - INFO - test loss 0.03267901434650664
2023-07-01 00:20:12,287 - INFO - test acc 0.550000011920929
2023-07-01 00:20:13,529 - INFO - Distilling data from client: Client03
2023-07-01 00:20:13,529 - INFO - train loss: 0.0009434714315491115
2023-07-01 00:20:13,529 - INFO - train acc: 1.0
2023-07-01 00:20:13,553 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.58      0.58        73
           4       0.46      0.45      0.46        66
           6       0.58      0.61      0.59        61

    accuracy                           0.55       200
   macro avg       0.54      0.55      0.54       200
weighted avg       0.54      0.55      0.54       200

2023-07-01 00:20:13,553 - INFO - test loss 0.0334723423335623
2023-07-01 00:20:13,553 - INFO - test acc 0.5450000166893005
2023-07-01 00:20:14,787 - INFO - Distilling data from client: Client03
2023-07-01 00:20:14,787 - INFO - train loss: 0.000935832643182942
2023-07-01 00:20:14,787 - INFO - train acc: 1.0
2023-07-01 00:20:14,810 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.58      0.58        73
           4       0.47      0.53      0.50        66
           6       0.58      0.51      0.54        61

    accuracy                           0.54       200
   macro avg       0.54      0.54      0.54       200
weighted avg       0.54      0.54      0.54       200

2023-07-01 00:20:14,810 - INFO - test loss 0.033305087925333834
2023-07-01 00:20:14,810 - INFO - test acc 0.5399999618530273
2023-07-01 00:20:16,039 - INFO - Distilling data from client: Client03
2023-07-01 00:20:16,039 - INFO - train loss: 0.0009500534020126107
2023-07-01 00:20:16,040 - INFO - train acc: 1.0
2023-07-01 00:20:16,063 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.64      0.64        73
           4       0.50      0.55      0.52        66
           6       0.63      0.56      0.59        61

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.59       200

2023-07-01 00:20:16,063 - INFO - test loss 0.03270184215400216
2023-07-01 00:20:16,063 - INFO - test acc 0.5849999785423279
2023-07-01 00:20:17,315 - INFO - Distilling data from client: Client03
2023-07-01 00:20:17,315 - INFO - train loss: 0.0007868022649771977
2023-07-01 00:20:17,315 - INFO - train acc: 1.0
2023-07-01 00:20:17,338 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.58      0.58        73
           4       0.48      0.47      0.47        66
           6       0.58      0.59      0.59        61

    accuracy                           0.55       200
   macro avg       0.54      0.55      0.54       200
weighted avg       0.54      0.55      0.54       200

2023-07-01 00:20:17,338 - INFO - test loss 0.03289751975678957
2023-07-01 00:20:17,339 - INFO - test acc 0.5450000166893005
2023-07-01 00:20:18,581 - INFO - Distilling data from client: Client03
2023-07-01 00:20:18,581 - INFO - train loss: 0.0007939659356370307
2023-07-01 00:20:18,581 - INFO - train acc: 1.0
2023-07-01 00:20:18,605 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.59      0.60        73
           4       0.51      0.50      0.50        66
           6       0.58      0.61      0.59        61

    accuracy                           0.56       200
   macro avg       0.56      0.57      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-07-01 00:20:18,605 - INFO - test loss 0.03245998733172269
2023-07-01 00:20:18,605 - INFO - test acc 0.5649999976158142
2023-07-01 00:20:19,841 - INFO - Distilling data from client: Client03
2023-07-01 00:20:19,842 - INFO - train loss: 0.0008972220539793427
2023-07-01 00:20:19,842 - INFO - train acc: 1.0
2023-07-01 00:20:19,865 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.58      0.60        73
           4       0.51      0.56      0.53        66
           6       0.61      0.59      0.60        61

    accuracy                           0.57       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.57      0.58       200

2023-07-01 00:20:19,865 - INFO - test loss 0.032622001593429636
2023-07-01 00:20:19,865 - INFO - test acc 0.574999988079071
2023-07-01 00:20:21,119 - INFO - Distilling data from client: Client03
2023-07-01 00:20:21,119 - INFO - train loss: 0.0005886959736404305
2023-07-01 00:20:21,119 - INFO - train acc: 1.0
2023-07-01 00:20:21,143 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.58      0.60        73
           4       0.45      0.52      0.48        66
           6       0.59      0.54      0.56        61

    accuracy                           0.55       200
   macro avg       0.55      0.54      0.55       200
weighted avg       0.55      0.55      0.55       200

2023-07-01 00:20:21,143 - INFO - test loss 0.0332313342303395
2023-07-01 00:20:21,143 - INFO - test acc 0.5450000166893005
2023-07-01 00:20:22,378 - INFO - Distilling data from client: Client03
2023-07-01 00:20:22,378 - INFO - train loss: 0.0007424955454620964
2023-07-01 00:20:22,378 - INFO - train acc: 1.0
2023-07-01 00:20:22,404 - INFO - report:               precision    recall  f1-score   support

           2       0.60      0.60      0.60        73
           4       0.49      0.52      0.50        66
           6       0.62      0.59      0.61        61

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-07-01 00:20:22,404 - INFO - test loss 0.03260364377355957
2023-07-01 00:20:22,404 - INFO - test acc 0.5699999928474426
2023-07-01 00:20:23,637 - INFO - Distilling data from client: Client03
2023-07-01 00:20:23,637 - INFO - train loss: 0.0006949124781497926
2023-07-01 00:20:23,638 - INFO - train acc: 1.0
2023-07-01 00:20:23,664 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.60      0.61        73
           4       0.46      0.50      0.48        66
           6       0.61      0.57      0.59        61

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-07-01 00:20:23,664 - INFO - test loss 0.03306003216534928
2023-07-01 00:20:23,665 - INFO - test acc 0.5600000023841858
2023-07-01 00:20:24,905 - INFO - Distilling data from client: Client03
2023-07-01 00:20:24,905 - INFO - train loss: 0.0006125332624105947
2023-07-01 00:20:24,905 - INFO - train acc: 1.0
2023-07-01 00:20:24,928 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.55      0.57        73
           4       0.48      0.53      0.50        66
           6       0.63      0.61      0.62        61

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-07-01 00:20:24,928 - INFO - test loss 0.033803496978572566
2023-07-01 00:20:24,928 - INFO - test acc 0.5600000023841858
2023-07-01 00:20:26,162 - INFO - Distilling data from client: Client03
2023-07-01 00:20:26,162 - INFO - train loss: 0.0005970923177774192
2023-07-01 00:20:26,162 - INFO - train acc: 1.0
2023-07-01 00:20:26,186 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.56      0.58        73
           4       0.50      0.56      0.53        66
           6       0.63      0.59      0.61        61

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-07-01 00:20:26,186 - INFO - test loss 0.03343628794378169
2023-07-01 00:20:26,186 - INFO - test acc 0.5699999928474426
2023-07-01 00:20:27,425 - INFO - Distilling data from client: Client03
2023-07-01 00:20:27,425 - INFO - train loss: 0.0006435969548045032
2023-07-01 00:20:27,425 - INFO - train acc: 1.0
2023-07-01 00:20:27,449 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.59      0.59        73
           4       0.50      0.55      0.52        66
           6       0.64      0.57      0.60        61

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-07-01 00:20:27,449 - INFO - test loss 0.03335649298440856
2023-07-01 00:20:27,450 - INFO - test acc 0.5699999928474426
2023-07-01 00:20:28,700 - INFO - Distilling data from client: Client03
2023-07-01 00:20:28,700 - INFO - train loss: 0.0004735850905535679
2023-07-01 00:20:28,700 - INFO - train acc: 1.0
2023-07-01 00:20:28,723 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.60      0.61        73
           4       0.45      0.45      0.45        66
           6       0.61      0.61      0.61        61

    accuracy                           0.56       200
   macro avg       0.56      0.55      0.55       200
weighted avg       0.56      0.56      0.56       200

2023-07-01 00:20:28,723 - INFO - test loss 0.0329242156436172
2023-07-01 00:20:28,723 - INFO - test acc 0.5550000071525574
2023-07-01 00:20:29,965 - INFO - Distilling data from client: Client03
2023-07-01 00:20:29,965 - INFO - train loss: 0.0004880675969590685
2023-07-01 00:20:29,965 - INFO - train acc: 1.0
2023-07-01 00:20:29,989 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.56      0.59        73
           4       0.47      0.56      0.51        66
           6       0.63      0.59      0.61        61

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-07-01 00:20:29,989 - INFO - test loss 0.03276679421097478
2023-07-01 00:20:29,989 - INFO - test acc 0.5699999928474426
2023-07-01 00:20:31,234 - INFO - Distilling data from client: Client03
2023-07-01 00:20:31,235 - INFO - train loss: 0.0005066442795747564
2023-07-01 00:20:31,235 - INFO - train acc: 1.0
2023-07-01 00:20:31,259 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.55      0.56        73
           4       0.45      0.52      0.48        66
           6       0.56      0.49      0.52        61

    accuracy                           0.52       200
   macro avg       0.52      0.52      0.52       200
weighted avg       0.53      0.52      0.52       200

2023-07-01 00:20:31,259 - INFO - test loss 0.03449585184999357
2023-07-01 00:20:31,260 - INFO - test acc 0.5199999809265137
2023-07-01 00:20:32,514 - INFO - Distilling data from client: Client03
2023-07-01 00:20:32,514 - INFO - train loss: 0.0006501577246884648
2023-07-01 00:20:32,514 - INFO - train acc: 1.0
2023-07-01 00:20:32,538 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.59      0.61        73
           4       0.48      0.56      0.52        66
           6       0.62      0.57      0.60        61

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.58       200
weighted avg       0.58      0.57      0.58       200

2023-07-01 00:20:32,538 - INFO - test loss 0.03319855864988097
2023-07-01 00:20:32,538 - INFO - test acc 0.574999988079071
2023-07-01 00:20:32,541 - WARNING - Finished tracing + transforming jit(gather) in 0.00025010108947753906 sec
2023-07-01 00:20:32,541 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:20:32,542 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0011615753173828125 sec
2023-07-01 00:20:32,542 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:32,552 - WARNING - Finished XLA compilation of jit(gather) in 0.009728193283081055 sec
2023-07-01 00:20:32,562 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:32,572 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:32,580 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:32,589 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:32,597 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:32,606 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:32,615 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:32,624 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:32,633 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:33,009 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client03//synthetic.png
2023-07-01 00:20:33,022 - INFO - c: 6.0 and total_data_in_this_class: 531
2023-07-01 00:20:33,022 - INFO - c: 7.0 and total_data_in_this_class: 268
2023-07-01 00:20:33,022 - INFO - c: 6.0 and total_data_in_this_class: 135
2023-07-01 00:20:33,022 - INFO - c: 7.0 and total_data_in_this_class: 65
2023-07-01 00:20:33,041 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025153160095214844 sec
2023-07-01 00:20:33,042 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:20:33,043 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0011534690856933594 sec
2023-07-01 00:20:33,043 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:33,054 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010494232177734375 sec
2023-07-01 00:20:33,055 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025153160095214844 sec
2023-07-01 00:20:33,056 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:20:33,057 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0010066032409667969 sec
2023-07-01 00:20:33,057 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:33,065 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008301019668579102 sec
2023-07-01 00:20:33,069 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013327598571777344 sec
2023-07-01 00:20:33,070 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001246929168701172 sec
2023-07-01 00:20:33,070 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003025531768798828 sec
2023-07-01 00:20:33,072 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020551681518554688 sec
2023-07-01 00:20:33,072 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012040138244628906 sec
2023-07-01 00:20:33,073 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00028014183044433594 sec
2023-07-01 00:20:33,073 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002465248107910156 sec
2023-07-01 00:20:33,074 - WARNING - Finished tracing + transforming absolute for pjit in 0.00016689300537109375 sec
2023-07-01 00:20:33,074 - WARNING - Finished tracing + transforming fn for pjit in 0.0002853870391845703 sec
2023-07-01 00:20:33,075 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0003151893615722656 sec
2023-07-01 00:20:33,076 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001990795135498047 sec
2023-07-01 00:20:33,077 - WARNING - Finished tracing + transforming fn for pjit in 0.00023627281188964844 sec
2023-07-01 00:20:33,078 - WARNING - Finished tracing + transforming fn for pjit in 0.00027060508728027344 sec
2023-07-01 00:20:33,078 - WARNING - Finished tracing + transforming fn for pjit in 0.00022721290588378906 sec
2023-07-01 00:20:33,079 - WARNING - Finished tracing + transforming fn for pjit in 0.0002636909484863281 sec
2023-07-01 00:20:33,080 - WARNING - Finished tracing + transforming fn for pjit in 0.00022339820861816406 sec
2023-07-01 00:20:33,082 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001666545867919922 sec
2023-07-01 00:20:33,083 - WARNING - Finished tracing + transforming fn for pjit in 0.00022721290588378906 sec
2023-07-01 00:20:33,084 - WARNING - Finished tracing + transforming fn for pjit in 0.0002372264862060547 sec
2023-07-01 00:20:33,087 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00035858154296875 sec
2023-07-01 00:20:33,088 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009543895721435547 sec
2023-07-01 00:20:33,089 - WARNING - Finished tracing + transforming fn for pjit in 0.00023293495178222656 sec
2023-07-01 00:20:33,089 - WARNING - Finished tracing + transforming fn for pjit in 0.00022149085998535156 sec
2023-07-01 00:20:33,090 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002193450927734375 sec
2023-07-01 00:20:33,091 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003361701965332031 sec
2023-07-01 00:20:33,091 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017571449279785156 sec
2023-07-01 00:20:33,092 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002651214599609375 sec
2023-07-01 00:20:33,093 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024175643920898438 sec
2023-07-01 00:20:33,093 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023126602172851562 sec
2023-07-01 00:20:33,094 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002689361572265625 sec
2023-07-01 00:20:33,095 - WARNING - Finished tracing + transforming _where for pjit in 0.0008625984191894531 sec
2023-07-01 00:20:33,095 - WARNING - Finished tracing + transforming fn for pjit in 0.0002617835998535156 sec
2023-07-01 00:20:33,096 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003218650817871094 sec
2023-07-01 00:20:33,097 - WARNING - Finished tracing + transforming fn for pjit in 0.00022721290588378906 sec
2023-07-01 00:20:33,097 - WARNING - Finished tracing + transforming fn for pjit in 0.0002231597900390625 sec
2023-07-01 00:20:33,098 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002186298370361328 sec
2023-07-01 00:20:33,099 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025725364685058594 sec
2023-07-01 00:20:33,099 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017523765563964844 sec
2023-07-01 00:20:33,100 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025916099548339844 sec
2023-07-01 00:20:33,101 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022530555725097656 sec
2023-07-01 00:20:33,102 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002219676971435547 sec
2023-07-01 00:20:33,102 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025534629821777344 sec
2023-07-01 00:20:33,103 - WARNING - Finished tracing + transforming _where for pjit in 0.0008330345153808594 sec
2023-07-01 00:20:33,103 - WARNING - Finished tracing + transforming fn for pjit in 0.00025725364685058594 sec
2023-07-01 00:20:33,104 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002529621124267578 sec
2023-07-01 00:20:33,105 - WARNING - Finished tracing + transforming fn for pjit in 0.00022172927856445312 sec
2023-07-01 00:20:33,109 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002739429473876953 sec
2023-07-01 00:20:33,110 - WARNING - Finished tracing + transforming fn for pjit in 0.00027251243591308594 sec
2023-07-01 00:20:33,110 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026726722717285156 sec
2023-07-01 00:20:33,111 - WARNING - Finished tracing + transforming fn for pjit in 0.00029087066650390625 sec
2023-07-01 00:20:33,115 - WARNING - Finished tracing + transforming fn for pjit in 0.00022149085998535156 sec
2023-07-01 00:20:33,117 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001652240753173828 sec
2023-07-01 00:20:33,117 - WARNING - Finished tracing + transforming fn for pjit in 0.000293731689453125 sec
2023-07-01 00:20:33,118 - WARNING - Finished tracing + transforming fn for pjit in 0.00022840499877929688 sec
2023-07-01 00:20:33,136 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06774401664733887 sec
2023-07-01 00:20:33,139 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012493133544921875 sec
2023-07-01 00:20:33,139 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011301040649414062 sec
2023-07-01 00:20:33,140 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002646446228027344 sec
2023-07-01 00:20:33,142 - WARNING - Finished tracing + transforming fn for pjit in 0.00022077560424804688 sec
2023-07-01 00:20:33,143 - WARNING - Finished tracing + transforming fn for pjit in 0.0002532005310058594 sec
2023-07-01 00:20:33,144 - WARNING - Finished tracing + transforming fn for pjit in 0.0002181529998779297 sec
2023-07-01 00:20:33,150 - WARNING - Finished tracing + transforming fn for pjit in 0.00023221969604492188 sec
2023-07-01 00:20:33,151 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002200603485107422 sec
2023-07-01 00:20:33,152 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025653839111328125 sec
2023-07-01 00:20:33,152 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001735687255859375 sec
2023-07-01 00:20:33,153 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003249645233154297 sec
2023-07-01 00:20:33,154 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022411346435546875 sec
2023-07-01 00:20:33,154 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022220611572265625 sec
2023-07-01 00:20:33,155 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00026226043701171875 sec
2023-07-01 00:20:33,156 - WARNING - Finished tracing + transforming _where for pjit in 0.0008416175842285156 sec
2023-07-01 00:20:33,156 - WARNING - Finished tracing + transforming fn for pjit in 0.0002570152282714844 sec
2023-07-01 00:20:33,157 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002582073211669922 sec
2023-07-01 00:20:33,158 - WARNING - Finished tracing + transforming fn for pjit in 0.0002155303955078125 sec
2023-07-01 00:20:33,158 - WARNING - Finished tracing + transforming fn for pjit in 0.00027370452880859375 sec
2023-07-01 00:20:33,170 - WARNING - Finished tracing + transforming fn for pjit in 0.00021910667419433594 sec
2023-07-01 00:20:33,190 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05287480354309082 sec
2023-07-01 00:20:33,191 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001227855682373047 sec
2023-07-01 00:20:33,192 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013256072998046875 sec
2023-07-01 00:20:33,192 - WARNING - Finished tracing + transforming _where for pjit in 0.000598907470703125 sec
2023-07-01 00:20:33,193 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002846717834472656 sec
2023-07-01 00:20:33,193 - WARNING - Finished tracing + transforming trace for pjit in 0.002449512481689453 sec
2023-07-01 00:20:33,195 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010371208190917969 sec
2023-07-01 00:20:33,196 - WARNING - Finished tracing + transforming tril for pjit in 0.0006592273712158203 sec
2023-07-01 00:20:33,197 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.001749277114868164 sec
2023-07-01 00:20:33,197 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001068115234375 sec
2023-07-01 00:20:33,198 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010466575622558594 sec
2023-07-01 00:20:33,200 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014014244079589844 sec
2023-07-01 00:20:33,203 - WARNING - Finished tracing + transforming _solve for pjit in 0.009021759033203125 sec
2023-07-01 00:20:33,204 - WARNING - Finished tracing + transforming dot for pjit in 0.00031113624572753906 sec
2023-07-01 00:20:33,207 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.1396780014038086 sec
2023-07-01 00:20:33,208 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:20:33,241 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.0327911376953125 sec
2023-07-01 00:20:33,242 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:33,369 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12711024284362793 sec
2023-07-01 00:20:33,385 - INFO - initial test loss: 0.015037239803721186
2023-07-01 00:20:33,385 - INFO - initial test acc: 0.8499999642372131
2023-07-01 00:20:33,391 - WARNING - Finished tracing + transforming dot for pjit in 0.00038552284240722656 sec
2023-07-01 00:20:33,392 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00030350685119628906 sec
2023-07-01 00:20:33,393 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00038504600524902344 sec
2023-07-01 00:20:33,393 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010101795196533203 sec
2023-07-01 00:20:33,394 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001990795135498047 sec
2023-07-01 00:20:33,395 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001838207244873047 sec
2023-07-01 00:20:33,396 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002484321594238281 sec
2023-07-01 00:20:33,396 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00036525726318359375 sec
2023-07-01 00:20:33,397 - WARNING - Finished tracing + transforming _mean for pjit in 0.001066446304321289 sec
2023-07-01 00:20:33,398 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.010361671447753906 sec
2023-07-01 00:20:33,406 - WARNING - Finished tracing + transforming fn for pjit in 0.00031304359436035156 sec
2023-07-01 00:20:33,407 - WARNING - Finished tracing + transforming fn for pjit in 0.00026869773864746094 sec
2023-07-01 00:20:33,407 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022029876708984375 sec
2023-07-01 00:20:33,408 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00026106834411621094 sec
2023-07-01 00:20:33,409 - WARNING - Finished tracing + transforming _where for pjit in 0.0008416175842285156 sec
2023-07-01 00:20:33,417 - WARNING - Finished tracing + transforming fn for pjit in 0.00024628639221191406 sec
2023-07-01 00:20:33,417 - WARNING - Finished tracing + transforming fn for pjit in 0.00025725364685058594 sec
2023-07-01 00:20:33,418 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002086162567138672 sec
2023-07-01 00:20:33,419 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00024771690368652344 sec
2023-07-01 00:20:33,419 - WARNING - Finished tracing + transforming _where for pjit in 0.0008187294006347656 sec
2023-07-01 00:20:33,453 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002167224884033203 sec
2023-07-01 00:20:33,508 - WARNING - Finished tracing + transforming fn for pjit in 0.0002765655517578125 sec
2023-07-01 00:20:33,509 - WARNING - Finished tracing + transforming fn for pjit in 0.00023031234741210938 sec
2023-07-01 00:20:33,509 - WARNING - Finished tracing + transforming square for pjit in 0.00017690658569335938 sec
2023-07-01 00:20:33,511 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022864341735839844 sec
2023-07-01 00:20:33,513 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005488395690917969 sec
2023-07-01 00:20:33,514 - WARNING - Finished tracing + transforming fn for pjit in 0.0002665519714355469 sec
2023-07-01 00:20:33,515 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022220611572265625 sec
2023-07-01 00:20:33,515 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023293495178222656 sec
2023-07-01 00:20:33,516 - WARNING - Finished tracing + transforming fn for pjit in 0.0002655982971191406 sec
2023-07-01 00:20:33,516 - WARNING - Finished tracing + transforming fn for pjit in 0.0002257823944091797 sec
2023-07-01 00:20:33,517 - WARNING - Finished tracing + transforming square for pjit in 0.00016760826110839844 sec
2023-07-01 00:20:33,519 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021648406982421875 sec
2023-07-01 00:20:33,520 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001678466796875 sec
2023-07-01 00:20:33,521 - WARNING - Finished tracing + transforming fn for pjit in 0.0002522468566894531 sec
2023-07-01 00:20:33,522 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021696090698242188 sec
2023-07-01 00:20:33,522 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022530555725097656 sec
2023-07-01 00:20:33,523 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1362619400024414 sec
2023-07-01 00:20:33,527 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:20:33,588 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.061026811599731445 sec
2023-07-01 00:20:33,588 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:33,910 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32146644592285156 sec
2023-07-01 00:20:34,664 - INFO - Distilling data from client: Client04
2023-07-01 00:20:34,664 - INFO - train loss: 0.0030304324536273223
2023-07-01 00:20:34,664 - INFO - train acc: 0.9832401871681213
2023-07-01 00:20:34,704 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.89      0.90       135
           7       0.78      0.82      0.80        65

    accuracy                           0.86       200
   macro avg       0.84      0.85      0.85       200
weighted avg       0.87      0.86      0.87       200

2023-07-01 00:20:34,704 - INFO - test loss 0.012249882212608636
2023-07-01 00:20:34,705 - INFO - test acc 0.8650000095367432
2023-07-01 00:20:35,446 - INFO - Distilling data from client: Client04
2023-07-01 00:20:35,446 - INFO - train loss: 0.002001664223471091
2023-07-01 00:20:35,446 - INFO - train acc: 0.9972066879272461
2023-07-01 00:20:35,490 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.79      0.83      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:20:35,490 - INFO - test loss 0.01181087368241471
2023-07-01 00:20:35,491 - INFO - test acc 0.875
2023-07-01 00:20:36,233 - INFO - Distilling data from client: Client04
2023-07-01 00:20:36,233 - INFO - train loss: 0.0019254022085720246
2023-07-01 00:20:36,233 - INFO - train acc: 0.9972066879272461
2023-07-01 00:20:36,251 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.89      0.90       135
           7       0.78      0.83      0.81        65

    accuracy                           0.87       200
   macro avg       0.85      0.86      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:20:36,251 - INFO - test loss 0.01178680847537796
2023-07-01 00:20:36,251 - INFO - test acc 0.8700000047683716
2023-07-01 00:20:36,997 - INFO - Distilling data from client: Client04
2023-07-01 00:20:36,997 - INFO - train loss: 0.0015978500739735832
2023-07-01 00:20:36,997 - INFO - train acc: 0.9944133758544922
2023-07-01 00:20:37,019 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.91      0.91       135
           7       0.81      0.80      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.87      0.88      0.87       200

2023-07-01 00:20:37,019 - INFO - test loss 0.012237086917449283
2023-07-01 00:20:37,019 - INFO - test acc 0.875
2023-07-01 00:20:37,759 - INFO - Distilling data from client: Client04
2023-07-01 00:20:37,759 - INFO - train loss: 0.001617688400126399
2023-07-01 00:20:37,760 - INFO - train acc: 0.9972066879272461
2023-07-01 00:20:37,778 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.90      0.90       135
           7       0.79      0.82      0.80        65

    accuracy                           0.87       200
   macro avg       0.85      0.86      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:20:37,778 - INFO - test loss 0.011632908874740262
2023-07-01 00:20:37,779 - INFO - test acc 0.8700000047683716
2023-07-01 00:20:38,522 - INFO - Distilling data from client: Client04
2023-07-01 00:20:38,522 - INFO - train loss: 0.0013919155686941086
2023-07-01 00:20:38,522 - INFO - train acc: 1.0
2023-07-01 00:20:38,565 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.80      0.85      0.82        65

    accuracy                           0.88       200
   macro avg       0.86      0.87      0.87       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:20:38,566 - INFO - test loss 0.012367828178110012
2023-07-01 00:20:38,566 - INFO - test acc 0.8799999952316284
2023-07-01 00:20:39,311 - INFO - Distilling data from client: Client04
2023-07-01 00:20:39,311 - INFO - train loss: 0.0013297051644342915
2023-07-01 00:20:39,311 - INFO - train acc: 1.0
2023-07-01 00:20:39,328 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.79      0.83      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:20:39,329 - INFO - test loss 0.012078277442020205
2023-07-01 00:20:39,329 - INFO - test acc 0.875
2023-07-01 00:20:40,074 - INFO - Distilling data from client: Client04
2023-07-01 00:20:40,075 - INFO - train loss: 0.001577385670044494
2023-07-01 00:20:40,075 - INFO - train acc: 1.0
2023-07-01 00:20:40,092 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.79      0.83      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:20:40,092 - INFO - test loss 0.011844747066084798
2023-07-01 00:20:40,092 - INFO - test acc 0.875
2023-07-01 00:20:40,846 - INFO - Distilling data from client: Client04
2023-07-01 00:20:40,846 - INFO - train loss: 0.0011260043219097028
2023-07-01 00:20:40,846 - INFO - train acc: 1.0
2023-07-01 00:20:40,866 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.87      0.89       135
           7       0.76      0.83      0.79        65

    accuracy                           0.86       200
   macro avg       0.84      0.85      0.84       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:20:40,866 - INFO - test loss 0.011962044128491368
2023-07-01 00:20:40,866 - INFO - test acc 0.85999995470047
2023-07-01 00:20:41,610 - INFO - Distilling data from client: Client04
2023-07-01 00:20:41,611 - INFO - train loss: 0.0011773345555564972
2023-07-01 00:20:41,611 - INFO - train acc: 0.9972066879272461
2023-07-01 00:20:41,629 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.90      0.91       135
           7       0.80      0.82      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:20:41,629 - INFO - test loss 0.01162268455476946
2023-07-01 00:20:41,629 - INFO - test acc 0.875
2023-07-01 00:20:42,373 - INFO - Distilling data from client: Client04
2023-07-01 00:20:42,373 - INFO - train loss: 0.00112349148803014
2023-07-01 00:20:42,373 - INFO - train acc: 1.0
2023-07-01 00:20:42,390 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.91      0.90       135
           7       0.81      0.78      0.80        65

    accuracy                           0.87       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:20:42,391 - INFO - test loss 0.011794774738889598
2023-07-01 00:20:42,391 - INFO - test acc 0.8700000047683716
2023-07-01 00:20:43,135 - INFO - Distilling data from client: Client04
2023-07-01 00:20:43,136 - INFO - train loss: 0.0011601227713490504
2023-07-01 00:20:43,136 - INFO - train acc: 1.0
2023-07-01 00:20:43,153 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.90      0.91       135
           7       0.80      0.82      0.81        65

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:20:43,153 - INFO - test loss 0.01152541795869512
2023-07-01 00:20:43,153 - INFO - test acc 0.875
2023-07-01 00:20:43,901 - INFO - Distilling data from client: Client04
2023-07-01 00:20:43,901 - INFO - train loss: 0.001196514293305659
2023-07-01 00:20:43,901 - INFO - train acc: 0.9972066879272461
2023-07-01 00:20:43,918 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.90      0.90       135
           7       0.80      0.78      0.79        65

    accuracy                           0.86       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:20:43,918 - INFO - test loss 0.012066657418706214
2023-07-01 00:20:43,918 - INFO - test acc 0.8650000095367432
2023-07-01 00:20:44,667 - INFO - Distilling data from client: Client04
2023-07-01 00:20:44,667 - INFO - train loss: 0.0009074464161777983
2023-07-01 00:20:44,667 - INFO - train acc: 1.0
2023-07-01 00:20:44,711 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.91      0.92       135
           7       0.82      0.85      0.83        65

    accuracy                           0.89       200
   macro avg       0.87      0.88      0.88       200
weighted avg       0.89      0.89      0.89       200

2023-07-01 00:20:44,711 - INFO - test loss 0.011177067667513654
2023-07-01 00:20:44,711 - INFO - test acc 0.8899999856948853
2023-07-01 00:20:45,450 - INFO - Distilling data from client: Client04
2023-07-01 00:20:45,450 - INFO - train loss: 0.0011434677908485013
2023-07-01 00:20:45,451 - INFO - train acc: 1.0
2023-07-01 00:20:45,469 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.91      0.91       135
           7       0.82      0.83      0.82        65

    accuracy                           0.89       200
   macro avg       0.87      0.87      0.87       200
weighted avg       0.89      0.89      0.89       200

2023-07-01 00:20:45,469 - INFO - test loss 0.011708466107574004
2023-07-01 00:20:45,469 - INFO - test acc 0.8849999904632568
2023-07-01 00:20:46,218 - INFO - Distilling data from client: Client04
2023-07-01 00:20:46,218 - INFO - train loss: 0.001126767698393562
2023-07-01 00:20:46,218 - INFO - train acc: 1.0
2023-07-01 00:20:46,235 - INFO - report:               precision    recall  f1-score   support

           6       0.93      0.90      0.92       135
           7       0.81      0.86      0.84        65

    accuracy                           0.89       200
   macro avg       0.87      0.88      0.88       200
weighted avg       0.89      0.89      0.89       200

2023-07-01 00:20:46,235 - INFO - test loss 0.011521975482157271
2023-07-01 00:20:46,235 - INFO - test acc 0.8899999856948853
2023-07-01 00:20:46,984 - INFO - Distilling data from client: Client04
2023-07-01 00:20:46,984 - INFO - train loss: 0.0009712756101258006
2023-07-01 00:20:46,984 - INFO - train acc: 1.0
2023-07-01 00:20:47,003 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.91      0.92       135
           7       0.82      0.85      0.83        65

    accuracy                           0.89       200
   macro avg       0.87      0.88      0.88       200
weighted avg       0.89      0.89      0.89       200

2023-07-01 00:20:47,003 - INFO - test loss 0.010881118654416468
2023-07-01 00:20:47,003 - INFO - test acc 0.8899999856948853
2023-07-01 00:20:47,755 - INFO - Distilling data from client: Client04
2023-07-01 00:20:47,755 - INFO - train loss: 0.0010724603920574368
2023-07-01 00:20:47,755 - INFO - train acc: 1.0
2023-07-01 00:20:47,772 - INFO - report:               precision    recall  f1-score   support

           6       0.93      0.90      0.91       135
           7       0.80      0.86      0.83        65

    accuracy                           0.89       200
   macro avg       0.87      0.88      0.87       200
weighted avg       0.89      0.89      0.89       200

2023-07-01 00:20:47,772 - INFO - test loss 0.011329242628846435
2023-07-01 00:20:47,772 - INFO - test acc 0.8849999904632568
2023-07-01 00:20:48,518 - INFO - Distilling data from client: Client04
2023-07-01 00:20:48,518 - INFO - train loss: 0.0010843325136252854
2023-07-01 00:20:48,518 - INFO - train acc: 1.0
2023-07-01 00:20:48,536 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.88      0.89       135
           7       0.77      0.82      0.79        65

    accuracy                           0.86       200
   macro avg       0.84      0.85      0.84       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:20:48,536 - INFO - test loss 0.01172887007968317
2023-07-01 00:20:48,536 - INFO - test acc 0.85999995470047
2023-07-01 00:20:49,281 - INFO - Distilling data from client: Client04
2023-07-01 00:20:49,281 - INFO - train loss: 0.0010442105481547092
2023-07-01 00:20:49,281 - INFO - train acc: 1.0
2023-07-01 00:20:49,301 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.81      0.83      0.82        65

    accuracy                           0.88       200
   macro avg       0.86      0.87      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:20:49,301 - INFO - test loss 0.01228371157608822
2023-07-01 00:20:49,301 - INFO - test acc 0.8799999952316284
2023-07-01 00:20:50,045 - INFO - Distilling data from client: Client04
2023-07-01 00:20:50,045 - INFO - train loss: 0.0009553675487944656
2023-07-01 00:20:50,045 - INFO - train acc: 1.0
2023-07-01 00:20:50,062 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.91      0.90       135
           7       0.81      0.78      0.80        65

    accuracy                           0.87       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:20:50,062 - INFO - test loss 0.012179786193443669
2023-07-01 00:20:50,063 - INFO - test acc 0.8700000047683716
2023-07-01 00:20:50,807 - INFO - Distilling data from client: Client04
2023-07-01 00:20:50,807 - INFO - train loss: 0.001164663807300699
2023-07-01 00:20:50,807 - INFO - train acc: 0.9972066879272461
2023-07-01 00:20:50,826 - INFO - report:               precision    recall  f1-score   support

           6       0.92      0.90      0.91       135
           7       0.81      0.83      0.82        65

    accuracy                           0.88       200
   macro avg       0.86      0.87      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:20:50,826 - INFO - test loss 0.011573726796976885
2023-07-01 00:20:50,826 - INFO - test acc 0.8799999952316284
2023-07-01 00:20:51,567 - INFO - Distilling data from client: Client04
2023-07-01 00:20:51,567 - INFO - train loss: 0.0009176921037066688
2023-07-01 00:20:51,567 - INFO - train acc: 1.0
2023-07-01 00:20:51,584 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.90      0.90       135
           7       0.80      0.78      0.79        65

    accuracy                           0.86       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:20:51,584 - INFO - test loss 0.011732218653450937
2023-07-01 00:20:51,584 - INFO - test acc 0.8650000095367432
2023-07-01 00:20:52,330 - INFO - Distilling data from client: Client04
2023-07-01 00:20:52,330 - INFO - train loss: 0.0008510398771706138
2023-07-01 00:20:52,330 - INFO - train acc: 1.0
2023-07-01 00:20:52,347 - INFO - report:               precision    recall  f1-score   support

           6       0.90      0.92      0.91       135
           7       0.82      0.78      0.80        65

    accuracy                           0.88       200
   macro avg       0.86      0.85      0.86       200
weighted avg       0.87      0.88      0.87       200

2023-07-01 00:20:52,347 - INFO - test loss 0.01173908798213164
2023-07-01 00:20:52,347 - INFO - test acc 0.875
2023-07-01 00:20:53,091 - INFO - Distilling data from client: Client04
2023-07-01 00:20:53,091 - INFO - train loss: 0.0008576477059405712
2023-07-01 00:20:53,091 - INFO - train acc: 1.0
2023-07-01 00:20:53,108 - INFO - report:               precision    recall  f1-score   support

           6       0.91      0.92      0.92       135
           7       0.83      0.82      0.82        65

    accuracy                           0.89       200
   macro avg       0.87      0.87      0.87       200
weighted avg       0.88      0.89      0.88       200

2023-07-01 00:20:53,109 - INFO - test loss 0.012194642182906898
2023-07-01 00:20:53,109 - INFO - test acc 0.8849999904632568
2023-07-01 00:20:53,110 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.00019621849060058594 sec
2023-07-01 00:20:53,111 - WARNING - Finished tracing + transforming fn for pjit in 0.0003142356872558594 sec
2023-07-01 00:20:53,111 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(int64[2]), ShapedArray(int64[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:20:53,113 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.0012927055358886719 sec
2023-07-01 00:20:53,113 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:53,120 - WARNING - Finished XLA compilation of jit(fn) in 0.006712675094604492 sec
2023-07-01 00:20:53,120 - WARNING - Finished tracing + transforming jit(add) in 0.00022912025451660156 sec
2023-07-01 00:20:53,121 - DEBUG - Compiling add for with global shapes and types [ShapedArray(int64[2]), ShapedArray(int64[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:20:53,122 - WARNING - Finished jaxpr to MLIR module conversion jit(add) in 0.0012354850769042969 sec
2023-07-01 00:20:53,122 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:53,128 - WARNING - Finished XLA compilation of jit(add) in 0.0061223506927490234 sec
2023-07-01 00:20:53,129 - WARNING - Finished tracing + transforming jit(select_n) in 0.00022482872009277344 sec
2023-07-01 00:20:53,129 - DEBUG - Compiling select_n for with global shapes and types [ShapedArray(bool[2]), ShapedArray(int64[2]), ShapedArray(int64[2])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:20:53,131 - WARNING - Finished jaxpr to MLIR module conversion jit(select_n) in 0.0010097026824951172 sec
2023-07-01 00:20:53,131 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:53,137 - WARNING - Finished XLA compilation of jit(select_n) in 0.006505012512207031 sec
2023-07-01 00:20:53,138 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00018525123596191406 sec
2023-07-01 00:20:53,139 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.00014400482177734375 sec
2023-07-01 00:20:53,139 - DEBUG - Compiling convert_element_type for with global shapes and types [ShapedArray(int64[2])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:20:53,140 - WARNING - Finished jaxpr to MLIR module conversion jit(convert_element_type) in 0.0009195804595947266 sec
2023-07-01 00:20:53,141 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:53,147 - WARNING - Finished XLA compilation of jit(convert_element_type) in 0.005880594253540039 sec
2023-07-01 00:20:53,147 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.000225067138671875 sec
2023-07-01 00:20:53,148 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(int32[2])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:20:53,149 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009300708770751953 sec
2023-07-01 00:20:53,149 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:53,154 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.0049402713775634766 sec
2023-07-01 00:20:53,155 - WARNING - Finished tracing + transforming jit(gather) in 0.00023412704467773438 sec
2023-07-01 00:20:53,155 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[358,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:20:53,156 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0010232925415039062 sec
2023-07-01 00:20:53,156 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:53,166 - WARNING - Finished XLA compilation of jit(gather) in 0.009204864501953125 sec
2023-07-01 00:20:53,167 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002446174621582031 sec
2023-07-01 00:20:53,167 - WARNING - Finished tracing + transforming jit(copy) in 0.0001049041748046875 sec
2023-07-01 00:20:53,167 - DEBUG - Compiling copy for with global shapes and types [ShapedArray(float32[2,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:20:53,168 - WARNING - Finished jaxpr to MLIR module conversion jit(copy) in 0.0008776187896728516 sec
2023-07-01 00:20:53,168 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:53,174 - WARNING - Finished XLA compilation of jit(copy) in 0.0050067901611328125 sec
2023-07-01 00:20:53,183 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:53,192 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:53,201 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:53,209 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:53,211 - WARNING - Finished tracing + transforming _unstack for pjit in 0.0005202293395996094 sec
2023-07-01 00:20:53,211 - DEBUG - Compiling _unstack for with global shapes and types [ShapedArray(float32[2,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:20:53,212 - WARNING - Finished jaxpr to MLIR module conversion jit(_unstack) in 0.0013320446014404297 sec
2023-07-01 00:20:53,213 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:53,224 - WARNING - Finished XLA compilation of jit(_unstack) in 0.010765314102172852 sec
2023-07-01 00:20:53,233 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:53,242 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:20:53,523 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client04//synthetic.png
2023-07-01 00:20:53,536 - INFO - c: 1.0 and total_data_in_this_class: 262
2023-07-01 00:20:53,536 - INFO - c: 2.0 and total_data_in_this_class: 269
2023-07-01 00:20:53,536 - INFO - c: 8.0 and total_data_in_this_class: 268
2023-07-01 00:20:53,536 - INFO - c: 1.0 and total_data_in_this_class: 71
2023-07-01 00:20:53,536 - INFO - c: 2.0 and total_data_in_this_class: 64
2023-07-01 00:20:53,536 - INFO - c: 8.0 and total_data_in_this_class: 65
2023-07-01 00:20:53,557 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00028133392333984375 sec
2023-07-01 00:20:53,557 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:20:53,559 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0011756420135498047 sec
2023-07-01 00:20:53,559 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:53,569 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010379791259765625 sec
2023-07-01 00:20:53,571 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002474784851074219 sec
2023-07-01 00:20:53,572 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:20:53,573 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009741783142089844 sec
2023-07-01 00:20:53,573 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:53,581 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008356809616088867 sec
2023-07-01 00:20:53,584 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013875961303710938 sec
2023-07-01 00:20:53,585 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001232624053955078 sec
2023-07-01 00:20:53,586 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00029778480529785156 sec
2023-07-01 00:20:53,587 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002040863037109375 sec
2023-07-01 00:20:53,588 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011944770812988281 sec
2023-07-01 00:20:53,588 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002796649932861328 sec
2023-07-01 00:20:53,589 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002472400665283203 sec
2023-07-01 00:20:53,590 - WARNING - Finished tracing + transforming absolute for pjit in 0.00017023086547851562 sec
2023-07-01 00:20:53,590 - WARNING - Finished tracing + transforming fn for pjit in 0.00027370452880859375 sec
2023-07-01 00:20:53,591 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0003159046173095703 sec
2023-07-01 00:20:53,592 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020647048950195312 sec
2023-07-01 00:20:53,593 - WARNING - Finished tracing + transforming fn for pjit in 0.00023055076599121094 sec
2023-07-01 00:20:53,593 - WARNING - Finished tracing + transforming fn for pjit in 0.00026607513427734375 sec
2023-07-01 00:20:53,594 - WARNING - Finished tracing + transforming fn for pjit in 0.00022459030151367188 sec
2023-07-01 00:20:53,594 - WARNING - Finished tracing + transforming fn for pjit in 0.0002646446228027344 sec
2023-07-01 00:20:53,596 - WARNING - Finished tracing + transforming fn for pjit in 0.00022411346435546875 sec
2023-07-01 00:20:53,597 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016951560974121094 sec
2023-07-01 00:20:53,598 - WARNING - Finished tracing + transforming fn for pjit in 0.0002589225769042969 sec
2023-07-01 00:20:53,599 - WARNING - Finished tracing + transforming fn for pjit in 0.0002307891845703125 sec
2023-07-01 00:20:53,602 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003597736358642578 sec
2023-07-01 00:20:53,603 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009546279907226562 sec
2023-07-01 00:20:53,604 - WARNING - Finished tracing + transforming fn for pjit in 0.0002307891845703125 sec
2023-07-01 00:20:53,604 - WARNING - Finished tracing + transforming fn for pjit in 0.0002186298370361328 sec
2023-07-01 00:20:53,605 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021839141845703125 sec
2023-07-01 00:20:53,606 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002529621124267578 sec
2023-07-01 00:20:53,606 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017380714416503906 sec
2023-07-01 00:20:53,607 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025963783264160156 sec
2023-07-01 00:20:53,608 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022673606872558594 sec
2023-07-01 00:20:53,609 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022649765014648438 sec
2023-07-01 00:20:53,609 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003376007080078125 sec
2023-07-01 00:20:53,610 - WARNING - Finished tracing + transforming _where for pjit in 0.0009353160858154297 sec
2023-07-01 00:20:53,611 - WARNING - Finished tracing + transforming fn for pjit in 0.0002684593200683594 sec
2023-07-01 00:20:53,611 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002567768096923828 sec
2023-07-01 00:20:53,612 - WARNING - Finished tracing + transforming fn for pjit in 0.0002219676971435547 sec
2023-07-01 00:20:53,613 - WARNING - Finished tracing + transforming fn for pjit in 0.00022268295288085938 sec
2023-07-01 00:20:53,613 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002200603485107422 sec
2023-07-01 00:20:53,614 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002567768096923828 sec
2023-07-01 00:20:53,615 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000244140625 sec
2023-07-01 00:20:53,615 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025653839111328125 sec
2023-07-01 00:20:53,616 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022530555725097656 sec
2023-07-01 00:20:53,617 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002231597900390625 sec
2023-07-01 00:20:53,618 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002560615539550781 sec
2023-07-01 00:20:53,618 - WARNING - Finished tracing + transforming _where for pjit in 0.0008273124694824219 sec
2023-07-01 00:20:53,619 - WARNING - Finished tracing + transforming fn for pjit in 0.0002560615539550781 sec
2023-07-01 00:20:53,619 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025081634521484375 sec
2023-07-01 00:20:53,620 - WARNING - Finished tracing + transforming fn for pjit in 0.00021791458129882812 sec
2023-07-01 00:20:53,624 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000270843505859375 sec
2023-07-01 00:20:53,625 - WARNING - Finished tracing + transforming fn for pjit in 0.0003426074981689453 sec
2023-07-01 00:20:53,626 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026106834411621094 sec
2023-07-01 00:20:53,626 - WARNING - Finished tracing + transforming fn for pjit in 0.00021910667419433594 sec
2023-07-01 00:20:53,630 - WARNING - Finished tracing + transforming fn for pjit in 0.00021886825561523438 sec
2023-07-01 00:20:53,632 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016260147094726562 sec
2023-07-01 00:20:53,633 - WARNING - Finished tracing + transforming fn for pjit in 0.0008945465087890625 sec
2023-07-01 00:20:53,634 - WARNING - Finished tracing + transforming fn for pjit in 0.00023293495178222656 sec
2023-07-01 00:20:53,651 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06728100776672363 sec
2023-07-01 00:20:53,654 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012350082397460938 sec
2023-07-01 00:20:53,655 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011348724365234375 sec
2023-07-01 00:20:53,655 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002646446228027344 sec
2023-07-01 00:20:53,657 - WARNING - Finished tracing + transforming fn for pjit in 0.00021719932556152344 sec
2023-07-01 00:20:53,658 - WARNING - Finished tracing + transforming fn for pjit in 0.0002503395080566406 sec
2023-07-01 00:20:53,659 - WARNING - Finished tracing + transforming fn for pjit in 0.0002148151397705078 sec
2023-07-01 00:20:53,665 - WARNING - Finished tracing + transforming fn for pjit in 0.00022363662719726562 sec
2023-07-01 00:20:53,666 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002181529998779297 sec
2023-07-01 00:20:53,666 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025081634521484375 sec
2023-07-01 00:20:53,667 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017023086547851562 sec
2023-07-01 00:20:53,668 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003223419189453125 sec
2023-07-01 00:20:53,669 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022220611572265625 sec
2023-07-01 00:20:53,669 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002193450927734375 sec
2023-07-01 00:20:53,670 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00026035308837890625 sec
2023-07-01 00:20:53,670 - WARNING - Finished tracing + transforming _where for pjit in 0.0008358955383300781 sec
2023-07-01 00:20:53,671 - WARNING - Finished tracing + transforming fn for pjit in 0.0002532005310058594 sec
2023-07-01 00:20:53,672 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025177001953125 sec
2023-07-01 00:20:53,672 - WARNING - Finished tracing + transforming fn for pjit in 0.00021505355834960938 sec
2023-07-01 00:20:53,673 - WARNING - Finished tracing + transforming fn for pjit in 0.00033092498779296875 sec
2023-07-01 00:20:53,685 - WARNING - Finished tracing + transforming fn for pjit in 0.0002193450927734375 sec
2023-07-01 00:20:53,705 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0529322624206543 sec
2023-07-01 00:20:53,706 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012445449829101562 sec
2023-07-01 00:20:53,707 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013589859008789062 sec
2023-07-01 00:20:53,707 - WARNING - Finished tracing + transforming _where for pjit in 0.0006060600280761719 sec
2023-07-01 00:20:53,708 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00028705596923828125 sec
2023-07-01 00:20:53,708 - WARNING - Finished tracing + transforming trace for pjit in 0.0024657249450683594 sec
2023-07-01 00:20:53,711 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010418891906738281 sec
2023-07-01 00:20:53,712 - WARNING - Finished tracing + transforming tril for pjit in 0.0006673336029052734 sec
2023-07-01 00:20:53,712 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0017740726470947266 sec
2023-07-01 00:20:53,713 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010728836059570312 sec
2023-07-01 00:20:53,713 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010657310485839844 sec
2023-07-01 00:20:53,715 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014052391052246094 sec
2023-07-01 00:20:53,719 - WARNING - Finished tracing + transforming _solve for pjit in 0.009116411209106445 sec
2023-07-01 00:20:53,720 - WARNING - Finished tracing + transforming dot for pjit in 0.00031375885009765625 sec
2023-07-01 00:20:53,722 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.13926935195922852 sec
2023-07-01 00:20:53,724 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:20:53,757 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03260445594787598 sec
2023-07-01 00:20:53,757 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:53,880 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1231534481048584 sec
2023-07-01 00:20:53,903 - INFO - initial test loss: 0.01804188091857634
2023-07-01 00:20:53,903 - INFO - initial test acc: 0.8100000023841858
2023-07-01 00:20:53,909 - WARNING - Finished tracing + transforming dot for pjit in 0.00037384033203125 sec
2023-07-01 00:20:53,910 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029468536376953125 sec
2023-07-01 00:20:53,911 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003705024719238281 sec
2023-07-01 00:20:53,911 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009927749633789062 sec
2023-07-01 00:20:53,912 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001957416534423828 sec
2023-07-01 00:20:53,913 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001983642578125 sec
2023-07-01 00:20:53,913 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024509429931640625 sec
2023-07-01 00:20:53,914 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.000370025634765625 sec
2023-07-01 00:20:53,915 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010631084442138672 sec
2023-07-01 00:20:53,916 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009826898574829102 sec
2023-07-01 00:20:53,924 - WARNING - Finished tracing + transforming fn for pjit in 0.0003123283386230469 sec
2023-07-01 00:20:53,925 - WARNING - Finished tracing + transforming fn for pjit in 0.00027108192443847656 sec
2023-07-01 00:20:53,925 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002181529998779297 sec
2023-07-01 00:20:53,926 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002646446228027344 sec
2023-07-01 00:20:53,927 - WARNING - Finished tracing + transforming _where for pjit in 0.0008497238159179688 sec
2023-07-01 00:20:53,935 - WARNING - Finished tracing + transforming fn for pjit in 0.0002620220184326172 sec
2023-07-01 00:20:53,936 - WARNING - Finished tracing + transforming fn for pjit in 0.00026726722717285156 sec
2023-07-01 00:20:53,937 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021791458129882812 sec
2023-07-01 00:20:53,937 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002484321594238281 sec
2023-07-01 00:20:53,938 - WARNING - Finished tracing + transforming _where for pjit in 0.0008182525634765625 sec
2023-07-01 00:20:53,972 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001499652862548828 sec
2023-07-01 00:20:54,028 - WARNING - Finished tracing + transforming fn for pjit in 0.0002682209014892578 sec
2023-07-01 00:20:54,029 - WARNING - Finished tracing + transforming fn for pjit in 0.00023293495178222656 sec
2023-07-01 00:20:54,030 - WARNING - Finished tracing + transforming square for pjit in 0.0001697540283203125 sec
2023-07-01 00:20:54,032 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021958351135253906 sec
2023-07-01 00:20:54,033 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024771690368652344 sec
2023-07-01 00:20:54,034 - WARNING - Finished tracing + transforming fn for pjit in 0.0002765655517578125 sec
2023-07-01 00:20:54,035 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022172927856445312 sec
2023-07-01 00:20:54,035 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002288818359375 sec
2023-07-01 00:20:54,036 - WARNING - Finished tracing + transforming fn for pjit in 0.0002620220184326172 sec
2023-07-01 00:20:54,037 - WARNING - Finished tracing + transforming fn for pjit in 0.0002276897430419922 sec
2023-07-01 00:20:54,037 - WARNING - Finished tracing + transforming square for pjit in 0.0001621246337890625 sec
2023-07-01 00:20:54,039 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021696090698242188 sec
2023-07-01 00:20:54,040 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001704692840576172 sec
2023-07-01 00:20:54,041 - WARNING - Finished tracing + transforming fn for pjit in 0.00027298927307128906 sec
2023-07-01 00:20:54,042 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021600723266601562 sec
2023-07-01 00:20:54,042 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022101402282714844 sec
2023-07-01 00:20:54,043 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13776469230651855 sec
2023-07-01 00:20:54,047 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:20:54,108 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06164050102233887 sec
2023-07-01 00:20:54,109 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:20:54,439 - WARNING - Finished XLA compilation of jit(update_fn) in 0.33046674728393555 sec
2023-07-01 00:20:55,705 - INFO - Distilling data from client: Client05
2023-07-01 00:20:55,706 - INFO - train loss: 0.0019406408740370881
2023-07-01 00:20:55,706 - INFO - train acc: 0.9961904883384705
2023-07-01 00:20:55,768 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.68      0.74        71
           2       0.75      0.86      0.80        64
           8       0.72      0.77      0.75        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.76       200
weighted avg       0.77      0.77      0.76       200

2023-07-01 00:20:55,768 - INFO - test loss 0.018053640408580648
2023-07-01 00:20:55,768 - INFO - test acc 0.7649999856948853
2023-07-01 00:20:57,029 - INFO - Distilling data from client: Client05
2023-07-01 00:20:57,029 - INFO - train loss: 0.0009889922117788624
2023-07-01 00:20:57,029 - INFO - train acc: 1.0
2023-07-01 00:20:57,093 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.68      0.76        71
           2       0.78      0.84      0.81        64
           8       0.71      0.83      0.77        65

    accuracy                           0.78       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.79      0.78      0.78       200

2023-07-01 00:20:57,093 - INFO - test loss 0.018329932600010725
2023-07-01 00:20:57,093 - INFO - test acc 0.7799999713897705
2023-07-01 00:20:58,343 - INFO - Distilling data from client: Client05
2023-07-01 00:20:58,343 - INFO - train loss: 0.0006984868238803798
2023-07-01 00:20:58,343 - INFO - train acc: 1.0
2023-07-01 00:20:58,367 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.58      0.68        71
           2       0.74      0.83      0.78        64
           8       0.67      0.82      0.74        65

    accuracy                           0.73       200
   macro avg       0.75      0.74      0.73       200
weighted avg       0.75      0.73      0.73       200

2023-07-01 00:20:58,367 - INFO - test loss 0.01870231270913656
2023-07-01 00:20:58,367 - INFO - test acc 0.73499995470047
2023-07-01 00:20:59,614 - INFO - Distilling data from client: Client05
2023-07-01 00:20:59,614 - INFO - train loss: 0.0006353362836797884
2023-07-01 00:20:59,614 - INFO - train acc: 1.0
2023-07-01 00:20:59,639 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.62      0.72        71
           2       0.79      0.88      0.83        64
           8       0.68      0.82      0.74        65

    accuracy                           0.77       200
   macro avg       0.78      0.77      0.76       200
weighted avg       0.78      0.77      0.76       200

2023-07-01 00:20:59,639 - INFO - test loss 0.018825322420429877
2023-07-01 00:20:59,639 - INFO - test acc 0.7649999856948853
2023-07-01 00:21:00,893 - INFO - Distilling data from client: Client05
2023-07-01 00:21:00,893 - INFO - train loss: 0.00045439463201124575
2023-07-01 00:21:00,893 - INFO - train acc: 1.0
2023-07-01 00:21:00,919 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.66      0.73        71
           2       0.76      0.81      0.79        64
           8       0.68      0.78      0.73        65

    accuracy                           0.75       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-07-01 00:21:00,919 - INFO - test loss 0.01923013445219197
2023-07-01 00:21:00,919 - INFO - test acc 0.75
2023-07-01 00:21:02,172 - INFO - Distilling data from client: Client05
2023-07-01 00:21:02,172 - INFO - train loss: 0.00045025526901192465
2023-07-01 00:21:02,172 - INFO - train acc: 1.0
2023-07-01 00:21:02,196 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.65      0.72        71
           2       0.78      0.84      0.81        64
           8       0.69      0.80      0.74        65

    accuracy                           0.76       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:21:02,196 - INFO - test loss 0.018464714740766667
2023-07-01 00:21:02,196 - INFO - test acc 0.7599999904632568
2023-07-01 00:21:03,450 - INFO - Distilling data from client: Client05
2023-07-01 00:21:03,450 - INFO - train loss: 0.00044723407502418784
2023-07-01 00:21:03,450 - INFO - train acc: 1.0
2023-07-01 00:21:03,474 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.66      0.72        71
           2       0.76      0.88      0.81        64
           8       0.70      0.72      0.71        65

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-07-01 00:21:03,474 - INFO - test loss 0.018955303190677775
2023-07-01 00:21:03,474 - INFO - test acc 0.75
2023-07-01 00:21:04,733 - INFO - Distilling data from client: Client05
2023-07-01 00:21:04,733 - INFO - train loss: 0.00031576559757442394
2023-07-01 00:21:04,734 - INFO - train acc: 1.0
2023-07-01 00:21:04,759 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.63      0.73        71
           2       0.79      0.86      0.82        64
           8       0.69      0.82      0.75        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.76       200
weighted avg       0.78      0.77      0.76       200

2023-07-01 00:21:04,759 - INFO - test loss 0.019158379371441485
2023-07-01 00:21:04,759 - INFO - test acc 0.7649999856948853
2023-07-01 00:21:06,015 - INFO - Distilling data from client: Client05
2023-07-01 00:21:06,016 - INFO - train loss: 0.0003493774631898421
2023-07-01 00:21:06,016 - INFO - train acc: 1.0
2023-07-01 00:21:06,041 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.61      0.70        71
           2       0.78      0.88      0.82        64
           8       0.68      0.80      0.74        65

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:21:06,041 - INFO - test loss 0.018490364174094064
2023-07-01 00:21:06,041 - INFO - test acc 0.7549999952316284
2023-07-01 00:21:07,295 - INFO - Distilling data from client: Client05
2023-07-01 00:21:07,296 - INFO - train loss: 0.00033751473082165696
2023-07-01 00:21:07,296 - INFO - train acc: 1.0
2023-07-01 00:21:07,358 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.66      0.74        71
           2       0.81      0.86      0.83        64
           8       0.74      0.86      0.79        65

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.80      0.79      0.79       200

2023-07-01 00:21:07,358 - INFO - test loss 0.018664819947072146
2023-07-01 00:21:07,358 - INFO - test acc 0.7899999618530273
2023-07-01 00:21:08,615 - INFO - Distilling data from client: Client05
2023-07-01 00:21:08,615 - INFO - train loss: 0.00037623191055685353
2023-07-01 00:21:08,615 - INFO - train acc: 1.0
2023-07-01 00:21:08,638 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.65      0.72        71
           2       0.79      0.88      0.83        64
           8       0.71      0.80      0.75        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-07-01 00:21:08,638 - INFO - test loss 0.018773206870465332
2023-07-01 00:21:08,638 - INFO - test acc 0.7699999809265137
2023-07-01 00:21:09,894 - INFO - Distilling data from client: Client05
2023-07-01 00:21:09,894 - INFO - train loss: 0.000297852055827694
2023-07-01 00:21:09,894 - INFO - train acc: 1.0
2023-07-01 00:21:09,923 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.65      0.73        71
           2       0.78      0.88      0.82        64
           8       0.68      0.77      0.72        65

    accuracy                           0.76       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:21:09,923 - INFO - test loss 0.019313675775912045
2023-07-01 00:21:09,923 - INFO - test acc 0.7599999904632568
2023-07-01 00:21:11,170 - INFO - Distilling data from client: Client05
2023-07-01 00:21:11,170 - INFO - train loss: 0.0002608369318514876
2023-07-01 00:21:11,170 - INFO - train acc: 1.0
2023-07-01 00:21:11,195 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.62      0.70        71
           2       0.76      0.83      0.79        64
           8       0.68      0.78      0.73        65

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:21:11,195 - INFO - test loss 0.018527661383460278
2023-07-01 00:21:11,195 - INFO - test acc 0.7400000095367432
2023-07-01 00:21:12,452 - INFO - Distilling data from client: Client05
2023-07-01 00:21:12,452 - INFO - train loss: 0.00023840638177855252
2023-07-01 00:21:12,452 - INFO - train acc: 1.0
2023-07-01 00:21:12,475 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.66      0.73        71
           2       0.76      0.86      0.81        64
           8       0.70      0.77      0.74        65

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:21:12,475 - INFO - test loss 0.01856818411391943
2023-07-01 00:21:12,475 - INFO - test acc 0.7599999904632568
2023-07-01 00:21:13,738 - INFO - Distilling data from client: Client05
2023-07-01 00:21:13,738 - INFO - train loss: 0.00029110830596017955
2023-07-01 00:21:13,738 - INFO - train acc: 1.0
2023-07-01 00:21:13,761 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.65      0.72        71
           2       0.82      0.86      0.84        64
           8       0.69      0.82      0.75        65

    accuracy                           0.77       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-07-01 00:21:13,762 - INFO - test loss 0.019040367738541564
2023-07-01 00:21:13,762 - INFO - test acc 0.7699999809265137
2023-07-01 00:21:15,022 - INFO - Distilling data from client: Client05
2023-07-01 00:21:15,022 - INFO - train loss: 0.0002213233262851871
2023-07-01 00:21:15,022 - INFO - train acc: 1.0
2023-07-01 00:21:15,045 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.65      0.73        71
           2       0.75      0.84      0.79        64
           8       0.67      0.75      0.71        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.76      0.74      0.74       200

2023-07-01 00:21:15,045 - INFO - test loss 0.019055854753885257
2023-07-01 00:21:15,045 - INFO - test acc 0.7450000047683716
2023-07-01 00:21:16,292 - INFO - Distilling data from client: Client05
2023-07-01 00:21:16,292 - INFO - train loss: 0.0002065485167061395
2023-07-01 00:21:16,292 - INFO - train acc: 1.0
2023-07-01 00:21:16,315 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.65      0.73        71
           2       0.79      0.88      0.83        64
           8       0.69      0.78      0.73        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.76       200
weighted avg       0.77      0.77      0.76       200

2023-07-01 00:21:16,316 - INFO - test loss 0.018968513106256402
2023-07-01 00:21:16,316 - INFO - test acc 0.7649999856948853
2023-07-01 00:21:17,549 - INFO - Distilling data from client: Client05
2023-07-01 00:21:17,549 - INFO - train loss: 0.00021927512805756147
2023-07-01 00:21:17,549 - INFO - train acc: 1.0
2023-07-01 00:21:17,577 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.65      0.73        71
           2       0.81      0.86      0.83        64
           8       0.69      0.82      0.75        65

    accuracy                           0.77       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-07-01 00:21:17,577 - INFO - test loss 0.01847777223531444
2023-07-01 00:21:17,577 - INFO - test acc 0.7699999809265137
2023-07-01 00:21:18,826 - INFO - Distilling data from client: Client05
2023-07-01 00:21:18,827 - INFO - train loss: 0.00016317044206580583
2023-07-01 00:21:18,827 - INFO - train acc: 1.0
2023-07-01 00:21:18,850 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.66      0.75        71
           2       0.80      0.89      0.84        64
           8       0.70      0.80      0.75        65

    accuracy                           0.78       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.79      0.78      0.78       200

2023-07-01 00:21:18,850 - INFO - test loss 0.018830864023059748
2023-07-01 00:21:18,850 - INFO - test acc 0.7799999713897705
2023-07-01 00:21:20,101 - INFO - Distilling data from client: Client05
2023-07-01 00:21:20,101 - INFO - train loss: 0.00018172783480997645
2023-07-01 00:21:20,101 - INFO - train acc: 1.0
2023-07-01 00:21:20,126 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.68      0.75        71
           2       0.81      0.88      0.84        64
           8       0.72      0.82      0.76        65

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.78       200
weighted avg       0.79      0.79      0.78       200

2023-07-01 00:21:20,126 - INFO - test loss 0.018496495555792237
2023-07-01 00:21:20,127 - INFO - test acc 0.7849999666213989
2023-07-01 00:21:21,381 - INFO - Distilling data from client: Client05
2023-07-01 00:21:21,381 - INFO - train loss: 0.0002172327509156947
2023-07-01 00:21:21,381 - INFO - train acc: 1.0
2023-07-01 00:21:21,405 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.66      0.74        71
           2       0.79      0.88      0.83        64
           8       0.70      0.78      0.74        65

    accuracy                           0.77       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-07-01 00:21:21,405 - INFO - test loss 0.01851321447414428
2023-07-01 00:21:21,405 - INFO - test acc 0.7699999809265137
2023-07-01 00:21:22,660 - INFO - Distilling data from client: Client05
2023-07-01 00:21:22,660 - INFO - train loss: 0.00016724978502040066
2023-07-01 00:21:22,660 - INFO - train acc: 1.0
2023-07-01 00:21:22,684 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.69      0.75        71
           2       0.75      0.84      0.79        64
           8       0.70      0.74      0.72        65

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:21:22,684 - INFO - test loss 0.018769726845846043
2023-07-01 00:21:22,684 - INFO - test acc 0.7549999952316284
2023-07-01 00:21:23,950 - INFO - Distilling data from client: Client05
2023-07-01 00:21:23,950 - INFO - train loss: 0.000159105460435719
2023-07-01 00:21:23,950 - INFO - train acc: 1.0
2023-07-01 00:21:23,974 - INFO - report:               precision    recall  f1-score   support

           1       0.81      0.68      0.74        71
           2       0.81      0.84      0.82        64
           8       0.66      0.75      0.71        65

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:21:23,974 - INFO - test loss 0.01870981248107691
2023-07-01 00:21:23,974 - INFO - test acc 0.7549999952316284
2023-07-01 00:21:25,237 - INFO - Distilling data from client: Client05
2023-07-01 00:21:25,237 - INFO - train loss: 0.00014810591537296695
2023-07-01 00:21:25,237 - INFO - train acc: 1.0
2023-07-01 00:21:25,260 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.68      0.76        71
           2       0.81      0.88      0.84        64
           8       0.71      0.82      0.76        65

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.78       200

2023-07-01 00:21:25,260 - INFO - test loss 0.019012843469321514
2023-07-01 00:21:25,260 - INFO - test acc 0.7849999666213989
2023-07-01 00:21:26,514 - INFO - Distilling data from client: Client05
2023-07-01 00:21:26,514 - INFO - train loss: 0.00015225522965630532
2023-07-01 00:21:26,514 - INFO - train acc: 1.0
2023-07-01 00:21:26,539 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.68      0.76        71
           2       0.81      0.84      0.82        64
           8       0.71      0.85      0.77        65

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.80      0.79      0.78       200

2023-07-01 00:21:26,539 - INFO - test loss 0.018573480328748213
2023-07-01 00:21:26,539 - INFO - test acc 0.7849999666213989
2023-07-01 00:21:26,541 - WARNING - Finished tracing + transforming jit(gather) in 0.0002357959747314453 sec
2023-07-01 00:21:26,542 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:21:26,543 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0011680126190185547 sec
2023-07-01 00:21:26,543 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:21:26,553 - WARNING - Finished XLA compilation of jit(gather) in 0.009696722030639648 sec
2023-07-01 00:21:26,564 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:26,572 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:26,581 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:26,589 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:26,598 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:26,606 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:26,615 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:26,624 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:26,633 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:27,001 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client05//synthetic.png
2023-07-01 00:21:27,012 - INFO - c: 4.0 and total_data_in_this_class: 272
2023-07-01 00:21:27,012 - INFO - c: 7.0 and total_data_in_this_class: 259
2023-07-01 00:21:27,012 - INFO - c: 8.0 and total_data_in_this_class: 268
2023-07-01 00:21:27,012 - INFO - c: 4.0 and total_data_in_this_class: 61
2023-07-01 00:21:27,012 - INFO - c: 7.0 and total_data_in_this_class: 74
2023-07-01 00:21:27,012 - INFO - c: 8.0 and total_data_in_this_class: 65
2023-07-01 00:21:27,084 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04843425750732422 sec
2023-07-01 00:21:27,130 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04539370536804199 sec
2023-07-01 00:21:27,136 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.10122346878051758 sec
2023-07-01 00:21:27,138 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:21:27,171 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.033217430114746094 sec
2023-07-01 00:21:27,171 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:21:27,295 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12324929237365723 sec
2023-07-01 00:21:27,318 - INFO - initial test loss: 0.0218890622711591
2023-07-01 00:21:27,318 - INFO - initial test acc: 0.7149999737739563
2023-07-01 00:21:27,327 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.00566554069519043 sec
2023-07-01 00:21:27,434 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11314988136291504 sec
2023-07-01 00:21:27,437 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:21:27,498 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.0610201358795166 sec
2023-07-01 00:21:27,499 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:21:27,835 - WARNING - Finished XLA compilation of jit(update_fn) in 0.33614277839660645 sec
2023-07-01 00:21:29,083 - INFO - Distilling data from client: Client06
2023-07-01 00:21:29,083 - INFO - train loss: 0.0024569279907317473
2023-07-01 00:21:29,084 - INFO - train acc: 0.9961464405059814
2023-07-01 00:21:29,142 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.72      0.67        61
           7       0.71      0.55      0.62        74
           8       0.77      0.85      0.81        65

    accuracy                           0.70       200
   macro avg       0.70      0.71      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:21:29,142 - INFO - test loss 0.020532277548713708
2023-07-01 00:21:29,142 - INFO - test acc 0.699999988079071
2023-07-01 00:21:30,382 - INFO - Distilling data from client: Client06
2023-07-01 00:21:30,382 - INFO - train loss: 0.0012059570838288194
2023-07-01 00:21:30,382 - INFO - train acc: 1.0
2023-07-01 00:21:30,406 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.70      0.64        61
           7       0.70      0.54      0.61        74
           8       0.77      0.82      0.79        65

    accuracy                           0.68       200
   macro avg       0.68      0.69      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:21:30,406 - INFO - test loss 0.02193742699185084
2023-07-01 00:21:30,407 - INFO - test acc 0.6800000071525574
2023-07-01 00:21:31,661 - INFO - Distilling data from client: Client06
2023-07-01 00:21:31,661 - INFO - train loss: 0.0010533486754289193
2023-07-01 00:21:31,661 - INFO - train acc: 1.0
2023-07-01 00:21:31,729 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.75      0.68        61
           7       0.75      0.57      0.65        74
           8       0.77      0.83      0.80        65

    accuracy                           0.71       200
   macro avg       0.71      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:21:31,729 - INFO - test loss 0.020938300899393747
2023-07-01 00:21:31,729 - INFO - test acc 0.7099999785423279
2023-07-01 00:21:32,979 - INFO - Distilling data from client: Client06
2023-07-01 00:21:32,979 - INFO - train loss: 0.0007528927753914631
2023-07-01 00:21:32,980 - INFO - train acc: 1.0
2023-07-01 00:21:33,005 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.72      0.65        61
           7       0.72      0.57      0.64        74
           8       0.78      0.80      0.79        65

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:21:33,005 - INFO - test loss 0.021349311996814023
2023-07-01 00:21:33,005 - INFO - test acc 0.6899999976158142
2023-07-01 00:21:34,256 - INFO - Distilling data from client: Client06
2023-07-01 00:21:34,257 - INFO - train loss: 0.0006340393498704389
2023-07-01 00:21:34,257 - INFO - train acc: 1.0
2023-07-01 00:21:34,281 - INFO - report:               precision    recall  f1-score   support

           4       0.54      0.70      0.61        61
           7       0.73      0.50      0.59        74
           8       0.75      0.80      0.78        65

    accuracy                           0.66       200
   macro avg       0.67      0.67      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-07-01 00:21:34,281 - INFO - test loss 0.022422911081265005
2023-07-01 00:21:34,281 - INFO - test acc 0.6599999666213989
2023-07-01 00:21:35,534 - INFO - Distilling data from client: Client06
2023-07-01 00:21:35,535 - INFO - train loss: 0.0006252945171598807
2023-07-01 00:21:35,535 - INFO - train acc: 1.0
2023-07-01 00:21:35,558 - INFO - report:               precision    recall  f1-score   support

           4       0.57      0.69      0.62        61
           7       0.69      0.58      0.63        74
           8       0.80      0.78      0.79        65

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:21:35,558 - INFO - test loss 0.02205908263411015
2023-07-01 00:21:35,558 - INFO - test acc 0.6800000071525574
2023-07-01 00:21:36,807 - INFO - Distilling data from client: Client06
2023-07-01 00:21:36,807 - INFO - train loss: 0.0006940783650402303
2023-07-01 00:21:36,807 - INFO - train acc: 1.0
2023-07-01 00:21:36,833 - INFO - report:               precision    recall  f1-score   support

           4       0.57      0.70      0.63        61
           7       0.72      0.55      0.63        74
           8       0.78      0.82      0.80        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.68       200

2023-07-01 00:21:36,833 - INFO - test loss 0.022235185238267355
2023-07-01 00:21:36,833 - INFO - test acc 0.6850000023841858
2023-07-01 00:21:38,077 - INFO - Distilling data from client: Client06
2023-07-01 00:21:38,077 - INFO - train loss: 0.0007242409553522825
2023-07-01 00:21:38,077 - INFO - train acc: 1.0
2023-07-01 00:21:38,104 - INFO - report:               precision    recall  f1-score   support

           4       0.57      0.72      0.64        61
           7       0.70      0.53      0.60        74
           8       0.78      0.80      0.79        65

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.67       200

2023-07-01 00:21:38,104 - INFO - test loss 0.022485300894185184
2023-07-01 00:21:38,104 - INFO - test acc 0.675000011920929
2023-07-01 00:21:39,351 - INFO - Distilling data from client: Client06
2023-07-01 00:21:39,351 - INFO - train loss: 0.0004972463174257922
2023-07-01 00:21:39,351 - INFO - train acc: 1.0
2023-07-01 00:21:39,374 - INFO - report:               precision    recall  f1-score   support

           4       0.55      0.69      0.61        61
           7       0.70      0.57      0.63        74
           8       0.78      0.77      0.78        65

    accuracy                           0.67       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:21:39,374 - INFO - test loss 0.02230138750612723
2023-07-01 00:21:39,374 - INFO - test acc 0.6699999570846558
2023-07-01 00:21:40,616 - INFO - Distilling data from client: Client06
2023-07-01 00:21:40,616 - INFO - train loss: 0.0004523370733996874
2023-07-01 00:21:40,616 - INFO - train acc: 1.0
2023-07-01 00:21:40,639 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.74      0.65        61
           7       0.73      0.58      0.65        74
           8       0.81      0.80      0.81        65

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:21:40,640 - INFO - test loss 0.02180725274510554
2023-07-01 00:21:40,640 - INFO - test acc 0.699999988079071
2023-07-01 00:21:41,890 - INFO - Distilling data from client: Client06
2023-07-01 00:21:41,891 - INFO - train loss: 0.0004546996444003519
2023-07-01 00:21:41,891 - INFO - train acc: 1.0
2023-07-01 00:21:41,914 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.67      0.63        61
           7       0.70      0.58      0.64        74
           8       0.74      0.80      0.77        65

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:21:41,914 - INFO - test loss 0.022250415265319914
2023-07-01 00:21:41,914 - INFO - test acc 0.6800000071525574
2023-07-01 00:21:43,160 - INFO - Distilling data from client: Client06
2023-07-01 00:21:43,160 - INFO - train loss: 0.0004019373790637381
2023-07-01 00:21:43,160 - INFO - train acc: 1.0
2023-07-01 00:21:43,183 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.72      0.65        61
           7       0.68      0.54      0.60        74
           8       0.80      0.82      0.81        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.68       200

2023-07-01 00:21:43,184 - INFO - test loss 0.02227138693198978
2023-07-01 00:21:43,184 - INFO - test acc 0.6850000023841858
2023-07-01 00:21:44,424 - INFO - Distilling data from client: Client06
2023-07-01 00:21:44,424 - INFO - train loss: 0.00036732994758510896
2023-07-01 00:21:44,425 - INFO - train acc: 1.0
2023-07-01 00:21:44,448 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.69      0.64        61
           7       0.70      0.59      0.64        74
           8       0.79      0.80      0.79        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:21:44,449 - INFO - test loss 0.021866655712861168
2023-07-01 00:21:44,449 - INFO - test acc 0.6899999976158142
2023-07-01 00:21:45,683 - INFO - Distilling data from client: Client06
2023-07-01 00:21:45,684 - INFO - train loss: 0.00026909463771084615
2023-07-01 00:21:45,684 - INFO - train acc: 1.0
2023-07-01 00:21:45,706 - INFO - report:               precision    recall  f1-score   support

           4       0.57      0.70      0.63        61
           7       0.69      0.54      0.61        74
           8       0.80      0.82      0.81        65

    accuracy                           0.68       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:21:45,707 - INFO - test loss 0.022409089696369098
2023-07-01 00:21:45,707 - INFO - test acc 0.6800000071525574
2023-07-01 00:21:46,958 - INFO - Distilling data from client: Client06
2023-07-01 00:21:46,958 - INFO - train loss: 0.0003228113773575992
2023-07-01 00:21:46,958 - INFO - train acc: 1.0
2023-07-01 00:21:47,022 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.74      0.68        61
           7       0.73      0.59      0.66        74
           8       0.78      0.83      0.81        65

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:21:47,022 - INFO - test loss 0.021492613160912145
2023-07-01 00:21:47,022 - INFO - test acc 0.7149999737739563
2023-07-01 00:21:48,259 - INFO - Distilling data from client: Client06
2023-07-01 00:21:48,260 - INFO - train loss: 0.00034381079973301804
2023-07-01 00:21:48,260 - INFO - train acc: 1.0
2023-07-01 00:21:48,283 - INFO - report:               precision    recall  f1-score   support

           4       0.57      0.70      0.63        61
           7       0.69      0.54      0.61        74
           8       0.79      0.80      0.79        65

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.67       200

2023-07-01 00:21:48,283 - INFO - test loss 0.02220600796825493
2023-07-01 00:21:48,283 - INFO - test acc 0.675000011920929
2023-07-01 00:21:49,521 - INFO - Distilling data from client: Client06
2023-07-01 00:21:49,521 - INFO - train loss: 0.0003202232383158456
2023-07-01 00:21:49,521 - INFO - train acc: 1.0
2023-07-01 00:21:49,545 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.72      0.64        61
           7       0.71      0.55      0.62        74
           8       0.79      0.80      0.79        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.68       200

2023-07-01 00:21:49,545 - INFO - test loss 0.02237559838759931
2023-07-01 00:21:49,546 - INFO - test acc 0.6850000023841858
2023-07-01 00:21:50,783 - INFO - Distilling data from client: Client06
2023-07-01 00:21:50,783 - INFO - train loss: 0.0003610572400725018
2023-07-01 00:21:50,783 - INFO - train acc: 1.0
2023-07-01 00:21:50,807 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.70      0.64        61
           7       0.70      0.58      0.64        74
           8       0.77      0.78      0.78        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.68       200

2023-07-01 00:21:50,807 - INFO - test loss 0.022515279440084947
2023-07-01 00:21:50,807 - INFO - test acc 0.6850000023841858
2023-07-01 00:21:52,045 - INFO - Distilling data from client: Client06
2023-07-01 00:21:52,045 - INFO - train loss: 0.00023733261878172918
2023-07-01 00:21:52,045 - INFO - train acc: 1.0
2023-07-01 00:21:52,068 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.67      0.62        61
           7       0.66      0.55      0.60        74
           8       0.79      0.82      0.80        65

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.67       200

2023-07-01 00:21:52,068 - INFO - test loss 0.022255568181126676
2023-07-01 00:21:52,068 - INFO - test acc 0.675000011920929
2023-07-01 00:21:53,303 - INFO - Distilling data from client: Client06
2023-07-01 00:21:53,303 - INFO - train loss: 0.0003205935636598807
2023-07-01 00:21:53,303 - INFO - train acc: 1.0
2023-07-01 00:21:53,327 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.74      0.65        61
           7       0.74      0.54      0.62        74
           8       0.78      0.82      0.80        65

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:21:53,327 - INFO - test loss 0.0222690947233772
2023-07-01 00:21:53,327 - INFO - test acc 0.6899999976158142
2023-07-01 00:21:54,578 - INFO - Distilling data from client: Client06
2023-07-01 00:21:54,579 - INFO - train loss: 0.00023930538084390853
2023-07-01 00:21:54,579 - INFO - train acc: 1.0
2023-07-01 00:21:54,603 - INFO - report:               precision    recall  f1-score   support

           4       0.56      0.72      0.63        61
           7       0.72      0.51      0.60        74
           8       0.75      0.80      0.78        65

    accuracy                           0.67       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:21:54,603 - INFO - test loss 0.022152167166854562
2023-07-01 00:21:54,603 - INFO - test acc 0.6699999570846558
2023-07-01 00:21:55,844 - INFO - Distilling data from client: Client06
2023-07-01 00:21:55,845 - INFO - train loss: 0.00024771259552025553
2023-07-01 00:21:55,845 - INFO - train acc: 1.0
2023-07-01 00:21:55,869 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.69      0.63        61
           7       0.69      0.55      0.62        74
           8       0.75      0.80      0.78        65

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.68      0.68      0.67       200

2023-07-01 00:21:55,869 - INFO - test loss 0.02171519478803067
2023-07-01 00:21:55,869 - INFO - test acc 0.675000011920929
2023-07-01 00:21:57,116 - INFO - Distilling data from client: Client06
2023-07-01 00:21:57,116 - INFO - train loss: 0.00025316667265460653
2023-07-01 00:21:57,116 - INFO - train acc: 1.0
2023-07-01 00:21:57,141 - INFO - report:               precision    recall  f1-score   support

           4       0.56      0.69      0.62        61
           7       0.70      0.53      0.60        74
           8       0.77      0.82      0.79        65

    accuracy                           0.67       200
   macro avg       0.67      0.68      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:21:57,141 - INFO - test loss 0.021951635066618118
2023-07-01 00:21:57,141 - INFO - test acc 0.6699999570846558
2023-07-01 00:21:58,381 - INFO - Distilling data from client: Client06
2023-07-01 00:21:58,381 - INFO - train loss: 0.00021999029300159723
2023-07-01 00:21:58,381 - INFO - train acc: 1.0
2023-07-01 00:21:58,404 - INFO - report:               precision    recall  f1-score   support

           4       0.58      0.69      0.63        61
           7       0.69      0.57      0.62        74
           8       0.79      0.82      0.80        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.68       200

2023-07-01 00:21:58,404 - INFO - test loss 0.02201614282533813
2023-07-01 00:21:58,404 - INFO - test acc 0.6850000023841858
2023-07-01 00:21:59,648 - INFO - Distilling data from client: Client06
2023-07-01 00:21:59,648 - INFO - train loss: 0.00034091984939124475
2023-07-01 00:21:59,648 - INFO - train acc: 1.0
2023-07-01 00:21:59,672 - INFO - report:               precision    recall  f1-score   support

           4       0.59      0.72      0.65        61
           7       0.71      0.54      0.62        74
           8       0.77      0.82      0.79        65

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.69      0.68       200

2023-07-01 00:21:59,672 - INFO - test loss 0.022492985808209368
2023-07-01 00:21:59,672 - INFO - test acc 0.6850000023841858
2023-07-01 00:21:59,684 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:59,693 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:59,702 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:59,710 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:59,719 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:59,728 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:59,737 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:59,746 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:21:59,755 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:22:00,128 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client06//synthetic.png
2023-07-01 00:22:00,141 - INFO - c: 2.0 and total_data_in_this_class: 277
2023-07-01 00:22:00,141 - INFO - c: 5.0 and total_data_in_this_class: 266
2023-07-01 00:22:00,141 - INFO - c: 8.0 and total_data_in_this_class: 256
2023-07-01 00:22:00,141 - INFO - c: 2.0 and total_data_in_this_class: 56
2023-07-01 00:22:00,141 - INFO - c: 5.0 and total_data_in_this_class: 67
2023-07-01 00:22:00,141 - INFO - c: 8.0 and total_data_in_this_class: 77
2023-07-01 00:22:00,162 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025534629821777344 sec
2023-07-01 00:22:00,162 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:22:00,164 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0011823177337646484 sec
2023-07-01 00:22:00,164 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:22:00,174 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010505437850952148 sec
2023-07-01 00:22:00,176 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025343894958496094 sec
2023-07-01 00:22:00,177 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:22:00,178 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0010349750518798828 sec
2023-07-01 00:22:00,178 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:22:00,187 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008404970169067383 sec
2023-07-01 00:22:00,190 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001373291015625 sec
2023-07-01 00:22:00,191 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011897087097167969 sec
2023-07-01 00:22:00,192 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00030541419982910156 sec
2023-07-01 00:22:00,193 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013017654418945312 sec
2023-07-01 00:22:00,194 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019788742065429688 sec
2023-07-01 00:22:00,194 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002789497375488281 sec
2023-07-01 00:22:00,195 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024080276489257812 sec
2023-07-01 00:22:00,195 - WARNING - Finished tracing + transforming absolute for pjit in 0.0001652240753173828 sec
2023-07-01 00:22:00,196 - WARNING - Finished tracing + transforming fn for pjit in 0.00027298927307128906 sec
2023-07-01 00:22:00,197 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00031495094299316406 sec
2023-07-01 00:22:00,197 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012922286987304688 sec
2023-07-01 00:22:00,198 - WARNING - Finished tracing + transforming fn for pjit in 0.00023126602172851562 sec
2023-07-01 00:22:00,199 - WARNING - Finished tracing + transforming fn for pjit in 0.0003387928009033203 sec
2023-07-01 00:22:00,200 - WARNING - Finished tracing + transforming fn for pjit in 0.0002300739288330078 sec
2023-07-01 00:22:00,200 - WARNING - Finished tracing + transforming fn for pjit in 0.0002639293670654297 sec
2023-07-01 00:22:00,202 - WARNING - Finished tracing + transforming fn for pjit in 0.00022482872009277344 sec
2023-07-01 00:22:00,203 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002384185791015625 sec
2023-07-01 00:22:00,204 - WARNING - Finished tracing + transforming fn for pjit in 0.00022792816162109375 sec
2023-07-01 00:22:00,205 - WARNING - Finished tracing + transforming fn for pjit in 0.0002384185791015625 sec
2023-07-01 00:22:00,208 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003466606140136719 sec
2023-07-01 00:22:00,209 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009322166442871094 sec
2023-07-01 00:22:00,210 - WARNING - Finished tracing + transforming fn for pjit in 0.00022721290588378906 sec
2023-07-01 00:22:00,210 - WARNING - Finished tracing + transforming fn for pjit in 0.0002181529998779297 sec
2023-07-01 00:22:00,211 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002238750457763672 sec
2023-07-01 00:22:00,212 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00034618377685546875 sec
2023-07-01 00:22:00,212 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00018095970153808594 sec
2023-07-01 00:22:00,213 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026345252990722656 sec
2023-07-01 00:22:00,214 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002295970916748047 sec
2023-07-01 00:22:00,214 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023031234741210938 sec
2023-07-01 00:22:00,215 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00026488304138183594 sec
2023-07-01 00:22:00,216 - WARNING - Finished tracing + transforming _where for pjit in 0.0008587837219238281 sec
2023-07-01 00:22:00,216 - WARNING - Finished tracing + transforming fn for pjit in 0.00026154518127441406 sec
2023-07-01 00:22:00,217 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000335693359375 sec
2023-07-01 00:22:00,218 - WARNING - Finished tracing + transforming fn for pjit in 0.00022745132446289062 sec
2023-07-01 00:22:00,218 - WARNING - Finished tracing + transforming fn for pjit in 0.00022745132446289062 sec
2023-07-01 00:22:00,219 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002200603485107422 sec
2023-07-01 00:22:00,220 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002627372741699219 sec
2023-07-01 00:22:00,220 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017714500427246094 sec
2023-07-01 00:22:00,221 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026106834411621094 sec
2023-07-01 00:22:00,222 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022602081298828125 sec
2023-07-01 00:22:00,223 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023484230041503906 sec
2023-07-01 00:22:00,223 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025916099548339844 sec
2023-07-01 00:22:00,224 - WARNING - Finished tracing + transforming _where for pjit in 0.0008478164672851562 sec
2023-07-01 00:22:00,225 - WARNING - Finished tracing + transforming fn for pjit in 0.0002636909484863281 sec
2023-07-01 00:22:00,225 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002582073211669922 sec
2023-07-01 00:22:00,226 - WARNING - Finished tracing + transforming fn for pjit in 0.0002224445343017578 sec
2023-07-01 00:22:00,230 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002701282501220703 sec
2023-07-01 00:22:00,231 - WARNING - Finished tracing + transforming fn for pjit in 0.00027060508728027344 sec
2023-07-01 00:22:00,232 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026416778564453125 sec
2023-07-01 00:22:00,233 - WARNING - Finished tracing + transforming fn for pjit in 0.0002968311309814453 sec
2023-07-01 00:22:00,236 - WARNING - Finished tracing + transforming fn for pjit in 0.0002224445343017578 sec
2023-07-01 00:22:00,238 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016307830810546875 sec
2023-07-01 00:22:00,239 - WARNING - Finished tracing + transforming fn for pjit in 0.0002956390380859375 sec
2023-07-01 00:22:00,239 - WARNING - Finished tracing + transforming fn for pjit in 0.0002315044403076172 sec
2023-07-01 00:22:00,257 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0677347183227539 sec
2023-07-01 00:22:00,261 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001239776611328125 sec
2023-07-01 00:22:00,261 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011873245239257812 sec
2023-07-01 00:22:00,261 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002613067626953125 sec
2023-07-01 00:22:00,263 - WARNING - Finished tracing + transforming fn for pjit in 0.00022077560424804688 sec
2023-07-01 00:22:00,264 - WARNING - Finished tracing + transforming fn for pjit in 0.0002486705780029297 sec
2023-07-01 00:22:00,265 - WARNING - Finished tracing + transforming fn for pjit in 0.00021195411682128906 sec
2023-07-01 00:22:00,271 - WARNING - Finished tracing + transforming fn for pjit in 0.0002319812774658203 sec
2023-07-01 00:22:00,272 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021457672119140625 sec
2023-07-01 00:22:00,273 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002567768096923828 sec
2023-07-01 00:22:00,273 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00016760826110839844 sec
2023-07-01 00:22:00,274 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003185272216796875 sec
2023-07-01 00:22:00,275 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002219676971435547 sec
2023-07-01 00:22:00,275 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021696090698242188 sec
2023-07-01 00:22:00,276 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002579689025878906 sec
2023-07-01 00:22:00,277 - WARNING - Finished tracing + transforming _where for pjit in 0.0008320808410644531 sec
2023-07-01 00:22:00,277 - WARNING - Finished tracing + transforming fn for pjit in 0.0002503395080566406 sec
2023-07-01 00:22:00,278 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002446174621582031 sec
2023-07-01 00:22:00,279 - WARNING - Finished tracing + transforming fn for pjit in 0.00021266937255859375 sec
2023-07-01 00:22:00,279 - WARNING - Finished tracing + transforming fn for pjit in 0.0002713203430175781 sec
2023-07-01 00:22:00,291 - WARNING - Finished tracing + transforming fn for pjit in 0.00021505355834960938 sec
2023-07-01 00:22:00,310 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05148196220397949 sec
2023-07-01 00:22:00,311 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011897087097167969 sec
2023-07-01 00:22:00,312 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013256072998046875 sec
2023-07-01 00:22:00,312 - WARNING - Finished tracing + transforming _where for pjit in 0.0006036758422851562 sec
2023-07-01 00:22:00,313 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00028967857360839844 sec
2023-07-01 00:22:00,313 - WARNING - Finished tracing + transforming trace for pjit in 0.0024466514587402344 sec
2023-07-01 00:22:00,315 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010132789611816406 sec
2023-07-01 00:22:00,316 - WARNING - Finished tracing + transforming tril for pjit in 0.0006458759307861328 sec
2023-07-01 00:22:00,317 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0019273757934570312 sec
2023-07-01 00:22:00,318 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00011229515075683594 sec
2023-07-01 00:22:00,318 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010418891906738281 sec
2023-07-01 00:22:00,320 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.001405954360961914 sec
2023-07-01 00:22:00,324 - WARNING - Finished tracing + transforming _solve for pjit in 0.009174346923828125 sec
2023-07-01 00:22:00,324 - WARNING - Finished tracing + transforming dot for pjit in 0.0003173351287841797 sec
2023-07-01 00:22:00,327 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.13876676559448242 sec
2023-07-01 00:22:00,329 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:22:00,361 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.0322263240814209 sec
2023-07-01 00:22:00,361 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:22:00,483 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12175655364990234 sec
2023-07-01 00:22:00,506 - INFO - initial test loss: 0.02413335062602263
2023-07-01 00:22:00,506 - INFO - initial test acc: 0.6800000071525574
2023-07-01 00:22:00,513 - WARNING - Finished tracing + transforming dot for pjit in 0.00036263465881347656 sec
2023-07-01 00:22:00,513 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002913475036621094 sec
2023-07-01 00:22:00,514 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003643035888671875 sec
2023-07-01 00:22:00,515 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009770393371582031 sec
2023-07-01 00:22:00,516 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001919269561767578 sec
2023-07-01 00:22:00,516 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001800060272216797 sec
2023-07-01 00:22:00,517 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025153160095214844 sec
2023-07-01 00:22:00,518 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003726482391357422 sec
2023-07-01 00:22:00,518 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010585784912109375 sec
2023-07-01 00:22:00,519 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009685516357421875 sec
2023-07-01 00:22:00,527 - WARNING - Finished tracing + transforming fn for pjit in 0.00030541419982910156 sec
2023-07-01 00:22:00,528 - WARNING - Finished tracing + transforming fn for pjit in 0.00025343894958496094 sec
2023-07-01 00:22:00,528 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021266937255859375 sec
2023-07-01 00:22:00,529 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002627372741699219 sec
2023-07-01 00:22:00,530 - WARNING - Finished tracing + transforming _where for pjit in 0.0008466243743896484 sec
2023-07-01 00:22:00,538 - WARNING - Finished tracing + transforming fn for pjit in 0.0002505779266357422 sec
2023-07-01 00:22:00,538 - WARNING - Finished tracing + transforming fn for pjit in 0.0002579689025878906 sec
2023-07-01 00:22:00,539 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002079010009765625 sec
2023-07-01 00:22:00,540 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00024819374084472656 sec
2023-07-01 00:22:00,540 - WARNING - Finished tracing + transforming _where for pjit in 0.0008053779602050781 sec
2023-07-01 00:22:00,574 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00020933151245117188 sec
2023-07-01 00:22:00,628 - WARNING - Finished tracing + transforming fn for pjit in 0.0002677440643310547 sec
2023-07-01 00:22:00,629 - WARNING - Finished tracing + transforming fn for pjit in 0.00023102760314941406 sec
2023-07-01 00:22:00,630 - WARNING - Finished tracing + transforming square for pjit in 0.00016498565673828125 sec
2023-07-01 00:22:00,632 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022125244140625 sec
2023-07-01 00:22:00,633 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000247955322265625 sec
2023-07-01 00:22:00,634 - WARNING - Finished tracing + transforming fn for pjit in 0.0002658367156982422 sec
2023-07-01 00:22:00,635 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022149085998535156 sec
2023-07-01 00:22:00,635 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023055076599121094 sec
2023-07-01 00:22:00,636 - WARNING - Finished tracing + transforming fn for pjit in 0.0002675056457519531 sec
2023-07-01 00:22:00,636 - WARNING - Finished tracing + transforming fn for pjit in 0.00022840499877929688 sec
2023-07-01 00:22:00,637 - WARNING - Finished tracing + transforming square for pjit in 0.00016760826110839844 sec
2023-07-01 00:22:00,639 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021600723266601562 sec
2023-07-01 00:22:00,640 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017142295837402344 sec
2023-07-01 00:22:00,641 - WARNING - Finished tracing + transforming fn for pjit in 0.0002601146697998047 sec
2023-07-01 00:22:00,642 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002186298370361328 sec
2023-07-01 00:22:00,642 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002262592315673828 sec
2023-07-01 00:22:00,643 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13435959815979004 sec
2023-07-01 00:22:00,647 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:22:00,708 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06080365180969238 sec
2023-07-01 00:22:00,708 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:22:01,034 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32636451721191406 sec
2023-07-01 00:22:02,261 - INFO - Distilling data from client: Client07
2023-07-01 00:22:02,261 - INFO - train loss: 0.003125137660858208
2023-07-01 00:22:02,261 - INFO - train acc: 0.9902534484863281
2023-07-01 00:22:02,316 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.66      0.60        56
           5       0.68      0.60      0.63        67
           8       0.81      0.77      0.79        77

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:22:02,317 - INFO - test loss 0.02307388647745684
2023-07-01 00:22:02,317 - INFO - test acc 0.6800000071525574
2023-07-01 00:22:03,548 - INFO - Distilling data from client: Client07
2023-07-01 00:22:03,548 - INFO - train loss: 0.0016466450936451957
2023-07-01 00:22:03,548 - INFO - train acc: 0.9961013793945312
2023-07-01 00:22:03,624 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.62      0.57        56
           5       0.70      0.60      0.65        67
           8       0.82      0.81      0.81        77

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:22:03,624 - INFO - test loss 0.022133899765939036
2023-07-01 00:22:03,624 - INFO - test acc 0.6850000023841858
2023-07-01 00:22:04,857 - INFO - Distilling data from client: Client07
2023-07-01 00:22:04,858 - INFO - train loss: 0.0012577700558912494
2023-07-01 00:22:04,858 - INFO - train acc: 1.0
2023-07-01 00:22:04,881 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.62      0.57        56
           5       0.67      0.57      0.61        67
           8       0.79      0.78      0.78        77

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:22:04,882 - INFO - test loss 0.02322694478650535
2023-07-01 00:22:04,882 - INFO - test acc 0.6649999618530273
2023-07-01 00:22:06,110 - INFO - Distilling data from client: Client07
2023-07-01 00:22:06,110 - INFO - train loss: 0.000955214803921342
2023-07-01 00:22:06,110 - INFO - train acc: 1.0
2023-07-01 00:22:06,134 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.61      0.55        56
           5       0.67      0.60      0.63        67
           8       0.81      0.77      0.79        77

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:22:06,134 - INFO - test loss 0.023429057768463997
2023-07-01 00:22:06,134 - INFO - test acc 0.6649999618530273
2023-07-01 00:22:07,351 - INFO - Distilling data from client: Client07
2023-07-01 00:22:07,352 - INFO - train loss: 0.0008461274458452431
2023-07-01 00:22:07,352 - INFO - train acc: 1.0
2023-07-01 00:22:07,375 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.68      0.60        56
           5       0.68      0.60      0.63        67
           8       0.83      0.75      0.79        77

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.70      0.68      0.68       200

2023-07-01 00:22:07,376 - INFO - test loss 0.023920677179755774
2023-07-01 00:22:07,376 - INFO - test acc 0.6800000071525574
2023-07-01 00:22:08,596 - INFO - Distilling data from client: Client07
2023-07-01 00:22:08,596 - INFO - train loss: 0.0006546909811612468
2023-07-01 00:22:08,596 - INFO - train acc: 1.0
2023-07-01 00:22:08,620 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.68      0.58        56
           5       0.69      0.55      0.61        67
           8       0.83      0.77      0.80        77

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.66       200
weighted avg       0.69      0.67      0.67       200

2023-07-01 00:22:08,621 - INFO - test loss 0.024688959821542764
2023-07-01 00:22:08,621 - INFO - test acc 0.6699999570846558
2023-07-01 00:22:09,845 - INFO - Distilling data from client: Client07
2023-07-01 00:22:09,845 - INFO - train loss: 0.000874354504141688
2023-07-01 00:22:09,845 - INFO - train acc: 1.0
2023-07-01 00:22:09,869 - INFO - report:               precision    recall  f1-score   support

           2       0.55      0.66      0.60        56
           5       0.64      0.58      0.61        67
           8       0.82      0.77      0.79        77

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:22:09,869 - INFO - test loss 0.024200930905495254
2023-07-01 00:22:09,901 - INFO - test acc 0.675000011920929
2023-07-01 00:22:11,127 - INFO - Distilling data from client: Client07
2023-07-01 00:22:11,127 - INFO - train loss: 0.0007156299467129558
2023-07-01 00:22:11,127 - INFO - train acc: 1.0
2023-07-01 00:22:11,151 - INFO - report:               precision    recall  f1-score   support

           2       0.49      0.59      0.54        56
           5       0.67      0.60      0.63        67
           8       0.81      0.77      0.79        77

    accuracy                           0.66       200
   macro avg       0.66      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:22:11,151 - INFO - test loss 0.024571266582068083
2023-07-01 00:22:11,151 - INFO - test acc 0.6599999666213989
2023-07-01 00:22:12,381 - INFO - Distilling data from client: Client07
2023-07-01 00:22:12,381 - INFO - train loss: 0.0005676648148452274
2023-07-01 00:22:12,381 - INFO - train acc: 1.0
2023-07-01 00:22:12,404 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.62      0.57        56
           5       0.71      0.61      0.66        67
           8       0.80      0.78      0.79        77

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:22:12,404 - INFO - test loss 0.023864791627678086
2023-07-01 00:22:12,404 - INFO - test acc 0.6800000071525574
2023-07-01 00:22:13,636 - INFO - Distilling data from client: Client07
2023-07-01 00:22:13,636 - INFO - train loss: 0.0004933935466375313
2023-07-01 00:22:13,636 - INFO - train acc: 1.0
2023-07-01 00:22:13,662 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.68      0.60        56
           5       0.68      0.58      0.63        67
           8       0.81      0.75      0.78        77

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:22:13,662 - INFO - test loss 0.0240419405368688
2023-07-01 00:22:13,662 - INFO - test acc 0.675000011920929
2023-07-01 00:22:14,891 - INFO - Distilling data from client: Client07
2023-07-01 00:22:14,891 - INFO - train loss: 0.0006069319125697021
2023-07-01 00:22:14,920 - INFO - train acc: 1.0
2023-07-01 00:22:14,943 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.62      0.56        56
           5       0.62      0.57      0.59        67
           8       0.81      0.74      0.78        77

    accuracy                           0.65       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.66      0.65      0.65       200

2023-07-01 00:22:14,943 - INFO - test loss 0.024650610576066337
2023-07-01 00:22:14,943 - INFO - test acc 0.6499999761581421
2023-07-01 00:22:16,177 - INFO - Distilling data from client: Client07
2023-07-01 00:22:16,178 - INFO - train loss: 0.0006148424165622837
2023-07-01 00:22:16,178 - INFO - train acc: 1.0
2023-07-01 00:22:16,201 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.61      0.56        56
           5       0.67      0.61      0.64        67
           8       0.81      0.77      0.79        77

    accuracy                           0.67       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:22:16,201 - INFO - test loss 0.024063287244489587
2023-07-01 00:22:16,201 - INFO - test acc 0.6699999570846558
2023-07-01 00:22:17,427 - INFO - Distilling data from client: Client07
2023-07-01 00:22:17,427 - INFO - train loss: 0.0004706226388460575
2023-07-01 00:22:17,427 - INFO - train acc: 1.0
2023-07-01 00:22:17,451 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.66      0.57        56
           5       0.64      0.54      0.59        67
           8       0.80      0.74      0.77        77

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.64       200
weighted avg       0.67      0.65      0.65       200

2023-07-01 00:22:17,451 - INFO - test loss 0.024769372295422854
2023-07-01 00:22:17,451 - INFO - test acc 0.6499999761581421
2023-07-01 00:22:18,677 - INFO - Distilling data from client: Client07
2023-07-01 00:22:18,677 - INFO - train loss: 0.0004679909682340417
2023-07-01 00:22:18,677 - INFO - train acc: 1.0
2023-07-01 00:22:18,700 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.66      0.58        56
           5       0.64      0.52      0.57        67
           8       0.80      0.77      0.78        77

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:22:18,700 - INFO - test loss 0.024203681620052358
2023-07-01 00:22:18,700 - INFO - test acc 0.6549999713897705
2023-07-01 00:22:19,914 - INFO - Distilling data from client: Client07
2023-07-01 00:22:19,914 - INFO - train loss: 0.00044668205995299824
2023-07-01 00:22:19,914 - INFO - train acc: 1.0
2023-07-01 00:22:19,937 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.64      0.57        56
           5       0.65      0.55      0.60        67
           8       0.81      0.77      0.79        77

    accuracy                           0.66       200
   macro avg       0.66      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:22:19,937 - INFO - test loss 0.024608708935868456
2023-07-01 00:22:19,937 - INFO - test acc 0.6599999666213989
2023-07-01 00:22:21,167 - INFO - Distilling data from client: Client07
2023-07-01 00:22:21,167 - INFO - train loss: 0.0004567348669063685
2023-07-01 00:22:21,167 - INFO - train acc: 1.0
2023-07-01 00:22:21,191 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.66      0.57        56
           5       0.65      0.54      0.59        67
           8       0.82      0.77      0.79        77

    accuracy                           0.66       200
   macro avg       0.66      0.65      0.65       200
weighted avg       0.68      0.66      0.66       200

2023-07-01 00:22:21,191 - INFO - test loss 0.024026470610686794
2023-07-01 00:22:21,191 - INFO - test acc 0.6599999666213989
2023-07-01 00:22:22,413 - INFO - Distilling data from client: Client07
2023-07-01 00:22:22,413 - INFO - train loss: 0.0004979802222419035
2023-07-01 00:22:22,413 - INFO - train acc: 1.0
2023-07-01 00:22:22,437 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.64      0.57        56
           5       0.64      0.55      0.59        67
           8       0.82      0.75      0.78        77

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:22:22,437 - INFO - test loss 0.02395637783441826
2023-07-01 00:22:22,437 - INFO - test acc 0.6549999713897705
2023-07-01 00:22:23,656 - INFO - Distilling data from client: Client07
2023-07-01 00:22:23,656 - INFO - train loss: 0.0004914772554462907
2023-07-01 00:22:23,656 - INFO - train acc: 1.0
2023-07-01 00:22:23,681 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.66      0.57        56
           5       0.67      0.55      0.61        67
           8       0.81      0.75      0.78        77

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.65       200
weighted avg       0.68      0.66      0.66       200

2023-07-01 00:22:23,681 - INFO - test loss 0.024944235915295265
2023-07-01 00:22:23,681 - INFO - test acc 0.6599999666213989
2023-07-01 00:22:24,901 - INFO - Distilling data from client: Client07
2023-07-01 00:22:24,901 - INFO - train loss: 0.0003899602237554627
2023-07-01 00:22:24,901 - INFO - train acc: 1.0
2023-07-01 00:22:24,924 - INFO - report:               precision    recall  f1-score   support

           2       0.50      0.57      0.53        56
           5       0.61      0.58      0.60        67
           8       0.81      0.75      0.78        77

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:22:24,924 - INFO - test loss 0.02454481994273836
2023-07-01 00:22:24,924 - INFO - test acc 0.6449999809265137
2023-07-01 00:22:26,158 - INFO - Distilling data from client: Client07
2023-07-01 00:22:26,158 - INFO - train loss: 0.0004031702890090289
2023-07-01 00:22:26,158 - INFO - train acc: 1.0
2023-07-01 00:22:26,185 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.66      0.59        56
           5       0.63      0.57      0.60        67
           8       0.81      0.74      0.78        77

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:22:26,185 - INFO - test loss 0.024801665640720325
2023-07-01 00:22:26,185 - INFO - test acc 0.6599999666213989
2023-07-01 00:22:27,408 - INFO - Distilling data from client: Client07
2023-07-01 00:22:27,408 - INFO - train loss: 0.0003586467567566224
2023-07-01 00:22:27,408 - INFO - train acc: 1.0
2023-07-01 00:22:27,431 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.62      0.56        56
           5       0.65      0.58      0.61        67
           8       0.82      0.77      0.79        77

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:22:27,431 - INFO - test loss 0.023961436587641494
2023-07-01 00:22:27,431 - INFO - test acc 0.6649999618530273
2023-07-01 00:22:28,656 - INFO - Distilling data from client: Client07
2023-07-01 00:22:28,656 - INFO - train loss: 0.00028078695361308503
2023-07-01 00:22:28,656 - INFO - train acc: 1.0
2023-07-01 00:22:28,679 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.64      0.57        56
           5       0.64      0.57      0.60        67
           8       0.82      0.75      0.78        77

    accuracy                           0.66       200
   macro avg       0.66      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:22:28,679 - INFO - test loss 0.024855198266533072
2023-07-01 00:22:28,679 - INFO - test acc 0.6599999666213989
2023-07-01 00:22:29,927 - INFO - Distilling data from client: Client07
2023-07-01 00:22:29,927 - INFO - train loss: 0.00038833811705518706
2023-07-01 00:22:29,927 - INFO - train acc: 1.0
2023-07-01 00:22:29,951 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.64      0.57        56
           5       0.66      0.58      0.62        67
           8       0.81      0.74      0.78        77

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.65       200
weighted avg       0.68      0.66      0.66       200

2023-07-01 00:22:29,952 - INFO - test loss 0.024158520461922118
2023-07-01 00:22:29,952 - INFO - test acc 0.6599999666213989
2023-07-01 00:22:31,198 - INFO - Distilling data from client: Client07
2023-07-01 00:22:31,198 - INFO - train loss: 0.0004315864762786751
2023-07-01 00:22:31,198 - INFO - train acc: 1.0
2023-07-01 00:22:31,221 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.66      0.59        56
           5       0.68      0.61      0.65        67
           8       0.83      0.77      0.80        77

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:22:31,222 - INFO - test loss 0.024432781955929208
2023-07-01 00:22:31,222 - INFO - test acc 0.6850000023841858
2023-07-01 00:22:32,460 - INFO - Distilling data from client: Client07
2023-07-01 00:22:32,460 - INFO - train loss: 0.0003244916779290405
2023-07-01 00:22:32,460 - INFO - train acc: 1.0
2023-07-01 00:22:32,485 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.68      0.60        56
           5       0.66      0.58      0.62        67
           8       0.83      0.75      0.79        77

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:22:32,485 - INFO - test loss 0.024633620085243585
2023-07-01 00:22:32,485 - INFO - test acc 0.675000011920929
2023-07-01 00:22:32,488 - WARNING - Finished tracing + transforming jit(gather) in 0.00023627281188964844 sec
2023-07-01 00:22:32,488 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:22:32,489 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0012049674987792969 sec
2023-07-01 00:22:32,489 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:22:32,499 - WARNING - Finished XLA compilation of jit(gather) in 0.009798526763916016 sec
2023-07-01 00:22:32,510 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:22:32,519 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:22:32,528 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:22:32,537 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:22:32,545 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:22:32,554 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:22:32,564 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:22:32,573 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:22:32,582 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:22:32,966 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client07//synthetic.png
2023-07-01 00:22:32,977 - INFO - c: 0.0 and total_data_in_this_class: 260
2023-07-01 00:22:32,977 - INFO - c: 1.0 and total_data_in_this_class: 269
2023-07-01 00:22:32,977 - INFO - c: 5.0 and total_data_in_this_class: 270
2023-07-01 00:22:32,977 - INFO - c: 0.0 and total_data_in_this_class: 73
2023-07-01 00:22:32,977 - INFO - c: 1.0 and total_data_in_this_class: 64
2023-07-01 00:22:32,977 - INFO - c: 5.0 and total_data_in_this_class: 63
2023-07-01 00:22:33,047 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0476224422454834 sec
2023-07-01 00:22:33,094 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04508471488952637 sec
2023-07-01 00:22:33,098 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.09972167015075684 sec
2023-07-01 00:22:33,100 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:22:33,133 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03259849548339844 sec
2023-07-01 00:22:33,133 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:22:33,258 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12438559532165527 sec
2023-07-01 00:22:33,281 - INFO - initial test loss: 0.022924551037423156
2023-07-01 00:22:33,281 - INFO - initial test acc: 0.75
2023-07-01 00:22:33,289 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.005709648132324219 sec
2023-07-01 00:22:33,398 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11471223831176758 sec
2023-07-01 00:22:33,401 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:22:33,464 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.062444210052490234 sec
2023-07-01 00:22:33,464 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:22:33,798 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3340141773223877 sec
2023-07-01 00:22:35,049 - INFO - Distilling data from client: Client08
2023-07-01 00:22:35,049 - INFO - train loss: 0.001707038411079494
2023-07-01 00:22:35,049 - INFO - train acc: 0.9980732202529907
2023-07-01 00:22:35,111 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.78      0.78        73
           1       0.74      0.72      0.73        64
           5       0.80      0.81      0.80        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:22:35,111 - INFO - test loss 0.0195849901518431
2023-07-01 00:22:35,111 - INFO - test acc 0.7699999809265137
2023-07-01 00:22:36,351 - INFO - Distilling data from client: Client08
2023-07-01 00:22:36,351 - INFO - train loss: 0.0008966371248022915
2023-07-01 00:22:36,351 - INFO - train acc: 1.0
2023-07-01 00:22:36,375 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.74      0.74        73
           1       0.74      0.70      0.72        64
           5       0.77      0.81      0.79        63

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-07-01 00:22:36,375 - INFO - test loss 0.019344997356881586
2023-07-01 00:22:36,375 - INFO - test acc 0.75
2023-07-01 00:22:37,617 - INFO - Distilling data from client: Client08
2023-07-01 00:22:37,617 - INFO - train loss: 0.0007151586190056666
2023-07-01 00:22:37,617 - INFO - train acc: 1.0
2023-07-01 00:22:37,640 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.79      0.77        73
           1       0.75      0.70      0.73        64
           5       0.79      0.78      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:22:37,640 - INFO - test loss 0.019867558041393684
2023-07-01 00:22:37,640 - INFO - test acc 0.7599999904632568
2023-07-01 00:22:38,884 - INFO - Distilling data from client: Client08
2023-07-01 00:22:38,884 - INFO - train loss: 0.0005283992637796141
2023-07-01 00:22:38,884 - INFO - train acc: 1.0
2023-07-01 00:22:38,908 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.74      0.75        73
           1       0.75      0.72      0.74        64
           5       0.75      0.81      0.78        63

    accuracy                           0.76       200
   macro avg       0.75      0.76      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:22:38,908 - INFO - test loss 0.01968950460507034
2023-07-01 00:22:38,909 - INFO - test acc 0.7549999952316284
2023-07-01 00:22:40,146 - INFO - Distilling data from client: Client08
2023-07-01 00:22:40,147 - INFO - train loss: 0.0004392995186877509
2023-07-01 00:22:40,147 - INFO - train acc: 1.0
2023-07-01 00:22:40,170 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.78      0.76        73
           1       0.78      0.73      0.76        64
           5       0.79      0.79      0.79        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:22:40,170 - INFO - test loss 0.019210309554803913
2023-07-01 00:22:40,170 - INFO - test acc 0.7699999809265137
2023-07-01 00:22:41,409 - INFO - Distilling data from client: Client08
2023-07-01 00:22:41,409 - INFO - train loss: 0.00046128509421051555
2023-07-01 00:22:41,409 - INFO - train acc: 1.0
2023-07-01 00:22:41,433 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.75      0.75        73
           1       0.74      0.67      0.70        64
           5       0.74      0.79      0.76        63

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:22:41,433 - INFO - test loss 0.019907555395004942
2023-07-01 00:22:41,433 - INFO - test acc 0.7400000095367432
2023-07-01 00:22:42,670 - INFO - Distilling data from client: Client08
2023-07-01 00:22:42,670 - INFO - train loss: 0.0003620566332317774
2023-07-01 00:22:42,670 - INFO - train acc: 1.0
2023-07-01 00:22:42,695 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.77      0.76        73
           1       0.77      0.72      0.74        64
           5       0.75      0.78      0.77        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:22:42,695 - INFO - test loss 0.019659716505180967
2023-07-01 00:22:42,696 - INFO - test acc 0.7549999952316284
2023-07-01 00:22:43,941 - INFO - Distilling data from client: Client08
2023-07-01 00:22:43,941 - INFO - train loss: 0.00028883081379339683
2023-07-01 00:22:43,941 - INFO - train acc: 1.0
2023-07-01 00:22:44,003 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.78      0.78        73
           1       0.77      0.73      0.75        64
           5       0.79      0.83      0.81        63

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:22:44,003 - INFO - test loss 0.019783724651784303
2023-07-01 00:22:44,003 - INFO - test acc 0.7799999713897705
2023-07-01 00:22:45,238 - INFO - Distilling data from client: Client08
2023-07-01 00:22:45,238 - INFO - train loss: 0.00032509529787859686
2023-07-01 00:22:45,238 - INFO - train acc: 1.0
2023-07-01 00:22:45,299 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.78      0.78        73
           1       0.78      0.72      0.75        64
           5       0.81      0.86      0.83        63

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.78       200
weighted avg       0.78      0.79      0.78       200

2023-07-01 00:22:45,299 - INFO - test loss 0.01974256087245774
2023-07-01 00:22:45,299 - INFO - test acc 0.7849999666213989
2023-07-01 00:22:46,527 - INFO - Distilling data from client: Client08
2023-07-01 00:22:46,527 - INFO - train loss: 0.0003625724090926354
2023-07-01 00:22:46,527 - INFO - train acc: 1.0
2023-07-01 00:22:46,550 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.77      0.75        73
           1       0.77      0.72      0.74        64
           5       0.78      0.78      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.76       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:22:46,550 - INFO - test loss 0.020298933790505912
2023-07-01 00:22:46,550 - INFO - test acc 0.7549999952316284
2023-07-01 00:22:47,797 - INFO - Distilling data from client: Client08
2023-07-01 00:22:47,797 - INFO - train loss: 0.00032885659861456475
2023-07-01 00:22:47,797 - INFO - train acc: 1.0
2023-07-01 00:22:47,820 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.77      0.75        73
           1       0.75      0.72      0.74        64
           5       0.78      0.78      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.76       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:22:47,821 - INFO - test loss 0.019805340515899973
2023-07-01 00:22:47,821 - INFO - test acc 0.7549999952316284
2023-07-01 00:22:49,066 - INFO - Distilling data from client: Client08
2023-07-01 00:22:49,066 - INFO - train loss: 0.00023119449174523703
2023-07-01 00:22:49,066 - INFO - train acc: 1.0
2023-07-01 00:22:49,089 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.78      0.75        73
           1       0.78      0.72      0.75        64
           5       0.78      0.78      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:22:49,089 - INFO - test loss 0.019966636919904978
2023-07-01 00:22:49,089 - INFO - test acc 0.7599999904632568
2023-07-01 00:22:50,325 - INFO - Distilling data from client: Client08
2023-07-01 00:22:50,325 - INFO - train loss: 0.00023509858310328825
2023-07-01 00:22:50,325 - INFO - train acc: 1.0
2023-07-01 00:22:50,351 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.75      0.75        73
           1       0.77      0.73      0.75        64
           5       0.79      0.83      0.81        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:22:50,351 - INFO - test loss 0.019938912382021536
2023-07-01 00:22:50,352 - INFO - test acc 0.7699999809265137
2023-07-01 00:22:51,585 - INFO - Distilling data from client: Client08
2023-07-01 00:22:51,585 - INFO - train loss: 0.00020938982854298673
2023-07-01 00:22:51,585 - INFO - train acc: 1.0
2023-07-01 00:22:51,609 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.74      0.75        73
           1       0.75      0.73      0.74        64
           5       0.77      0.81      0.79        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:22:51,609 - INFO - test loss 0.019615355296871067
2023-07-01 00:22:51,609 - INFO - test acc 0.7599999904632568
2023-07-01 00:22:52,850 - INFO - Distilling data from client: Client08
2023-07-01 00:22:52,850 - INFO - train loss: 0.00022248862754350248
2023-07-01 00:22:52,850 - INFO - train acc: 1.0
2023-07-01 00:22:52,874 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.78      0.77        73
           1       0.77      0.69      0.73        64
           5       0.76      0.81      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:22:52,874 - INFO - test loss 0.01959071301849783
2023-07-01 00:22:52,875 - INFO - test acc 0.7599999904632568
2023-07-01 00:22:54,122 - INFO - Distilling data from client: Client08
2023-07-01 00:22:54,122 - INFO - train loss: 0.00022852603847866595
2023-07-01 00:22:54,122 - INFO - train acc: 1.0
2023-07-01 00:22:54,146 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.78      0.77        73
           1       0.77      0.73      0.75        64
           5       0.80      0.81      0.80        63

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.78       200
weighted avg       0.77      0.78      0.77       200

2023-07-01 00:22:54,146 - INFO - test loss 0.019784573568101184
2023-07-01 00:22:54,146 - INFO - test acc 0.7749999761581421
2023-07-01 00:22:55,392 - INFO - Distilling data from client: Client08
2023-07-01 00:22:55,392 - INFO - train loss: 0.00021676212009983593
2023-07-01 00:22:55,393 - INFO - train acc: 1.0
2023-07-01 00:22:55,419 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.75      0.74        73
           1       0.77      0.72      0.74        64
           5       0.75      0.78      0.77        63

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-07-01 00:22:55,419 - INFO - test loss 0.019737596832846094
2023-07-01 00:22:55,419 - INFO - test acc 0.75
2023-07-01 00:22:56,662 - INFO - Distilling data from client: Client08
2023-07-01 00:22:56,662 - INFO - train loss: 0.00019723161025484836
2023-07-01 00:22:56,662 - INFO - train acc: 1.0
2023-07-01 00:22:56,686 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.77      0.76        73
           1       0.77      0.72      0.74        64
           5       0.78      0.81      0.80        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.76       200

2023-07-01 00:22:56,686 - INFO - test loss 0.020031100575252137
2023-07-01 00:22:56,686 - INFO - test acc 0.7649999856948853
2023-07-01 00:22:57,935 - INFO - Distilling data from client: Client08
2023-07-01 00:22:57,935 - INFO - train loss: 0.00019839699842878682
2023-07-01 00:22:57,936 - INFO - train acc: 1.0
2023-07-01 00:22:57,960 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.78      0.77        73
           1       0.78      0.73      0.76        64
           5       0.78      0.79      0.79        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:22:57,960 - INFO - test loss 0.01913865137909324
2023-07-01 00:22:57,960 - INFO - test acc 0.7699999809265137
2023-07-01 00:22:59,199 - INFO - Distilling data from client: Client08
2023-07-01 00:22:59,199 - INFO - train loss: 0.00021681993918711134
2023-07-01 00:22:59,199 - INFO - train acc: 1.0
2023-07-01 00:22:59,223 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.73        73
           1       0.77      0.73      0.75        64
           5       0.76      0.81      0.78        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:22:59,223 - INFO - test loss 0.019530636062075463
2023-07-01 00:22:59,223 - INFO - test acc 0.7549999952316284
2023-07-01 00:23:00,469 - INFO - Distilling data from client: Client08
2023-07-01 00:23:00,469 - INFO - train loss: 0.00017861644271881613
2023-07-01 00:23:00,469 - INFO - train acc: 1.0
2023-07-01 00:23:00,493 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.78      0.76        73
           1       0.77      0.75      0.76        64
           5       0.80      0.78      0.79        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:23:00,493 - INFO - test loss 0.019650765981027957
2023-07-01 00:23:00,493 - INFO - test acc 0.7699999809265137
2023-07-01 00:23:01,734 - INFO - Distilling data from client: Client08
2023-07-01 00:23:01,734 - INFO - train loss: 0.00017937196461358974
2023-07-01 00:23:01,734 - INFO - train acc: 1.0
2023-07-01 00:23:01,758 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.78      0.76        73
           1       0.80      0.73      0.76        64
           5       0.78      0.79      0.79        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:23:01,758 - INFO - test loss 0.019785268250278328
2023-07-01 00:23:01,758 - INFO - test acc 0.7699999809265137
2023-07-01 00:23:03,015 - INFO - Distilling data from client: Client08
2023-07-01 00:23:03,015 - INFO - train loss: 0.00015501467226937255
2023-07-01 00:23:03,015 - INFO - train acc: 1.0
2023-07-01 00:23:03,039 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.78      0.77        73
           1       0.76      0.73      0.75        64
           5       0.81      0.79      0.80        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:23:03,039 - INFO - test loss 0.01995134039036791
2023-07-01 00:23:03,040 - INFO - test acc 0.7699999809265137
2023-07-01 00:23:04,277 - INFO - Distilling data from client: Client08
2023-07-01 00:23:04,277 - INFO - train loss: 0.0001637896034809156
2023-07-01 00:23:04,277 - INFO - train acc: 1.0
2023-07-01 00:23:04,300 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.79      0.78        73
           1       0.76      0.70      0.73        64
           5       0.77      0.79      0.78        63

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.76      0.77      0.76       200

2023-07-01 00:23:04,300 - INFO - test loss 0.019728633271249196
2023-07-01 00:23:04,300 - INFO - test acc 0.7649999856948853
2023-07-01 00:23:05,555 - INFO - Distilling data from client: Client08
2023-07-01 00:23:05,555 - INFO - train loss: 0.00015225737582383762
2023-07-01 00:23:05,555 - INFO - train acc: 1.0
2023-07-01 00:23:05,579 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.75      0.73        73
           1       0.75      0.72      0.74        64
           5       0.77      0.76      0.77        63

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.75       200
weighted avg       0.75      0.74      0.75       200

2023-07-01 00:23:05,579 - INFO - test loss 0.020071251585298128
2023-07-01 00:23:05,579 - INFO - test acc 0.7450000047683716
2023-07-01 00:23:05,591 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:05,834 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:05,842 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:05,850 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:05,859 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:05,867 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:05,876 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:05,885 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:05,894 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:06,266 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client08//synthetic.png
2023-07-01 00:23:06,279 - INFO - c: 2.0 and total_data_in_this_class: 262
2023-07-01 00:23:06,279 - INFO - c: 5.0 and total_data_in_this_class: 272
2023-07-01 00:23:06,279 - INFO - c: 6.0 and total_data_in_this_class: 265
2023-07-01 00:23:06,279 - INFO - c: 2.0 and total_data_in_this_class: 71
2023-07-01 00:23:06,279 - INFO - c: 5.0 and total_data_in_this_class: 61
2023-07-01 00:23:06,279 - INFO - c: 6.0 and total_data_in_this_class: 68
2023-07-01 00:23:06,348 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04639434814453125 sec
2023-07-01 00:23:06,393 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04448080062866211 sec
2023-07-01 00:23:06,398 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.09775352478027344 sec
2023-07-01 00:23:06,400 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:23:06,432 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03189706802368164 sec
2023-07-01 00:23:06,432 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:23:06,557 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12464666366577148 sec
2023-07-01 00:23:06,581 - INFO - initial test loss: 0.030013232514811996
2023-07-01 00:23:06,581 - INFO - initial test acc: 0.6150000095367432
2023-07-01 00:23:06,589 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.005662679672241211 sec
2023-07-01 00:23:06,698 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1145317554473877 sec
2023-07-01 00:23:06,701 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:23:06,763 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.061948299407958984 sec
2023-07-01 00:23:06,764 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:23:07,088 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3241841793060303 sec
2023-07-01 00:23:08,341 - INFO - Distilling data from client: Client09
2023-07-01 00:23:08,341 - INFO - train loss: 0.003358547094792051
2023-07-01 00:23:08,341 - INFO - train acc: 0.9961904883384705
2023-07-01 00:23:08,402 - INFO - report:               precision    recall  f1-score   support

           2       0.55      0.48      0.51        71
           5       0.53      0.51      0.52        61
           6       0.53      0.62      0.57        68

    accuracy                           0.54       200
   macro avg       0.54      0.53      0.53       200
weighted avg       0.54      0.54      0.53       200

2023-07-01 00:23:08,402 - INFO - test loss 0.029551134900994088
2023-07-01 00:23:08,402 - INFO - test acc 0.5349999666213989
2023-07-01 00:23:09,651 - INFO - Distilling data from client: Client09
2023-07-01 00:23:09,651 - INFO - train loss: 0.001776116397943819
2023-07-01 00:23:09,651 - INFO - train acc: 1.0
2023-07-01 00:23:09,717 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.51      0.55        71
           5       0.59      0.61      0.60        61
           6       0.61      0.68      0.64        68

    accuracy                           0.59       200
   macro avg       0.59      0.60      0.59       200
weighted avg       0.59      0.59      0.59       200

2023-07-01 00:23:09,718 - INFO - test loss 0.029090757954969213
2023-07-01 00:23:09,718 - INFO - test acc 0.5949999690055847
2023-07-01 00:23:10,996 - INFO - Distilling data from client: Client09
2023-07-01 00:23:10,996 - INFO - train loss: 0.0012751001646509519
2023-07-01 00:23:10,996 - INFO - train acc: 1.0
2023-07-01 00:23:11,019 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.54      0.56        71
           5       0.58      0.57      0.58        61
           6       0.59      0.65      0.62        68

    accuracy                           0.58       200
   macro avg       0.58      0.59      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-07-01 00:23:11,019 - INFO - test loss 0.02962779120029488
2023-07-01 00:23:11,020 - INFO - test acc 0.5849999785423279
2023-07-01 00:23:12,286 - INFO - Distilling data from client: Client09
2023-07-01 00:23:12,286 - INFO - train loss: 0.0011015007078165526
2023-07-01 00:23:12,286 - INFO - train acc: 1.0
2023-07-01 00:23:12,309 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.55      0.56        71
           5       0.58      0.62      0.60        61
           6       0.59      0.57      0.58        68

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-07-01 00:23:12,309 - INFO - test loss 0.029784772662513555
2023-07-01 00:23:12,310 - INFO - test acc 0.5799999833106995
2023-07-01 00:23:13,567 - INFO - Distilling data from client: Client09
2023-07-01 00:23:13,567 - INFO - train loss: 0.001183269472608193
2023-07-01 00:23:13,567 - INFO - train acc: 1.0
2023-07-01 00:23:13,591 - INFO - report:               precision    recall  f1-score   support

           2       0.55      0.51      0.53        71
           5       0.56      0.54      0.55        61
           6       0.57      0.63      0.60        68

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-07-01 00:23:13,592 - INFO - test loss 0.030667764389900588
2023-07-01 00:23:13,592 - INFO - test acc 0.5600000023841858
2023-07-01 00:23:14,850 - INFO - Distilling data from client: Client09
2023-07-01 00:23:14,850 - INFO - train loss: 0.0008708596785959946
2023-07-01 00:23:14,850 - INFO - train acc: 1.0
2023-07-01 00:23:14,876 - INFO - report:               precision    recall  f1-score   support

           2       0.55      0.54      0.54        71
           5       0.58      0.56      0.57        61
           6       0.60      0.63      0.61        68

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-07-01 00:23:14,876 - INFO - test loss 0.03058577439885818
2023-07-01 00:23:14,876 - INFO - test acc 0.574999988079071
2023-07-01 00:23:16,128 - INFO - Distilling data from client: Client09
2023-07-01 00:23:16,128 - INFO - train loss: 0.0009169591850174372
2023-07-01 00:23:16,128 - INFO - train acc: 1.0
2023-07-01 00:23:16,151 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.56      0.57        71
           5       0.59      0.57      0.58        61
           6       0.58      0.60      0.59        68

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-07-01 00:23:16,152 - INFO - test loss 0.030132625639267777
2023-07-01 00:23:16,152 - INFO - test acc 0.5799999833106995
2023-07-01 00:23:17,402 - INFO - Distilling data from client: Client09
2023-07-01 00:23:17,402 - INFO - train loss: 0.0006879432405839728
2023-07-01 00:23:17,402 - INFO - train acc: 1.0
2023-07-01 00:23:17,425 - INFO - report:               precision    recall  f1-score   support

           2       0.56      0.49      0.53        71
           5       0.55      0.54      0.55        61
           6       0.58      0.66      0.62        68

    accuracy                           0.56       200
   macro avg       0.56      0.57      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-07-01 00:23:17,425 - INFO - test loss 0.03100812256245173
2023-07-01 00:23:17,425 - INFO - test acc 0.5649999976158142
2023-07-01 00:23:18,688 - INFO - Distilling data from client: Client09
2023-07-01 00:23:18,689 - INFO - train loss: 0.0007261838488854929
2023-07-01 00:23:18,689 - INFO - train acc: 1.0
2023-07-01 00:23:18,712 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.54      0.55        71
           5       0.56      0.57      0.56        61
           6       0.59      0.60      0.59        68

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-07-01 00:23:18,712 - INFO - test loss 0.03019645444812695
2023-07-01 00:23:18,712 - INFO - test acc 0.5699999928474426
2023-07-01 00:23:19,980 - INFO - Distilling data from client: Client09
2023-07-01 00:23:19,980 - INFO - train loss: 0.0006216933207116247
2023-07-01 00:23:19,980 - INFO - train acc: 1.0
2023-07-01 00:23:20,003 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.52      0.55        71
           5       0.52      0.52      0.52        61
           6       0.56      0.62      0.59        68

    accuracy                           0.56       200
   macro avg       0.55      0.55      0.55       200
weighted avg       0.56      0.56      0.55       200

2023-07-01 00:23:20,003 - INFO - test loss 0.029880490724631538
2023-07-01 00:23:20,003 - INFO - test acc 0.5550000071525574
2023-07-01 00:23:21,263 - INFO - Distilling data from client: Client09
2023-07-01 00:23:21,263 - INFO - train loss: 0.0005468136004884633
2023-07-01 00:23:21,263 - INFO - train acc: 1.0
2023-07-01 00:23:21,327 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.59      0.58        71
           5       0.64      0.57      0.60        61
           6       0.61      0.65      0.63        68

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.61       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:23:21,328 - INFO - test loss 0.030432906058871723
2023-07-01 00:23:21,328 - INFO - test acc 0.6049999594688416
2023-07-01 00:23:22,577 - INFO - Distilling data from client: Client09
2023-07-01 00:23:22,577 - INFO - train loss: 0.0005461863695208169
2023-07-01 00:23:22,577 - INFO - train acc: 1.0
2023-07-01 00:23:22,600 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.55      0.57        71
           5       0.57      0.56      0.56        61
           6       0.58      0.62      0.60        68

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-07-01 00:23:22,600 - INFO - test loss 0.030720048587016116
2023-07-01 00:23:22,600 - INFO - test acc 0.574999988079071
2023-07-01 00:23:23,859 - INFO - Distilling data from client: Client09
2023-07-01 00:23:23,859 - INFO - train loss: 0.000468346430664553
2023-07-01 00:23:23,859 - INFO - train acc: 1.0
2023-07-01 00:23:23,926 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.61      0.62        71
           5       0.59      0.57      0.58        61
           6       0.60      0.65      0.62        68

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-07-01 00:23:23,926 - INFO - test loss 0.030149292560968165
2023-07-01 00:23:23,926 - INFO - test acc 0.6100000143051147
2023-07-01 00:23:25,183 - INFO - Distilling data from client: Client09
2023-07-01 00:23:25,183 - INFO - train loss: 0.0005635936282943584
2023-07-01 00:23:25,183 - INFO - train acc: 1.0
2023-07-01 00:23:25,207 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.52      0.54        71
           5       0.54      0.59      0.56        61
           6       0.60      0.60      0.60        68

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-07-01 00:23:25,208 - INFO - test loss 0.030929168955893933
2023-07-01 00:23:25,208 - INFO - test acc 0.5699999928474426
2023-07-01 00:23:26,479 - INFO - Distilling data from client: Client09
2023-07-01 00:23:26,479 - INFO - train loss: 0.00043174777736199136
2023-07-01 00:23:26,479 - INFO - train acc: 1.0
2023-07-01 00:23:26,503 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.56      0.58        71
           5       0.56      0.57      0.56        61
           6       0.58      0.59      0.58        68

    accuracy                           0.57       200
   macro avg       0.57      0.58      0.57       200
weighted avg       0.58      0.57      0.58       200

2023-07-01 00:23:26,503 - INFO - test loss 0.03097100252577698
2023-07-01 00:23:26,503 - INFO - test acc 0.574999988079071
2023-07-01 00:23:27,754 - INFO - Distilling data from client: Client09
2023-07-01 00:23:27,755 - INFO - train loss: 0.0003647670875760717
2023-07-01 00:23:27,755 - INFO - train acc: 1.0
2023-07-01 00:23:27,780 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.52      0.54        71
           5       0.58      0.57      0.58        61
           6       0.57      0.63      0.60        68

    accuracy                           0.57       200
   macro avg       0.58      0.58      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-07-01 00:23:27,780 - INFO - test loss 0.030569299498390507
2023-07-01 00:23:27,780 - INFO - test acc 0.574999988079071
2023-07-01 00:23:29,035 - INFO - Distilling data from client: Client09
2023-07-01 00:23:29,035 - INFO - train loss: 0.0003627419930108398
2023-07-01 00:23:29,035 - INFO - train acc: 1.0
2023-07-01 00:23:29,059 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.54      0.57        71
           5       0.59      0.57      0.58        61
           6       0.58      0.68      0.63        68

    accuracy                           0.59       200
   macro avg       0.60      0.60      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-07-01 00:23:29,059 - INFO - test loss 0.03133177892680174
2023-07-01 00:23:29,059 - INFO - test acc 0.5949999690055847
2023-07-01 00:23:30,324 - INFO - Distilling data from client: Client09
2023-07-01 00:23:30,324 - INFO - train loss: 0.00043050482223203074
2023-07-01 00:23:30,324 - INFO - train acc: 1.0
2023-07-01 00:23:30,347 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.49      0.51        71
           5       0.56      0.57      0.56        61
           6       0.55      0.57      0.56        68

    accuracy                           0.55       200
   macro avg       0.55      0.55      0.55       200
weighted avg       0.54      0.55      0.54       200

2023-07-01 00:23:30,347 - INFO - test loss 0.03136398674943827
2023-07-01 00:23:30,347 - INFO - test acc 0.5450000166893005
2023-07-01 00:23:31,605 - INFO - Distilling data from client: Client09
2023-07-01 00:23:31,605 - INFO - train loss: 0.000487317713299991
2023-07-01 00:23:31,606 - INFO - train acc: 1.0
2023-07-01 00:23:31,629 - INFO - report:               precision    recall  f1-score   support

           2       0.58      0.56      0.57        71
           5       0.55      0.51      0.53        61
           6       0.56      0.62      0.59        68

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.57      0.56      0.56       200

2023-07-01 00:23:31,629 - INFO - test loss 0.03035271566433
2023-07-01 00:23:31,629 - INFO - test acc 0.5649999976158142
2023-07-01 00:23:32,903 - INFO - Distilling data from client: Client09
2023-07-01 00:23:32,903 - INFO - train loss: 0.00041377963391384906
2023-07-01 00:23:32,903 - INFO - train acc: 1.0
2023-07-01 00:23:32,928 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.52      0.52        71
           5       0.58      0.52      0.55        61
           6       0.56      0.62      0.59        68

    accuracy                           0.56       200
   macro avg       0.56      0.55      0.55       200
weighted avg       0.56      0.56      0.55       200

2023-07-01 00:23:32,928 - INFO - test loss 0.03091361791855754
2023-07-01 00:23:32,928 - INFO - test acc 0.5550000071525574
2023-07-01 00:23:34,183 - INFO - Distilling data from client: Client09
2023-07-01 00:23:34,183 - INFO - train loss: 0.0003704733607649972
2023-07-01 00:23:34,183 - INFO - train acc: 1.0
2023-07-01 00:23:34,208 - INFO - report:               precision    recall  f1-score   support

           2       0.57      0.54      0.55        71
           5       0.58      0.56      0.57        61
           6       0.55      0.60      0.58        68

    accuracy                           0.56       200
   macro avg       0.57      0.57      0.56       200
weighted avg       0.57      0.56      0.56       200

2023-07-01 00:23:34,208 - INFO - test loss 0.030561776769672686
2023-07-01 00:23:34,208 - INFO - test acc 0.5649999976158142
2023-07-01 00:23:35,468 - INFO - Distilling data from client: Client09
2023-07-01 00:23:35,468 - INFO - train loss: 0.00034154778979758886
2023-07-01 00:23:35,468 - INFO - train acc: 1.0
2023-07-01 00:23:35,493 - INFO - report:               precision    recall  f1-score   support

           2       0.60      0.62      0.61        71
           5       0.58      0.52      0.55        61
           6       0.60      0.63      0.61        68

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.59      0.59      0.59       200

2023-07-01 00:23:35,493 - INFO - test loss 0.030813717700543145
2023-07-01 00:23:35,493 - INFO - test acc 0.5949999690055847
2023-07-01 00:23:36,747 - INFO - Distilling data from client: Client09
2023-07-01 00:23:36,747 - INFO - train loss: 0.0004772979020468786
2023-07-01 00:23:36,747 - INFO - train acc: 1.0
2023-07-01 00:23:36,771 - INFO - report:               precision    recall  f1-score   support

           2       0.59      0.54      0.56        71
           5       0.57      0.59      0.58        61
           6       0.59      0.63      0.61        68

    accuracy                           0.58       200
   macro avg       0.58      0.59      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-07-01 00:23:36,772 - INFO - test loss 0.031004138654697564
2023-07-01 00:23:36,772 - INFO - test acc 0.5849999785423279
2023-07-01 00:23:38,033 - INFO - Distilling data from client: Client09
2023-07-01 00:23:38,033 - INFO - train loss: 0.0004396924240064013
2023-07-01 00:23:38,033 - INFO - train acc: 1.0
2023-07-01 00:23:38,056 - INFO - report:               precision    recall  f1-score   support

           2       0.56      0.56      0.56        71
           5       0.58      0.57      0.58        61
           6       0.59      0.60      0.60        68

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-07-01 00:23:38,057 - INFO - test loss 0.03139193080316567
2023-07-01 00:23:38,057 - INFO - test acc 0.5799999833106995
2023-07-01 00:23:39,305 - INFO - Distilling data from client: Client09
2023-07-01 00:23:39,305 - INFO - train loss: 0.00035175838668044057
2023-07-01 00:23:39,305 - INFO - train acc: 1.0
2023-07-01 00:23:39,328 - INFO - report:               precision    recall  f1-score   support

           2       0.56      0.52      0.54        71
           5       0.54      0.54      0.54        61
           6       0.56      0.60      0.58        68

    accuracy                           0.56       200
   macro avg       0.55      0.56      0.55       200
weighted avg       0.55      0.56      0.55       200

2023-07-01 00:23:39,328 - INFO - test loss 0.030982305707148307
2023-07-01 00:23:39,328 - INFO - test acc 0.5550000071525574
2023-07-01 00:23:39,340 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:39,349 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:39,358 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:39,366 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:39,375 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:39,383 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:39,392 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:39,402 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:39,411 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:23:39,787 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client09//synthetic.png
2023-07-01 00:23:39,799 - INFO - c: 1.0 and total_data_in_this_class: 265
2023-07-01 00:23:39,799 - INFO - c: 6.0 and total_data_in_this_class: 266
2023-07-01 00:23:39,799 - INFO - c: 8.0 and total_data_in_this_class: 268
2023-07-01 00:23:39,799 - INFO - c: 1.0 and total_data_in_this_class: 68
2023-07-01 00:23:39,799 - INFO - c: 6.0 and total_data_in_this_class: 67
2023-07-01 00:23:39,799 - INFO - c: 8.0 and total_data_in_this_class: 65
2023-07-01 00:23:39,819 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002570152282714844 sec
2023-07-01 00:23:39,819 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:23:39,820 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0011837482452392578 sec
2023-07-01 00:23:39,821 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:23:39,831 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010595321655273438 sec
2023-07-01 00:23:39,833 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002346038818359375 sec
2023-07-01 00:23:39,834 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:23:39,835 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009291172027587891 sec
2023-07-01 00:23:39,835 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:23:39,843 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008356094360351562 sec
2023-07-01 00:23:39,846 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013685226440429688 sec
2023-07-01 00:23:39,847 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012040138244628906 sec
2023-07-01 00:23:39,848 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003006458282470703 sec
2023-07-01 00:23:39,850 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002048015594482422 sec
2023-07-01 00:23:39,850 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011897087097167969 sec
2023-07-01 00:23:39,850 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002803802490234375 sec
2023-07-01 00:23:39,851 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024390220642089844 sec
2023-07-01 00:23:39,852 - WARNING - Finished tracing + transforming absolute for pjit in 0.00016546249389648438 sec
2023-07-01 00:23:39,852 - WARNING - Finished tracing + transforming fn for pjit in 0.0002751350402832031 sec
2023-07-01 00:23:39,853 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0003287792205810547 sec
2023-07-01 00:23:39,854 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019741058349609375 sec
2023-07-01 00:23:39,855 - WARNING - Finished tracing + transforming fn for pjit in 0.0002257823944091797 sec
2023-07-01 00:23:39,855 - WARNING - Finished tracing + transforming fn for pjit in 0.00026702880859375 sec
2023-07-01 00:23:39,856 - WARNING - Finished tracing + transforming fn for pjit in 0.00022840499877929688 sec
2023-07-01 00:23:39,857 - WARNING - Finished tracing + transforming fn for pjit in 0.0002617835998535156 sec
2023-07-01 00:23:39,858 - WARNING - Finished tracing + transforming fn for pjit in 0.00022411346435546875 sec
2023-07-01 00:23:39,860 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00017023086547851562 sec
2023-07-01 00:23:39,860 - WARNING - Finished tracing + transforming fn for pjit in 0.0002257823944091797 sec
2023-07-01 00:23:39,861 - WARNING - Finished tracing + transforming fn for pjit in 0.00023603439331054688 sec
2023-07-01 00:23:39,864 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003590583801269531 sec
2023-07-01 00:23:39,865 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009586811065673828 sec
2023-07-01 00:23:39,866 - WARNING - Finished tracing + transforming fn for pjit in 0.00023508071899414062 sec
2023-07-01 00:23:39,866 - WARNING - Finished tracing + transforming fn for pjit in 0.00022125244140625 sec
2023-07-01 00:23:39,867 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029277801513671875 sec
2023-07-01 00:23:39,868 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025963783264160156 sec
2023-07-01 00:23:39,869 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001735687255859375 sec
2023-07-01 00:23:39,869 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002598762512207031 sec
2023-07-01 00:23:39,870 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022745132446289062 sec
2023-07-01 00:23:39,871 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022840499877929688 sec
2023-07-01 00:23:39,872 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00034737586975097656 sec
2023-07-01 00:23:39,872 - WARNING - Finished tracing + transforming _where for pjit in 0.0009427070617675781 sec
2023-07-01 00:23:39,873 - WARNING - Finished tracing + transforming fn for pjit in 0.0002624988555908203 sec
2023-07-01 00:23:39,873 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002644062042236328 sec
2023-07-01 00:23:39,874 - WARNING - Finished tracing + transforming fn for pjit in 0.0002238750457763672 sec
2023-07-01 00:23:39,875 - WARNING - Finished tracing + transforming fn for pjit in 0.0002200603485107422 sec
2023-07-01 00:23:39,875 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022077560424804688 sec
2023-07-01 00:23:39,876 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025653839111328125 sec
2023-07-01 00:23:39,877 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024819374084472656 sec
2023-07-01 00:23:39,878 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025963783264160156 sec
2023-07-01 00:23:39,878 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023126602172851562 sec
2023-07-01 00:23:39,879 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022864341735839844 sec
2023-07-01 00:23:39,880 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002574920654296875 sec
2023-07-01 00:23:39,880 - WARNING - Finished tracing + transforming _where for pjit in 0.0008401870727539062 sec
2023-07-01 00:23:39,881 - WARNING - Finished tracing + transforming fn for pjit in 0.00025916099548339844 sec
2023-07-01 00:23:39,881 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025725364685058594 sec
2023-07-01 00:23:39,882 - WARNING - Finished tracing + transforming fn for pjit in 0.00022554397583007812 sec
2023-07-01 00:23:39,887 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002741813659667969 sec
2023-07-01 00:23:39,888 - WARNING - Finished tracing + transforming fn for pjit in 0.001005411148071289 sec
2023-07-01 00:23:39,889 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002701282501220703 sec
2023-07-01 00:23:39,889 - WARNING - Finished tracing + transforming fn for pjit in 0.0002257823944091797 sec
2023-07-01 00:23:39,893 - WARNING - Finished tracing + transforming fn for pjit in 0.0002219676971435547 sec
2023-07-01 00:23:39,895 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016379356384277344 sec
2023-07-01 00:23:39,895 - WARNING - Finished tracing + transforming fn for pjit in 0.0002987384796142578 sec
2023-07-01 00:23:39,896 - WARNING - Finished tracing + transforming fn for pjit in 0.00022864341735839844 sec
2023-07-01 00:23:39,914 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0679781436920166 sec
2023-07-01 00:23:39,917 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012540817260742188 sec
2023-07-01 00:23:39,918 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011372566223144531 sec
2023-07-01 00:23:39,918 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00026702880859375 sec
2023-07-01 00:23:39,920 - WARNING - Finished tracing + transforming fn for pjit in 0.00022149085998535156 sec
2023-07-01 00:23:39,921 - WARNING - Finished tracing + transforming fn for pjit in 0.0002713203430175781 sec
2023-07-01 00:23:39,922 - WARNING - Finished tracing + transforming fn for pjit in 0.00021719932556152344 sec
2023-07-01 00:23:39,928 - WARNING - Finished tracing + transforming fn for pjit in 0.00023102760314941406 sec
2023-07-01 00:23:39,929 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023031234741210938 sec
2023-07-01 00:23:39,930 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002505779266357422 sec
2023-07-01 00:23:39,930 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00016999244689941406 sec
2023-07-01 00:23:39,931 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032401084899902344 sec
2023-07-01 00:23:39,932 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022077560424804688 sec
2023-07-01 00:23:39,932 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002219676971435547 sec
2023-07-01 00:23:39,933 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002675056457519531 sec
2023-07-01 00:23:39,933 - WARNING - Finished tracing + transforming _where for pjit in 0.0008630752563476562 sec
2023-07-01 00:23:39,934 - WARNING - Finished tracing + transforming fn for pjit in 0.00026035308837890625 sec
2023-07-01 00:23:39,935 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002524852752685547 sec
2023-07-01 00:23:39,936 - WARNING - Finished tracing + transforming fn for pjit in 0.00021958351135253906 sec
2023-07-01 00:23:39,936 - WARNING - Finished tracing + transforming fn for pjit in 0.0002789497375488281 sec
2023-07-01 00:23:39,948 - WARNING - Finished tracing + transforming fn for pjit in 0.00022101402282714844 sec
2023-07-01 00:23:39,968 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.053056955337524414 sec
2023-07-01 00:23:39,969 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012254714965820312 sec
2023-07-01 00:23:39,970 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.000133514404296875 sec
2023-07-01 00:23:39,970 - WARNING - Finished tracing + transforming _where for pjit in 0.0006020069122314453 sec
2023-07-01 00:23:39,971 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00028824806213378906 sec
2023-07-01 00:23:39,971 - WARNING - Finished tracing + transforming trace for pjit in 0.0024614334106445312 sec
2023-07-01 00:23:39,974 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010418891906738281 sec
2023-07-01 00:23:39,975 - WARNING - Finished tracing + transforming tril for pjit in 0.0006763935089111328 sec
2023-07-01 00:23:39,975 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0017001628875732422 sec
2023-07-01 00:23:39,976 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00011086463928222656 sec
2023-07-01 00:23:39,976 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010561943054199219 sec
2023-07-01 00:23:39,978 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014231204986572266 sec
2023-07-01 00:23:39,982 - WARNING - Finished tracing + transforming _solve for pjit in 0.009164094924926758 sec
2023-07-01 00:23:39,983 - WARNING - Finished tracing + transforming dot for pjit in 0.00032138824462890625 sec
2023-07-01 00:23:39,985 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.14043569564819336 sec
2023-07-01 00:23:39,987 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:23:40,020 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03260397911071777 sec
2023-07-01 00:23:40,020 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:23:40,146 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12531518936157227 sec
2023-07-01 00:23:40,169 - INFO - initial test loss: 0.019266028931610925
2023-07-01 00:23:40,169 - INFO - initial test acc: 0.7849999666213989
2023-07-01 00:23:40,175 - WARNING - Finished tracing + transforming dot for pjit in 0.0003731250762939453 sec
2023-07-01 00:23:40,176 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002930164337158203 sec
2023-07-01 00:23:40,177 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00030994415283203125 sec
2023-07-01 00:23:40,178 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009596347808837891 sec
2023-07-01 00:23:40,178 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00019598007202148438 sec
2023-07-01 00:23:40,179 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00024318695068359375 sec
2023-07-01 00:23:40,180 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024509429931640625 sec
2023-07-01 00:23:40,181 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003657341003417969 sec
2023-07-01 00:23:40,181 - WARNING - Finished tracing + transforming _mean for pjit in 0.001069784164428711 sec
2023-07-01 00:23:40,182 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009910821914672852 sec
2023-07-01 00:23:40,190 - WARNING - Finished tracing + transforming fn for pjit in 0.0002536773681640625 sec
2023-07-01 00:23:40,191 - WARNING - Finished tracing + transforming fn for pjit in 0.0002608299255371094 sec
2023-07-01 00:23:40,192 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0007944107055664062 sec
2023-07-01 00:23:40,193 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00027251243591308594 sec
2023-07-01 00:23:40,193 - WARNING - Finished tracing + transforming _where for pjit in 0.0008487701416015625 sec
2023-07-01 00:23:40,201 - WARNING - Finished tracing + transforming fn for pjit in 0.00025200843811035156 sec
2023-07-01 00:23:40,202 - WARNING - Finished tracing + transforming fn for pjit in 0.0002663135528564453 sec
2023-07-01 00:23:40,202 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021123886108398438 sec
2023-07-01 00:23:40,203 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00024056434631347656 sec
2023-07-01 00:23:40,204 - WARNING - Finished tracing + transforming _where for pjit in 0.0007987022399902344 sec
2023-07-01 00:23:40,237 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021004676818847656 sec
2023-07-01 00:23:40,293 - WARNING - Finished tracing + transforming fn for pjit in 0.0002779960632324219 sec
2023-07-01 00:23:40,294 - WARNING - Finished tracing + transforming fn for pjit in 0.00023174285888671875 sec
2023-07-01 00:23:40,294 - WARNING - Finished tracing + transforming square for pjit in 0.0001704692840576172 sec
2023-07-01 00:23:40,296 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022101402282714844 sec
2023-07-01 00:23:40,298 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023365020751953125 sec
2023-07-01 00:23:40,298 - WARNING - Finished tracing + transforming fn for pjit in 0.0002613067626953125 sec
2023-07-01 00:23:40,299 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002315044403076172 sec
2023-07-01 00:23:40,300 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022864341735839844 sec
2023-07-01 00:23:40,300 - WARNING - Finished tracing + transforming fn for pjit in 0.0002696514129638672 sec
2023-07-01 00:23:40,301 - WARNING - Finished tracing + transforming fn for pjit in 0.00024127960205078125 sec
2023-07-01 00:23:40,301 - WARNING - Finished tracing + transforming square for pjit in 0.0001671314239501953 sec
2023-07-01 00:23:40,303 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021505355834960938 sec
2023-07-01 00:23:40,305 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001742839813232422 sec
2023-07-01 00:23:40,305 - WARNING - Finished tracing + transforming fn for pjit in 0.0002639293670654297 sec
2023-07-01 00:23:40,306 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021696090698242188 sec
2023-07-01 00:23:40,307 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022983551025390625 sec
2023-07-01 00:23:40,307 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13615012168884277 sec
2023-07-01 00:23:40,311 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[531,10]), ShapedArray(float32[531,10]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:23:40,373 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.062101125717163086 sec
2023-07-01 00:23:40,374 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:23:40,707 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3331882953643799 sec
2023-07-01 00:23:42,001 - INFO - Distilling data from client: Client10
2023-07-01 00:23:42,001 - INFO - train loss: 0.0015600975164059385
2023-07-01 00:23:42,001 - INFO - train acc: 0.9981167316436768
2023-07-01 00:23:42,064 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.66      0.70        68
           6       0.70      0.78      0.74        67
           8       0.79      0.80      0.79        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:23:42,064 - INFO - test loss 0.019705266758662656
2023-07-01 00:23:42,064 - INFO - test acc 0.7450000047683716
2023-07-01 00:23:43,342 - INFO - Distilling data from client: Client10
2023-07-01 00:23:43,342 - INFO - train loss: 0.0007477777710048687
2023-07-01 00:23:43,342 - INFO - train acc: 1.0
2023-07-01 00:23:43,410 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.65      0.70        68
           6       0.72      0.82      0.77        67
           8       0.78      0.80      0.79        65

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:23:43,410 - INFO - test loss 0.019451448593085615
2023-07-01 00:23:43,410 - INFO - test acc 0.7549999952316284
2023-07-01 00:23:44,694 - INFO - Distilling data from client: Client10
2023-07-01 00:23:44,694 - INFO - train loss: 0.00045651875369629287
2023-07-01 00:23:44,694 - INFO - train acc: 1.0
2023-07-01 00:23:44,718 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.63      0.70        68
           6       0.72      0.81      0.76        67
           8       0.76      0.82      0.79        65

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-07-01 00:23:44,718 - INFO - test loss 0.0201317991205771
2023-07-01 00:23:44,718 - INFO - test acc 0.75
2023-07-01 00:23:46,004 - INFO - Distilling data from client: Client10
2023-07-01 00:23:46,004 - INFO - train loss: 0.00041992893058015706
2023-07-01 00:23:46,004 - INFO - train acc: 1.0
2023-07-01 00:23:46,027 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.62      0.69        68
           6       0.71      0.81      0.76        67
           8       0.74      0.80      0.77        65

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:23:46,027 - INFO - test loss 0.020481482718812664
2023-07-01 00:23:46,028 - INFO - test acc 0.7400000095367432
2023-07-01 00:23:47,322 - INFO - Distilling data from client: Client10
2023-07-01 00:23:47,322 - INFO - train loss: 0.0004475053241338047
2023-07-01 00:23:47,322 - INFO - train acc: 1.0
2023-07-01 00:23:47,344 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.60      0.68        68
           6       0.69      0.81      0.74        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:23:47,345 - INFO - test loss 0.020089077172760303
2023-07-01 00:23:47,345 - INFO - test acc 0.7299999594688416
2023-07-01 00:23:48,629 - INFO - Distilling data from client: Client10
2023-07-01 00:23:48,629 - INFO - train loss: 0.00032822207202783994
2023-07-01 00:23:48,629 - INFO - train acc: 1.0
2023-07-01 00:23:48,654 - INFO - report:               precision    recall  f1-score   support

           1       0.74      0.62      0.67        68
           6       0.72      0.76      0.74        67
           8       0.74      0.82      0.77        65

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-07-01 00:23:48,654 - INFO - test loss 0.020416608096907554
2023-07-01 00:23:48,654 - INFO - test acc 0.7299999594688416
2023-07-01 00:23:49,934 - INFO - Distilling data from client: Client10
2023-07-01 00:23:49,935 - INFO - train loss: 0.00026619880041656526
2023-07-01 00:23:49,935 - INFO - train acc: 1.0
2023-07-01 00:23:49,958 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.62      0.69        68
           6       0.72      0.79      0.75        67
           8       0.72      0.80      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:23:49,958 - INFO - test loss 0.020750212052123365
2023-07-01 00:23:49,959 - INFO - test acc 0.73499995470047
2023-07-01 00:23:51,246 - INFO - Distilling data from client: Client10
2023-07-01 00:23:51,246 - INFO - train loss: 0.00024333401155257308
2023-07-01 00:23:51,247 - INFO - train acc: 1.0
2023-07-01 00:23:51,270 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.62      0.69        68
           6       0.71      0.79      0.75        67
           8       0.72      0.80      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:23:51,270 - INFO - test loss 0.020489935900292737
2023-07-01 00:23:51,270 - INFO - test acc 0.73499995470047
2023-07-01 00:23:52,555 - INFO - Distilling data from client: Client10
2023-07-01 00:23:52,555 - INFO - train loss: 0.00024015446679176677
2023-07-01 00:23:52,555 - INFO - train acc: 1.0
2023-07-01 00:23:52,579 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.59      0.68        68
           6       0.71      0.79      0.75        67
           8       0.73      0.85      0.79        65

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:23:52,579 - INFO - test loss 0.020496139178259123
2023-07-01 00:23:52,580 - INFO - test acc 0.7400000095367432
2023-07-01 00:23:53,860 - INFO - Distilling data from client: Client10
2023-07-01 00:23:53,860 - INFO - train loss: 0.00019327193914563413
2023-07-01 00:23:53,860 - INFO - train acc: 1.0
2023-07-01 00:23:53,884 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.63      0.70        68
           6       0.71      0.79      0.75        67
           8       0.72      0.78      0.75        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:23:53,885 - INFO - test loss 0.020283142096071606
2023-07-01 00:23:53,885 - INFO - test acc 0.73499995470047
2023-07-01 00:23:55,161 - INFO - Distilling data from client: Client10
2023-07-01 00:23:55,161 - INFO - train loss: 0.00018481572503998916
2023-07-01 00:23:55,161 - INFO - train acc: 1.0
2023-07-01 00:23:55,185 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.63      0.70        68
           6       0.71      0.79      0.75        67
           8       0.75      0.82      0.78        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:23:55,185 - INFO - test loss 0.020361080442402504
2023-07-01 00:23:55,185 - INFO - test acc 0.7450000047683716
2023-07-01 00:23:56,463 - INFO - Distilling data from client: Client10
2023-07-01 00:23:56,463 - INFO - train loss: 0.00015003649105676734
2023-07-01 00:23:56,463 - INFO - train acc: 1.0
2023-07-01 00:23:56,487 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.63      0.70        68
           6       0.72      0.79      0.75        67
           8       0.75      0.83      0.79        65

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-07-01 00:23:56,487 - INFO - test loss 0.020094149769047328
2023-07-01 00:23:56,487 - INFO - test acc 0.75
2023-07-01 00:23:57,766 - INFO - Distilling data from client: Client10
2023-07-01 00:23:57,766 - INFO - train loss: 0.00013469756549736188
2023-07-01 00:23:57,767 - INFO - train acc: 1.0
2023-07-01 00:23:57,792 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.60      0.68        68
           6       0.69      0.81      0.74        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:23:57,792 - INFO - test loss 0.02059262153215819
2023-07-01 00:23:57,792 - INFO - test acc 0.7299999594688416
2023-07-01 00:23:59,088 - INFO - Distilling data from client: Client10
2023-07-01 00:23:59,088 - INFO - train loss: 0.00015713870233143333
2023-07-01 00:23:59,089 - INFO - train acc: 1.0
2023-07-01 00:23:59,112 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.63      0.70        68
           6       0.72      0.79      0.75        67
           8       0.75      0.82      0.78        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:23:59,112 - INFO - test loss 0.02078301299261145
2023-07-01 00:23:59,113 - INFO - test acc 0.7450000047683716
2023-07-01 00:24:00,384 - INFO - Distilling data from client: Client10
2023-07-01 00:24:00,384 - INFO - train loss: 0.0001304823062272907
2023-07-01 00:24:00,384 - INFO - train acc: 1.0
2023-07-01 00:24:00,409 - INFO - report:               precision    recall  f1-score   support

           1       0.76      0.65      0.70        68
           6       0.74      0.79      0.76        67
           8       0.74      0.80      0.77        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:24:00,409 - INFO - test loss 0.020217545848362183
2023-07-01 00:24:00,409 - INFO - test acc 0.7450000047683716
2023-07-01 00:24:01,683 - INFO - Distilling data from client: Client10
2023-07-01 00:24:01,683 - INFO - train loss: 0.00016835043201350141
2023-07-01 00:24:01,683 - INFO - train acc: 1.0
2023-07-01 00:24:01,707 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.63      0.70        68
           6       0.71      0.79      0.75        67
           8       0.73      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:24:01,707 - INFO - test loss 0.020183094580427694
2023-07-01 00:24:01,707 - INFO - test acc 0.73499995470047
2023-07-01 00:24:03,001 - INFO - Distilling data from client: Client10
2023-07-01 00:24:03,002 - INFO - train loss: 0.00014981517828654238
2023-07-01 00:24:03,002 - INFO - train acc: 1.0
2023-07-01 00:24:03,025 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.65      0.71        68
           6       0.70      0.79      0.74        67
           8       0.76      0.80      0.78        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:24:03,025 - INFO - test loss 0.020089196777369974
2023-07-01 00:24:03,025 - INFO - test acc 0.7450000047683716
2023-07-01 00:24:04,328 - INFO - Distilling data from client: Client10
2023-07-01 00:24:04,328 - INFO - train loss: 0.00011042584264852743
2023-07-01 00:24:04,329 - INFO - train acc: 1.0
2023-07-01 00:24:04,355 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.63      0.70        68
           6       0.68      0.78      0.73        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:24:04,356 - INFO - test loss 0.020290520835903932
2023-07-01 00:24:04,356 - INFO - test acc 0.7299999594688416
2023-07-01 00:24:05,651 - INFO - Distilling data from client: Client10
2023-07-01 00:24:05,651 - INFO - train loss: 0.00017596816927866087
2023-07-01 00:24:05,651 - INFO - train acc: 1.0
2023-07-01 00:24:05,676 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.62      0.69        68
           6       0.68      0.79      0.73        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:24:05,676 - INFO - test loss 0.020256947248200236
2023-07-01 00:24:05,676 - INFO - test acc 0.7299999594688416
2023-07-01 00:24:06,960 - INFO - Distilling data from client: Client10
2023-07-01 00:24:06,960 - INFO - train loss: 0.00014062441115440935
2023-07-01 00:24:06,960 - INFO - train acc: 1.0
2023-07-01 00:24:06,985 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.63      0.70        68
           6       0.70      0.78      0.74        67
           8       0.75      0.82      0.78        65

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:24:06,985 - INFO - test loss 0.02040245410777302
2023-07-01 00:24:06,985 - INFO - test acc 0.7400000095367432
2023-07-01 00:24:08,281 - INFO - Distilling data from client: Client10
2023-07-01 00:24:08,281 - INFO - train loss: 0.00014069817784160727
2023-07-01 00:24:08,281 - INFO - train acc: 1.0
2023-07-01 00:24:08,307 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.62      0.69        68
           6       0.73      0.79      0.76        67
           8       0.73      0.82      0.77        65

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:24:08,307 - INFO - test loss 0.02031301593811172
2023-07-01 00:24:08,308 - INFO - test acc 0.7400000095367432
2023-07-01 00:24:09,596 - INFO - Distilling data from client: Client10
2023-07-01 00:24:09,596 - INFO - train loss: 0.00012970834262511746
2023-07-01 00:24:09,596 - INFO - train acc: 1.0
2023-07-01 00:24:09,620 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.62      0.69        68
           6       0.73      0.79      0.76        67
           8       0.73      0.83      0.78        65

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:24:09,620 - INFO - test loss 0.020494804466411016
2023-07-01 00:24:09,620 - INFO - test acc 0.7450000047683716
2023-07-01 00:24:10,907 - INFO - Distilling data from client: Client10
2023-07-01 00:24:10,907 - INFO - train loss: 0.0001228192865159241
2023-07-01 00:24:10,907 - INFO - train acc: 1.0
2023-07-01 00:24:10,931 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.62      0.68        68
           6       0.71      0.79      0.75        67
           8       0.72      0.77      0.75        65

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:24:10,931 - INFO - test loss 0.020355302948826648
2023-07-01 00:24:10,931 - INFO - test acc 0.7249999642372131
2023-07-01 00:24:12,211 - INFO - Distilling data from client: Client10
2023-07-01 00:24:12,211 - INFO - train loss: 0.0001607197287380108
2023-07-01 00:24:12,212 - INFO - train acc: 1.0
2023-07-01 00:24:12,235 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.63      0.69        68
           6       0.72      0.79      0.75        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:24:12,235 - INFO - test loss 0.02033452336722352
2023-07-01 00:24:12,235 - INFO - test acc 0.73499995470047
2023-07-01 00:24:13,512 - INFO - Distilling data from client: Client10
2023-07-01 00:24:13,513 - INFO - train loss: 0.00013723598661088762
2023-07-01 00:24:13,513 - INFO - train acc: 1.0
2023-07-01 00:24:13,537 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.63      0.69        68
           6       0.72      0.79      0.75        67
           8       0.74      0.78      0.76        65

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:24:13,537 - INFO - test loss 0.020449748669580558
2023-07-01 00:24:13,537 - INFO - test acc 0.73499995470047
2023-07-01 00:24:13,539 - WARNING - Finished tracing + transforming jit(gather) in 0.00025582313537597656 sec
2023-07-01 00:24:13,539 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[531,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:24:13,541 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0011649131774902344 sec
2023-07-01 00:24:13,541 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:24:13,551 - WARNING - Finished XLA compilation of jit(gather) in 0.009755849838256836 sec
2023-07-01 00:24:13,561 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:13,570 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:13,578 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:13,587 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:13,596 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:13,605 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:13,614 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:13,623 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:13,632 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:14,004 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client10//synthetic.png
2023-07-01 00:24:14,015 - INFO - c: 0.0 and total_data_in_this_class: 268
2023-07-01 00:24:14,015 - INFO - c: 5.0 and total_data_in_this_class: 268
2023-07-01 00:24:14,015 - INFO - c: 7.0 and total_data_in_this_class: 263
2023-07-01 00:24:14,015 - INFO - c: 0.0 and total_data_in_this_class: 65
2023-07-01 00:24:14,015 - INFO - c: 5.0 and total_data_in_this_class: 65
2023-07-01 00:24:14,015 - INFO - c: 7.0 and total_data_in_this_class: 70
2023-07-01 00:24:14,084 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04631829261779785 sec
2023-07-01 00:24:14,130 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04496002197265625 sec
2023-07-01 00:24:14,135 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.09821391105651855 sec
2023-07-01 00:24:14,136 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:24:14,169 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03205108642578125 sec
2023-07-01 00:24:14,169 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:24:14,294 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12535524368286133 sec
2023-07-01 00:24:14,317 - INFO - initial test loss: 0.022151852191614863
2023-07-01 00:24:14,317 - INFO - initial test acc: 0.7099999785423279
2023-07-01 00:24:14,326 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.005751609802246094 sec
2023-07-01 00:24:14,436 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11690902709960938 sec
2023-07-01 00:24:14,440 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:24:14,501 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.0610966682434082 sec
2023-07-01 00:24:14,501 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:24:14,825 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32365918159484863 sec
2023-07-01 00:24:16,076 - INFO - Distilling data from client: Client11
2023-07-01 00:24:16,077 - INFO - train loss: 0.002086484649907249
2023-07-01 00:24:16,077 - INFO - train acc: 1.0
2023-07-01 00:24:16,132 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.86      0.79        65
           5       0.71      0.68      0.69        65
           7       0.67      0.59      0.63        70

    accuracy                           0.70       200
   macro avg       0.70      0.71      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:24:16,132 - INFO - test loss 0.021850958964100678
2023-07-01 00:24:16,132 - INFO - test acc 0.7049999833106995
2023-07-01 00:24:17,380 - INFO - Distilling data from client: Client11
2023-07-01 00:24:17,381 - INFO - train loss: 0.0010362348308592532
2023-07-01 00:24:17,381 - INFO - train acc: 0.9980952143669128
2023-07-01 00:24:17,448 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.88      0.80        65
           5       0.73      0.68      0.70        65
           7       0.71      0.63      0.67        70

    accuracy                           0.73       200
   macro avg       0.72      0.73      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:24:17,448 - INFO - test loss 0.021086494188225294
2023-07-01 00:24:17,448 - INFO - test acc 0.7249999642372131
2023-07-01 00:24:18,715 - INFO - Distilling data from client: Client11
2023-07-01 00:24:18,715 - INFO - train loss: 0.0007368500642904364
2023-07-01 00:24:18,715 - INFO - train acc: 1.0
2023-07-01 00:24:18,739 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.88      0.80        65
           5       0.72      0.66      0.69        65
           7       0.71      0.63      0.67        70

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:24:18,739 - INFO - test loss 0.02121971686416059
2023-07-01 00:24:18,739 - INFO - test acc 0.7199999690055847
2023-07-01 00:24:20,002 - INFO - Distilling data from client: Client11
2023-07-01 00:24:20,002 - INFO - train loss: 0.0005507830850071121
2023-07-01 00:24:20,002 - INFO - train acc: 1.0
2023-07-01 00:24:20,025 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.88      0.78        65
           5       0.73      0.68      0.70        65
           7       0.69      0.59      0.64        70

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:24:20,025 - INFO - test loss 0.02153613847728603
2023-07-01 00:24:20,025 - INFO - test acc 0.7099999785423279
2023-07-01 00:24:21,279 - INFO - Distilling data from client: Client11
2023-07-01 00:24:21,279 - INFO - train loss: 0.0005484663939400559
2023-07-01 00:24:21,279 - INFO - train acc: 1.0
2023-07-01 00:24:21,309 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.86      0.80        65
           5       0.75      0.66      0.70        65
           7       0.68      0.66      0.67        70

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:24:21,309 - INFO - test loss 0.02187964682158776
2023-07-01 00:24:21,309 - INFO - test acc 0.7249999642372131
2023-07-01 00:24:22,572 - INFO - Distilling data from client: Client11
2023-07-01 00:24:22,572 - INFO - train loss: 0.0004764109472946377
2023-07-01 00:24:22,572 - INFO - train acc: 1.0
2023-07-01 00:24:22,595 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.85      0.78        65
           5       0.68      0.66      0.67        65
           7       0.69      0.60      0.64        70

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:24:22,596 - INFO - test loss 0.022117930265166234
2023-07-01 00:24:22,596 - INFO - test acc 0.699999988079071
2023-07-01 00:24:23,853 - INFO - Distilling data from client: Client11
2023-07-01 00:24:23,853 - INFO - train loss: 0.0003850856453759674
2023-07-01 00:24:23,853 - INFO - train acc: 1.0
2023-07-01 00:24:23,877 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.85      0.78        65
           5       0.75      0.71      0.73        65
           7       0.68      0.61      0.65        70

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:24:23,877 - INFO - test loss 0.021705896372206993
2023-07-01 00:24:23,877 - INFO - test acc 0.7199999690055847
2023-07-01 00:24:25,130 - INFO - Distilling data from client: Client11
2023-07-01 00:24:25,131 - INFO - train loss: 0.0003914083983871516
2023-07-01 00:24:25,131 - INFO - train acc: 1.0
2023-07-01 00:24:25,155 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.85      0.76        65
           5       0.72      0.66      0.69        65
           7       0.67      0.59      0.63        70

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:24:25,155 - INFO - test loss 0.022046926889702022
2023-07-01 00:24:25,155 - INFO - test acc 0.6949999928474426
2023-07-01 00:24:26,412 - INFO - Distilling data from client: Client11
2023-07-01 00:24:26,412 - INFO - train loss: 0.0003060570463158996
2023-07-01 00:24:26,412 - INFO - train acc: 1.0
2023-07-01 00:24:26,437 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.86      0.79        65
           5       0.74      0.69      0.71        65
           7       0.71      0.63      0.67        70

    accuracy                           0.73       200
   macro avg       0.72      0.73      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:24:26,437 - INFO - test loss 0.02165076166518805
2023-07-01 00:24:26,437 - INFO - test acc 0.7249999642372131
2023-07-01 00:24:27,692 - INFO - Distilling data from client: Client11
2023-07-01 00:24:27,692 - INFO - train loss: 0.00035749187280186874
2023-07-01 00:24:27,692 - INFO - train acc: 1.0
2023-07-01 00:24:27,718 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.88      0.78        65
           5       0.73      0.62      0.67        65
           7       0.69      0.63      0.66        70

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:24:27,718 - INFO - test loss 0.022077901848400663
2023-07-01 00:24:27,718 - INFO - test acc 0.7049999833106995
2023-07-01 00:24:28,972 - INFO - Distilling data from client: Client11
2023-07-01 00:24:28,972 - INFO - train loss: 0.0002671567860162899
2023-07-01 00:24:28,972 - INFO - train acc: 1.0
2023-07-01 00:24:28,999 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.88      0.79        65
           5       0.73      0.66      0.69        65
           7       0.70      0.61      0.66        70

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:24:28,999 - INFO - test loss 0.021102291002318127
2023-07-01 00:24:28,999 - INFO - test acc 0.7149999737739563
2023-07-01 00:24:30,257 - INFO - Distilling data from client: Client11
2023-07-01 00:24:30,257 - INFO - train loss: 0.00021099610368259986
2023-07-01 00:24:30,257 - INFO - train acc: 1.0
2023-07-01 00:24:30,281 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.78      0.74        65
           5       0.73      0.69      0.71        65
           7       0.68      0.63      0.65        70

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:24:30,282 - INFO - test loss 0.02150888889459299
2023-07-01 00:24:30,282 - INFO - test acc 0.699999988079071
2023-07-01 00:24:31,546 - INFO - Distilling data from client: Client11
2023-07-01 00:24:31,546 - INFO - train loss: 0.0002646684779777761
2023-07-01 00:24:31,546 - INFO - train acc: 1.0
2023-07-01 00:24:31,569 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.86      0.79        65
           5       0.74      0.65      0.69        65
           7       0.70      0.66      0.68        70

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:24:31,570 - INFO - test loss 0.021469162405342423
2023-07-01 00:24:31,570 - INFO - test acc 0.7199999690055847
2023-07-01 00:24:32,824 - INFO - Distilling data from client: Client11
2023-07-01 00:24:32,824 - INFO - train loss: 0.0002495967201752775
2023-07-01 00:24:32,824 - INFO - train acc: 1.0
2023-07-01 00:24:32,847 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.86      0.77        65
           5       0.75      0.66      0.70        65
           7       0.70      0.63      0.66        70

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:24:32,848 - INFO - test loss 0.02198129193516261
2023-07-01 00:24:32,848 - INFO - test acc 0.7149999737739563
2023-07-01 00:24:34,110 - INFO - Distilling data from client: Client11
2023-07-01 00:24:34,110 - INFO - train loss: 0.0002169998883541346
2023-07-01 00:24:34,110 - INFO - train acc: 1.0
2023-07-01 00:24:34,135 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.86      0.77        65
           5       0.72      0.65      0.68        65
           7       0.69      0.61      0.65        70

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:24:34,136 - INFO - test loss 0.021703799754320803
2023-07-01 00:24:34,136 - INFO - test acc 0.7049999833106995
2023-07-01 00:24:35,404 - INFO - Distilling data from client: Client11
2023-07-01 00:24:35,405 - INFO - train loss: 0.00019552599376195147
2023-07-01 00:24:35,405 - INFO - train acc: 1.0
2023-07-01 00:24:35,428 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.85      0.77        65
           5       0.73      0.66      0.69        65
           7       0.73      0.67      0.70        70

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:24:35,428 - INFO - test loss 0.021427283224263773
2023-07-01 00:24:35,428 - INFO - test acc 0.7249999642372131
2023-07-01 00:24:36,697 - INFO - Distilling data from client: Client11
2023-07-01 00:24:36,697 - INFO - train loss: 0.0001793562671111377
2023-07-01 00:24:36,697 - INFO - train acc: 1.0
2023-07-01 00:24:36,720 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.86      0.78        65
           5       0.72      0.68      0.70        65
           7       0.70      0.61      0.66        70

    accuracy                           0.71       200
   macro avg       0.71      0.72      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:24:36,720 - INFO - test loss 0.02134966043672836
2023-07-01 00:24:36,720 - INFO - test acc 0.7149999737739563
2023-07-01 00:24:37,984 - INFO - Distilling data from client: Client11
2023-07-01 00:24:37,984 - INFO - train loss: 0.00019039598982109412
2023-07-01 00:24:37,984 - INFO - train acc: 1.0
2023-07-01 00:24:38,008 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.85      0.77        65
           5       0.73      0.68      0.70        65
           7       0.70      0.63      0.66        70

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:24:38,008 - INFO - test loss 0.021594045691745806
2023-07-01 00:24:38,008 - INFO - test acc 0.7149999737739563
2023-07-01 00:24:39,259 - INFO - Distilling data from client: Client11
2023-07-01 00:24:39,259 - INFO - train loss: 0.00016960860857966598
2023-07-01 00:24:39,260 - INFO - train acc: 1.0
2023-07-01 00:24:39,283 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.83      0.77        65
           5       0.70      0.66      0.68        65
           7       0.69      0.63      0.66        70

    accuracy                           0.70       200
   macro avg       0.70      0.71      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:24:39,283 - INFO - test loss 0.021795141504936062
2023-07-01 00:24:39,283 - INFO - test acc 0.7049999833106995
2023-07-01 00:24:40,540 - INFO - Distilling data from client: Client11
2023-07-01 00:24:40,540 - INFO - train loss: 0.00019495557805314695
2023-07-01 00:24:40,540 - INFO - train acc: 1.0
2023-07-01 00:24:40,603 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.91      0.80        65
           5       0.75      0.66      0.70        65
           7       0.74      0.64      0.69        70

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:24:40,604 - INFO - test loss 0.02160867674660976
2023-07-01 00:24:40,604 - INFO - test acc 0.73499995470047
2023-07-01 00:24:41,855 - INFO - Distilling data from client: Client11
2023-07-01 00:24:41,855 - INFO - train loss: 0.0001736787569857892
2023-07-01 00:24:41,855 - INFO - train acc: 1.0
2023-07-01 00:24:41,917 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.89      0.79        65
           5       0.76      0.68      0.72        65
           7       0.75      0.66      0.70        70

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:24:41,918 - INFO - test loss 0.021369521871917405
2023-07-01 00:24:41,918 - INFO - test acc 0.7400000095367432
2023-07-01 00:24:43,170 - INFO - Distilling data from client: Client11
2023-07-01 00:24:43,170 - INFO - train loss: 0.00017931405176649248
2023-07-01 00:24:43,170 - INFO - train acc: 1.0
2023-07-01 00:24:43,196 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.85      0.79        65
           5       0.75      0.69      0.72        65
           7       0.71      0.66      0.68        70

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-07-01 00:24:43,196 - INFO - test loss 0.022001168334285597
2023-07-01 00:24:43,196 - INFO - test acc 0.7299999594688416
2023-07-01 00:24:44,435 - INFO - Distilling data from client: Client11
2023-07-01 00:24:44,436 - INFO - train loss: 0.0001502858689802042
2023-07-01 00:24:44,436 - INFO - train acc: 1.0
2023-07-01 00:24:44,459 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.83      0.77        65
           5       0.72      0.68      0.70        65
           7       0.69      0.63      0.66        70

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:24:44,459 - INFO - test loss 0.021740454411435326
2023-07-01 00:24:44,459 - INFO - test acc 0.7099999785423279
2023-07-01 00:24:45,730 - INFO - Distilling data from client: Client11
2023-07-01 00:24:45,730 - INFO - train loss: 0.00015318879594440767
2023-07-01 00:24:45,730 - INFO - train acc: 1.0
2023-07-01 00:24:45,754 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.85      0.78        65
           5       0.73      0.69      0.71        65
           7       0.69      0.61      0.65        70

    accuracy                           0.71       200
   macro avg       0.71      0.72      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:24:45,754 - INFO - test loss 0.021681437248636967
2023-07-01 00:24:45,754 - INFO - test acc 0.7149999737739563
2023-07-01 00:24:47,010 - INFO - Distilling data from client: Client11
2023-07-01 00:24:47,011 - INFO - train loss: 0.00016937914207085182
2023-07-01 00:24:47,011 - INFO - train acc: 1.0
2023-07-01 00:24:47,035 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.85      0.77        65
           5       0.75      0.68      0.71        65
           7       0.70      0.63      0.66        70

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:24:47,035 - INFO - test loss 0.021196287406225506
2023-07-01 00:24:47,036 - INFO - test acc 0.7149999737739563
2023-07-01 00:24:47,047 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:47,055 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:47,064 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:47,073 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:47,082 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:47,091 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:47,100 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:47,110 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:47,119 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:24:47,494 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client11//synthetic.png
2023-07-01 00:24:47,507 - INFO - c: 1.0 and total_data_in_this_class: 272
2023-07-01 00:24:47,507 - INFO - c: 6.0 and total_data_in_this_class: 267
2023-07-01 00:24:47,508 - INFO - c: 7.0 and total_data_in_this_class: 260
2023-07-01 00:24:47,508 - INFO - c: 1.0 and total_data_in_this_class: 61
2023-07-01 00:24:47,508 - INFO - c: 6.0 and total_data_in_this_class: 66
2023-07-01 00:24:47,508 - INFO - c: 7.0 and total_data_in_this_class: 73
2023-07-01 00:24:47,579 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0476837158203125 sec
2023-07-01 00:24:47,626 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.045950889587402344 sec
2023-07-01 00:24:47,631 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.10071492195129395 sec
2023-07-01 00:24:47,632 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:24:47,665 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03277778625488281 sec
2023-07-01 00:24:47,666 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:24:47,789 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1236414909362793 sec
2023-07-01 00:24:47,812 - INFO - initial test loss: 0.023340683899852523
2023-07-01 00:24:47,812 - INFO - initial test acc: 0.7149999737739563
2023-07-01 00:24:47,820 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.0057506561279296875 sec
2023-07-01 00:24:48,209 - WARNING - Finished tracing + transforming update_fn for pjit in 0.39469408988952637 sec
2023-07-01 00:24:48,212 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:24:48,275 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06181645393371582 sec
2023-07-01 00:24:48,275 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:24:48,602 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32669758796691895 sec
2023-07-01 00:24:49,854 - INFO - Distilling data from client: Client12
2023-07-01 00:24:49,854 - INFO - train loss: 0.001893336084016186
2023-07-01 00:24:49,854 - INFO - train acc: 1.0
2023-07-01 00:24:49,910 - INFO - report:               precision    recall  f1-score   support

           1       0.65      0.74      0.69        61
           6       0.67      0.74      0.71        66
           7       0.76      0.60      0.67        73

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:24:49,910 - INFO - test loss 0.022449521223204018
2023-07-01 00:24:49,910 - INFO - test acc 0.6899999976158142
2023-07-01 00:24:51,158 - INFO - Distilling data from client: Client12
2023-07-01 00:24:51,158 - INFO - train loss: 0.0009339936267457063
2023-07-01 00:24:51,158 - INFO - train acc: 1.0
2023-07-01 00:24:51,227 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.80      0.74        61
           6       0.67      0.74      0.71        66
           7       0.78      0.59      0.67        73

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:24:51,227 - INFO - test loss 0.023188995306286726
2023-07-01 00:24:51,228 - INFO - test acc 0.7049999833106995
2023-07-01 00:24:52,482 - INFO - Distilling data from client: Client12
2023-07-01 00:24:52,482 - INFO - train loss: 0.0007391885341689335
2023-07-01 00:24:52,482 - INFO - train acc: 1.0
2023-07-01 00:24:52,542 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.80      0.74        61
           6       0.74      0.77      0.76        66
           7       0.76      0.62      0.68        73

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:24:52,542 - INFO - test loss 0.02319126045992161
2023-07-01 00:24:52,542 - INFO - test acc 0.7249999642372131
2023-07-01 00:24:53,786 - INFO - Distilling data from client: Client12
2023-07-01 00:24:53,786 - INFO - train loss: 0.0005980966513057956
2023-07-01 00:24:53,786 - INFO - train acc: 1.0
2023-07-01 00:24:53,809 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.82      0.73        61
           6       0.65      0.73      0.69        66
           7       0.78      0.53      0.63        73

    accuracy                           0.69       200
   macro avg       0.70      0.69      0.68       200
weighted avg       0.70      0.69      0.68       200

2023-07-01 00:24:53,809 - INFO - test loss 0.023800614964545053
2023-07-01 00:24:53,809 - INFO - test acc 0.6850000023841858
2023-07-01 00:24:55,044 - INFO - Distilling data from client: Client12
2023-07-01 00:24:55,044 - INFO - train loss: 0.0006729939236495902
2023-07-01 00:24:55,045 - INFO - train acc: 1.0
2023-07-01 00:24:55,068 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.82      0.73        61
           6       0.68      0.71      0.70        66
           7       0.78      0.59      0.67        73

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:24:55,068 - INFO - test loss 0.02312424390061907
2023-07-01 00:24:55,068 - INFO - test acc 0.699999988079071
2023-07-01 00:24:56,305 - INFO - Distilling data from client: Client12
2023-07-01 00:24:56,305 - INFO - train loss: 0.0005163526483252574
2023-07-01 00:24:56,305 - INFO - train acc: 1.0
2023-07-01 00:24:56,328 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.75      0.72        61
           6       0.63      0.76      0.69        66
           7       0.76      0.56      0.65        73

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.70      0.69      0.68       200

2023-07-01 00:24:56,329 - INFO - test loss 0.02370243362704621
2023-07-01 00:24:56,329 - INFO - test acc 0.6850000023841858
2023-07-01 00:24:57,577 - INFO - Distilling data from client: Client12
2023-07-01 00:24:57,577 - INFO - train loss: 0.00045738502984470726
2023-07-01 00:24:57,577 - INFO - train acc: 1.0
2023-07-01 00:24:57,599 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.77      0.73        61
           6       0.67      0.76      0.71        66
           7       0.75      0.59      0.66        73

    accuracy                           0.70       200
   macro avg       0.70      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:24:57,599 - INFO - test loss 0.02404870226796567
2023-07-01 00:24:57,600 - INFO - test acc 0.699999988079071
2023-07-01 00:24:58,837 - INFO - Distilling data from client: Client12
2023-07-01 00:24:58,838 - INFO - train loss: 0.00033173867489376076
2023-07-01 00:24:58,838 - INFO - train acc: 1.0
2023-07-01 00:24:58,861 - INFO - report:               precision    recall  f1-score   support

           1       0.64      0.77      0.70        61
           6       0.63      0.67      0.65        66
           7       0.75      0.58      0.65        73

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.66       200
weighted avg       0.67      0.67      0.66       200

2023-07-01 00:24:58,861 - INFO - test loss 0.024250673314107365
2023-07-01 00:24:58,861 - INFO - test acc 0.6649999618530273
2023-07-01 00:25:00,106 - INFO - Distilling data from client: Client12
2023-07-01 00:25:00,106 - INFO - train loss: 0.0003552005000529259
2023-07-01 00:25:00,106 - INFO - train acc: 1.0
2023-07-01 00:25:00,130 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.80      0.74        61
           6       0.63      0.70      0.66        66
           7       0.76      0.58      0.66        73

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.69      0.68       200

2023-07-01 00:25:00,130 - INFO - test loss 0.02360878764370097
2023-07-01 00:25:00,131 - INFO - test acc 0.6850000023841858
2023-07-01 00:25:01,365 - INFO - Distilling data from client: Client12
2023-07-01 00:25:01,365 - INFO - train loss: 0.000284226045899991
2023-07-01 00:25:01,365 - INFO - train acc: 1.0
2023-07-01 00:25:01,389 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.75      0.71        61
           6       0.64      0.74      0.69        66
           7       0.75      0.56      0.64        73

    accuracy                           0.68       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:25:01,389 - INFO - test loss 0.02357081460184137
2023-07-01 00:25:01,389 - INFO - test acc 0.6800000071525574
2023-07-01 00:25:02,629 - INFO - Distilling data from client: Client12
2023-07-01 00:25:02,630 - INFO - train loss: 0.00033036339568143084
2023-07-01 00:25:02,630 - INFO - train acc: 1.0
2023-07-01 00:25:02,654 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.80      0.73        61
           6       0.63      0.71      0.67        66
           7       0.80      0.56      0.66        73

    accuracy                           0.69       200
   macro avg       0.70      0.69      0.68       200
weighted avg       0.70      0.69      0.68       200

2023-07-01 00:25:02,654 - INFO - test loss 0.023579980766450518
2023-07-01 00:25:02,654 - INFO - test acc 0.6850000023841858
2023-07-01 00:25:03,909 - INFO - Distilling data from client: Client12
2023-07-01 00:25:03,909 - INFO - train loss: 0.00026720316505258004
2023-07-01 00:25:03,909 - INFO - train acc: 1.0
2023-07-01 00:25:03,933 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.79      0.72        61
           6       0.62      0.71      0.66        66
           7       0.80      0.56      0.66        73

    accuracy                           0.68       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.70      0.68      0.68       200

2023-07-01 00:25:03,934 - INFO - test loss 0.02384437458759156
2023-07-01 00:25:03,934 - INFO - test acc 0.6800000071525574
2023-07-01 00:25:05,170 - INFO - Distilling data from client: Client12
2023-07-01 00:25:05,170 - INFO - train loss: 0.00026324198237073586
2023-07-01 00:25:05,170 - INFO - train acc: 1.0
2023-07-01 00:25:05,193 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.80      0.74        61
           6       0.66      0.71      0.69        66
           7       0.79      0.63      0.70        73

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:25:05,194 - INFO - test loss 0.02339491748596029
2023-07-01 00:25:05,194 - INFO - test acc 0.7099999785423279
2023-07-01 00:25:06,438 - INFO - Distilling data from client: Client12
2023-07-01 00:25:06,438 - INFO - train loss: 0.00021737511160141692
2023-07-01 00:25:06,438 - INFO - train acc: 1.0
2023-07-01 00:25:06,461 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.79      0.73        61
           6       0.65      0.71      0.68        66
           7       0.77      0.60      0.68        73

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:25:06,461 - INFO - test loss 0.023543441573168927
2023-07-01 00:25:06,461 - INFO - test acc 0.6949999928474426
2023-07-01 00:25:07,699 - INFO - Distilling data from client: Client12
2023-07-01 00:25:07,700 - INFO - train loss: 0.000293529273772992
2023-07-01 00:25:07,700 - INFO - train acc: 1.0
2023-07-01 00:25:07,723 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.79      0.73        61
           6       0.63      0.73      0.68        66
           7       0.80      0.59      0.68        73

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.69      0.69       200

2023-07-01 00:25:07,723 - INFO - test loss 0.023501057812989806
2023-07-01 00:25:07,723 - INFO - test acc 0.6949999928474426
2023-07-01 00:25:08,967 - INFO - Distilling data from client: Client12
2023-07-01 00:25:08,968 - INFO - train loss: 0.00027407212865135757
2023-07-01 00:25:08,968 - INFO - train acc: 1.0
2023-07-01 00:25:08,991 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.79      0.73        61
           6       0.65      0.73      0.69        66
           7       0.78      0.59      0.67        73

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.71      0.69      0.69       200

2023-07-01 00:25:08,991 - INFO - test loss 0.02364701032157664
2023-07-01 00:25:08,992 - INFO - test acc 0.6949999928474426
2023-07-01 00:25:10,236 - INFO - Distilling data from client: Client12
2023-07-01 00:25:10,236 - INFO - train loss: 0.0002897302881251418
2023-07-01 00:25:10,236 - INFO - train acc: 1.0
2023-07-01 00:25:10,263 - INFO - report:               precision    recall  f1-score   support

           1       0.65      0.74      0.69        61
           6       0.64      0.74      0.69        66
           7       0.76      0.58      0.66        73

    accuracy                           0.68       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:25:10,263 - INFO - test loss 0.023505179534589984
2023-07-01 00:25:10,263 - INFO - test acc 0.6800000071525574
2023-07-01 00:25:11,514 - INFO - Distilling data from client: Client12
2023-07-01 00:25:11,514 - INFO - train loss: 0.0002128883090728767
2023-07-01 00:25:11,514 - INFO - train acc: 1.0
2023-07-01 00:25:11,540 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.77      0.71        61
           6       0.62      0.70      0.66        66
           7       0.75      0.56      0.64        73

    accuracy                           0.67       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:25:11,540 - INFO - test loss 0.024003643413786423
2023-07-01 00:25:11,540 - INFO - test acc 0.6699999570846558
2023-07-01 00:25:12,781 - INFO - Distilling data from client: Client12
2023-07-01 00:25:12,782 - INFO - train loss: 0.00019595338676759138
2023-07-01 00:25:12,782 - INFO - train acc: 1.0
2023-07-01 00:25:12,805 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.79      0.73        61
           6       0.65      0.70      0.67        66
           7       0.78      0.63      0.70        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:25:12,805 - INFO - test loss 0.023893784259677517
2023-07-01 00:25:12,805 - INFO - test acc 0.699999988079071
2023-07-01 00:25:14,038 - INFO - Distilling data from client: Client12
2023-07-01 00:25:14,038 - INFO - train loss: 0.00019833829247152616
2023-07-01 00:25:14,038 - INFO - train acc: 1.0
2023-07-01 00:25:14,061 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.74      0.70        61
           6       0.63      0.73      0.68        66
           7       0.77      0.59      0.67        73

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:25:14,062 - INFO - test loss 0.02456838020953392
2023-07-01 00:25:14,062 - INFO - test acc 0.6800000071525574
2023-07-01 00:25:15,298 - INFO - Distilling data from client: Client12
2023-07-01 00:25:15,298 - INFO - train loss: 0.0002197708847265327
2023-07-01 00:25:15,298 - INFO - train acc: 1.0
2023-07-01 00:25:15,320 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.79      0.73        61
           6       0.68      0.76      0.72        66
           7       0.79      0.62      0.69        73

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:25:15,321 - INFO - test loss 0.02319016577971914
2023-07-01 00:25:15,321 - INFO - test acc 0.7149999737739563
2023-07-01 00:25:16,564 - INFO - Distilling data from client: Client12
2023-07-01 00:25:16,564 - INFO - train loss: 0.0002565752803696921
2023-07-01 00:25:16,564 - INFO - train acc: 1.0
2023-07-01 00:25:16,588 - INFO - report:               precision    recall  f1-score   support

           1       0.65      0.77      0.71        61
           6       0.63      0.73      0.68        66
           7       0.81      0.58      0.67        73

    accuracy                           0.69       200
   macro avg       0.70      0.69      0.68       200
weighted avg       0.70      0.69      0.68       200

2023-07-01 00:25:16,588 - INFO - test loss 0.02360629397741588
2023-07-01 00:25:16,588 - INFO - test acc 0.6850000023841858
2023-07-01 00:25:17,834 - INFO - Distilling data from client: Client12
2023-07-01 00:25:17,834 - INFO - train loss: 0.0002372237564116549
2023-07-01 00:25:17,834 - INFO - train acc: 1.0
2023-07-01 00:25:17,860 - INFO - report:               precision    recall  f1-score   support

           1       0.65      0.77      0.71        61
           6       0.66      0.73      0.69        66
           7       0.82      0.62      0.70        73

    accuracy                           0.70       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:25:17,860 - INFO - test loss 0.02369302579503907
2023-07-01 00:25:17,860 - INFO - test acc 0.699999988079071
2023-07-01 00:25:19,097 - INFO - Distilling data from client: Client12
2023-07-01 00:25:19,097 - INFO - train loss: 0.00019491421989299728
2023-07-01 00:25:19,097 - INFO - train acc: 1.0
2023-07-01 00:25:19,120 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.80      0.74        61
           6       0.66      0.73      0.69        66
           7       0.78      0.59      0.67        73

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:25:19,121 - INFO - test loss 0.023492502601736156
2023-07-01 00:25:19,121 - INFO - test acc 0.699999988079071
2023-07-01 00:25:20,362 - INFO - Distilling data from client: Client12
2023-07-01 00:25:20,362 - INFO - train loss: 0.00023806425844423803
2023-07-01 00:25:20,362 - INFO - train acc: 1.0
2023-07-01 00:25:20,387 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.77      0.72        61
           6       0.63      0.73      0.68        66
           7       0.75      0.56      0.64        73

    accuracy                           0.68       200
   macro avg       0.69      0.69      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:25:20,387 - INFO - test loss 0.024066462071097156
2023-07-01 00:25:20,387 - INFO - test acc 0.6800000071525574
2023-07-01 00:25:20,399 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:20,408 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:20,417 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:20,425 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:20,434 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:20,442 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:20,452 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:20,460 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:20,469 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:20,849 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client12//synthetic.png
2023-07-01 00:25:20,860 - INFO - c: 0.0 and total_data_in_this_class: 270
2023-07-01 00:25:20,861 - INFO - c: 4.0 and total_data_in_this_class: 256
2023-07-01 00:25:20,861 - INFO - c: 7.0 and total_data_in_this_class: 273
2023-07-01 00:25:20,861 - INFO - c: 0.0 and total_data_in_this_class: 63
2023-07-01 00:25:20,861 - INFO - c: 4.0 and total_data_in_this_class: 77
2023-07-01 00:25:20,861 - INFO - c: 7.0 and total_data_in_this_class: 60
2023-07-01 00:25:20,931 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04727935791015625 sec
2023-07-01 00:25:20,977 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04497694969177246 sec
2023-07-01 00:25:20,982 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.09911680221557617 sec
2023-07-01 00:25:20,984 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:25:21,016 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03252530097961426 sec
2023-07-01 00:25:21,016 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:25:21,142 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1251084804534912 sec
2023-07-01 00:25:21,164 - INFO - initial test loss: 0.024530602142511394
2023-07-01 00:25:21,164 - INFO - initial test acc: 0.6599999666213989
2023-07-01 00:25:21,172 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.00564885139465332 sec
2023-07-01 00:25:21,281 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11483407020568848 sec
2023-07-01 00:25:21,285 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:25:21,347 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06165146827697754 sec
2023-07-01 00:25:21,347 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:25:21,678 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3313751220703125 sec
2023-07-01 00:25:22,894 - INFO - Distilling data from client: Client13
2023-07-01 00:25:22,894 - INFO - train loss: 0.0024920281675113484
2023-07-01 00:25:22,894 - INFO - train acc: 0.9941520690917969
2023-07-01 00:25:22,951 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.75      0.76        63
           4       0.67      0.69      0.68        77
           7       0.58      0.58      0.58        60

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:25:22,951 - INFO - test loss 0.02383671885115298
2023-07-01 00:25:22,951 - INFO - test acc 0.675000011920929
2023-07-01 00:25:24,174 - INFO - Distilling data from client: Client13
2023-07-01 00:25:24,175 - INFO - train loss: 0.001383928378288416
2023-07-01 00:25:24,175 - INFO - train acc: 0.9980506896972656
2023-07-01 00:25:24,199 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        63
           4       0.68      0.68      0.68        77
           7       0.60      0.62      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:25:24,199 - INFO - test loss 0.023495421972231388
2023-07-01 00:25:24,199 - INFO - test acc 0.675000011920929
2023-07-01 00:25:25,428 - INFO - Distilling data from client: Client13
2023-07-01 00:25:25,428 - INFO - train loss: 0.001141171402853887
2023-07-01 00:25:25,428 - INFO - train acc: 1.0
2023-07-01 00:25:25,452 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.70      0.73        63
           4       0.62      0.65      0.63        77
           7       0.55      0.57      0.56        60

    accuracy                           0.64       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.64      0.64       200

2023-07-01 00:25:25,452 - INFO - test loss 0.02384639726143648
2023-07-01 00:25:25,452 - INFO - test acc 0.6399999856948853
2023-07-01 00:25:26,679 - INFO - Distilling data from client: Client13
2023-07-01 00:25:26,679 - INFO - train loss: 0.0009659095292068122
2023-07-01 00:25:26,680 - INFO - train acc: 1.0
2023-07-01 00:25:26,705 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.73      0.75        63
           4       0.64      0.69      0.66        77
           7       0.60      0.58      0.59        60

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:25:26,706 - INFO - test loss 0.023818294498004794
2023-07-01 00:25:26,706 - INFO - test acc 0.6699999570846558
2023-07-01 00:25:27,948 - INFO - Distilling data from client: Client13
2023-07-01 00:25:27,948 - INFO - train loss: 0.0008561507483854642
2023-07-01 00:25:27,948 - INFO - train acc: 1.0
2023-07-01 00:25:28,007 - INFO - report:               precision    recall  f1-score   support

           0       0.81      0.75      0.78        63
           4       0.70      0.68      0.69        77
           7       0.60      0.68      0.64        60

    accuracy                           0.70       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:25:28,008 - INFO - test loss 0.024056024882588727
2023-07-01 00:25:28,008 - INFO - test acc 0.699999988079071
2023-07-01 00:25:29,232 - INFO - Distilling data from client: Client13
2023-07-01 00:25:29,232 - INFO - train loss: 0.0006609202427631162
2023-07-01 00:25:29,232 - INFO - train acc: 0.9980506896972656
2023-07-01 00:25:29,254 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.71      0.73        63
           4       0.68      0.70      0.69        77
           7       0.60      0.60      0.60        60

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:25:29,255 - INFO - test loss 0.02437064651714406
2023-07-01 00:25:29,255 - INFO - test acc 0.675000011920929
2023-07-01 00:25:30,472 - INFO - Distilling data from client: Client13
2023-07-01 00:25:30,473 - INFO - train loss: 0.0008240391440573789
2023-07-01 00:25:30,473 - INFO - train acc: 1.0
2023-07-01 00:25:30,495 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.79      0.78        63
           4       0.69      0.66      0.68        77
           7       0.61      0.62      0.61        60

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:25:30,496 - INFO - test loss 0.02380000195260895
2023-07-01 00:25:30,496 - INFO - test acc 0.6899999976158142
2023-07-01 00:25:31,721 - INFO - Distilling data from client: Client13
2023-07-01 00:25:31,721 - INFO - train loss: 0.0007472503267620578
2023-07-01 00:25:31,722 - INFO - train acc: 1.0
2023-07-01 00:25:31,751 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.78      0.77        63
           4       0.69      0.69      0.69        77
           7       0.62      0.60      0.61        60

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:25:31,751 - INFO - test loss 0.02377597999359776
2023-07-01 00:25:31,752 - INFO - test acc 0.6899999976158142
2023-07-01 00:25:32,973 - INFO - Distilling data from client: Client13
2023-07-01 00:25:32,973 - INFO - train loss: 0.0006456968682165688
2023-07-01 00:25:32,973 - INFO - train acc: 1.0
2023-07-01 00:25:32,996 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.76      0.75        63
           4       0.67      0.69      0.68        77
           7       0.61      0.57      0.59        60

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.67      0.68      0.67       200

2023-07-01 00:25:32,997 - INFO - test loss 0.023828031048563632
2023-07-01 00:25:32,997 - INFO - test acc 0.675000011920929
2023-07-01 00:25:34,225 - INFO - Distilling data from client: Client13
2023-07-01 00:25:34,225 - INFO - train loss: 0.0006205794781523334
2023-07-01 00:25:34,225 - INFO - train acc: 1.0
2023-07-01 00:25:34,247 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.77        63
           4       0.64      0.69      0.66        77
           7       0.60      0.58      0.59        60

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:25:34,248 - INFO - test loss 0.024112209225440717
2023-07-01 00:25:34,248 - INFO - test acc 0.675000011920929
2023-07-01 00:25:35,472 - INFO - Distilling data from client: Client13
2023-07-01 00:25:35,472 - INFO - train loss: 0.00048093018767490325
2023-07-01 00:25:35,472 - INFO - train acc: 1.0
2023-07-01 00:25:35,495 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.79      0.79        63
           4       0.66      0.68      0.67        77
           7       0.61      0.58      0.60        60

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.69      0.68       200

2023-07-01 00:25:35,496 - INFO - test loss 0.02369214270739871
2023-07-01 00:25:35,496 - INFO - test acc 0.6850000023841858
2023-07-01 00:25:36,731 - INFO - Distilling data from client: Client13
2023-07-01 00:25:36,731 - INFO - train loss: 0.0003890904135655523
2023-07-01 00:25:36,731 - INFO - train acc: 1.0
2023-07-01 00:25:36,756 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.75      0.75        63
           4       0.68      0.69      0.68        77
           7       0.58      0.58      0.58        60

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:25:36,756 - INFO - test loss 0.024534226471958444
2023-07-01 00:25:36,757 - INFO - test acc 0.675000011920929
2023-07-01 00:25:37,978 - INFO - Distilling data from client: Client13
2023-07-01 00:25:37,978 - INFO - train loss: 0.0004508070313115946
2023-07-01 00:25:37,978 - INFO - train acc: 1.0
2023-07-01 00:25:38,001 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.77        63
           4       0.67      0.66      0.67        77
           7       0.62      0.67      0.64        60

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:25:38,001 - INFO - test loss 0.023933096944279118
2023-07-01 00:25:38,001 - INFO - test acc 0.6899999976158142
2023-07-01 00:25:39,227 - INFO - Distilling data from client: Client13
2023-07-01 00:25:39,227 - INFO - train loss: 0.0004057743798855572
2023-07-01 00:25:39,227 - INFO - train acc: 1.0
2023-07-01 00:25:39,251 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.75      0.76        63
           4       0.65      0.68      0.66        77
           7       0.59      0.58      0.59        60

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:25:39,251 - INFO - test loss 0.023981094127797507
2023-07-01 00:25:39,251 - INFO - test acc 0.6699999570846558
2023-07-01 00:25:40,476 - INFO - Distilling data from client: Client13
2023-07-01 00:25:40,476 - INFO - train loss: 0.0003510612276078684
2023-07-01 00:25:40,476 - INFO - train acc: 1.0
2023-07-01 00:25:40,499 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.76      0.77        63
           4       0.66      0.68      0.67        77
           7       0.63      0.62      0.62        60

    accuracy                           0.69       200
   macro avg       0.69      0.68      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:25:40,499 - INFO - test loss 0.024137715913180265
2023-07-01 00:25:40,500 - INFO - test acc 0.6850000023841858
2023-07-01 00:25:41,726 - INFO - Distilling data from client: Client13
2023-07-01 00:25:41,726 - INFO - train loss: 0.00044018448619404757
2023-07-01 00:25:41,726 - INFO - train acc: 1.0
2023-07-01 00:25:41,750 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.77        63
           4       0.68      0.70      0.69        77
           7       0.63      0.65      0.64        60

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:25:41,750 - INFO - test loss 0.024065037984113666
2023-07-01 00:25:41,750 - INFO - test acc 0.699999988079071
2023-07-01 00:25:42,984 - INFO - Distilling data from client: Client13
2023-07-01 00:25:42,985 - INFO - train loss: 0.00041190230744207035
2023-07-01 00:25:42,985 - INFO - train acc: 1.0
2023-07-01 00:25:43,008 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.76      0.76        63
           4       0.67      0.68      0.67        77
           7       0.61      0.60      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:25:43,008 - INFO - test loss 0.024337024041827405
2023-07-01 00:25:43,008 - INFO - test acc 0.6800000071525574
2023-07-01 00:25:44,228 - INFO - Distilling data from client: Client13
2023-07-01 00:25:44,228 - INFO - train loss: 0.0003957106156505937
2023-07-01 00:25:44,228 - INFO - train acc: 1.0
2023-07-01 00:25:44,252 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.78      0.78        63
           4       0.67      0.64      0.65        77
           7       0.59      0.63      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:25:44,252 - INFO - test loss 0.02478698205697171
2023-07-01 00:25:44,252 - INFO - test acc 0.6800000071525574
2023-07-01 00:25:45,484 - INFO - Distilling data from client: Client13
2023-07-01 00:25:45,484 - INFO - train loss: 0.00030002593424229235
2023-07-01 00:25:45,484 - INFO - train acc: 1.0
2023-07-01 00:25:45,507 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.75      0.76        63
           4       0.65      0.69      0.67        77
           7       0.61      0.58      0.60        60

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.68      0.68      0.67       200

2023-07-01 00:25:45,508 - INFO - test loss 0.023672817491045734
2023-07-01 00:25:45,508 - INFO - test acc 0.675000011920929
2023-07-01 00:25:46,735 - INFO - Distilling data from client: Client13
2023-07-01 00:25:46,735 - INFO - train loss: 0.0003153077525364447
2023-07-01 00:25:46,735 - INFO - train acc: 1.0
2023-07-01 00:25:46,759 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.75      0.76        63
           4       0.65      0.66      0.66        77
           7       0.60      0.62      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:25:46,759 - INFO - test loss 0.02429574605610048
2023-07-01 00:25:46,759 - INFO - test acc 0.675000011920929
2023-07-01 00:25:47,987 - INFO - Distilling data from client: Client13
2023-07-01 00:25:47,987 - INFO - train loss: 0.0002673068673507079
2023-07-01 00:25:47,987 - INFO - train acc: 1.0
2023-07-01 00:25:48,011 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.76      0.77        63
           4       0.65      0.66      0.66        77
           7       0.61      0.62      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:25:48,011 - INFO - test loss 0.024003113499518797
2023-07-01 00:25:48,011 - INFO - test acc 0.6800000071525574
2023-07-01 00:25:49,242 - INFO - Distilling data from client: Client13
2023-07-01 00:25:49,243 - INFO - train loss: 0.0002944816876319769
2023-07-01 00:25:49,243 - INFO - train acc: 1.0
2023-07-01 00:25:49,265 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.76      0.78        63
           4       0.65      0.68      0.66        77
           7       0.58      0.58      0.58        60

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:25:49,265 - INFO - test loss 0.02433361489717757
2023-07-01 00:25:49,265 - INFO - test acc 0.675000011920929
2023-07-01 00:25:50,499 - INFO - Distilling data from client: Client13
2023-07-01 00:25:50,499 - INFO - train loss: 0.0002509189264056399
2023-07-01 00:25:50,499 - INFO - train acc: 1.0
2023-07-01 00:25:50,522 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.78      0.80        63
           4       0.66      0.69      0.68        77
           7       0.62      0.63      0.63        60

    accuracy                           0.70       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:25:50,522 - INFO - test loss 0.023998960190241385
2023-07-01 00:25:50,522 - INFO - test acc 0.699999988079071
2023-07-01 00:25:51,757 - INFO - Distilling data from client: Client13
2023-07-01 00:25:51,758 - INFO - train loss: 0.00029186035601144213
2023-07-01 00:25:51,758 - INFO - train acc: 1.0
2023-07-01 00:25:51,782 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.76      0.77        63
           4       0.65      0.66      0.65        77
           7       0.61      0.60      0.61        60

    accuracy                           0.68       200
   macro avg       0.68      0.67      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:25:51,782 - INFO - test loss 0.024158420239133643
2023-07-01 00:25:51,782 - INFO - test acc 0.675000011920929
2023-07-01 00:25:53,005 - INFO - Distilling data from client: Client13
2023-07-01 00:25:53,005 - INFO - train loss: 0.0002790282419752574
2023-07-01 00:25:53,005 - INFO - train acc: 1.0
2023-07-01 00:25:53,028 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.75      0.76        63
           4       0.66      0.69      0.68        77
           7       0.62      0.62      0.62        60

    accuracy                           0.69       200
   macro avg       0.69      0.68      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:25:53,028 - INFO - test loss 0.0239100597508473
2023-07-01 00:25:53,028 - INFO - test acc 0.6850000023841858
2023-07-01 00:25:53,040 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:53,048 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:53,057 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:53,067 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:53,076 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:53,085 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:53,094 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:53,103 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:53,112 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:25:53,481 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client13//synthetic.png
2023-07-01 00:25:53,494 - INFO - c: 3.0 and total_data_in_this_class: 271
2023-07-01 00:25:53,494 - INFO - c: 6.0 and total_data_in_this_class: 266
2023-07-01 00:25:53,494 - INFO - c: 9.0 and total_data_in_this_class: 262
2023-07-01 00:25:53,494 - INFO - c: 3.0 and total_data_in_this_class: 62
2023-07-01 00:25:53,494 - INFO - c: 6.0 and total_data_in_this_class: 67
2023-07-01 00:25:53,494 - INFO - c: 9.0 and total_data_in_this_class: 71
2023-07-01 00:25:53,565 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04787731170654297 sec
2023-07-01 00:25:53,611 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04535079002380371 sec
2023-07-01 00:25:53,616 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.1003122329711914 sec
2023-07-01 00:25:53,618 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:25:53,651 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03272652626037598 sec
2023-07-01 00:25:53,651 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:25:53,774 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12318038940429688 sec
2023-07-01 00:25:53,797 - INFO - initial test loss: 0.024398382369826666
2023-07-01 00:25:53,797 - INFO - initial test acc: 0.6549999713897705
2023-07-01 00:25:53,805 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.0054476261138916016 sec
2023-07-01 00:25:53,916 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11689615249633789 sec
2023-07-01 00:25:53,919 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:25:53,983 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06388187408447266 sec
2023-07-01 00:25:53,983 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:25:54,319 - WARNING - Finished XLA compilation of jit(update_fn) in 0.33516693115234375 sec
2023-07-01 00:25:55,577 - INFO - Distilling data from client: Client14
2023-07-01 00:25:55,577 - INFO - train loss: 0.002260313958100144
2023-07-01 00:25:55,578 - INFO - train acc: 0.9961904883384705
2023-07-01 00:25:55,640 - INFO - report:               precision    recall  f1-score   support

           3       0.56      0.63      0.59        62
           6       0.64      0.73      0.68        67
           9       0.85      0.63      0.73        71

    accuracy                           0.67       200
   macro avg       0.68      0.66      0.67       200
weighted avg       0.69      0.67      0.67       200

2023-07-01 00:25:55,640 - INFO - test loss 0.022061632898906683
2023-07-01 00:25:55,640 - INFO - test acc 0.6649999618530273
2023-07-01 00:25:56,895 - INFO - Distilling data from client: Client14
2023-07-01 00:25:56,895 - INFO - train loss: 0.0011213997562850213
2023-07-01 00:25:56,895 - INFO - train acc: 1.0
2023-07-01 00:25:56,921 - INFO - report:               precision    recall  f1-score   support

           3       0.57      0.61      0.59        62
           6       0.64      0.73      0.68        67
           9       0.82      0.65      0.72        71

    accuracy                           0.67       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:25:56,921 - INFO - test loss 0.02200075794457463
2023-07-01 00:25:56,921 - INFO - test acc 0.6649999618530273
2023-07-01 00:25:58,173 - INFO - Distilling data from client: Client14
2023-07-01 00:25:58,173 - INFO - train loss: 0.0008301813723117472
2023-07-01 00:25:58,173 - INFO - train acc: 1.0
2023-07-01 00:25:58,239 - INFO - report:               precision    recall  f1-score   support

           3       0.59      0.61      0.60        62
           6       0.64      0.75      0.69        67
           9       0.83      0.68      0.74        71

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:25:58,239 - INFO - test loss 0.02227370339677935
2023-07-01 00:25:58,239 - INFO - test acc 0.6800000071525574
2023-07-01 00:25:59,487 - INFO - Distilling data from client: Client14
2023-07-01 00:25:59,487 - INFO - train loss: 0.0006282957891459164
2023-07-01 00:25:59,487 - INFO - train acc: 1.0
2023-07-01 00:25:59,511 - INFO - report:               precision    recall  f1-score   support

           3       0.55      0.60      0.57        62
           6       0.61      0.70      0.65        67
           9       0.86      0.68      0.76        71

    accuracy                           0.66       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-07-01 00:25:59,511 - INFO - test loss 0.022957467706908617
2023-07-01 00:25:59,511 - INFO - test acc 0.6599999666213989
2023-07-01 00:26:00,756 - INFO - Distilling data from client: Client14
2023-07-01 00:26:00,756 - INFO - train loss: 0.0005328467960458025
2023-07-01 00:26:00,756 - INFO - train acc: 1.0
2023-07-01 00:26:00,779 - INFO - report:               precision    recall  f1-score   support

           3       0.58      0.61      0.59        62
           6       0.63      0.73      0.68        67
           9       0.88      0.69      0.77        71

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.70      0.68      0.68       200

2023-07-01 00:26:00,780 - INFO - test loss 0.022728982400585778
2023-07-01 00:26:00,780 - INFO - test acc 0.6800000071525574
2023-07-01 00:26:02,017 - INFO - Distilling data from client: Client14
2023-07-01 00:26:02,017 - INFO - train loss: 0.00046892012960969283
2023-07-01 00:26:02,017 - INFO - train acc: 1.0
2023-07-01 00:26:02,040 - INFO - report:               precision    recall  f1-score   support

           3       0.58      0.61      0.59        62
           6       0.64      0.75      0.69        67
           9       0.82      0.65      0.72        71

    accuracy                           0.67       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:26:02,041 - INFO - test loss 0.022938495716177874
2023-07-01 00:26:02,041 - INFO - test acc 0.6699999570846558
2023-07-01 00:26:03,291 - INFO - Distilling data from client: Client14
2023-07-01 00:26:03,291 - INFO - train loss: 0.00042646171161195864
2023-07-01 00:26:03,291 - INFO - train acc: 1.0
2023-07-01 00:26:03,314 - INFO - report:               precision    recall  f1-score   support

           3       0.56      0.61      0.58        62
           6       0.64      0.72      0.68        67
           9       0.82      0.66      0.73        71

    accuracy                           0.67       200
   macro avg       0.67      0.66      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:26:03,314 - INFO - test loss 0.0227386459688124
2023-07-01 00:26:03,314 - INFO - test acc 0.6649999618530273
2023-07-01 00:26:04,546 - INFO - Distilling data from client: Client14
2023-07-01 00:26:04,546 - INFO - train loss: 0.0004009749779749394
2023-07-01 00:26:04,546 - INFO - train acc: 1.0
2023-07-01 00:26:04,571 - INFO - report:               precision    recall  f1-score   support

           3       0.56      0.61      0.58        62
           6       0.61      0.70      0.65        67
           9       0.85      0.66      0.75        71

    accuracy                           0.66       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-07-01 00:26:04,571 - INFO - test loss 0.023215913560962805
2023-07-01 00:26:04,571 - INFO - test acc 0.6599999666213989
2023-07-01 00:26:05,826 - INFO - Distilling data from client: Client14
2023-07-01 00:26:05,826 - INFO - train loss: 0.0005305062142561048
2023-07-01 00:26:05,826 - INFO - train acc: 1.0
2023-07-01 00:26:05,849 - INFO - report:               precision    recall  f1-score   support

           3       0.51      0.60      0.55        62
           6       0.59      0.67      0.63        67
           9       0.85      0.62      0.72        71

    accuracy                           0.63       200
   macro avg       0.65      0.63      0.63       200
weighted avg       0.66      0.63      0.64       200

2023-07-01 00:26:05,849 - INFO - test loss 0.023783070541010618
2023-07-01 00:26:05,849 - INFO - test acc 0.6299999952316284
2023-07-01 00:26:07,101 - INFO - Distilling data from client: Client14
2023-07-01 00:26:07,101 - INFO - train loss: 0.0003724331988426569
2023-07-01 00:26:07,101 - INFO - train acc: 1.0
2023-07-01 00:26:07,126 - INFO - report:               precision    recall  f1-score   support

           3       0.53      0.58      0.55        62
           6       0.59      0.70      0.64        67
           9       0.87      0.63      0.73        71

    accuracy                           0.64       200
   macro avg       0.66      0.64      0.64       200
weighted avg       0.67      0.64      0.65       200

2023-07-01 00:26:07,126 - INFO - test loss 0.02351525362406552
2023-07-01 00:26:07,126 - INFO - test acc 0.6399999856948853
2023-07-01 00:26:08,380 - INFO - Distilling data from client: Client14
2023-07-01 00:26:08,380 - INFO - train loss: 0.0003427791306583634
2023-07-01 00:26:08,380 - INFO - train acc: 1.0
2023-07-01 00:26:08,405 - INFO - report:               precision    recall  f1-score   support

           3       0.58      0.61      0.60        62
           6       0.62      0.75      0.68        67
           9       0.85      0.65      0.74        71

    accuracy                           0.67       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.69      0.67      0.67       200

2023-07-01 00:26:08,405 - INFO - test loss 0.022511968876736836
2023-07-01 00:26:08,405 - INFO - test acc 0.6699999570846558
2023-07-01 00:26:09,656 - INFO - Distilling data from client: Client14
2023-07-01 00:26:09,657 - INFO - train loss: 0.00027227568040057826
2023-07-01 00:26:09,657 - INFO - train acc: 1.0
2023-07-01 00:26:09,679 - INFO - report:               precision    recall  f1-score   support

           3       0.57      0.61      0.59        62
           6       0.61      0.73      0.67        67
           9       0.83      0.62      0.71        71

    accuracy                           0.66       200
   macro avg       0.67      0.65      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-07-01 00:26:09,679 - INFO - test loss 0.02332317242998417
2023-07-01 00:26:09,680 - INFO - test acc 0.6549999713897705
2023-07-01 00:26:10,931 - INFO - Distilling data from client: Client14
2023-07-01 00:26:10,931 - INFO - train loss: 0.0003083314751345091
2023-07-01 00:26:10,931 - INFO - train acc: 1.0
2023-07-01 00:26:10,957 - INFO - report:               precision    recall  f1-score   support

           3       0.59      0.63      0.61        62
           6       0.63      0.76      0.69        67
           9       0.83      0.62      0.71        71

    accuracy                           0.67       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.69      0.67      0.67       200

2023-07-01 00:26:10,957 - INFO - test loss 0.022528209462933404
2023-07-01 00:26:10,957 - INFO - test acc 0.6699999570846558
2023-07-01 00:26:12,219 - INFO - Distilling data from client: Client14
2023-07-01 00:26:12,220 - INFO - train loss: 0.0004975460979937114
2023-07-01 00:26:12,220 - INFO - train acc: 1.0
2023-07-01 00:26:12,243 - INFO - report:               precision    recall  f1-score   support

           3       0.56      0.58      0.57        62
           6       0.57      0.73      0.64        67
           9       0.84      0.59      0.69        71

    accuracy                           0.64       200
   macro avg       0.66      0.63      0.64       200
weighted avg       0.66      0.64      0.64       200

2023-07-01 00:26:12,243 - INFO - test loss 0.02365472642096617
2023-07-01 00:26:12,244 - INFO - test acc 0.6349999904632568
2023-07-01 00:26:13,493 - INFO - Distilling data from client: Client14
2023-07-01 00:26:13,493 - INFO - train loss: 0.00030518042480317875
2023-07-01 00:26:13,493 - INFO - train acc: 1.0
2023-07-01 00:26:13,517 - INFO - report:               precision    recall  f1-score   support

           3       0.53      0.58      0.55        62
           6       0.61      0.73      0.67        67
           9       0.85      0.62      0.72        71

    accuracy                           0.65       200
   macro avg       0.66      0.64      0.65       200
weighted avg       0.67      0.65      0.65       200

2023-07-01 00:26:13,517 - INFO - test loss 0.023728255541624065
2023-07-01 00:26:13,517 - INFO - test acc 0.6449999809265137
2023-07-01 00:26:14,768 - INFO - Distilling data from client: Client14
2023-07-01 00:26:14,768 - INFO - train loss: 0.000259624380222963
2023-07-01 00:26:14,768 - INFO - train acc: 1.0
2023-07-01 00:26:14,795 - INFO - report:               precision    recall  f1-score   support

           3       0.51      0.60      0.55        62
           6       0.60      0.67      0.63        67
           9       0.85      0.63      0.73        71

    accuracy                           0.64       200
   macro avg       0.65      0.63      0.64       200
weighted avg       0.66      0.64      0.64       200

2023-07-01 00:26:14,795 - INFO - test loss 0.023623737534603707
2023-07-01 00:26:14,795 - INFO - test acc 0.6349999904632568
2023-07-01 00:26:16,042 - INFO - Distilling data from client: Client14
2023-07-01 00:26:16,043 - INFO - train loss: 0.0002160274384721396
2023-07-01 00:26:16,043 - INFO - train acc: 1.0
2023-07-01 00:26:16,066 - INFO - report:               precision    recall  f1-score   support

           3       0.57      0.60      0.58        62
           6       0.62      0.73      0.67        67
           9       0.84      0.66      0.74        71

    accuracy                           0.67       200
   macro avg       0.68      0.66      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:26:16,066 - INFO - test loss 0.02303781948287619
2023-07-01 00:26:16,066 - INFO - test acc 0.6649999618530273
2023-07-01 00:26:17,317 - INFO - Distilling data from client: Client14
2023-07-01 00:26:17,317 - INFO - train loss: 0.00025726241510845045
2023-07-01 00:26:17,317 - INFO - train acc: 1.0
2023-07-01 00:26:17,341 - INFO - report:               precision    recall  f1-score   support

           3       0.55      0.60      0.57        62
           6       0.62      0.73      0.67        67
           9       0.85      0.65      0.74        71

    accuracy                           0.66       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-07-01 00:26:17,341 - INFO - test loss 0.02318691181425815
2023-07-01 00:26:17,341 - INFO - test acc 0.6599999666213989
2023-07-01 00:26:18,584 - INFO - Distilling data from client: Client14
2023-07-01 00:26:18,584 - INFO - train loss: 0.00021576483920316867
2023-07-01 00:26:18,584 - INFO - train acc: 1.0
2023-07-01 00:26:18,607 - INFO - report:               precision    recall  f1-score   support

           3       0.54      0.58      0.56        62
           6       0.59      0.70      0.64        67
           9       0.81      0.62      0.70        71

    accuracy                           0.64       200
   macro avg       0.65      0.63      0.64       200
weighted avg       0.66      0.64      0.64       200

2023-07-01 00:26:18,607 - INFO - test loss 0.02310655568734083
2023-07-01 00:26:18,607 - INFO - test acc 0.6349999904632568
2023-07-01 00:26:19,860 - INFO - Distilling data from client: Client14
2023-07-01 00:26:19,860 - INFO - train loss: 0.000213926203732053
2023-07-01 00:26:19,860 - INFO - train acc: 1.0
2023-07-01 00:26:19,883 - INFO - report:               precision    recall  f1-score   support

           3       0.55      0.58      0.57        62
           6       0.59      0.72      0.65        67
           9       0.83      0.63      0.72        71

    accuracy                           0.65       200
   macro avg       0.66      0.64      0.65       200
weighted avg       0.67      0.65      0.65       200

2023-07-01 00:26:19,884 - INFO - test loss 0.023377904102228207
2023-07-01 00:26:19,884 - INFO - test acc 0.6449999809265137
2023-07-01 00:26:21,134 - INFO - Distilling data from client: Client14
2023-07-01 00:26:21,134 - INFO - train loss: 0.000214982110798567
2023-07-01 00:26:21,134 - INFO - train acc: 1.0
2023-07-01 00:26:21,157 - INFO - report:               precision    recall  f1-score   support

           3       0.54      0.60      0.57        62
           6       0.62      0.72      0.66        67
           9       0.85      0.65      0.74        71

    accuracy                           0.66       200
   macro avg       0.67      0.65      0.66       200
weighted avg       0.68      0.66      0.66       200

2023-07-01 00:26:21,157 - INFO - test loss 0.022956960999076657
2023-07-01 00:26:21,157 - INFO - test acc 0.6549999713897705
2023-07-01 00:26:22,401 - INFO - Distilling data from client: Client14
2023-07-01 00:26:22,402 - INFO - train loss: 0.00023233957787336
2023-07-01 00:26:22,402 - INFO - train acc: 1.0
2023-07-01 00:26:22,425 - INFO - report:               precision    recall  f1-score   support

           3       0.52      0.61      0.56        62
           6       0.61      0.67      0.64        67
           9       0.85      0.63      0.73        71

    accuracy                           0.64       200
   macro avg       0.66      0.64      0.64       200
weighted avg       0.67      0.64      0.65       200

2023-07-01 00:26:22,425 - INFO - test loss 0.02304465803696612
2023-07-01 00:26:22,425 - INFO - test acc 0.6399999856948853
2023-07-01 00:26:23,671 - INFO - Distilling data from client: Client14
2023-07-01 00:26:23,671 - INFO - train loss: 0.00021545802560195304
2023-07-01 00:26:23,671 - INFO - train acc: 1.0
2023-07-01 00:26:23,695 - INFO - report:               precision    recall  f1-score   support

           3       0.51      0.58      0.55        62
           6       0.59      0.70      0.64        67
           9       0.86      0.61      0.71        71

    accuracy                           0.63       200
   macro avg       0.65      0.63      0.63       200
weighted avg       0.66      0.63      0.64       200

2023-07-01 00:26:23,695 - INFO - test loss 0.023829275633413027
2023-07-01 00:26:23,695 - INFO - test acc 0.6299999952316284
2023-07-01 00:26:24,947 - INFO - Distilling data from client: Client14
2023-07-01 00:26:24,947 - INFO - train loss: 0.00018958112225231282
2023-07-01 00:26:24,947 - INFO - train acc: 1.0
2023-07-01 00:26:24,973 - INFO - report:               precision    recall  f1-score   support

           3       0.57      0.63      0.60        62
           6       0.62      0.73      0.67        67
           9       0.87      0.63      0.73        71

    accuracy                           0.67       200
   macro avg       0.68      0.66      0.67       200
weighted avg       0.69      0.67      0.67       200

2023-07-01 00:26:24,973 - INFO - test loss 0.02326697052238512
2023-07-01 00:26:24,973 - INFO - test acc 0.6649999618530273
2023-07-01 00:26:26,223 - INFO - Distilling data from client: Client14
2023-07-01 00:26:26,223 - INFO - train loss: 0.00022610843306892546
2023-07-01 00:26:26,223 - INFO - train acc: 1.0
2023-07-01 00:26:26,246 - INFO - report:               precision    recall  f1-score   support

           3       0.54      0.56      0.55        62
           6       0.59      0.72      0.64        67
           9       0.87      0.65      0.74        71

    accuracy                           0.65       200
   macro avg       0.66      0.64      0.65       200
weighted avg       0.67      0.65      0.65       200

2023-07-01 00:26:26,247 - INFO - test loss 0.023332031324094005
2023-07-01 00:26:26,247 - INFO - test acc 0.6449999809265137
2023-07-01 00:26:26,258 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:26,267 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:26,276 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:26,285 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:26,294 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:26,303 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:26,313 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:26,322 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:26,331 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:26,712 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client14//synthetic.png
2023-07-01 00:26:26,725 - INFO - c: 2.0 and total_data_in_this_class: 271
2023-07-01 00:26:26,725 - INFO - c: 3.0 and total_data_in_this_class: 268
2023-07-01 00:26:26,725 - INFO - c: 8.0 and total_data_in_this_class: 260
2023-07-01 00:26:26,725 - INFO - c: 2.0 and total_data_in_this_class: 62
2023-07-01 00:26:26,725 - INFO - c: 3.0 and total_data_in_this_class: 65
2023-07-01 00:26:26,725 - INFO - c: 8.0 and total_data_in_this_class: 73
2023-07-01 00:26:26,796 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04834461212158203 sec
2023-07-01 00:26:26,843 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04607796669006348 sec
2023-07-01 00:26:26,848 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.10161733627319336 sec
2023-07-01 00:26:26,850 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:26:26,883 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03322172164916992 sec
2023-07-01 00:26:26,884 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:26:27,010 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12605810165405273 sec
2023-07-01 00:26:27,033 - INFO - initial test loss: 0.020395313983975635
2023-07-01 00:26:27,033 - INFO - initial test acc: 0.7450000047683716
2023-07-01 00:26:27,041 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.005768299102783203 sec
2023-07-01 00:26:27,152 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11689066886901855 sec
2023-07-01 00:26:27,156 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:26:27,219 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06315898895263672 sec
2023-07-01 00:26:27,219 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:26:27,553 - WARNING - Finished XLA compilation of jit(update_fn) in 0.33361077308654785 sec
2023-07-01 00:26:28,790 - INFO - Distilling data from client: Client15
2023-07-01 00:26:28,790 - INFO - train loss: 0.0027770038758893048
2023-07-01 00:26:28,790 - INFO - train acc: 0.9922928810119629
2023-07-01 00:26:28,845 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.58      0.61        62
           3       0.66      0.65      0.65        65
           8       0.76      0.82      0.79        73

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:26:28,845 - INFO - test loss 0.0206394510099429
2023-07-01 00:26:28,845 - INFO - test acc 0.6899999976158142
2023-07-01 00:26:30,083 - INFO - Distilling data from client: Client15
2023-07-01 00:26:30,083 - INFO - train loss: 0.0014171337837275725
2023-07-01 00:26:30,083 - INFO - train acc: 0.9980732202529907
2023-07-01 00:26:30,148 - INFO - report:               precision    recall  f1-score   support

           2       0.65      0.66      0.66        62
           3       0.72      0.68      0.70        65
           8       0.79      0.82      0.81        73

    accuracy                           0.73       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:26:30,148 - INFO - test loss 0.020267110264747267
2023-07-01 00:26:30,148 - INFO - test acc 0.7249999642372131
2023-07-01 00:26:31,395 - INFO - Distilling data from client: Client15
2023-07-01 00:26:31,395 - INFO - train loss: 0.0010580881184756685
2023-07-01 00:26:31,395 - INFO - train acc: 1.0
2023-07-01 00:26:31,419 - INFO - report:               precision    recall  f1-score   support

           2       0.60      0.61      0.61        62
           3       0.70      0.65      0.67        65
           8       0.75      0.79      0.77        73

    accuracy                           0.69       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:26:31,420 - INFO - test loss 0.020230620862981517
2023-07-01 00:26:31,420 - INFO - test acc 0.6899999976158142
2023-07-01 00:26:32,660 - INFO - Distilling data from client: Client15
2023-07-01 00:26:32,660 - INFO - train loss: 0.0010256299843992326
2023-07-01 00:26:32,660 - INFO - train acc: 0.9980732202529907
2023-07-01 00:26:32,684 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.61      0.62        62
           3       0.68      0.68      0.68        65
           8       0.77      0.79      0.78        73

    accuracy                           0.70       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:26:32,684 - INFO - test loss 0.020405490739350867
2023-07-01 00:26:32,684 - INFO - test acc 0.699999988079071
2023-07-01 00:26:33,919 - INFO - Distilling data from client: Client15
2023-07-01 00:26:33,919 - INFO - train loss: 0.0008403236541286385
2023-07-01 00:26:33,919 - INFO - train acc: 1.0
2023-07-01 00:26:33,945 - INFO - report:               precision    recall  f1-score   support

           2       0.67      0.66      0.67        62
           3       0.69      0.69      0.69        65
           8       0.80      0.81      0.80        73

    accuracy                           0.73       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:26:33,945 - INFO - test loss 0.02028390598874951
2023-07-01 00:26:33,946 - INFO - test acc 0.7249999642372131
2023-07-01 00:26:35,193 - INFO - Distilling data from client: Client15
2023-07-01 00:26:35,193 - INFO - train loss: 0.0007081562791771866
2023-07-01 00:26:35,193 - INFO - train acc: 1.0
2023-07-01 00:26:35,217 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.68      0.66        62
           3       0.72      0.68      0.70        65
           8       0.81      0.81      0.81        73

    accuracy                           0.73       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.73      0.72      0.73       200

2023-07-01 00:26:35,217 - INFO - test loss 0.020946465160699562
2023-07-01 00:26:35,217 - INFO - test acc 0.7249999642372131
2023-07-01 00:26:36,449 - INFO - Distilling data from client: Client15
2023-07-01 00:26:36,450 - INFO - train loss: 0.0007311588796978708
2023-07-01 00:26:36,450 - INFO - train acc: 1.0
2023-07-01 00:26:36,473 - INFO - report:               precision    recall  f1-score   support

           2       0.61      0.60      0.60        62
           3       0.64      0.69      0.67        65
           8       0.77      0.73      0.75        73

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:26:36,473 - INFO - test loss 0.021280762855802642
2023-07-01 00:26:36,473 - INFO - test acc 0.675000011920929
2023-07-01 00:26:37,720 - INFO - Distilling data from client: Client15
2023-07-01 00:26:37,720 - INFO - train loss: 0.0007303341255016462
2023-07-01 00:26:37,720 - INFO - train acc: 1.0
2023-07-01 00:26:37,743 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.68      0.66        62
           3       0.70      0.66      0.68        65
           8       0.79      0.79      0.79        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:26:37,744 - INFO - test loss 0.020760970678936223
2023-07-01 00:26:37,744 - INFO - test acc 0.7149999737739563
2023-07-01 00:26:38,988 - INFO - Distilling data from client: Client15
2023-07-01 00:26:38,988 - INFO - train loss: 0.0005344791574517069
2023-07-01 00:26:38,988 - INFO - train acc: 1.0
2023-07-01 00:26:39,011 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.66      0.65        62
           3       0.66      0.66      0.66        65
           8       0.79      0.77      0.78        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:26:39,011 - INFO - test loss 0.02140760790208833
2023-07-01 00:26:39,011 - INFO - test acc 0.699999988079071
2023-07-01 00:26:40,253 - INFO - Distilling data from client: Client15
2023-07-01 00:26:40,253 - INFO - train loss: 0.00047346849908529366
2023-07-01 00:26:40,253 - INFO - train acc: 1.0
2023-07-01 00:26:40,277 - INFO - report:               precision    recall  f1-score   support

           2       0.65      0.68      0.66        62
           3       0.70      0.65      0.67        65
           8       0.76      0.78      0.77        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:26:40,277 - INFO - test loss 0.021322260664407768
2023-07-01 00:26:40,277 - INFO - test acc 0.7049999833106995
2023-07-01 00:26:41,507 - INFO - Distilling data from client: Client15
2023-07-01 00:26:41,507 - INFO - train loss: 0.0005343373314368797
2023-07-01 00:26:41,507 - INFO - train acc: 1.0
2023-07-01 00:26:41,531 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.63      0.62        62
           3       0.66      0.68      0.67        65
           8       0.79      0.75      0.77        73

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:26:41,532 - INFO - test loss 0.021441036683564407
2023-07-01 00:26:41,532 - INFO - test acc 0.6899999976158142
2023-07-01 00:26:42,772 - INFO - Distilling data from client: Client15
2023-07-01 00:26:42,772 - INFO - train loss: 0.00044637266370117085
2023-07-01 00:26:42,773 - INFO - train acc: 1.0
2023-07-01 00:26:42,797 - INFO - report:               precision    recall  f1-score   support

           2       0.65      0.73      0.69        62
           3       0.71      0.63      0.67        65
           8       0.79      0.79      0.79        73

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:26:42,797 - INFO - test loss 0.021324488079726308
2023-07-01 00:26:42,797 - INFO - test acc 0.7199999690055847
2023-07-01 00:26:44,036 - INFO - Distilling data from client: Client15
2023-07-01 00:26:44,036 - INFO - train loss: 0.0004326582386806463
2023-07-01 00:26:44,036 - INFO - train acc: 1.0
2023-07-01 00:26:44,059 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.63      0.63        62
           3       0.66      0.69      0.68        65
           8       0.81      0.78      0.80        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-07-01 00:26:44,059 - INFO - test loss 0.0213671867324596
2023-07-01 00:26:44,059 - INFO - test acc 0.7049999833106995
2023-07-01 00:26:45,295 - INFO - Distilling data from client: Client15
2023-07-01 00:26:45,295 - INFO - train loss: 0.0004136878224146691
2023-07-01 00:26:45,296 - INFO - train acc: 1.0
2023-07-01 00:26:45,324 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.63      0.63        62
           3       0.64      0.65      0.64        65
           8       0.79      0.78      0.79        73

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:26:45,324 - INFO - test loss 0.021703316222395495
2023-07-01 00:26:45,324 - INFO - test acc 0.6899999976158142
2023-07-01 00:26:46,566 - INFO - Distilling data from client: Client15
2023-07-01 00:26:46,566 - INFO - train loss: 0.00037456225915246793
2023-07-01 00:26:46,566 - INFO - train acc: 1.0
2023-07-01 00:26:46,633 - INFO - report:               precision    recall  f1-score   support

           2       0.67      0.69      0.68        62
           3       0.71      0.71      0.71        65
           8       0.83      0.81      0.82        73

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:26:46,633 - INFO - test loss 0.021707801264361438
2023-07-01 00:26:46,633 - INFO - test acc 0.7400000095367432
2023-07-01 00:26:47,888 - INFO - Distilling data from client: Client15
2023-07-01 00:26:47,888 - INFO - train loss: 0.00034225566317205223
2023-07-01 00:26:47,888 - INFO - train acc: 1.0
2023-07-01 00:26:47,911 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.68      0.65        62
           3       0.70      0.66      0.68        65
           8       0.81      0.79      0.80        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:26:47,911 - INFO - test loss 0.02077007550629726
2023-07-01 00:26:47,911 - INFO - test acc 0.7149999737739563
2023-07-01 00:26:49,148 - INFO - Distilling data from client: Client15
2023-07-01 00:26:49,148 - INFO - train loss: 0.0003141843076739399
2023-07-01 00:26:49,148 - INFO - train acc: 1.0
2023-07-01 00:26:49,172 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.65      0.63        62
           3       0.68      0.69      0.69        65
           8       0.79      0.75      0.77        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:26:49,172 - INFO - test loss 0.021018345032194106
2023-07-01 00:26:49,172 - INFO - test acc 0.699999988079071
2023-07-01 00:26:50,412 - INFO - Distilling data from client: Client15
2023-07-01 00:26:50,412 - INFO - train loss: 0.00033760033811424696
2023-07-01 00:26:50,412 - INFO - train acc: 1.0
2023-07-01 00:26:50,435 - INFO - report:               precision    recall  f1-score   support

           2       0.63      0.68      0.65        62
           3       0.67      0.66      0.67        65
           8       0.81      0.77      0.79        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-07-01 00:26:50,435 - INFO - test loss 0.02141694913249093
2023-07-01 00:26:50,436 - INFO - test acc 0.7049999833106995
2023-07-01 00:26:51,665 - INFO - Distilling data from client: Client15
2023-07-01 00:26:51,665 - INFO - train loss: 0.00031573112070549516
2023-07-01 00:26:51,665 - INFO - train acc: 1.0
2023-07-01 00:26:51,689 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.71      0.68        62
           3       0.71      0.69      0.70        65
           8       0.81      0.78      0.80        73

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-07-01 00:26:51,689 - INFO - test loss 0.020930669364061124
2023-07-01 00:26:51,689 - INFO - test acc 0.7299999594688416
2023-07-01 00:26:52,926 - INFO - Distilling data from client: Client15
2023-07-01 00:26:52,926 - INFO - train loss: 0.0002834857018348389
2023-07-01 00:26:52,926 - INFO - train acc: 1.0
2023-07-01 00:26:52,949 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.69      0.67        62
           3       0.69      0.66      0.68        65
           8       0.80      0.78      0.79        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:26:52,949 - INFO - test loss 0.021025779031403475
2023-07-01 00:26:52,949 - INFO - test acc 0.7149999737739563
2023-07-01 00:26:54,192 - INFO - Distilling data from client: Client15
2023-07-01 00:26:54,192 - INFO - train loss: 0.00033636315602487163
2023-07-01 00:26:54,192 - INFO - train acc: 1.0
2023-07-01 00:26:54,217 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.65      0.65        62
           3       0.68      0.69      0.69        65
           8       0.79      0.79      0.79        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:26:54,217 - INFO - test loss 0.021309822677504343
2023-07-01 00:26:54,217 - INFO - test acc 0.7149999737739563
2023-07-01 00:26:55,460 - INFO - Distilling data from client: Client15
2023-07-01 00:26:55,460 - INFO - train loss: 0.0003637780540769504
2023-07-01 00:26:55,460 - INFO - train acc: 0.9980732202529907
2023-07-01 00:26:55,484 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.69      0.67        62
           3       0.70      0.66      0.68        65
           8       0.78      0.77      0.77        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:26:55,484 - INFO - test loss 0.021594910798253212
2023-07-01 00:26:55,484 - INFO - test acc 0.7099999785423279
2023-07-01 00:26:56,719 - INFO - Distilling data from client: Client15
2023-07-01 00:26:56,719 - INFO - train loss: 0.0003256878181930578
2023-07-01 00:26:56,719 - INFO - train acc: 1.0
2023-07-01 00:26:56,743 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.66      0.66        62
           3       0.68      0.69      0.69        65
           8       0.78      0.77      0.77        73

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:26:56,743 - INFO - test loss 0.02116093326899247
2023-07-01 00:26:56,743 - INFO - test acc 0.7099999785423279
2023-07-01 00:26:57,996 - INFO - Distilling data from client: Client15
2023-07-01 00:26:57,996 - INFO - train loss: 0.0003602325451200321
2023-07-01 00:26:57,996 - INFO - train acc: 1.0
2023-07-01 00:26:58,021 - INFO - report:               precision    recall  f1-score   support

           2       0.62      0.66      0.64        62
           3       0.70      0.65      0.67        65
           8       0.78      0.79      0.79        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-07-01 00:26:58,021 - INFO - test loss 0.021092342811323724
2023-07-01 00:26:58,021 - INFO - test acc 0.7049999833106995
2023-07-01 00:26:59,262 - INFO - Distilling data from client: Client15
2023-07-01 00:26:59,262 - INFO - train loss: 0.00026559732405468983
2023-07-01 00:26:59,262 - INFO - train acc: 1.0
2023-07-01 00:26:59,286 - INFO - report:               precision    recall  f1-score   support

           2       0.65      0.63      0.64        62
           3       0.68      0.69      0.69        65
           8       0.77      0.78      0.78        73

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:26:59,286 - INFO - test loss 0.02112514091667047
2023-07-01 00:26:59,286 - INFO - test acc 0.7049999833106995
2023-07-01 00:26:59,298 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:59,307 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:59,315 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:59,324 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:59,333 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:59,342 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:59,351 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:59,360 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:59,369 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:26:59,755 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client15//synthetic.png
2023-07-01 00:26:59,768 - INFO - c: 2.0 and total_data_in_this_class: 548
2023-07-01 00:26:59,768 - INFO - c: 5.0 and total_data_in_this_class: 251
2023-07-01 00:26:59,768 - INFO - c: 2.0 and total_data_in_this_class: 118
2023-07-01 00:26:59,768 - INFO - c: 5.0 and total_data_in_this_class: 82
2023-07-01 00:26:59,787 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002567768096923828 sec
2023-07-01 00:26:59,787 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:26:59,788 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012438297271728516 sec
2023-07-01 00:26:59,788 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:26:59,799 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010702371597290039 sec
2023-07-01 00:26:59,801 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00024509429931640625 sec
2023-07-01 00:26:59,801 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:26:59,803 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009484291076660156 sec
2023-07-01 00:26:59,803 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:26:59,811 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008559465408325195 sec
2023-07-01 00:26:59,815 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013589859008789062 sec
2023-07-01 00:26:59,816 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012493133544921875 sec
2023-07-01 00:26:59,816 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002970695495605469 sec
2023-07-01 00:26:59,818 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020503997802734375 sec
2023-07-01 00:26:59,818 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011777877807617188 sec
2023-07-01 00:26:59,819 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000274658203125 sec
2023-07-01 00:26:59,819 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002434253692626953 sec
2023-07-01 00:26:59,820 - WARNING - Finished tracing + transforming absolute for pjit in 0.0001666545867919922 sec
2023-07-01 00:26:59,821 - WARNING - Finished tracing + transforming fn for pjit in 0.0002741813659667969 sec
2023-07-01 00:26:59,821 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00032138824462890625 sec
2023-07-01 00:26:59,822 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020194053649902344 sec
2023-07-01 00:26:59,823 - WARNING - Finished tracing + transforming fn for pjit in 0.00023245811462402344 sec
2023-07-01 00:26:59,824 - WARNING - Finished tracing + transforming fn for pjit in 0.0002682209014892578 sec
2023-07-01 00:26:59,824 - WARNING - Finished tracing + transforming fn for pjit in 0.00022864341735839844 sec
2023-07-01 00:26:59,825 - WARNING - Finished tracing + transforming fn for pjit in 0.0002658367156982422 sec
2023-07-01 00:26:59,826 - WARNING - Finished tracing + transforming fn for pjit in 0.00023174285888671875 sec
2023-07-01 00:26:59,828 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001747608184814453 sec
2023-07-01 00:26:59,829 - WARNING - Finished tracing + transforming fn for pjit in 0.00023102760314941406 sec
2023-07-01 00:26:59,829 - WARNING - Finished tracing + transforming fn for pjit in 0.00023484230041503906 sec
2023-07-01 00:26:59,833 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00037670135498046875 sec
2023-07-01 00:26:59,834 - WARNING - Finished tracing + transforming _mean for pjit in 0.000982522964477539 sec
2023-07-01 00:26:59,835 - WARNING - Finished tracing + transforming fn for pjit in 0.00023603439331054688 sec
2023-07-01 00:26:59,835 - WARNING - Finished tracing + transforming fn for pjit in 0.00022125244140625 sec
2023-07-01 00:26:59,836 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029921531677246094 sec
2023-07-01 00:26:59,837 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026106834411621094 sec
2023-07-01 00:26:59,837 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001785755157470703 sec
2023-07-01 00:26:59,838 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002655982971191406 sec
2023-07-01 00:26:59,839 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002315044403076172 sec
2023-07-01 00:26:59,839 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024008750915527344 sec
2023-07-01 00:26:59,840 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00036644935607910156 sec
2023-07-01 00:26:59,841 - WARNING - Finished tracing + transforming _where for pjit in 0.0009872913360595703 sec
2023-07-01 00:26:59,842 - WARNING - Finished tracing + transforming fn for pjit in 0.0002703666687011719 sec
2023-07-01 00:26:59,842 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026035308837890625 sec
2023-07-01 00:26:59,843 - WARNING - Finished tracing + transforming fn for pjit in 0.00022673606872558594 sec
2023-07-01 00:26:59,844 - WARNING - Finished tracing + transforming fn for pjit in 0.000225067138671875 sec
2023-07-01 00:26:59,844 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022339820861816406 sec
2023-07-01 00:26:59,845 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002620220184326172 sec
2023-07-01 00:26:59,846 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002560615539550781 sec
2023-07-01 00:26:59,846 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026226043701171875 sec
2023-07-01 00:26:59,847 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022840499877929688 sec
2023-07-01 00:26:59,848 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022554397583007812 sec
2023-07-01 00:26:59,849 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002694129943847656 sec
2023-07-01 00:26:59,849 - WARNING - Finished tracing + transforming _where for pjit in 0.0008702278137207031 sec
2023-07-01 00:26:59,850 - WARNING - Finished tracing + transforming fn for pjit in 0.0002605915069580078 sec
2023-07-01 00:26:59,850 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025653839111328125 sec
2023-07-01 00:26:59,852 - WARNING - Finished tracing + transforming fn for pjit in 0.00022602081298828125 sec
2023-07-01 00:26:59,856 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002799034118652344 sec
2023-07-01 00:26:59,856 - WARNING - Finished tracing + transforming fn for pjit in 0.0003490447998046875 sec
2023-07-01 00:26:59,857 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027298927307128906 sec
2023-07-01 00:26:59,858 - WARNING - Finished tracing + transforming fn for pjit in 0.0002307891845703125 sec
2023-07-01 00:26:59,862 - WARNING - Finished tracing + transforming fn for pjit in 0.00021648406982421875 sec
2023-07-01 00:26:59,863 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00017499923706054688 sec
2023-07-01 00:26:59,865 - WARNING - Finished tracing + transforming fn for pjit in 0.0012645721435546875 sec
2023-07-01 00:26:59,866 - WARNING - Finished tracing + transforming fn for pjit in 0.00023245811462402344 sec
2023-07-01 00:26:59,884 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06944870948791504 sec
2023-07-01 00:26:59,887 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012993812561035156 sec
2023-07-01 00:26:59,887 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011324882507324219 sec
2023-07-01 00:26:59,888 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002696514129638672 sec
2023-07-01 00:26:59,890 - WARNING - Finished tracing + transforming fn for pjit in 0.00022411346435546875 sec
2023-07-01 00:26:59,891 - WARNING - Finished tracing + transforming fn for pjit in 0.00025773048400878906 sec
2023-07-01 00:26:59,892 - WARNING - Finished tracing + transforming fn for pjit in 0.00021982192993164062 sec
2023-07-01 00:26:59,898 - WARNING - Finished tracing + transforming fn for pjit in 0.00022912025451660156 sec
2023-07-01 00:26:59,899 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000225067138671875 sec
2023-07-01 00:26:59,900 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025844573974609375 sec
2023-07-01 00:26:59,900 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017547607421875 sec
2023-07-01 00:26:59,901 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000331878662109375 sec
2023-07-01 00:26:59,902 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022983551025390625 sec
2023-07-01 00:26:59,902 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022459030151367188 sec
2023-07-01 00:26:59,903 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002827644348144531 sec
2023-07-01 00:26:59,904 - WARNING - Finished tracing + transforming _where for pjit in 0.0008847713470458984 sec
2023-07-01 00:26:59,904 - WARNING - Finished tracing + transforming fn for pjit in 0.0002617835998535156 sec
2023-07-01 00:26:59,905 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002548694610595703 sec
2023-07-01 00:26:59,906 - WARNING - Finished tracing + transforming fn for pjit in 0.00022292137145996094 sec
2023-07-01 00:26:59,907 - WARNING - Finished tracing + transforming fn for pjit in 0.0002799034118652344 sec
2023-07-01 00:26:59,919 - WARNING - Finished tracing + transforming fn for pjit in 0.00022482872009277344 sec
2023-07-01 00:26:59,939 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05411076545715332 sec
2023-07-01 00:26:59,940 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012636184692382812 sec
2023-07-01 00:26:59,941 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013589859008789062 sec
2023-07-01 00:26:59,941 - WARNING - Finished tracing + transforming _where for pjit in 0.0006303787231445312 sec
2023-07-01 00:26:59,942 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003056526184082031 sec
2023-07-01 00:26:59,942 - WARNING - Finished tracing + transforming trace for pjit in 0.002576112747192383 sec
2023-07-01 00:26:59,945 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010776519775390625 sec
2023-07-01 00:26:59,946 - WARNING - Finished tracing + transforming tril for pjit in 0.0006780624389648438 sec
2023-07-01 00:26:59,946 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0017392635345458984 sec
2023-07-01 00:26:59,947 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00011205673217773438 sec
2023-07-01 00:26:59,947 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010967254638671875 sec
2023-07-01 00:26:59,949 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014662742614746094 sec
2023-07-01 00:26:59,953 - WARNING - Finished tracing + transforming _solve for pjit in 0.009377479553222656 sec
2023-07-01 00:26:59,954 - WARNING - Finished tracing + transforming dot for pjit in 0.0003178119659423828 sec
2023-07-01 00:26:59,956 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.14346098899841309 sec
2023-07-01 00:26:59,959 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:26:59,992 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03284001350402832 sec
2023-07-01 00:26:59,992 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:00,116 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12384867668151855 sec
2023-07-01 00:27:00,131 - INFO - initial test loss: 0.022935500093837092
2023-07-01 00:27:00,131 - INFO - initial test acc: 0.7099999785423279
2023-07-01 00:27:00,137 - WARNING - Finished tracing + transforming dot for pjit in 0.0003829002380371094 sec
2023-07-01 00:27:00,138 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029540061950683594 sec
2023-07-01 00:27:00,139 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00031495094299316406 sec
2023-07-01 00:27:00,139 - WARNING - Finished tracing + transforming _mean for pjit in 0.001001596450805664 sec
2023-07-01 00:27:00,140 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00019979476928710938 sec
2023-07-01 00:27:00,141 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0002512931823730469 sec
2023-07-01 00:27:00,142 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002536773681640625 sec
2023-07-01 00:27:00,143 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00038051605224609375 sec
2023-07-01 00:27:00,143 - WARNING - Finished tracing + transforming _mean for pjit in 0.001155853271484375 sec
2023-07-01 00:27:00,144 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.010479927062988281 sec
2023-07-01 00:27:00,153 - WARNING - Finished tracing + transforming fn for pjit in 0.0002620220184326172 sec
2023-07-01 00:27:00,153 - WARNING - Finished tracing + transforming fn for pjit in 0.000274658203125 sec
2023-07-01 00:27:00,154 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002777576446533203 sec
2023-07-01 00:27:00,155 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002701282501220703 sec
2023-07-01 00:27:00,155 - WARNING - Finished tracing + transforming _where for pjit in 0.0009253025054931641 sec
2023-07-01 00:27:00,164 - WARNING - Finished tracing + transforming fn for pjit in 0.00025343894958496094 sec
2023-07-01 00:27:00,165 - WARNING - Finished tracing + transforming fn for pjit in 0.00026726722717285156 sec
2023-07-01 00:27:00,166 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021195411682128906 sec
2023-07-01 00:27:00,167 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.000263214111328125 sec
2023-07-01 00:27:00,167 - WARNING - Finished tracing + transforming _where for pjit in 0.0008790493011474609 sec
2023-07-01 00:27:00,202 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021839141845703125 sec
2023-07-01 00:27:00,258 - WARNING - Finished tracing + transforming fn for pjit in 0.000274658203125 sec
2023-07-01 00:27:00,259 - WARNING - Finished tracing + transforming fn for pjit in 0.00028014183044433594 sec
2023-07-01 00:27:00,259 - WARNING - Finished tracing + transforming square for pjit in 0.00017547607421875 sec
2023-07-01 00:27:00,262 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002276897430419922 sec
2023-07-01 00:27:00,264 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024890899658203125 sec
2023-07-01 00:27:00,264 - WARNING - Finished tracing + transforming fn for pjit in 0.00026798248291015625 sec
2023-07-01 00:27:00,265 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00023317337036132812 sec
2023-07-01 00:27:00,265 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002377033233642578 sec
2023-07-01 00:27:00,266 - WARNING - Finished tracing + transforming fn for pjit in 0.0002715587615966797 sec
2023-07-01 00:27:00,267 - WARNING - Finished tracing + transforming fn for pjit in 0.00023555755615234375 sec
2023-07-01 00:27:00,268 - WARNING - Finished tracing + transforming square for pjit in 0.00017309188842773438 sec
2023-07-01 00:27:00,270 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022721290588378906 sec
2023-07-01 00:27:00,271 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00018310546875 sec
2023-07-01 00:27:00,272 - WARNING - Finished tracing + transforming fn for pjit in 0.0002727508544921875 sec
2023-07-01 00:27:00,273 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022172927856445312 sec
2023-07-01 00:27:00,273 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023412704467773438 sec
2023-07-01 00:27:00,274 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1412210464477539 sec
2023-07-01 00:27:00,278 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,10]), ShapedArray(float32[334,10]), ShapedArray(float32[334,10]), ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,10]), ShapedArray(float32[334,3,32,32]), ShapedArray(float32[334,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:27:00,341 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06251096725463867 sec
2023-07-01 00:27:00,341 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:00,671 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32970309257507324 sec
2023-07-01 00:27:01,351 - INFO - Distilling data from client: Client16
2023-07-01 00:27:01,352 - INFO - train loss: 0.004994063121692715
2023-07-01 00:27:01,352 - INFO - train acc: 0.9640718698501587
2023-07-01 00:27:01,390 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.83      0.79       118
           5       0.71      0.60      0.65        82

    accuracy                           0.73       200
   macro avg       0.73      0.71      0.72       200
weighted avg       0.73      0.73      0.73       200

2023-07-01 00:27:01,391 - INFO - test loss 0.020567092973865664
2023-07-01 00:27:01,391 - INFO - test acc 0.73499995470047
2023-07-01 00:27:02,069 - INFO - Distilling data from client: Client16
2023-07-01 00:27:02,069 - INFO - train loss: 0.0037964239381578736
2023-07-01 00:27:02,069 - INFO - train acc: 0.9790419340133667
2023-07-01 00:27:02,109 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.85      0.79       118
           5       0.73      0.59      0.65        82

    accuracy                           0.74       200
   macro avg       0.74      0.72      0.72       200
weighted avg       0.74      0.74      0.73       200

2023-07-01 00:27:02,109 - INFO - test loss 0.019428996089974015
2023-07-01 00:27:02,110 - INFO - test acc 0.7400000095367432
2023-07-01 00:27:02,783 - INFO - Distilling data from client: Client16
2023-07-01 00:27:02,783 - INFO - train loss: 0.0031967470976697
2023-07-01 00:27:02,783 - INFO - train acc: 0.9850299954414368
2023-07-01 00:27:02,800 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.87      0.79       118
           5       0.74      0.52      0.61        82

    accuracy                           0.73       200
   macro avg       0.73      0.70      0.70       200
weighted avg       0.73      0.73      0.72       200

2023-07-01 00:27:02,800 - INFO - test loss 0.019752065587224043
2023-07-01 00:27:02,800 - INFO - test acc 0.7299999594688416
2023-07-01 00:27:03,480 - INFO - Distilling data from client: Client16
2023-07-01 00:27:03,481 - INFO - train loss: 0.002930637628659736
2023-07-01 00:27:03,481 - INFO - train acc: 0.9880239963531494
2023-07-01 00:27:03,498 - INFO - report:               precision    recall  f1-score   support

           2       0.71      0.81      0.76       118
           5       0.66      0.52      0.59        82

    accuracy                           0.69       200
   macro avg       0.69      0.67      0.67       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:27:03,498 - INFO - test loss 0.020417024180378777
2023-07-01 00:27:03,498 - INFO - test acc 0.6949999928474426
2023-07-01 00:27:04,178 - INFO - Distilling data from client: Client16
2023-07-01 00:27:04,178 - INFO - train loss: 0.0026148847870889487
2023-07-01 00:27:04,178 - INFO - train acc: 0.9970059990882874
2023-07-01 00:27:04,219 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.86      0.80       118
           5       0.74      0.59      0.65        82

    accuracy                           0.74       200
   macro avg       0.74      0.72      0.73       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:27:04,219 - INFO - test loss 0.01958974832058148
2023-07-01 00:27:04,219 - INFO - test acc 0.7450000047683716
2023-07-01 00:27:04,911 - INFO - Distilling data from client: Client16
2023-07-01 00:27:04,912 - INFO - train loss: 0.0026183854186578614
2023-07-01 00:27:04,912 - INFO - train acc: 0.9970059990882874
2023-07-01 00:27:04,928 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.84      0.78       118
           5       0.70      0.54      0.61        82

    accuracy                           0.71       200
   macro avg       0.71      0.69      0.69       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:27:04,928 - INFO - test loss 0.02035401585759451
2023-07-01 00:27:04,928 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:05,606 - INFO - Distilling data from client: Client16
2023-07-01 00:27:05,606 - INFO - train loss: 0.0024652310810925606
2023-07-01 00:27:05,606 - INFO - train acc: 1.0
2023-07-01 00:27:05,623 - INFO - report:               precision    recall  f1-score   support

           2       0.71      0.83      0.76       118
           5       0.67      0.50      0.57        82

    accuracy                           0.69       200
   macro avg       0.69      0.67      0.67       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:27:05,623 - INFO - test loss 0.020525406644324753
2023-07-01 00:27:05,623 - INFO - test acc 0.6949999928474426
2023-07-01 00:27:06,306 - INFO - Distilling data from client: Client16
2023-07-01 00:27:06,306 - INFO - train loss: 0.0023644692336764043
2023-07-01 00:27:06,306 - INFO - train acc: 1.0
2023-07-01 00:27:06,322 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.83      0.78       118
           5       0.70      0.56      0.62        82

    accuracy                           0.72       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.72      0.72      0.71       200

2023-07-01 00:27:06,322 - INFO - test loss 0.019875354839406376
2023-07-01 00:27:06,322 - INFO - test acc 0.7199999690055847
2023-07-01 00:27:07,004 - INFO - Distilling data from client: Client16
2023-07-01 00:27:07,004 - INFO - train loss: 0.00279311688976175
2023-07-01 00:27:07,004 - INFO - train acc: 0.9910179972648621
2023-07-01 00:27:07,021 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.86      0.80       118
           5       0.74      0.59      0.65        82

    accuracy                           0.74       200
   macro avg       0.74      0.72      0.73       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:27:07,021 - INFO - test loss 0.018774606433245588
2023-07-01 00:27:07,021 - INFO - test acc 0.7450000047683716
2023-07-01 00:27:07,708 - INFO - Distilling data from client: Client16
2023-07-01 00:27:07,709 - INFO - train loss: 0.0026820152032188387
2023-07-01 00:27:07,709 - INFO - train acc: 1.0
2023-07-01 00:27:07,725 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.83      0.77       118
           5       0.68      0.52      0.59        82

    accuracy                           0.70       200
   macro avg       0.70      0.68      0.68       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:27:07,726 - INFO - test loss 0.019823284712253746
2023-07-01 00:27:07,726 - INFO - test acc 0.7049999833106995
2023-07-01 00:27:08,410 - INFO - Distilling data from client: Client16
2023-07-01 00:27:08,410 - INFO - train loss: 0.002049805814417854
2023-07-01 00:27:08,410 - INFO - train acc: 0.9970059990882874
2023-07-01 00:27:08,427 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.84      0.78       118
           5       0.70      0.54      0.61        82

    accuracy                           0.71       200
   macro avg       0.71      0.69      0.69       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:27:08,427 - INFO - test loss 0.021423047813198027
2023-07-01 00:27:08,427 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:09,124 - INFO - Distilling data from client: Client16
2023-07-01 00:27:09,124 - INFO - train loss: 0.002197043042423061
2023-07-01 00:27:09,124 - INFO - train acc: 0.9970059990882874
2023-07-01 00:27:09,140 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.85      0.78       118
           5       0.71      0.54      0.61        82

    accuracy                           0.72       200
   macro avg       0.72      0.69      0.70       200
weighted avg       0.72      0.72      0.71       200

2023-07-01 00:27:09,140 - INFO - test loss 0.02005340371161638
2023-07-01 00:27:09,141 - INFO - test acc 0.7199999690055847
2023-07-01 00:27:09,820 - INFO - Distilling data from client: Client16
2023-07-01 00:27:09,820 - INFO - train loss: 0.0024639472356212423
2023-07-01 00:27:09,820 - INFO - train acc: 0.9940119981765747
2023-07-01 00:27:09,837 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.85      0.78       118
           5       0.70      0.52      0.60        82

    accuracy                           0.71       200
   macro avg       0.71      0.69      0.69       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:27:09,837 - INFO - test loss 0.019458780284992615
2023-07-01 00:27:09,837 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:10,519 - INFO - Distilling data from client: Client16
2023-07-01 00:27:10,519 - INFO - train loss: 0.0019491827584849377
2023-07-01 00:27:10,519 - INFO - train acc: 1.0
2023-07-01 00:27:10,535 - INFO - report:               precision    recall  f1-score   support

           2       0.71      0.86      0.78       118
           5       0.71      0.49      0.58        82

    accuracy                           0.71       200
   macro avg       0.71      0.68      0.68       200
weighted avg       0.71      0.71      0.70       200

2023-07-01 00:27:10,535 - INFO - test loss 0.020274536377409568
2023-07-01 00:27:10,535 - INFO - test acc 0.7099999785423279
2023-07-01 00:27:11,218 - INFO - Distilling data from client: Client16
2023-07-01 00:27:11,218 - INFO - train loss: 0.002049490139299288
2023-07-01 00:27:11,218 - INFO - train acc: 1.0
2023-07-01 00:27:11,235 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.84      0.78       118
           5       0.71      0.56      0.63        82

    accuracy                           0.73       200
   macro avg       0.72      0.70      0.70       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:27:11,235 - INFO - test loss 0.019308815700266675
2023-07-01 00:27:11,235 - INFO - test acc 0.7249999642372131
2023-07-01 00:27:11,921 - INFO - Distilling data from client: Client16
2023-07-01 00:27:11,921 - INFO - train loss: 0.00239749398868375
2023-07-01 00:27:11,921 - INFO - train acc: 0.9940119981765747
2023-07-01 00:27:11,938 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.82      0.77       118
           5       0.68      0.55      0.61        82

    accuracy                           0.71       200
   macro avg       0.70      0.69      0.69       200
weighted avg       0.71      0.71      0.70       200

2023-07-01 00:27:11,938 - INFO - test loss 0.019696073523721398
2023-07-01 00:27:11,938 - INFO - test acc 0.7099999785423279
2023-07-01 00:27:12,617 - INFO - Distilling data from client: Client16
2023-07-01 00:27:12,618 - INFO - train loss: 0.0023805866096652467
2023-07-01 00:27:12,618 - INFO - train acc: 0.9970059990882874
2023-07-01 00:27:12,635 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.85      0.80       118
           5       0.73      0.60      0.66        82

    accuracy                           0.74       200
   macro avg       0.74      0.72      0.73       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:27:12,635 - INFO - test loss 0.019221046368313222
2023-07-01 00:27:12,635 - INFO - test acc 0.7450000047683716
2023-07-01 00:27:13,322 - INFO - Distilling data from client: Client16
2023-07-01 00:27:13,322 - INFO - train loss: 0.0019485447819391718
2023-07-01 00:27:13,322 - INFO - train acc: 1.0
2023-07-01 00:27:13,339 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.82      0.77       118
           5       0.68      0.55      0.61        82

    accuracy                           0.71       200
   macro avg       0.70      0.69      0.69       200
weighted avg       0.71      0.71      0.70       200

2023-07-01 00:27:13,339 - INFO - test loss 0.02092779263021237
2023-07-01 00:27:13,339 - INFO - test acc 0.7099999785423279
2023-07-01 00:27:14,024 - INFO - Distilling data from client: Client16
2023-07-01 00:27:14,024 - INFO - train loss: 0.0018264230439507883
2023-07-01 00:27:14,024 - INFO - train acc: 0.9940119981765747
2023-07-01 00:27:14,041 - INFO - report:               precision    recall  f1-score   support

           2       0.74      0.85      0.79       118
           5       0.72      0.57      0.64        82

    accuracy                           0.73       200
   macro avg       0.73      0.71      0.71       200
weighted avg       0.73      0.73      0.73       200

2023-07-01 00:27:14,041 - INFO - test loss 0.019558693618951743
2023-07-01 00:27:14,041 - INFO - test acc 0.73499995470047
2023-07-01 00:27:14,737 - INFO - Distilling data from client: Client16
2023-07-01 00:27:14,737 - INFO - train loss: 0.00170386025063477
2023-07-01 00:27:14,737 - INFO - train acc: 1.0
2023-07-01 00:27:14,754 - INFO - report:               precision    recall  f1-score   support

           2       0.70      0.85      0.77       118
           5       0.68      0.48      0.56        82

    accuracy                           0.69       200
   macro avg       0.69      0.66      0.66       200
weighted avg       0.69      0.69      0.68       200

2023-07-01 00:27:14,754 - INFO - test loss 0.019891080213414073
2023-07-01 00:27:14,754 - INFO - test acc 0.6949999928474426
2023-07-01 00:27:15,447 - INFO - Distilling data from client: Client16
2023-07-01 00:27:15,447 - INFO - train loss: 0.001970387618691453
2023-07-01 00:27:15,447 - INFO - train acc: 1.0
2023-07-01 00:27:15,463 - INFO - report:               precision    recall  f1-score   support

           2       0.74      0.83      0.78       118
           5       0.70      0.57      0.63        82

    accuracy                           0.73       200
   macro avg       0.72      0.70      0.71       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:27:15,464 - INFO - test loss 0.019379588408678268
2023-07-01 00:27:15,464 - INFO - test acc 0.7249999642372131
2023-07-01 00:27:16,152 - INFO - Distilling data from client: Client16
2023-07-01 00:27:16,152 - INFO - train loss: 0.0020734480584064102
2023-07-01 00:27:16,152 - INFO - train acc: 1.0
2023-07-01 00:27:16,170 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.86      0.79       118
           5       0.73      0.55      0.63        82

    accuracy                           0.73       200
   macro avg       0.73      0.70      0.71       200
weighted avg       0.73      0.73      0.72       200

2023-07-01 00:27:16,170 - INFO - test loss 0.020484899194889473
2023-07-01 00:27:16,170 - INFO - test acc 0.7299999594688416
2023-07-01 00:27:16,858 - INFO - Distilling data from client: Client16
2023-07-01 00:27:16,859 - INFO - train loss: 0.001937574109914281
2023-07-01 00:27:16,859 - INFO - train acc: 0.9970059990882874
2023-07-01 00:27:16,900 - INFO - report:               precision    recall  f1-score   support

           2       0.76      0.86      0.80       118
           5       0.75      0.61      0.67        82

    accuracy                           0.76       200
   macro avg       0.75      0.73      0.74       200
weighted avg       0.75      0.76      0.75       200

2023-07-01 00:27:16,900 - INFO - test loss 0.019699787121724634
2023-07-01 00:27:16,900 - INFO - test acc 0.7549999952316284
2023-07-01 00:27:17,581 - INFO - Distilling data from client: Client16
2023-07-01 00:27:17,581 - INFO - train loss: 0.0019945015697727885
2023-07-01 00:27:17,582 - INFO - train acc: 1.0
2023-07-01 00:27:17,599 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.87      0.80       118
           5       0.75      0.54      0.62        82

    accuracy                           0.73       200
   macro avg       0.74      0.70      0.71       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:27:17,599 - INFO - test loss 0.019374147783711458
2023-07-01 00:27:17,600 - INFO - test acc 0.73499995470047
2023-07-01 00:27:18,280 - INFO - Distilling data from client: Client16
2023-07-01 00:27:18,280 - INFO - train loss: 0.0018847878993990968
2023-07-01 00:27:18,280 - INFO - train acc: 1.0
2023-07-01 00:27:18,297 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.85      0.78       118
           5       0.71      0.54      0.61        82

    accuracy                           0.72       200
   macro avg       0.72      0.69      0.70       200
weighted avg       0.72      0.72      0.71       200

2023-07-01 00:27:18,297 - INFO - test loss 0.019588967341264425
2023-07-01 00:27:18,297 - INFO - test acc 0.7199999690055847
2023-07-01 00:27:18,299 - WARNING - Finished tracing + transforming jit(gather) in 0.0002460479736328125 sec
2023-07-01 00:27:18,300 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[334,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:27:18,301 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0011932849884033203 sec
2023-07-01 00:27:18,301 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:18,311 - WARNING - Finished XLA compilation of jit(gather) in 0.009860754013061523 sec
2023-07-01 00:27:18,322 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:18,331 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:18,340 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:18,348 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:18,358 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:18,367 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:18,647 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client16//synthetic.png
2023-07-01 00:27:18,658 - INFO - c: 4.0 and total_data_in_this_class: 269
2023-07-01 00:27:18,658 - INFO - c: 6.0 and total_data_in_this_class: 268
2023-07-01 00:27:18,658 - INFO - c: 8.0 and total_data_in_this_class: 262
2023-07-01 00:27:18,658 - INFO - c: 4.0 and total_data_in_this_class: 64
2023-07-01 00:27:18,659 - INFO - c: 6.0 and total_data_in_this_class: 65
2023-07-01 00:27:18,659 - INFO - c: 8.0 and total_data_in_this_class: 71
2023-07-01 00:27:19,049 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.3681201934814453 sec
2023-07-01 00:27:19,095 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04439806938171387 sec
2023-07-01 00:27:19,100 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.41973209381103516 sec
2023-07-01 00:27:19,102 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:27:19,134 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03243899345397949 sec
2023-07-01 00:27:19,134 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:19,257 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12243819236755371 sec
2023-07-01 00:27:19,280 - INFO - initial test loss: 0.020891491425479698
2023-07-01 00:27:19,280 - INFO - initial test acc: 0.7400000095367432
2023-07-01 00:27:19,288 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.0056917667388916016 sec
2023-07-01 00:27:19,397 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11562609672546387 sec
2023-07-01 00:27:19,401 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:27:19,464 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06230902671813965 sec
2023-07-01 00:27:19,464 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:19,788 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32433605194091797 sec
2023-07-01 00:27:21,048 - INFO - Distilling data from client: Client17
2023-07-01 00:27:21,048 - INFO - train loss: 0.0022151222802347555
2023-07-01 00:27:21,048 - INFO - train acc: 1.0
2023-07-01 00:27:21,110 - INFO - report:               precision    recall  f1-score   support

           4       0.64      0.64      0.64        64
           6       0.66      0.71      0.68        65
           8       0.89      0.83      0.86        71

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:27:21,110 - INFO - test loss 0.01965178549641284
2023-07-01 00:27:21,110 - INFO - test acc 0.7299999594688416
2023-07-01 00:27:22,375 - INFO - Distilling data from client: Client17
2023-07-01 00:27:22,375 - INFO - train loss: 0.0012343522061719256
2023-07-01 00:27:22,375 - INFO - train acc: 1.0
2023-07-01 00:27:22,442 - INFO - report:               precision    recall  f1-score   support

           4       0.65      0.70      0.68        64
           6       0.68      0.68      0.68        65
           8       0.89      0.83      0.86        71

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:27:22,442 - INFO - test loss 0.019715246133068236
2023-07-01 00:27:22,442 - INFO - test acc 0.7400000095367432
2023-07-01 00:27:23,699 - INFO - Distilling data from client: Client17
2023-07-01 00:27:23,699 - INFO - train loss: 0.0010336298612571683
2023-07-01 00:27:23,699 - INFO - train acc: 1.0
2023-07-01 00:27:23,723 - INFO - report:               precision    recall  f1-score   support

           4       0.67      0.62      0.65        64
           6       0.65      0.71      0.68        65
           8       0.87      0.85      0.86        71

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-07-01 00:27:23,723 - INFO - test loss 0.020195075478444673
2023-07-01 00:27:23,723 - INFO - test acc 0.7299999594688416
2023-07-01 00:27:24,973 - INFO - Distilling data from client: Client17
2023-07-01 00:27:24,973 - INFO - train loss: 0.0008773602094551918
2023-07-01 00:27:24,973 - INFO - train acc: 1.0
2023-07-01 00:27:24,995 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.64      0.64        64
           6       0.62      0.65      0.63        65
           8       0.90      0.85      0.87        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:27:24,996 - INFO - test loss 0.020131882057047766
2023-07-01 00:27:24,996 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:26,246 - INFO - Distilling data from client: Client17
2023-07-01 00:27:26,246 - INFO - train loss: 0.0008610825349998915
2023-07-01 00:27:26,246 - INFO - train acc: 0.9980952143669128
2023-07-01 00:27:26,268 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.66      0.64        64
           6       0.66      0.66      0.66        65
           8       0.88      0.83      0.86        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:27:26,269 - INFO - test loss 0.02006310239059225
2023-07-01 00:27:26,269 - INFO - test acc 0.7199999690055847
2023-07-01 00:27:27,510 - INFO - Distilling data from client: Client17
2023-07-01 00:27:27,511 - INFO - train loss: 0.0006488650586551178
2023-07-01 00:27:27,511 - INFO - train acc: 1.0
2023-07-01 00:27:27,534 - INFO - report:               precision    recall  f1-score   support

           4       0.65      0.69      0.67        64
           6       0.69      0.74      0.71        65
           8       0.90      0.79      0.84        71

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:27:27,534 - INFO - test loss 0.020476160441546765
2023-07-01 00:27:27,534 - INFO - test acc 0.7400000095367432
2023-07-01 00:27:28,785 - INFO - Distilling data from client: Client17
2023-07-01 00:27:28,785 - INFO - train loss: 0.0006783505598895797
2023-07-01 00:27:28,785 - INFO - train acc: 0.9980952143669128
2023-07-01 00:27:28,808 - INFO - report:               precision    recall  f1-score   support

           4       0.65      0.62      0.63        64
           6       0.63      0.69      0.66        65
           8       0.88      0.83      0.86        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:27:28,808 - INFO - test loss 0.020280744407344103
2023-07-01 00:27:28,809 - INFO - test acc 0.7199999690055847
2023-07-01 00:27:30,072 - INFO - Distilling data from client: Client17
2023-07-01 00:27:30,072 - INFO - train loss: 0.0005419358884065336
2023-07-01 00:27:30,072 - INFO - train acc: 1.0
2023-07-01 00:27:30,096 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.66      0.64        64
           6       0.63      0.63      0.63        65
           8       0.87      0.83      0.85        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:27:30,096 - INFO - test loss 0.021001518839512248
2023-07-01 00:27:30,096 - INFO - test acc 0.7099999785423279
2023-07-01 00:27:31,340 - INFO - Distilling data from client: Client17
2023-07-01 00:27:31,340 - INFO - train loss: 0.0006544455168404254
2023-07-01 00:27:31,340 - INFO - train acc: 1.0
2023-07-01 00:27:31,368 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.67      0.65        64
           6       0.65      0.65      0.65        65
           8       0.88      0.83      0.86        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:27:31,368 - INFO - test loss 0.020581320754099363
2023-07-01 00:27:31,368 - INFO - test acc 0.7199999690055847
2023-07-01 00:27:32,620 - INFO - Distilling data from client: Client17
2023-07-01 00:27:32,620 - INFO - train loss: 0.000474098197315981
2023-07-01 00:27:32,620 - INFO - train acc: 1.0
2023-07-01 00:27:32,645 - INFO - report:               precision    recall  f1-score   support

           4       0.60      0.64      0.62        64
           6       0.62      0.62      0.62        65
           8       0.87      0.83      0.85        71

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:27:32,645 - INFO - test loss 0.020820706335818104
2023-07-01 00:27:32,645 - INFO - test acc 0.699999988079071
2023-07-01 00:27:33,905 - INFO - Distilling data from client: Client17
2023-07-01 00:27:33,905 - INFO - train loss: 0.0004866956742840224
2023-07-01 00:27:33,905 - INFO - train acc: 1.0
2023-07-01 00:27:33,930 - INFO - report:               precision    recall  f1-score   support

           4       0.65      0.61      0.63        64
           6       0.63      0.69      0.66        65
           8       0.86      0.83      0.84        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:27:33,930 - INFO - test loss 0.020522156830885593
2023-07-01 00:27:33,930 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:35,178 - INFO - Distilling data from client: Client17
2023-07-01 00:27:35,178 - INFO - train loss: 0.0004164382545638036
2023-07-01 00:27:35,178 - INFO - train acc: 1.0
2023-07-01 00:27:35,202 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.66      0.64        64
           6       0.66      0.65      0.65        65
           8       0.87      0.85      0.86        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:27:35,202 - INFO - test loss 0.020075696139994072
2023-07-01 00:27:35,202 - INFO - test acc 0.7199999690055847
2023-07-01 00:27:36,451 - INFO - Distilling data from client: Client17
2023-07-01 00:27:36,451 - INFO - train loss: 0.000345328072826923
2023-07-01 00:27:36,451 - INFO - train acc: 1.0
2023-07-01 00:27:36,474 - INFO - report:               precision    recall  f1-score   support

           4       0.60      0.62      0.61        64
           6       0.64      0.65      0.64        65
           8       0.88      0.83      0.86        71

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-07-01 00:27:36,475 - INFO - test loss 0.020887564256613745
2023-07-01 00:27:36,475 - INFO - test acc 0.7049999833106995
2023-07-01 00:27:37,735 - INFO - Distilling data from client: Client17
2023-07-01 00:27:37,736 - INFO - train loss: 0.00044434076466348834
2023-07-01 00:27:37,736 - INFO - train acc: 1.0
2023-07-01 00:27:37,759 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.67      0.65        64
           6       0.68      0.66      0.67        65
           8       0.84      0.82      0.83        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:27:37,760 - INFO - test loss 0.02101038473397405
2023-07-01 00:27:37,760 - INFO - test acc 0.7199999690055847
2023-07-01 00:27:39,022 - INFO - Distilling data from client: Client17
2023-07-01 00:27:39,022 - INFO - train loss: 0.0003140024580918523
2023-07-01 00:27:39,022 - INFO - train acc: 1.0
2023-07-01 00:27:39,046 - INFO - report:               precision    recall  f1-score   support

           4       0.67      0.67      0.67        64
           6       0.67      0.69      0.68        65
           8       0.86      0.83      0.84        71

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.74      0.73      0.74       200

2023-07-01 00:27:39,046 - INFO - test loss 0.02065445821585429
2023-07-01 00:27:39,046 - INFO - test acc 0.73499995470047
2023-07-01 00:27:40,307 - INFO - Distilling data from client: Client17
2023-07-01 00:27:40,307 - INFO - train loss: 0.0005155757366437501
2023-07-01 00:27:40,307 - INFO - train acc: 1.0
2023-07-01 00:27:40,331 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.64      0.64        64
           6       0.66      0.68      0.67        65
           8       0.87      0.83      0.85        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:27:40,331 - INFO - test loss 0.020695828137680387
2023-07-01 00:27:40,332 - INFO - test acc 0.7199999690055847
2023-07-01 00:27:41,586 - INFO - Distilling data from client: Client17
2023-07-01 00:27:41,586 - INFO - train loss: 0.000458123665432199
2023-07-01 00:27:41,586 - INFO - train acc: 1.0
2023-07-01 00:27:41,610 - INFO - report:               precision    recall  f1-score   support

           4       0.60      0.59      0.60        64
           6       0.62      0.66      0.64        65
           8       0.87      0.83      0.85        71

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:27:41,610 - INFO - test loss 0.020466735974227695
2023-07-01 00:27:41,610 - INFO - test acc 0.699999988079071
2023-07-01 00:27:42,868 - INFO - Distilling data from client: Client17
2023-07-01 00:27:42,868 - INFO - train loss: 0.0003105767699068964
2023-07-01 00:27:42,868 - INFO - train acc: 1.0
2023-07-01 00:27:42,892 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.66      0.64        64
           6       0.64      0.66      0.65        65
           8       0.89      0.82      0.85        71

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:27:42,892 - INFO - test loss 0.020833573400702153
2023-07-01 00:27:42,892 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:44,142 - INFO - Distilling data from client: Client17
2023-07-01 00:27:44,143 - INFO - train loss: 0.0003750995687982548
2023-07-01 00:27:44,143 - INFO - train acc: 1.0
2023-07-01 00:27:44,169 - INFO - report:               precision    recall  f1-score   support

           4       0.65      0.66      0.65        64
           6       0.65      0.66      0.66        65
           8       0.86      0.83      0.84        71

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:27:44,169 - INFO - test loss 0.020425610003861963
2023-07-01 00:27:44,169 - INFO - test acc 0.7199999690055847
2023-07-01 00:27:45,415 - INFO - Distilling data from client: Client17
2023-07-01 00:27:45,415 - INFO - train loss: 0.00037380189569919536
2023-07-01 00:27:45,415 - INFO - train acc: 1.0
2023-07-01 00:27:45,439 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.64      0.63        64
           6       0.64      0.65      0.64        65
           8       0.87      0.83      0.85        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:27:45,439 - INFO - test loss 0.021121021209914107
2023-07-01 00:27:45,440 - INFO - test acc 0.7099999785423279
2023-07-01 00:27:46,683 - INFO - Distilling data from client: Client17
2023-07-01 00:27:46,684 - INFO - train loss: 0.0003760706443610431
2023-07-01 00:27:46,684 - INFO - train acc: 1.0
2023-07-01 00:27:46,708 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.62      0.62        64
           6       0.61      0.65      0.63        65
           8       0.85      0.80      0.83        71

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.70       200

2023-07-01 00:27:46,708 - INFO - test loss 0.020929017610289643
2023-07-01 00:27:46,708 - INFO - test acc 0.6949999928474426
2023-07-01 00:27:47,958 - INFO - Distilling data from client: Client17
2023-07-01 00:27:47,958 - INFO - train loss: 0.0003440379241882273
2023-07-01 00:27:47,958 - INFO - train acc: 1.0
2023-07-01 00:27:47,985 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.67      0.65        64
           6       0.65      0.63      0.64        65
           8       0.86      0.83      0.84        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:27:47,985 - INFO - test loss 0.020795898332603372
2023-07-01 00:27:47,985 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:49,231 - INFO - Distilling data from client: Client17
2023-07-01 00:27:49,231 - INFO - train loss: 0.0003257004207002265
2023-07-01 00:27:49,231 - INFO - train acc: 1.0
2023-07-01 00:27:49,255 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.70      0.66        64
           6       0.65      0.63      0.64        65
           8       0.88      0.80      0.84        71

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:27:49,255 - INFO - test loss 0.02056848923874919
2023-07-01 00:27:49,255 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:50,509 - INFO - Distilling data from client: Client17
2023-07-01 00:27:50,509 - INFO - train loss: 0.00024960258138374147
2023-07-01 00:27:50,509 - INFO - train acc: 1.0
2023-07-01 00:27:50,534 - INFO - report:               precision    recall  f1-score   support

           4       0.62      0.64      0.63        64
           6       0.65      0.66      0.66        65
           8       0.87      0.83      0.85        71

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:27:50,534 - INFO - test loss 0.02126704469641516
2023-07-01 00:27:50,534 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:51,777 - INFO - Distilling data from client: Client17
2023-07-01 00:27:51,777 - INFO - train loss: 0.0002830478566347576
2023-07-01 00:27:51,777 - INFO - train acc: 1.0
2023-07-01 00:27:51,801 - INFO - report:               precision    recall  f1-score   support

           4       0.63      0.67      0.65        64
           6       0.67      0.68      0.67        65
           8       0.89      0.83      0.86        71

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:27:51,801 - INFO - test loss 0.021100054509765247
2023-07-01 00:27:51,802 - INFO - test acc 0.7299999594688416
2023-07-01 00:27:51,813 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:51,822 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:51,831 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:51,840 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:51,849 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:51,857 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:51,866 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:51,876 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:51,885 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:52,258 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client17//synthetic.png
2023-07-01 00:27:52,270 - INFO - c: 0.0 and total_data_in_this_class: 3
2023-07-01 00:27:52,270 - INFO - c: 1.0 and total_data_in_this_class: 258
2023-07-01 00:27:52,271 - INFO - c: 2.0 and total_data_in_this_class: 12
2023-07-01 00:27:52,271 - INFO - c: 3.0 and total_data_in_this_class: 253
2023-07-01 00:27:52,271 - INFO - c: 4.0 and total_data_in_this_class: 273
2023-07-01 00:27:52,271 - INFO - c: 0.0 and total_data_in_this_class: 2
2023-07-01 00:27:52,271 - INFO - c: 1.0 and total_data_in_this_class: 70
2023-07-01 00:27:52,271 - INFO - c: 2.0 and total_data_in_this_class: 3
2023-07-01 00:27:52,271 - INFO - c: 3.0 and total_data_in_this_class: 65
2023-07-01 00:27:52,271 - INFO - c: 4.0 and total_data_in_this_class: 60
2023-07-01 00:27:52,288 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025916099548339844 sec
2023-07-01 00:27:52,288 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:27:52,290 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012331008911132812 sec
2023-07-01 00:27:52,290 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:52,299 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.00909733772277832 sec
2023-07-01 00:27:52,301 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002281665802001953 sec
2023-07-01 00:27:52,301 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:27:52,302 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009233951568603516 sec
2023-07-01 00:27:52,302 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:52,311 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008468866348266602 sec
2023-07-01 00:27:52,314 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00014209747314453125 sec
2023-07-01 00:27:52,315 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012564659118652344 sec
2023-07-01 00:27:52,316 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003085136413574219 sec
2023-07-01 00:27:52,317 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002124309539794922 sec
2023-07-01 00:27:52,318 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001246929168701172 sec
2023-07-01 00:27:52,318 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002903938293457031 sec
2023-07-01 00:27:52,319 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002529621124267578 sec
2023-07-01 00:27:52,320 - WARNING - Finished tracing + transforming absolute for pjit in 0.00017261505126953125 sec
2023-07-01 00:27:52,320 - WARNING - Finished tracing + transforming fn for pjit in 0.0002837181091308594 sec
2023-07-01 00:27:52,321 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00033402442932128906 sec
2023-07-01 00:27:52,322 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020742416381835938 sec
2023-07-01 00:27:52,323 - WARNING - Finished tracing + transforming fn for pjit in 0.0002334117889404297 sec
2023-07-01 00:27:52,323 - WARNING - Finished tracing + transforming fn for pjit in 0.00027108192443847656 sec
2023-07-01 00:27:52,324 - WARNING - Finished tracing + transforming fn for pjit in 0.00022673606872558594 sec
2023-07-01 00:27:52,325 - WARNING - Finished tracing + transforming fn for pjit in 0.0002658367156982422 sec
2023-07-01 00:27:52,326 - WARNING - Finished tracing + transforming fn for pjit in 0.0002269744873046875 sec
2023-07-01 00:27:52,328 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001761913299560547 sec
2023-07-01 00:27:52,328 - WARNING - Finished tracing + transforming fn for pjit in 0.0002319812774658203 sec
2023-07-01 00:27:52,329 - WARNING - Finished tracing + transforming fn for pjit in 0.00022649765014648438 sec
2023-07-01 00:27:52,333 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00036978721618652344 sec
2023-07-01 00:27:52,333 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009889602661132812 sec
2023-07-01 00:27:52,334 - WARNING - Finished tracing + transforming fn for pjit in 0.00023508071899414062 sec
2023-07-01 00:27:52,335 - WARNING - Finished tracing + transforming fn for pjit in 0.0002231597900390625 sec
2023-07-01 00:27:52,336 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029397010803222656 sec
2023-07-01 00:27:52,336 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002655982971191406 sec
2023-07-01 00:27:52,337 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00019407272338867188 sec
2023-07-01 00:27:52,338 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002644062042236328 sec
2023-07-01 00:27:52,339 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023055076599121094 sec
2023-07-01 00:27:52,339 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000232696533203125 sec
2023-07-01 00:27:52,340 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003561973571777344 sec
2023-07-01 00:27:52,340 - WARNING - Finished tracing + transforming _where for pjit in 0.0009682178497314453 sec
2023-07-01 00:27:52,341 - WARNING - Finished tracing + transforming fn for pjit in 0.0002582073211669922 sec
2023-07-01 00:27:52,342 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025534629821777344 sec
2023-07-01 00:27:52,343 - WARNING - Finished tracing + transforming fn for pjit in 0.00022077560424804688 sec
2023-07-01 00:27:52,343 - WARNING - Finished tracing + transforming fn for pjit in 0.00021839141845703125 sec
2023-07-01 00:27:52,344 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021409988403320312 sec
2023-07-01 00:27:52,345 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002560615539550781 sec
2023-07-01 00:27:52,345 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002524852752685547 sec
2023-07-01 00:27:52,346 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025844573974609375 sec
2023-07-01 00:27:52,347 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002238750457763672 sec
2023-07-01 00:27:52,347 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022077560424804688 sec
2023-07-01 00:27:52,348 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025391578674316406 sec
2023-07-01 00:27:52,348 - WARNING - Finished tracing + transforming _where for pjit in 0.0008406639099121094 sec
2023-07-01 00:27:52,349 - WARNING - Finished tracing + transforming fn for pjit in 0.00026226043701171875 sec
2023-07-01 00:27:52,350 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002646446228027344 sec
2023-07-01 00:27:52,351 - WARNING - Finished tracing + transforming fn for pjit in 0.0002224445343017578 sec
2023-07-01 00:27:52,355 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002732276916503906 sec
2023-07-01 00:27:52,356 - WARNING - Finished tracing + transforming fn for pjit in 0.0003380775451660156 sec
2023-07-01 00:27:52,356 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027060508728027344 sec
2023-07-01 00:27:52,357 - WARNING - Finished tracing + transforming fn for pjit in 0.00022935867309570312 sec
2023-07-01 00:27:52,361 - WARNING - Finished tracing + transforming fn for pjit in 0.0002243518829345703 sec
2023-07-01 00:27:52,363 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016927719116210938 sec
2023-07-01 00:27:52,364 - WARNING - Finished tracing + transforming fn for pjit in 0.001142263412475586 sec
2023-07-01 00:27:52,365 - WARNING - Finished tracing + transforming fn for pjit in 0.0002338886260986328 sec
2023-07-01 00:27:52,383 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06954550743103027 sec
2023-07-01 00:27:52,386 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001285076141357422 sec
2023-07-01 00:27:52,387 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011730194091796875 sec
2023-07-01 00:27:52,387 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002684593200683594 sec
2023-07-01 00:27:52,390 - WARNING - Finished tracing + transforming fn for pjit in 0.00022101402282714844 sec
2023-07-01 00:27:52,390 - WARNING - Finished tracing + transforming fn for pjit in 0.00025725364685058594 sec
2023-07-01 00:27:52,391 - WARNING - Finished tracing + transforming fn for pjit in 0.00021982192993164062 sec
2023-07-01 00:27:52,397 - WARNING - Finished tracing + transforming fn for pjit in 0.00023031234741210938 sec
2023-07-01 00:27:52,398 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002219676971435547 sec
2023-07-01 00:27:52,399 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002570152282714844 sec
2023-07-01 00:27:52,400 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017452239990234375 sec
2023-07-01 00:27:52,401 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003275871276855469 sec
2023-07-01 00:27:52,401 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022602081298828125 sec
2023-07-01 00:27:52,402 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022411346435546875 sec
2023-07-01 00:27:52,403 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002751350402832031 sec
2023-07-01 00:27:52,403 - WARNING - Finished tracing + transforming _where for pjit in 0.0008738040924072266 sec
2023-07-01 00:27:52,404 - WARNING - Finished tracing + transforming fn for pjit in 0.0002579689025878906 sec
2023-07-01 00:27:52,405 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002522468566894531 sec
2023-07-01 00:27:52,405 - WARNING - Finished tracing + transforming fn for pjit in 0.00021886825561523438 sec
2023-07-01 00:27:52,406 - WARNING - Finished tracing + transforming fn for pjit in 0.0002789497375488281 sec
2023-07-01 00:27:52,418 - WARNING - Finished tracing + transforming fn for pjit in 0.0002193450927734375 sec
2023-07-01 00:27:52,438 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05392813682556152 sec
2023-07-01 00:27:52,439 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001316070556640625 sec
2023-07-01 00:27:52,440 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013446807861328125 sec
2023-07-01 00:27:52,441 - WARNING - Finished tracing + transforming _where for pjit in 0.000621795654296875 sec
2023-07-01 00:27:52,441 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.000293731689453125 sec
2023-07-01 00:27:52,442 - WARNING - Finished tracing + transforming trace for pjit in 0.0025522708892822266 sec
2023-07-01 00:27:52,444 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010538101196289062 sec
2023-07-01 00:27:52,445 - WARNING - Finished tracing + transforming tril for pjit in 0.0006806850433349609 sec
2023-07-01 00:27:52,445 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0017197132110595703 sec
2023-07-01 00:27:52,446 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010895729064941406 sec
2023-07-01 00:27:52,446 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010728836059570312 sec
2023-07-01 00:27:52,448 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014081001281738281 sec
2023-07-01 00:27:52,452 - WARNING - Finished tracing + transforming _solve for pjit in 0.009203195571899414 sec
2023-07-01 00:27:52,453 - WARNING - Finished tracing + transforming dot for pjit in 0.0003254413604736328 sec
2023-07-01 00:27:52,455 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.14317035675048828 sec
2023-07-01 00:27:52,457 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:27:52,491 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03330373764038086 sec
2023-07-01 00:27:52,491 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:52,589 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.09771943092346191 sec
2023-07-01 00:27:52,592 - INFO - initial test loss: 0.03988313329532323
2023-07-01 00:27:52,593 - INFO - initial test acc: 0.38499999046325684
2023-07-01 00:27:52,597 - WARNING - Finished tracing + transforming dot for pjit in 0.0003590583801269531 sec
2023-07-01 00:27:52,598 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00028777122497558594 sec
2023-07-01 00:27:52,599 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003101825714111328 sec
2023-07-01 00:27:52,599 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009477138519287109 sec
2023-07-01 00:27:52,600 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00019502639770507812 sec
2023-07-01 00:27:52,601 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0002651214599609375 sec
2023-07-01 00:27:52,602 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024056434631347656 sec
2023-07-01 00:27:52,603 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00037169456481933594 sec
2023-07-01 00:27:52,603 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010828971862792969 sec
2023-07-01 00:27:52,604 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009902238845825195 sec
2023-07-01 00:27:52,612 - WARNING - Finished tracing + transforming fn for pjit in 0.00025916099548339844 sec
2023-07-01 00:27:52,613 - WARNING - Finished tracing + transforming fn for pjit in 0.00027942657470703125 sec
2023-07-01 00:27:52,614 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002837181091308594 sec
2023-07-01 00:27:52,614 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002665519714355469 sec
2023-07-01 00:27:52,615 - WARNING - Finished tracing + transforming _where for pjit in 0.0008742809295654297 sec
2023-07-01 00:27:52,624 - WARNING - Finished tracing + transforming fn for pjit in 0.00025200843811035156 sec
2023-07-01 00:27:52,624 - WARNING - Finished tracing + transforming fn for pjit in 0.00027561187744140625 sec
2023-07-01 00:27:52,625 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021314620971679688 sec
2023-07-01 00:27:52,626 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002532005310058594 sec
2023-07-01 00:27:52,626 - WARNING - Finished tracing + transforming _where for pjit in 0.0008485317230224609 sec
2023-07-01 00:27:52,660 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002079010009765625 sec
2023-07-01 00:27:52,715 - WARNING - Finished tracing + transforming fn for pjit in 0.0002703666687011719 sec
2023-07-01 00:27:52,715 - WARNING - Finished tracing + transforming fn for pjit in 0.0002238750457763672 sec
2023-07-01 00:27:52,716 - WARNING - Finished tracing + transforming square for pjit in 0.00017309188842773438 sec
2023-07-01 00:27:52,718 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022125244140625 sec
2023-07-01 00:27:52,720 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002391338348388672 sec
2023-07-01 00:27:52,720 - WARNING - Finished tracing + transforming fn for pjit in 0.0002675056457519531 sec
2023-07-01 00:27:52,721 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022721290588378906 sec
2023-07-01 00:27:52,721 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022649765014648438 sec
2023-07-01 00:27:52,722 - WARNING - Finished tracing + transforming fn for pjit in 0.00026488304138183594 sec
2023-07-01 00:27:52,723 - WARNING - Finished tracing + transforming fn for pjit in 0.00022339820861816406 sec
2023-07-01 00:27:52,723 - WARNING - Finished tracing + transforming square for pjit in 0.00018262863159179688 sec
2023-07-01 00:27:52,725 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002181529998779297 sec
2023-07-01 00:27:52,727 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001780986785888672 sec
2023-07-01 00:27:52,728 - WARNING - Finished tracing + transforming fn for pjit in 0.00026917457580566406 sec
2023-07-01 00:27:52,728 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000217437744140625 sec
2023-07-01 00:27:52,729 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021886825561523438 sec
2023-07-01 00:27:52,730 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13631153106689453 sec
2023-07-01 00:27:52,733 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,10]), ShapedArray(float32[32,10]), ShapedArray(float32[32,10]), ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,10]), ShapedArray(float32[32,3,32,32]), ShapedArray(float32[32,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:27:52,796 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06224536895751953 sec
2023-07-01 00:27:52,796 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:52,993 - WARNING - Finished XLA compilation of jit(update_fn) in 0.19703054428100586 sec
2023-07-01 00:27:53,040 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,040 - INFO - train loss: 0.02620768031715461
2023-07-01 00:27:53,040 - INFO - train acc: 0.65625
2023-07-01 00:27:53,048 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.81      0.79        70
           2       0.08      0.33      0.12         3
           3       0.63      0.63      0.63        65
           4       0.66      0.52      0.58        60

    accuracy                           0.65       200
   macro avg       0.43      0.46      0.42       200
weighted avg       0.67      0.65      0.66       200

2023-07-01 00:27:53,048 - INFO - test loss 0.02690352769296953
2023-07-01 00:27:53,048 - INFO - test acc 0.6499999761581421
2023-07-01 00:27:53,100 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,100 - INFO - train loss: 0.01937942100788706
2023-07-01 00:27:53,100 - INFO - train acc: 0.78125
2023-07-01 00:27:53,107 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.84      0.80        70
           2       0.07      0.33      0.12         3
           3       0.69      0.58      0.63        65
           4       0.70      0.62      0.65        60

    accuracy                           0.68       200
   macro avg       0.44      0.48      0.44       200
weighted avg       0.70      0.68      0.68       200

2023-07-01 00:27:53,107 - INFO - test loss 0.026148628734007348
2023-07-01 00:27:53,107 - INFO - test acc 0.675000011920929
2023-07-01 00:27:53,153 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,153 - INFO - train loss: 0.016873206590351696
2023-07-01 00:27:53,153 - INFO - train acc: 0.8125
2023-07-01 00:27:53,157 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.79      0.83      0.81        70
           2       0.00      0.00      0.00         3
           3       0.63      0.52      0.57        65
           4       0.66      0.63      0.64        60

    accuracy                           0.65       200
   macro avg       0.42      0.40      0.41       200
weighted avg       0.68      0.65      0.66       200

2023-07-01 00:27:53,158 - INFO - test loss 0.025929991981042448
2023-07-01 00:27:53,158 - INFO - test acc 0.6499999761581421
2023-07-01 00:27:53,203 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,203 - INFO - train loss: 0.02022676069473868
2023-07-01 00:27:53,203 - INFO - train acc: 0.8125
2023-07-01 00:27:53,210 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.77      0.87      0.82        70
           2       0.00      0.00      0.00         3
           3       0.69      0.57      0.62        65
           4       0.68      0.67      0.67        60

    accuracy                           0.69       200
   macro avg       0.43      0.42      0.42       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:27:53,210 - INFO - test loss 0.025624343372975693
2023-07-01 00:27:53,210 - INFO - test acc 0.6899999976158142
2023-07-01 00:27:53,258 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,258 - INFO - train loss: 0.014822006915326515
2023-07-01 00:27:53,258 - INFO - train acc: 0.90625
2023-07-01 00:27:53,263 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.77      0.87      0.82        70
           2       0.00      0.00      0.00         3
           3       0.65      0.52      0.58        65
           4       0.65      0.62      0.63        60

    accuracy                           0.66       200
   macro avg       0.42      0.40      0.41       200
weighted avg       0.68      0.66      0.67       200

2023-07-01 00:27:53,263 - INFO - test loss 0.024842998341743375
2023-07-01 00:27:53,263 - INFO - test acc 0.6599999666213989
2023-07-01 00:27:53,306 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,306 - INFO - train loss: 0.014610576284653743
2023-07-01 00:27:53,306 - INFO - train acc: 0.875
2023-07-01 00:27:53,311 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.78      0.81      0.80        70
           2       0.00      0.00      0.00         3
           3       0.65      0.65      0.65        65
           4       0.66      0.62      0.64        60

    accuracy                           0.68       200
   macro avg       0.42      0.42      0.42       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:27:53,311 - INFO - test loss 0.02523755880255259
2023-07-01 00:27:53,311 - INFO - test acc 0.6800000071525574
2023-07-01 00:27:53,358 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,358 - INFO - train loss: 0.020824730537385802
2023-07-01 00:27:53,358 - INFO - train acc: 0.75
2023-07-01 00:27:53,363 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.70      0.83      0.76        70
           2       0.00      0.00      0.00         3
           3       0.66      0.57      0.61        65
           4       0.69      0.60      0.64        60

    accuracy                           0.66       200
   macro avg       0.41      0.40      0.40       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:27:53,363 - INFO - test loss 0.025158866124852252
2023-07-01 00:27:53,363 - INFO - test acc 0.6549999713897705
2023-07-01 00:27:53,409 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,409 - INFO - train loss: 0.01905632365277901
2023-07-01 00:27:53,409 - INFO - train acc: 0.71875
2023-07-01 00:27:53,417 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.86      0.81        70
           2       0.00      0.00      0.00         3
           3       0.68      0.71      0.69        65
           4       0.74      0.58      0.65        60

    accuracy                           0.70       200
   macro avg       0.44      0.43      0.43       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:27:53,417 - INFO - test loss 0.025329108751547678
2023-07-01 00:27:53,417 - INFO - test acc 0.7049999833106995
2023-07-01 00:27:53,461 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,461 - INFO - train loss: 0.01441144775912804
2023-07-01 00:27:53,461 - INFO - train acc: 0.90625
2023-07-01 00:27:53,465 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.86      0.81        70
           2       0.00      0.00      0.00         3
           3       0.64      0.57      0.60        65
           4       0.70      0.63      0.67        60

    accuracy                           0.68       200
   macro avg       0.42      0.41      0.41       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:27:53,465 - INFO - test loss 0.02463892173414638
2023-07-01 00:27:53,465 - INFO - test acc 0.675000011920929
2023-07-01 00:27:53,508 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,508 - INFO - train loss: 0.016058712285025287
2023-07-01 00:27:53,508 - INFO - train acc: 0.875
2023-07-01 00:27:53,514 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.73      0.83      0.78        70
           2       0.00      0.00      0.00         3
           3       0.65      0.54      0.59        65
           4       0.64      0.65      0.64        60

    accuracy                           0.66       200
   macro avg       0.40      0.40      0.40       200
weighted avg       0.66      0.66      0.66       200

2023-07-01 00:27:53,514 - INFO - test loss 0.02446591671442445
2023-07-01 00:27:53,515 - INFO - test acc 0.6599999666213989
2023-07-01 00:27:53,557 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,558 - INFO - train loss: 0.015061557871391937
2023-07-01 00:27:53,558 - INFO - train acc: 0.84375
2023-07-01 00:27:53,562 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.75      0.89      0.81        70
           2       0.00      0.00      0.00         3
           3       0.68      0.60      0.64        65
           4       0.75      0.60      0.67        60

    accuracy                           0.69       200
   macro avg       0.44      0.42      0.42       200
weighted avg       0.71      0.69      0.69       200

2023-07-01 00:27:53,562 - INFO - test loss 0.025900441113150322
2023-07-01 00:27:53,562 - INFO - test acc 0.6850000023841858
2023-07-01 00:27:53,610 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,610 - INFO - train loss: 0.015822289054849094
2023-07-01 00:27:53,610 - INFO - train acc: 0.78125
2023-07-01 00:27:53,614 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.75      0.86      0.80        70
           2       0.00      0.00      0.00         3
           3       0.65      0.52      0.58        65
           4       0.68      0.70      0.69        60

    accuracy                           0.68       200
   macro avg       0.42      0.42      0.41       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:27:53,615 - INFO - test loss 0.02512909020125586
2023-07-01 00:27:53,615 - INFO - test acc 0.6800000071525574
2023-07-01 00:27:53,657 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,658 - INFO - train loss: 0.013733153043584946
2023-07-01 00:27:53,658 - INFO - train acc: 0.875
2023-07-01 00:27:53,665 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.78      0.83      0.81        70
           2       0.00      0.00      0.00         3
           3       0.71      0.71      0.71        65
           4       0.71      0.65      0.68        60

    accuracy                           0.71       200
   macro avg       0.44      0.44      0.44       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:27:53,665 - INFO - test loss 0.02359051065061403
2023-07-01 00:27:53,665 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:53,707 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,707 - INFO - train loss: 0.019078307287898135
2023-07-01 00:27:53,707 - INFO - train acc: 0.84375
2023-07-01 00:27:53,712 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.75      0.81      0.78        70
           2       0.00      0.00      0.00         3
           3       0.70      0.60      0.64        65
           4       0.67      0.70      0.68        60

    accuracy                           0.69       200
   macro avg       0.42      0.42      0.42       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:27:53,712 - INFO - test loss 0.02517938914998
2023-07-01 00:27:53,712 - INFO - test acc 0.6899999976158142
2023-07-01 00:27:53,756 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,756 - INFO - train loss: 0.01675713665921244
2023-07-01 00:27:53,756 - INFO - train acc: 0.875
2023-07-01 00:27:53,761 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.82      0.80      0.81        70
           2       0.00      0.00      0.00         3
           3       0.65      0.68      0.66        65
           4       0.67      0.65      0.66        60

    accuracy                           0.69       200
   macro avg       0.43      0.43      0.43       200
weighted avg       0.70      0.69      0.70       200

2023-07-01 00:27:53,761 - INFO - test loss 0.024286543265975985
2023-07-01 00:27:53,761 - INFO - test acc 0.6949999928474426
2023-07-01 00:27:53,804 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,804 - INFO - train loss: 0.01710359999523661
2023-07-01 00:27:53,804 - INFO - train acc: 0.75
2023-07-01 00:27:53,809 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.77      0.84      0.80        70
           2       0.00      0.00      0.00         3
           3       0.70      0.68      0.69        65
           4       0.73      0.67      0.70        60

    accuracy                           0.71       200
   macro avg       0.44      0.44      0.44       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:27:53,809 - INFO - test loss 0.023800527550757566
2023-07-01 00:27:53,809 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:53,859 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,859 - INFO - train loss: 0.017331314143818128
2023-07-01 00:27:53,859 - INFO - train acc: 0.84375
2023-07-01 00:27:53,864 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.76      0.86      0.81        70
           2       0.00      0.00      0.00         3
           3       0.66      0.66      0.66        65
           4       0.71      0.58      0.64        60

    accuracy                           0.69       200
   macro avg       0.43      0.42      0.42       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:27:53,864 - INFO - test loss 0.023839331085444053
2023-07-01 00:27:53,864 - INFO - test acc 0.6899999976158142
2023-07-01 00:27:53,907 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,907 - INFO - train loss: 0.0193096458219698
2023-07-01 00:27:53,907 - INFO - train acc: 0.78125
2023-07-01 00:27:53,912 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.74      0.86      0.79        70
           2       0.00      0.00      0.00         3
           3       0.70      0.60      0.64        65
           4       0.73      0.68      0.71        60

    accuracy                           0.70       200
   macro avg       0.43      0.43      0.43       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:27:53,912 - INFO - test loss 0.024384951537613667
2023-07-01 00:27:53,912 - INFO - test acc 0.699999988079071
2023-07-01 00:27:53,956 - INFO - Distilling data from client: Client18
2023-07-01 00:27:53,956 - INFO - train loss: 0.01754364091823254
2023-07-01 00:27:53,956 - INFO - train acc: 0.84375
2023-07-01 00:27:53,961 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.79      0.79      0.79        70
           2       0.00      0.00      0.00         3
           3       0.64      0.60      0.62        65
           4       0.63      0.65      0.64        60

    accuracy                           0.67       200
   macro avg       0.41      0.41      0.41       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:27:53,961 - INFO - test loss 0.02480563001904106
2023-07-01 00:27:53,961 - INFO - test acc 0.6649999618530273
2023-07-01 00:27:54,009 - INFO - Distilling data from client: Client18
2023-07-01 00:27:54,010 - INFO - train loss: 0.015054898191733532
2023-07-01 00:27:54,010 - INFO - train acc: 0.84375
2023-07-01 00:27:54,016 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.78      0.86      0.82        70
           2       0.00      0.00      0.00         3
           3       0.67      0.65      0.66        65
           4       0.67      0.58      0.63        60

    accuracy                           0.69       200
   macro avg       0.42      0.42      0.42       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:27:54,016 - INFO - test loss 0.024471866447276617
2023-07-01 00:27:54,016 - INFO - test acc 0.6850000023841858
2023-07-01 00:27:54,060 - INFO - Distilling data from client: Client18
2023-07-01 00:27:54,060 - INFO - train loss: 0.01737312617078727
2023-07-01 00:27:54,060 - INFO - train acc: 0.75
2023-07-01 00:27:54,065 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.79      0.81      0.80        70
           2       0.00      0.00      0.00         3
           3       0.69      0.66      0.68        65
           4       0.69      0.72      0.70        60

    accuracy                           0.71       200
   macro avg       0.44      0.44      0.44       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:27:54,065 - INFO - test loss 0.02387580027471215
2023-07-01 00:27:54,065 - INFO - test acc 0.7149999737739563
2023-07-01 00:27:54,108 - INFO - Distilling data from client: Client18
2023-07-01 00:27:54,108 - INFO - train loss: 0.01734671650314484
2023-07-01 00:27:54,108 - INFO - train acc: 0.8125
2023-07-01 00:27:54,113 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.69      0.84      0.76        70
           2       0.12      0.33      0.18         3
           3       0.69      0.58      0.63        65
           4       0.76      0.65      0.70        60

    accuracy                           0.69       200
   macro avg       0.45      0.48      0.45       200
weighted avg       0.70      0.69      0.68       200

2023-07-01 00:27:54,113 - INFO - test loss 0.02453211143945051
2023-07-01 00:27:54,113 - INFO - test acc 0.6850000023841858
2023-07-01 00:27:54,160 - INFO - Distilling data from client: Client18
2023-07-01 00:27:54,160 - INFO - train loss: 0.01820742629907162
2023-07-01 00:27:54,160 - INFO - train acc: 0.84375
2023-07-01 00:27:54,165 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.81      0.79      0.80        70
           2       0.00      0.00      0.00         3
           3       0.63      0.69      0.66        65
           4       0.62      0.60      0.61        60

    accuracy                           0.68       200
   macro avg       0.41      0.42      0.41       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:27:54,165 - INFO - test loss 0.025032126560549026
2023-07-01 00:27:54,165 - INFO - test acc 0.6800000071525574
2023-07-01 00:27:54,210 - INFO - Distilling data from client: Client18
2023-07-01 00:27:54,210 - INFO - train loss: 0.014574527071426558
2023-07-01 00:27:54,211 - INFO - train acc: 0.8125
2023-07-01 00:27:54,215 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.82      0.83      0.82        70
           2       0.00      0.00      0.00         3
           3       0.67      0.60      0.63        65
           4       0.60      0.63      0.62        60

    accuracy                           0.68       200
   macro avg       0.42      0.41      0.41       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:27:54,215 - INFO - test loss 0.024941922044502143
2023-07-01 00:27:54,216 - INFO - test acc 0.675000011920929
2023-07-01 00:27:54,260 - INFO - Distilling data from client: Client18
2023-07-01 00:27:54,260 - INFO - train loss: 0.018939522280508644
2023-07-01 00:27:54,261 - INFO - train acc: 0.84375
2023-07-01 00:27:54,268 - INFO - report:               precision    recall  f1-score   support

           0       0.00      0.00      0.00         2
           1       0.78      0.81      0.80        70
           2       0.00      0.00      0.00         3
           3       0.66      0.72      0.69        65
           4       0.82      0.67      0.73        60

    accuracy                           0.72       200
   macro avg       0.45      0.44      0.44       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:27:54,268 - INFO - test loss 0.024562584928647664
2023-07-01 00:27:54,268 - INFO - test acc 0.7199999690055847
2023-07-01 00:27:54,270 - WARNING - Finished tracing + transforming jit(gather) in 0.00023603439331054688 sec
2023-07-01 00:27:54,270 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[32,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:27:54,272 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0012123584747314453 sec
2023-07-01 00:27:54,272 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:54,282 - WARNING - Finished XLA compilation of jit(gather) in 0.009965658187866211 sec
2023-07-01 00:27:54,293 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:54,302 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:54,311 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:54,319 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:54,328 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:54,337 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:54,346 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:54,355 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:54,365 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:27:54,736 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client18//synthetic.png
2023-07-01 00:27:54,748 - INFO - c: 0.0 and total_data_in_this_class: 273
2023-07-01 00:27:54,748 - INFO - c: 4.0 and total_data_in_this_class: 256
2023-07-01 00:27:54,748 - INFO - c: 9.0 and total_data_in_this_class: 270
2023-07-01 00:27:54,748 - INFO - c: 0.0 and total_data_in_this_class: 60
2023-07-01 00:27:54,748 - INFO - c: 4.0 and total_data_in_this_class: 77
2023-07-01 00:27:54,748 - INFO - c: 9.0 and total_data_in_this_class: 63
2023-07-01 00:27:54,819 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0479588508605957 sec
2023-07-01 00:27:54,866 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.045209646224975586 sec
2023-07-01 00:27:54,871 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.10072660446166992 sec
2023-07-01 00:27:54,873 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:27:54,906 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03288626670837402 sec
2023-07-01 00:27:54,906 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:55,030 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12393021583557129 sec
2023-07-01 00:27:55,052 - INFO - initial test loss: 0.021735871310927992
2023-07-01 00:27:55,052 - INFO - initial test acc: 0.7249999642372131
2023-07-01 00:27:55,060 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.0057830810546875 sec
2023-07-01 00:27:55,171 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1165170669555664 sec
2023-07-01 00:27:55,174 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:27:55,237 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06254696846008301 sec
2023-07-01 00:27:55,237 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:27:55,566 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32907772064208984 sec
2023-07-01 00:27:56,789 - INFO - Distilling data from client: Client19
2023-07-01 00:27:56,789 - INFO - train loss: 0.0020213665855345494
2023-07-01 00:27:56,789 - INFO - train acc: 1.0
2023-07-01 00:27:56,848 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.75      0.77        60
           4       0.77      0.88      0.82        77
           9       0.84      0.73      0.78        63

    accuracy                           0.80       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.80      0.80      0.79       200

2023-07-01 00:27:56,848 - INFO - test loss 0.019784102972365374
2023-07-01 00:27:56,849 - INFO - test acc 0.7949999570846558
2023-07-01 00:27:58,059 - INFO - Distilling data from client: Client19
2023-07-01 00:27:58,059 - INFO - train loss: 0.0009861332413145293
2023-07-01 00:27:58,059 - INFO - train acc: 0.9980506896972656
2023-07-01 00:27:58,083 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.78        60
           4       0.73      0.81      0.77        77
           9       0.78      0.73      0.75        63

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.77       200
weighted avg       0.77      0.77      0.76       200

2023-07-01 00:27:58,083 - INFO - test loss 0.0202191246887704
2023-07-01 00:27:58,083 - INFO - test acc 0.7649999856948853
2023-07-01 00:27:59,306 - INFO - Distilling data from client: Client19
2023-07-01 00:27:59,306 - INFO - train loss: 0.0007887432952135116
2023-07-01 00:27:59,306 - INFO - train acc: 1.0
2023-07-01 00:27:59,330 - INFO - report:               precision    recall  f1-score   support

           0       0.82      0.75      0.78        60
           4       0.77      0.83      0.80        77
           9       0.77      0.76      0.77        63

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.79      0.79      0.78       200

2023-07-01 00:27:59,330 - INFO - test loss 0.019944971770575904
2023-07-01 00:27:59,330 - INFO - test acc 0.7849999666213989
2023-07-01 00:28:00,537 - INFO - Distilling data from client: Client19
2023-07-01 00:28:00,537 - INFO - train loss: 0.000683591718516228
2023-07-01 00:28:00,537 - INFO - train acc: 1.0
2023-07-01 00:28:00,561 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.73      0.76        60
           4       0.74      0.82      0.78        77
           9       0.80      0.75      0.77        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:28:00,561 - INFO - test loss 0.020350987846783398
2023-07-01 00:28:00,561 - INFO - test acc 0.7699999809265137
2023-07-01 00:28:01,782 - INFO - Distilling data from client: Client19
2023-07-01 00:28:01,783 - INFO - train loss: 0.0005488973170432747
2023-07-01 00:28:01,783 - INFO - train acc: 1.0
2023-07-01 00:28:01,805 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.75      0.77        60
           4       0.72      0.82      0.77        77
           9       0.77      0.68      0.72        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:28:01,806 - INFO - test loss 0.020410293102517515
2023-07-01 00:28:01,806 - INFO - test acc 0.7549999952316284
2023-07-01 00:28:03,035 - INFO - Distilling data from client: Client19
2023-07-01 00:28:03,035 - INFO - train loss: 0.0005127396658399222
2023-07-01 00:28:03,035 - INFO - train acc: 1.0
2023-07-01 00:28:03,059 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.73      0.77        60
           4       0.72      0.81      0.76        77
           9       0.76      0.71      0.74        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:28:03,059 - INFO - test loss 0.020988083190979973
2023-07-01 00:28:03,060 - INFO - test acc 0.7549999952316284
2023-07-01 00:28:04,288 - INFO - Distilling data from client: Client19
2023-07-01 00:28:04,288 - INFO - train loss: 0.0005016543757254757
2023-07-01 00:28:04,288 - INFO - train acc: 1.0
2023-07-01 00:28:04,310 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.73      0.76        60
           4       0.72      0.81      0.76        77
           9       0.78      0.71      0.74        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:28:04,310 - INFO - test loss 0.020796461678128125
2023-07-01 00:28:04,310 - INFO - test acc 0.7549999952316284
2023-07-01 00:28:05,527 - INFO - Distilling data from client: Client19
2023-07-01 00:28:05,527 - INFO - train loss: 0.0005188056011144815
2023-07-01 00:28:05,528 - INFO - train acc: 1.0
2023-07-01 00:28:05,552 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.78        60
           4       0.72      0.81      0.76        77
           9       0.78      0.71      0.74        63

    accuracy                           0.76       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:28:05,553 - INFO - test loss 0.020606217708317075
2023-07-01 00:28:05,553 - INFO - test acc 0.7599999904632568
2023-07-01 00:28:06,771 - INFO - Distilling data from client: Client19
2023-07-01 00:28:06,771 - INFO - train loss: 0.0004944005345040947
2023-07-01 00:28:06,771 - INFO - train acc: 1.0
2023-07-01 00:28:06,794 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.72      0.77        60
           4       0.71      0.82      0.76        77
           9       0.76      0.71      0.74        63

    accuracy                           0.76       200
   macro avg       0.77      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:28:06,795 - INFO - test loss 0.02082977195655221
2023-07-01 00:28:06,795 - INFO - test acc 0.7549999952316284
2023-07-01 00:28:08,019 - INFO - Distilling data from client: Client19
2023-07-01 00:28:08,019 - INFO - train loss: 0.00037341455191737007
2023-07-01 00:28:08,019 - INFO - train acc: 1.0
2023-07-01 00:28:08,042 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.73      0.77        60
           4       0.73      0.79      0.76        77
           9       0.77      0.75      0.76        63

    accuracy                           0.76       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:28:08,042 - INFO - test loss 0.020699860508940612
2023-07-01 00:28:08,042 - INFO - test acc 0.7599999904632568
2023-07-01 00:28:09,268 - INFO - Distilling data from client: Client19
2023-07-01 00:28:09,269 - INFO - train loss: 0.00039173972633858037
2023-07-01 00:28:09,269 - INFO - train acc: 1.0
2023-07-01 00:28:09,293 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.72      0.77        60
           4       0.72      0.82      0.76        77
           9       0.77      0.73      0.75        63

    accuracy                           0.76       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:28:09,293 - INFO - test loss 0.020993302839972913
2023-07-01 00:28:09,293 - INFO - test acc 0.7599999904632568
2023-07-01 00:28:10,517 - INFO - Distilling data from client: Client19
2023-07-01 00:28:10,517 - INFO - train loss: 0.00035813005116611536
2023-07-01 00:28:10,517 - INFO - train acc: 1.0
2023-07-01 00:28:10,541 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.72      0.77        60
           4       0.71      0.83      0.77        77
           9       0.76      0.70      0.73        63

    accuracy                           0.76       200
   macro avg       0.77      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:28:10,541 - INFO - test loss 0.02086180503709646
2023-07-01 00:28:10,541 - INFO - test acc 0.7549999952316284
2023-07-01 00:28:11,768 - INFO - Distilling data from client: Client19
2023-07-01 00:28:11,768 - INFO - train loss: 0.00038608481664317114
2023-07-01 00:28:11,768 - INFO - train acc: 1.0
2023-07-01 00:28:11,791 - INFO - report:               precision    recall  f1-score   support

           0       0.82      0.75      0.78        60
           4       0.73      0.83      0.78        77
           9       0.77      0.70      0.73        63

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.77      0.76       200

2023-07-01 00:28:11,792 - INFO - test loss 0.020715854905761246
2023-07-01 00:28:11,792 - INFO - test acc 0.7649999856948853
2023-07-01 00:28:13,012 - INFO - Distilling data from client: Client19
2023-07-01 00:28:13,012 - INFO - train loss: 0.00026499726132047757
2023-07-01 00:28:13,012 - INFO - train acc: 1.0
2023-07-01 00:28:13,036 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.73      0.77        60
           4       0.72      0.82      0.77        77
           9       0.76      0.70      0.73        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:28:13,037 - INFO - test loss 0.02091490535501995
2023-07-01 00:28:13,037 - INFO - test acc 0.7549999952316284
2023-07-01 00:28:14,252 - INFO - Distilling data from client: Client19
2023-07-01 00:28:14,252 - INFO - train loss: 0.00040841100432031756
2023-07-01 00:28:14,252 - INFO - train acc: 1.0
2023-07-01 00:28:14,274 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.70      0.73        60
           4       0.72      0.82      0.77        77
           9       0.76      0.70      0.73        63

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:28:14,275 - INFO - test loss 0.020832202534106684
2023-07-01 00:28:14,275 - INFO - test acc 0.7450000047683716
2023-07-01 00:28:15,484 - INFO - Distilling data from client: Client19
2023-07-01 00:28:15,484 - INFO - train loss: 0.00026661725828056233
2023-07-01 00:28:15,484 - INFO - train acc: 1.0
2023-07-01 00:28:15,507 - INFO - report:               precision    recall  f1-score   support

           0       0.81      0.77      0.79        60
           4       0.76      0.82      0.79        77
           9       0.78      0.75      0.76        63

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:28:15,508 - INFO - test loss 0.020421925822311372
2023-07-01 00:28:15,508 - INFO - test acc 0.7799999713897705
2023-07-01 00:28:16,732 - INFO - Distilling data from client: Client19
2023-07-01 00:28:16,733 - INFO - train loss: 0.00022237322795351185
2023-07-01 00:28:16,733 - INFO - train acc: 1.0
2023-07-01 00:28:16,762 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.78        60
           4       0.73      0.82      0.77        77
           9       0.76      0.70      0.73        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:28:16,762 - INFO - test loss 0.02100200994028212
2023-07-01 00:28:16,763 - INFO - test acc 0.7599999904632568
2023-07-01 00:28:17,986 - INFO - Distilling data from client: Client19
2023-07-01 00:28:17,986 - INFO - train loss: 0.00019069005089038878
2023-07-01 00:28:17,986 - INFO - train acc: 1.0
2023-07-01 00:28:18,009 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.75      0.79        60
           4       0.72      0.83      0.77        77
           9       0.77      0.70      0.73        63

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.77      0.76       200

2023-07-01 00:28:18,009 - INFO - test loss 0.02070063097250981
2023-07-01 00:28:18,009 - INFO - test acc 0.7649999856948853
2023-07-01 00:28:19,230 - INFO - Distilling data from client: Client19
2023-07-01 00:28:19,230 - INFO - train loss: 0.00019365983118537871
2023-07-01 00:28:19,230 - INFO - train acc: 1.0
2023-07-01 00:28:19,253 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.75      0.78        60
           4       0.74      0.83      0.79        77
           9       0.78      0.71      0.74        63

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:28:19,253 - INFO - test loss 0.020625026099975578
2023-07-01 00:28:19,253 - INFO - test acc 0.7699999809265137
2023-07-01 00:28:20,475 - INFO - Distilling data from client: Client19
2023-07-01 00:28:20,475 - INFO - train loss: 0.0002451960943687323
2023-07-01 00:28:20,475 - INFO - train acc: 1.0
2023-07-01 00:28:20,501 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.75      0.76        60
           4       0.73      0.84      0.78        77
           9       0.75      0.63      0.69        63

    accuracy                           0.75       200
   macro avg       0.75      0.74      0.75       200
weighted avg       0.75      0.75      0.75       200

2023-07-01 00:28:20,501 - INFO - test loss 0.021154382185077425
2023-07-01 00:28:20,501 - INFO - test acc 0.75
2023-07-01 00:28:21,728 - INFO - Distilling data from client: Client19
2023-07-01 00:28:21,728 - INFO - train loss: 0.0002411159390840048
2023-07-01 00:28:21,729 - INFO - train acc: 1.0
2023-07-01 00:28:21,756 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.73      0.76        60
           4       0.74      0.82      0.78        77
           9       0.78      0.73      0.75        63

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.77      0.77      0.76       200

2023-07-01 00:28:21,756 - INFO - test loss 0.020510254025014937
2023-07-01 00:28:21,756 - INFO - test acc 0.7649999856948853
2023-07-01 00:28:22,982 - INFO - Distilling data from client: Client19
2023-07-01 00:28:22,983 - INFO - train loss: 0.0002770236760708871
2023-07-01 00:28:22,983 - INFO - train acc: 1.0
2023-07-01 00:28:23,009 - INFO - report:               precision    recall  f1-score   support

           0       0.81      0.72      0.76        60
           4       0.72      0.82      0.76        77
           9       0.76      0.71      0.74        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:28:23,009 - INFO - test loss 0.020983011002363796
2023-07-01 00:28:23,009 - INFO - test acc 0.7549999952316284
2023-07-01 00:28:24,245 - INFO - Distilling data from client: Client19
2023-07-01 00:28:24,245 - INFO - train loss: 0.00023859406671709348
2023-07-01 00:28:24,245 - INFO - train acc: 1.0
2023-07-01 00:28:24,271 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.73      0.75        60
           4       0.74      0.82      0.78        77
           9       0.79      0.71      0.75        63

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:28:24,271 - INFO - test loss 0.02080720898488702
2023-07-01 00:28:24,272 - INFO - test acc 0.7599999904632568
2023-07-01 00:28:25,487 - INFO - Distilling data from client: Client19
2023-07-01 00:28:25,487 - INFO - train loss: 0.00027364137103234984
2023-07-01 00:28:25,488 - INFO - train acc: 1.0
2023-07-01 00:28:25,510 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.72      0.75        60
           4       0.72      0.84      0.78        77
           9       0.77      0.68      0.72        63

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:28:25,510 - INFO - test loss 0.02105582156049063
2023-07-01 00:28:25,510 - INFO - test acc 0.7549999952316284
2023-07-01 00:28:26,732 - INFO - Distilling data from client: Client19
2023-07-01 00:28:26,732 - INFO - train loss: 0.0001957359995201615
2023-07-01 00:28:26,732 - INFO - train acc: 1.0
2023-07-01 00:28:26,755 - INFO - report:               precision    recall  f1-score   support

           0       0.84      0.72      0.77        60
           4       0.70      0.83      0.76        77
           9       0.76      0.70      0.73        63

    accuracy                           0.76       200
   macro avg       0.77      0.75      0.75       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:28:26,755 - INFO - test loss 0.02122070825908621
2023-07-01 00:28:26,755 - INFO - test acc 0.7549999952316284
2023-07-01 00:28:26,770 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:26,778 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:26,787 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:26,796 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:26,805 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:26,814 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:26,823 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:26,833 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:26,841 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:27,213 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client19//synthetic.png
2023-07-01 00:28:27,224 - INFO - c: 0.0 and total_data_in_this_class: 260
2023-07-01 00:28:27,225 - INFO - c: 4.0 and total_data_in_this_class: 539
2023-07-01 00:28:27,225 - INFO - c: 0.0 and total_data_in_this_class: 73
2023-07-01 00:28:27,225 - INFO - c: 4.0 and total_data_in_this_class: 127
2023-07-01 00:28:27,243 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025844573974609375 sec
2023-07-01 00:28:27,244 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:28:27,245 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012133121490478516 sec
2023-07-01 00:28:27,245 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:28:27,256 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010699987411499023 sec
2023-07-01 00:28:27,258 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002605915069580078 sec
2023-07-01 00:28:27,258 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:28:27,259 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009706020355224609 sec
2023-07-01 00:28:27,260 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:28:27,268 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008519411087036133 sec
2023-07-01 00:28:27,272 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00014138221740722656 sec
2023-07-01 00:28:27,273 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001239776611328125 sec
2023-07-01 00:28:27,273 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003082752227783203 sec
2023-07-01 00:28:27,275 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021338462829589844 sec
2023-07-01 00:28:27,275 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012087821960449219 sec
2023-07-01 00:28:27,276 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00029349327087402344 sec
2023-07-01 00:28:27,277 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024771690368652344 sec
2023-07-01 00:28:27,277 - WARNING - Finished tracing + transforming absolute for pjit in 0.00017380714416503906 sec
2023-07-01 00:28:27,278 - WARNING - Finished tracing + transforming fn for pjit in 0.0002791881561279297 sec
2023-07-01 00:28:27,278 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00031948089599609375 sec
2023-07-01 00:28:27,279 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020122528076171875 sec
2023-07-01 00:28:27,280 - WARNING - Finished tracing + transforming fn for pjit in 0.00022840499877929688 sec
2023-07-01 00:28:27,281 - WARNING - Finished tracing + transforming fn for pjit in 0.0002791881561279297 sec
2023-07-01 00:28:27,281 - WARNING - Finished tracing + transforming fn for pjit in 0.00022459030151367188 sec
2023-07-01 00:28:27,282 - WARNING - Finished tracing + transforming fn for pjit in 0.00025200843811035156 sec
2023-07-01 00:28:27,283 - WARNING - Finished tracing + transforming fn for pjit in 0.00021529197692871094 sec
2023-07-01 00:28:27,285 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001678466796875 sec
2023-07-01 00:28:27,286 - WARNING - Finished tracing + transforming fn for pjit in 0.00022530555725097656 sec
2023-07-01 00:28:27,286 - WARNING - Finished tracing + transforming fn for pjit in 0.0002295970916748047 sec
2023-07-01 00:28:27,290 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003561973571777344 sec
2023-07-01 00:28:27,290 - WARNING - Finished tracing + transforming _mean for pjit in 0.000949859619140625 sec
2023-07-01 00:28:27,291 - WARNING - Finished tracing + transforming fn for pjit in 0.00022935867309570312 sec
2023-07-01 00:28:27,292 - WARNING - Finished tracing + transforming fn for pjit in 0.00022029876708984375 sec
2023-07-01 00:28:27,293 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029754638671875 sec
2023-07-01 00:28:27,293 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025391578674316406 sec
2023-07-01 00:28:27,294 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001735687255859375 sec
2023-07-01 00:28:27,295 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002636909484863281 sec
2023-07-01 00:28:27,296 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022411346435546875 sec
2023-07-01 00:28:27,296 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022125244140625 sec
2023-07-01 00:28:27,297 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003600120544433594 sec
2023-07-01 00:28:27,297 - WARNING - Finished tracing + transforming _where for pjit in 0.000957489013671875 sec
2023-07-01 00:28:27,298 - WARNING - Finished tracing + transforming fn for pjit in 0.00025582313537597656 sec
2023-07-01 00:28:27,299 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002617835998535156 sec
2023-07-01 00:28:27,300 - WARNING - Finished tracing + transforming fn for pjit in 0.0002224445343017578 sec
2023-07-01 00:28:27,300 - WARNING - Finished tracing + transforming fn for pjit in 0.00022125244140625 sec
2023-07-01 00:28:27,301 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002219676971435547 sec
2023-07-01 00:28:27,302 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002570152282714844 sec
2023-07-01 00:28:27,302 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002505779266357422 sec
2023-07-01 00:28:27,303 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002608299255371094 sec
2023-07-01 00:28:27,304 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022673606872558594 sec
2023-07-01 00:28:27,304 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002238750457763672 sec
2023-07-01 00:28:27,305 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002617835998535156 sec
2023-07-01 00:28:27,306 - WARNING - Finished tracing + transforming _where for pjit in 0.0008630752563476562 sec
2023-07-01 00:28:27,306 - WARNING - Finished tracing + transforming fn for pjit in 0.0002567768096923828 sec
2023-07-01 00:28:27,307 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002589225769042969 sec
2023-07-01 00:28:27,308 - WARNING - Finished tracing + transforming fn for pjit in 0.00022554397583007812 sec
2023-07-01 00:28:27,312 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002677440643310547 sec
2023-07-01 00:28:27,313 - WARNING - Finished tracing + transforming fn for pjit in 0.0003452301025390625 sec
2023-07-01 00:28:27,314 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002624988555908203 sec
2023-07-01 00:28:27,314 - WARNING - Finished tracing + transforming fn for pjit in 0.00021910667419433594 sec
2023-07-01 00:28:27,318 - WARNING - Finished tracing + transforming fn for pjit in 0.00022482872009277344 sec
2023-07-01 00:28:27,320 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001628398895263672 sec
2023-07-01 00:28:27,320 - WARNING - Finished tracing + transforming fn for pjit in 0.00029587745666503906 sec
2023-07-01 00:28:27,321 - WARNING - Finished tracing + transforming fn for pjit in 0.00022482872009277344 sec
2023-07-01 00:28:27,340 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06841111183166504 sec
2023-07-01 00:28:27,343 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013113021850585938 sec
2023-07-01 00:28:27,343 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011992454528808594 sec
2023-07-01 00:28:27,344 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00026917457580566406 sec
2023-07-01 00:28:27,346 - WARNING - Finished tracing + transforming fn for pjit in 0.0002262592315673828 sec
2023-07-01 00:28:27,347 - WARNING - Finished tracing + transforming fn for pjit in 0.0002601146697998047 sec
2023-07-01 00:28:27,348 - WARNING - Finished tracing + transforming fn for pjit in 0.00022172927856445312 sec
2023-07-01 00:28:27,354 - WARNING - Finished tracing + transforming fn for pjit in 0.0002219676971435547 sec
2023-07-01 00:28:27,355 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002162456512451172 sec
2023-07-01 00:28:27,355 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002617835998535156 sec
2023-07-01 00:28:27,356 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017499923706054688 sec
2023-07-01 00:28:27,357 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032401084899902344 sec
2023-07-01 00:28:27,358 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022935867309570312 sec
2023-07-01 00:28:27,358 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022602081298828125 sec
2023-07-01 00:28:27,359 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00028967857360839844 sec
2023-07-01 00:28:27,359 - WARNING - Finished tracing + transforming _where for pjit in 0.0009064674377441406 sec
2023-07-01 00:28:27,360 - WARNING - Finished tracing + transforming fn for pjit in 0.0002624988555908203 sec
2023-07-01 00:28:27,361 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002646446228027344 sec
2023-07-01 00:28:27,362 - WARNING - Finished tracing + transforming fn for pjit in 0.0002243518829345703 sec
2023-07-01 00:28:27,362 - WARNING - Finished tracing + transforming fn for pjit in 0.0002846717834472656 sec
2023-07-01 00:28:27,374 - WARNING - Finished tracing + transforming fn for pjit in 0.00022363662719726562 sec
2023-07-01 00:28:27,394 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05308985710144043 sec
2023-07-01 00:28:27,395 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001285076141357422 sec
2023-07-01 00:28:27,396 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013589859008789062 sec
2023-07-01 00:28:27,396 - WARNING - Finished tracing + transforming _where for pjit in 0.000640869140625 sec
2023-07-01 00:28:27,397 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003063678741455078 sec
2023-07-01 00:28:27,397 - WARNING - Finished tracing + transforming trace for pjit in 0.0026006698608398438 sec
2023-07-01 00:28:27,399 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010585784912109375 sec
2023-07-01 00:28:27,400 - WARNING - Finished tracing + transforming tril for pjit in 0.0006823539733886719 sec
2023-07-01 00:28:27,401 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.001741647720336914 sec
2023-07-01 00:28:27,402 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010848045349121094 sec
2023-07-01 00:28:27,402 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010609626770019531 sec
2023-07-01 00:28:27,404 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014328956604003906 sec
2023-07-01 00:28:27,408 - WARNING - Finished tracing + transforming _solve for pjit in 0.009351253509521484 sec
2023-07-01 00:28:27,409 - WARNING - Finished tracing + transforming dot for pjit in 0.0003037452697753906 sec
2023-07-01 00:28:27,411 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.14138174057006836 sec
2023-07-01 00:28:27,413 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:28:27,446 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.0327305793762207 sec
2023-07-01 00:28:27,446 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:28:27,570 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1232614517211914 sec
2023-07-01 00:28:27,587 - INFO - initial test loss: 0.017331103488643813
2023-07-01 00:28:27,587 - INFO - initial test acc: 0.8050000071525574
2023-07-01 00:28:27,594 - WARNING - Finished tracing + transforming dot for pjit in 0.0003705024719238281 sec
2023-07-01 00:28:27,595 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029754638671875 sec
2023-07-01 00:28:27,596 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003044605255126953 sec
2023-07-01 00:28:27,596 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009205341339111328 sec
2023-07-01 00:28:27,597 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001983642578125 sec
2023-07-01 00:28:27,598 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00023818016052246094 sec
2023-07-01 00:28:27,599 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002465248107910156 sec
2023-07-01 00:28:27,599 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003743171691894531 sec
2023-07-01 00:28:27,600 - WARNING - Finished tracing + transforming _mean for pjit in 0.001071929931640625 sec
2023-07-01 00:28:27,601 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.01107645034790039 sec
2023-07-01 00:28:27,609 - WARNING - Finished tracing + transforming fn for pjit in 0.0002486705780029297 sec
2023-07-01 00:28:27,610 - WARNING - Finished tracing + transforming fn for pjit in 0.00026607513427734375 sec
2023-07-01 00:28:27,610 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002751350402832031 sec
2023-07-01 00:28:27,611 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002703666687011719 sec
2023-07-01 00:28:27,612 - WARNING - Finished tracing + transforming _where for pjit in 0.0008552074432373047 sec
2023-07-01 00:28:27,620 - WARNING - Finished tracing + transforming fn for pjit in 0.00025463104248046875 sec
2023-07-01 00:28:27,621 - WARNING - Finished tracing + transforming fn for pjit in 0.000263214111328125 sec
2023-07-01 00:28:27,622 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022554397583007812 sec
2023-07-01 00:28:27,622 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025725364685058594 sec
2023-07-01 00:28:27,623 - WARNING - Finished tracing + transforming _where for pjit in 0.0008306503295898438 sec
2023-07-01 00:28:27,657 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002124309539794922 sec
2023-07-01 00:28:27,712 - WARNING - Finished tracing + transforming fn for pjit in 0.0002682209014892578 sec
2023-07-01 00:28:27,713 - WARNING - Finished tracing + transforming fn for pjit in 0.00024008750915527344 sec
2023-07-01 00:28:27,713 - WARNING - Finished tracing + transforming square for pjit in 0.0001697540283203125 sec
2023-07-01 00:28:27,715 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021696090698242188 sec
2023-07-01 00:28:27,717 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002384185791015625 sec
2023-07-01 00:28:27,718 - WARNING - Finished tracing + transforming fn for pjit in 0.0002694129943847656 sec
2023-07-01 00:28:27,718 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022172927856445312 sec
2023-07-01 00:28:27,719 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023865699768066406 sec
2023-07-01 00:28:27,719 - WARNING - Finished tracing + transforming fn for pjit in 0.00027251243591308594 sec
2023-07-01 00:28:27,720 - WARNING - Finished tracing + transforming fn for pjit in 0.00023031234741210938 sec
2023-07-01 00:28:27,721 - WARNING - Finished tracing + transforming square for pjit in 0.00016880035400390625 sec
2023-07-01 00:28:27,723 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002243518829345703 sec
2023-07-01 00:28:27,724 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017023086547851562 sec
2023-07-01 00:28:27,725 - WARNING - Finished tracing + transforming fn for pjit in 0.0002734661102294922 sec
2023-07-01 00:28:27,725 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002205371856689453 sec
2023-07-01 00:28:27,726 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023055076599121094 sec
2023-07-01 00:28:27,727 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13797593116760254 sec
2023-07-01 00:28:27,731 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,10]), ShapedArray(float32[346,10]), ShapedArray(float32[346,10]), ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,10]), ShapedArray(float32[346,3,32,32]), ShapedArray(float32[346,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:28:27,793 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06181168556213379 sec
2023-07-01 00:28:27,793 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:28:28,116 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32294344902038574 sec
2023-07-01 00:28:28,832 - INFO - Distilling data from client: Client20
2023-07-01 00:28:28,833 - INFO - train loss: 0.0028150287925196105
2023-07-01 00:28:28,833 - INFO - train acc: 0.9913294315338135
2023-07-01 00:28:28,872 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.67      0.73        73
           4       0.83      0.91      0.86       127

    accuracy                           0.82       200
   macro avg       0.82      0.79      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-07-01 00:28:28,873 - INFO - test loss 0.014632735507724893
2023-07-01 00:28:28,873 - INFO - test acc 0.8199999928474426
2023-07-01 00:28:29,593 - INFO - Distilling data from client: Client20
2023-07-01 00:28:29,594 - INFO - train loss: 0.0019477142415152478
2023-07-01 00:28:29,594 - INFO - train acc: 0.9942196607589722
2023-07-01 00:28:29,611 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.67      0.70        73
           4       0.82      0.86      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:28:29,611 - INFO - test loss 0.01568820731132532
2023-07-01 00:28:29,611 - INFO - test acc 0.7899999618530273
2023-07-01 00:28:30,342 - INFO - Distilling data from client: Client20
2023-07-01 00:28:30,342 - INFO - train loss: 0.001757128186137744
2023-07-01 00:28:30,342 - INFO - train acc: 0.9971098303794861
2023-07-01 00:28:30,360 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.63      0.68        73
           4       0.80      0.87      0.83       127

    accuracy                           0.78       200
   macro avg       0.77      0.75      0.75       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:28:30,360 - INFO - test loss 0.015798278270793775
2023-07-01 00:28:30,360 - INFO - test acc 0.7799999713897705
2023-07-01 00:28:31,082 - INFO - Distilling data from client: Client20
2023-07-01 00:28:31,082 - INFO - train loss: 0.0014615597246109195
2023-07-01 00:28:31,082 - INFO - train acc: 1.0
2023-07-01 00:28:31,100 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.63      0.70        73
           4       0.81      0.91      0.86       127

    accuracy                           0.81       200
   macro avg       0.80      0.77      0.78       200
weighted avg       0.80      0.81      0.80       200

2023-07-01 00:28:31,100 - INFO - test loss 0.015608014868048966
2023-07-01 00:28:31,100 - INFO - test acc 0.8050000071525574
2023-07-01 00:28:31,817 - INFO - Distilling data from client: Client20
2023-07-01 00:28:31,817 - INFO - train loss: 0.0013942868203465802
2023-07-01 00:28:31,817 - INFO - train acc: 0.9913294315338135
2023-07-01 00:28:31,834 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.66      0.71        73
           4       0.82      0.89      0.85       127

    accuracy                           0.81       200
   macro avg       0.80      0.77      0.78       200
weighted avg       0.80      0.81      0.80       200

2023-07-01 00:28:31,834 - INFO - test loss 0.015018256856478498
2023-07-01 00:28:31,834 - INFO - test acc 0.8050000071525574
2023-07-01 00:28:32,552 - INFO - Distilling data from client: Client20
2023-07-01 00:28:32,552 - INFO - train loss: 0.0011063022479428917
2023-07-01 00:28:32,552 - INFO - train acc: 1.0
2023-07-01 00:28:32,569 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.64      0.70        73
           4       0.81      0.88      0.85       127

    accuracy                           0.80       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-07-01 00:28:32,569 - INFO - test loss 0.01527759190587761
2023-07-01 00:28:32,569 - INFO - test acc 0.7949999570846558
2023-07-01 00:28:33,283 - INFO - Distilling data from client: Client20
2023-07-01 00:28:33,283 - INFO - train loss: 0.0011860509443699467
2023-07-01 00:28:33,283 - INFO - train acc: 1.0
2023-07-01 00:28:33,302 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.67      0.71        73
           4       0.82      0.87      0.84       127

    accuracy                           0.80       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-07-01 00:28:33,302 - INFO - test loss 0.01605238319813973
2023-07-01 00:28:33,302 - INFO - test acc 0.7949999570846558
2023-07-01 00:28:34,020 - INFO - Distilling data from client: Client20
2023-07-01 00:28:34,020 - INFO - train loss: 0.001227529170615073
2023-07-01 00:28:34,020 - INFO - train acc: 0.9971098303794861
2023-07-01 00:28:34,037 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.62      0.68        73
           4       0.80      0.88      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.75      0.76       200
weighted avg       0.78      0.79      0.78       200

2023-07-01 00:28:34,037 - INFO - test loss 0.01580815288703248
2023-07-01 00:28:34,037 - INFO - test acc 0.7849999666213989
2023-07-01 00:28:34,755 - INFO - Distilling data from client: Client20
2023-07-01 00:28:34,755 - INFO - train loss: 0.001167773687030402
2023-07-01 00:28:34,756 - INFO - train acc: 1.0
2023-07-01 00:28:34,772 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.66      0.72        73
           4       0.82      0.91      0.86       127

    accuracy                           0.81       200
   macro avg       0.81      0.78      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:28:34,772 - INFO - test loss 0.015958284900583992
2023-07-01 00:28:34,773 - INFO - test acc 0.8149999976158142
2023-07-01 00:28:35,504 - INFO - Distilling data from client: Client20
2023-07-01 00:28:35,504 - INFO - train loss: 0.0009689227355437483
2023-07-01 00:28:35,504 - INFO - train acc: 1.0
2023-07-01 00:28:35,521 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.60      0.68        73
           4       0.80      0.90      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.75      0.76       200
weighted avg       0.79      0.79      0.78       200

2023-07-01 00:28:35,521 - INFO - test loss 0.016285074713380795
2023-07-01 00:28:35,521 - INFO - test acc 0.7899999618530273
2023-07-01 00:28:36,249 - INFO - Distilling data from client: Client20
2023-07-01 00:28:36,249 - INFO - train loss: 0.0009885484912704147
2023-07-01 00:28:36,250 - INFO - train acc: 1.0
2023-07-01 00:28:36,266 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.64      0.69        73
           4       0.81      0.87      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:28:36,266 - INFO - test loss 0.016319699974066865
2023-07-01 00:28:36,266 - INFO - test acc 0.7899999618530273
2023-07-01 00:28:36,989 - INFO - Distilling data from client: Client20
2023-07-01 00:28:36,989 - INFO - train loss: 0.0009784072275667423
2023-07-01 00:28:36,989 - INFO - train acc: 1.0
2023-07-01 00:28:37,006 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.58      0.66        73
           4       0.79      0.91      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.74      0.75       200
weighted avg       0.78      0.79      0.78       200

2023-07-01 00:28:37,006 - INFO - test loss 0.0162003561446645
2023-07-01 00:28:37,006 - INFO - test acc 0.7849999666213989
2023-07-01 00:28:37,713 - INFO - Distilling data from client: Client20
2023-07-01 00:28:37,713 - INFO - train loss: 0.0008104219727018551
2023-07-01 00:28:37,713 - INFO - train acc: 1.0
2023-07-01 00:28:37,730 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.63      0.69        73
           4       0.81      0.88      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.76      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:28:37,730 - INFO - test loss 0.016212220631942994
2023-07-01 00:28:37,730 - INFO - test acc 0.7899999618530273
2023-07-01 00:28:38,431 - INFO - Distilling data from client: Client20
2023-07-01 00:28:38,431 - INFO - train loss: 0.0009598422923477436
2023-07-01 00:28:38,431 - INFO - train acc: 1.0
2023-07-01 00:28:38,448 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.62      0.69        73
           4       0.80      0.90      0.85       127

    accuracy                           0.80       200
   macro avg       0.79      0.76      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-07-01 00:28:38,448 - INFO - test loss 0.015769055609354363
2023-07-01 00:28:38,448 - INFO - test acc 0.7949999570846558
2023-07-01 00:28:39,162 - INFO - Distilling data from client: Client20
2023-07-01 00:28:39,162 - INFO - train loss: 0.0008515473953462568
2023-07-01 00:28:39,162 - INFO - train acc: 1.0
2023-07-01 00:28:39,179 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.66      0.70        73
           4       0.81      0.87      0.84       127

    accuracy                           0.79       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:28:39,180 - INFO - test loss 0.016450703052146586
2023-07-01 00:28:39,180 - INFO - test acc 0.7899999618530273
2023-07-01 00:28:39,899 - INFO - Distilling data from client: Client20
2023-07-01 00:28:39,899 - INFO - train loss: 0.0007858541004661996
2023-07-01 00:28:39,899 - INFO - train acc: 0.9971098303794861
2023-07-01 00:28:39,917 - INFO - report:               precision    recall  f1-score   support

           0       0.81      0.66      0.73        73
           4       0.82      0.91      0.87       127

    accuracy                           0.82       200
   macro avg       0.82      0.79      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-07-01 00:28:39,917 - INFO - test loss 0.01669661534478739
2023-07-01 00:28:39,917 - INFO - test acc 0.8199999928474426
2023-07-01 00:28:40,635 - INFO - Distilling data from client: Client20
2023-07-01 00:28:40,635 - INFO - train loss: 0.0008276240146864075
2023-07-01 00:28:40,635 - INFO - train acc: 1.0
2023-07-01 00:28:40,652 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.63      0.69        73
           4       0.81      0.89      0.85       127

    accuracy                           0.80       200
   macro avg       0.79      0.76      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-07-01 00:28:40,652 - INFO - test loss 0.016163176013724044
2023-07-01 00:28:40,652 - INFO - test acc 0.7949999570846558
2023-07-01 00:28:41,374 - INFO - Distilling data from client: Client20
2023-07-01 00:28:41,374 - INFO - train loss: 0.0009155204892414241
2023-07-01 00:28:41,374 - INFO - train acc: 1.0
2023-07-01 00:28:41,392 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.62      0.69        73
           4       0.80      0.90      0.85       127

    accuracy                           0.80       200
   macro avg       0.79      0.76      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-07-01 00:28:41,392 - INFO - test loss 0.01608635408005231
2023-07-01 00:28:41,392 - INFO - test acc 0.7949999570846558
2023-07-01 00:28:42,117 - INFO - Distilling data from client: Client20
2023-07-01 00:28:42,117 - INFO - train loss: 0.0009354473431362685
2023-07-01 00:28:42,117 - INFO - train acc: 1.0
2023-07-01 00:28:42,135 - INFO - report:               precision    recall  f1-score   support

           0       0.83      0.62      0.71        73
           4       0.81      0.93      0.86       127

    accuracy                           0.81       200
   macro avg       0.82      0.77      0.79       200
weighted avg       0.82      0.81      0.81       200

2023-07-01 00:28:42,135 - INFO - test loss 0.01606061499946494
2023-07-01 00:28:42,135 - INFO - test acc 0.8149999976158142
2023-07-01 00:28:42,854 - INFO - Distilling data from client: Client20
2023-07-01 00:28:42,854 - INFO - train loss: 0.0007342934634483104
2023-07-01 00:28:42,855 - INFO - train acc: 1.0
2023-07-01 00:28:42,873 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.60      0.68        73
           4       0.80      0.91      0.85       127

    accuracy                           0.80       200
   macro avg       0.79      0.75      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-07-01 00:28:42,873 - INFO - test loss 0.015583204397131045
2023-07-01 00:28:42,873 - INFO - test acc 0.7949999570846558
2023-07-01 00:28:43,594 - INFO - Distilling data from client: Client20
2023-07-01 00:28:43,595 - INFO - train loss: 0.0007509306118438138
2023-07-01 00:28:43,595 - INFO - train acc: 1.0
2023-07-01 00:28:43,613 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.64      0.71        73
           4       0.82      0.91      0.86       127

    accuracy                           0.81       200
   macro avg       0.81      0.77      0.79       200
weighted avg       0.81      0.81      0.80       200

2023-07-01 00:28:43,614 - INFO - test loss 0.016203047859224832
2023-07-01 00:28:43,614 - INFO - test acc 0.8100000023841858
2023-07-01 00:28:44,331 - INFO - Distilling data from client: Client20
2023-07-01 00:28:44,331 - INFO - train loss: 0.0007841716539055491
2023-07-01 00:28:44,331 - INFO - train acc: 1.0
2023-07-01 00:28:44,348 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.63      0.68        73
           4       0.80      0.87      0.84       127

    accuracy                           0.79       200
   macro avg       0.77      0.75      0.76       200
weighted avg       0.78      0.79      0.78       200

2023-07-01 00:28:44,348 - INFO - test loss 0.01561379923913114
2023-07-01 00:28:44,348 - INFO - test acc 0.7849999666213989
2023-07-01 00:28:45,075 - INFO - Distilling data from client: Client20
2023-07-01 00:28:45,076 - INFO - train loss: 0.0008563746805061651
2023-07-01 00:28:45,076 - INFO - train acc: 1.0
2023-07-01 00:28:45,094 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.62      0.69        73
           4       0.80      0.90      0.85       127

    accuracy                           0.80       200
   macro avg       0.79      0.76      0.77       200
weighted avg       0.79      0.80      0.79       200

2023-07-01 00:28:45,094 - INFO - test loss 0.016464387296287658
2023-07-01 00:28:45,094 - INFO - test acc 0.7949999570846558
2023-07-01 00:28:45,815 - INFO - Distilling data from client: Client20
2023-07-01 00:28:45,815 - INFO - train loss: 0.0006478135928168271
2023-07-01 00:28:45,815 - INFO - train acc: 1.0
2023-07-01 00:28:45,832 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.58      0.65        73
           4       0.78      0.88      0.83       127

    accuracy                           0.77       200
   macro avg       0.76      0.73      0.74       200
weighted avg       0.77      0.77      0.76       200

2023-07-01 00:28:45,832 - INFO - test loss 0.01653225779420238
2023-07-01 00:28:45,832 - INFO - test acc 0.7699999809265137
2023-07-01 00:28:46,552 - INFO - Distilling data from client: Client20
2023-07-01 00:28:46,552 - INFO - train loss: 0.0008422485030293185
2023-07-01 00:28:46,552 - INFO - train acc: 1.0
2023-07-01 00:28:46,569 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.53      0.63        73
           4       0.77      0.91      0.83       127

    accuracy                           0.77       200
   macro avg       0.77      0.72      0.73       200
weighted avg       0.77      0.77      0.76       200

2023-07-01 00:28:46,569 - INFO - test loss 0.016786920757251637
2023-07-01 00:28:46,569 - INFO - test acc 0.7699999809265137
2023-07-01 00:28:46,571 - WARNING - Finished tracing + transforming jit(gather) in 0.00023508071899414062 sec
2023-07-01 00:28:46,572 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[346,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:28:46,573 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0013663768768310547 sec
2023-07-01 00:28:46,573 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:28:46,584 - WARNING - Finished XLA compilation of jit(gather) in 0.010418415069580078 sec
2023-07-01 00:28:46,595 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:46,604 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:46,613 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:46,622 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:46,636 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:46,645 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:28:46,928 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client20//synthetic.png
2023-07-01 00:28:46,941 - INFO - c: 0.0 and total_data_in_this_class: 271
2023-07-01 00:28:46,941 - INFO - c: 6.0 and total_data_in_this_class: 259
2023-07-01 00:28:46,941 - INFO - c: 7.0 and total_data_in_this_class: 269
2023-07-01 00:28:46,941 - INFO - c: 0.0 and total_data_in_this_class: 62
2023-07-01 00:28:46,941 - INFO - c: 6.0 and total_data_in_this_class: 74
2023-07-01 00:28:46,941 - INFO - c: 7.0 and total_data_in_this_class: 64
2023-07-01 00:28:47,012 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04774141311645508 sec
2023-07-01 00:28:47,058 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.044771671295166016 sec
2023-07-01 00:28:47,063 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.09963583946228027 sec
2023-07-01 00:28:47,065 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:28:47,098 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.033449411392211914 sec
2023-07-01 00:28:47,099 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:28:47,224 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1252286434173584 sec
2023-07-01 00:28:47,246 - INFO - initial test loss: 0.019305157060978975
2023-07-01 00:28:47,246 - INFO - initial test acc: 0.7949999570846558
2023-07-01 00:28:47,254 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.006025552749633789 sec
2023-07-01 00:28:47,363 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11495351791381836 sec
2023-07-01 00:28:47,366 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:28:47,429 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06270599365234375 sec
2023-07-01 00:28:47,429 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:28:47,762 - WARNING - Finished XLA compilation of jit(update_fn) in 0.33206868171691895 sec
2023-07-01 00:28:49,009 - INFO - Distilling data from client: Client21
2023-07-01 00:28:49,009 - INFO - train loss: 0.0018944306018071726
2023-07-01 00:28:49,009 - INFO - train acc: 0.9961464405059814
2023-07-01 00:28:49,070 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.82      0.86        62
           6       0.79      0.86      0.83        74
           7       0.75      0.73      0.74        64

    accuracy                           0.81       200
   macro avg       0.82      0.81      0.81       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:28:49,070 - INFO - test loss 0.018046201192917517
2023-07-01 00:28:49,071 - INFO - test acc 0.8100000023841858
2023-07-01 00:28:50,304 - INFO - Distilling data from client: Client21
2023-07-01 00:28:50,304 - INFO - train loss: 0.0009076072865770347
2023-07-01 00:28:50,304 - INFO - train acc: 1.0
2023-07-01 00:28:50,328 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.82      0.86        62
           6       0.77      0.85      0.81        74
           7       0.74      0.72      0.73        64

    accuracy                           0.80       200
   macro avg       0.81      0.80      0.80       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:28:50,329 - INFO - test loss 0.01825387050579177
2023-07-01 00:28:50,329 - INFO - test acc 0.7999999523162842
2023-07-01 00:28:51,570 - INFO - Distilling data from client: Client21
2023-07-01 00:28:51,571 - INFO - train loss: 0.0006474536398157051
2023-07-01 00:28:51,571 - INFO - train acc: 1.0
2023-07-01 00:28:51,594 - INFO - report:               precision    recall  f1-score   support

           0       0.89      0.81      0.85        62
           6       0.76      0.85      0.80        74
           7       0.70      0.67      0.69        64

    accuracy                           0.78       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:28:51,595 - INFO - test loss 0.01827577560050562
2023-07-01 00:28:51,595 - INFO - test acc 0.7799999713897705
2023-07-01 00:28:52,841 - INFO - Distilling data from client: Client21
2023-07-01 00:28:52,841 - INFO - train loss: 0.0006253505677458971
2023-07-01 00:28:52,841 - INFO - train acc: 1.0
2023-07-01 00:28:52,866 - INFO - report:               precision    recall  f1-score   support

           0       0.86      0.82      0.84        62
           6       0.75      0.81      0.78        74
           7       0.70      0.67      0.69        64

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:28:52,866 - INFO - test loss 0.019079131799294814
2023-07-01 00:28:52,866 - INFO - test acc 0.7699999809265137
2023-07-01 00:28:54,092 - INFO - Distilling data from client: Client21
2023-07-01 00:28:54,092 - INFO - train loss: 0.0005866023955650704
2023-07-01 00:28:54,092 - INFO - train acc: 1.0
2023-07-01 00:28:54,116 - INFO - report:               precision    recall  f1-score   support

           0       0.86      0.81      0.83        62
           6       0.77      0.82      0.80        74
           7       0.70      0.69      0.69        64

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:28:54,116 - INFO - test loss 0.01907384898664188
2023-07-01 00:28:54,117 - INFO - test acc 0.7749999761581421
2023-07-01 00:28:55,348 - INFO - Distilling data from client: Client21
2023-07-01 00:28:55,348 - INFO - train loss: 0.0004762472278687006
2023-07-01 00:28:55,348 - INFO - train acc: 1.0
2023-07-01 00:28:55,372 - INFO - report:               precision    recall  f1-score   support

           0       0.86      0.81      0.83        62
           6       0.82      0.84      0.83        74
           7       0.70      0.72      0.71        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:28:55,372 - INFO - test loss 0.019029731455615866
2023-07-01 00:28:55,372 - INFO - test acc 0.7899999618530273
2023-07-01 00:28:56,603 - INFO - Distilling data from client: Client21
2023-07-01 00:28:56,603 - INFO - train loss: 0.0004142887910421597
2023-07-01 00:28:56,603 - INFO - train acc: 1.0
2023-07-01 00:28:56,627 - INFO - report:               precision    recall  f1-score   support

           0       0.86      0.82      0.84        62
           6       0.80      0.82      0.81        74
           7       0.71      0.72      0.71        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:28:56,627 - INFO - test loss 0.018636751451970888
2023-07-01 00:28:56,627 - INFO - test acc 0.7899999618530273
2023-07-01 00:28:57,869 - INFO - Distilling data from client: Client21
2023-07-01 00:28:57,869 - INFO - train loss: 0.0003771315156274674
2023-07-01 00:28:57,869 - INFO - train acc: 1.0
2023-07-01 00:28:57,891 - INFO - report:               precision    recall  f1-score   support

           0       0.85      0.82      0.84        62
           6       0.79      0.84      0.82        74
           7       0.73      0.70      0.71        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:28:57,892 - INFO - test loss 0.01864819673797451
2023-07-01 00:28:57,892 - INFO - test acc 0.7899999618530273
2023-07-01 00:28:59,133 - INFO - Distilling data from client: Client21
2023-07-01 00:28:59,133 - INFO - train loss: 0.0003272157994330472
2023-07-01 00:28:59,133 - INFO - train acc: 1.0
2023-07-01 00:28:59,156 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.85      0.87        62
           6       0.79      0.84      0.82        74
           7       0.73      0.70      0.71        64

    accuracy                           0.80       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:28:59,156 - INFO - test loss 0.01856333084614717
2023-07-01 00:28:59,156 - INFO - test acc 0.7999999523162842
2023-07-01 00:29:00,411 - INFO - Distilling data from client: Client21
2023-07-01 00:29:00,411 - INFO - train loss: 0.00027980699353480625
2023-07-01 00:29:00,411 - INFO - train acc: 1.0
2023-07-01 00:29:00,435 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.84      0.86        62
           6       0.78      0.84      0.81        74
           7       0.73      0.70      0.71        64

    accuracy                           0.80       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.80      0.80      0.79       200

2023-07-01 00:29:00,436 - INFO - test loss 0.01831042672094625
2023-07-01 00:29:00,436 - INFO - test acc 0.7949999570846558
2023-07-01 00:29:01,665 - INFO - Distilling data from client: Client21
2023-07-01 00:29:01,665 - INFO - train loss: 0.00029652822140365334
2023-07-01 00:29:01,665 - INFO - train acc: 1.0
2023-07-01 00:29:01,691 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.81      0.84        62
           6       0.78      0.82      0.80        74
           7       0.72      0.73      0.73        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:29:01,691 - INFO - test loss 0.018751826295327904
2023-07-01 00:29:01,691 - INFO - test acc 0.7899999618530273
2023-07-01 00:29:02,929 - INFO - Distilling data from client: Client21
2023-07-01 00:29:02,930 - INFO - train loss: 0.00023437239543444083
2023-07-01 00:29:02,930 - INFO - train acc: 1.0
2023-07-01 00:29:02,953 - INFO - report:               precision    recall  f1-score   support

           0       0.89      0.82      0.86        62
           6       0.77      0.81      0.79        74
           7       0.72      0.73      0.73        64

    accuracy                           0.79       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:29:02,953 - INFO - test loss 0.01863269040761066
2023-07-01 00:29:02,954 - INFO - test acc 0.7899999618530273
2023-07-01 00:29:04,192 - INFO - Distilling data from client: Client21
2023-07-01 00:29:04,192 - INFO - train loss: 0.0002437484373297142
2023-07-01 00:29:04,193 - INFO - train acc: 1.0
2023-07-01 00:29:04,216 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.84      0.86        62
           6       0.77      0.82      0.80        74
           7       0.73      0.70      0.71        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:29:04,216 - INFO - test loss 0.018602751225722913
2023-07-01 00:29:04,216 - INFO - test acc 0.7899999618530273
2023-07-01 00:29:05,453 - INFO - Distilling data from client: Client21
2023-07-01 00:29:05,453 - INFO - train loss: 0.0002165242289350072
2023-07-01 00:29:05,453 - INFO - train acc: 1.0
2023-07-01 00:29:05,477 - INFO - report:               precision    recall  f1-score   support

           0       0.89      0.82      0.86        62
           6       0.77      0.85      0.81        74
           7       0.70      0.67      0.69        64

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.79      0.79      0.78       200

2023-07-01 00:29:05,477 - INFO - test loss 0.018403407077673406
2023-07-01 00:29:05,478 - INFO - test acc 0.7849999666213989
2023-07-01 00:29:06,723 - INFO - Distilling data from client: Client21
2023-07-01 00:29:06,723 - INFO - train loss: 0.00020474461209169236
2023-07-01 00:29:06,723 - INFO - train acc: 1.0
2023-07-01 00:29:06,747 - INFO - report:               precision    recall  f1-score   support

           0       0.90      0.85      0.88        62
           6       0.79      0.88      0.83        74
           7       0.75      0.69      0.72        64

    accuracy                           0.81       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:29:06,747 - INFO - test loss 0.018745065989587723
2023-07-01 00:29:06,747 - INFO - test acc 0.8100000023841858
2023-07-01 00:29:07,988 - INFO - Distilling data from client: Client21
2023-07-01 00:29:07,988 - INFO - train loss: 0.00019083158640275315
2023-07-01 00:29:07,988 - INFO - train acc: 1.0
2023-07-01 00:29:08,012 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.81      0.84        62
           6       0.77      0.82      0.80        74
           7       0.69      0.69      0.69        64

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:29:08,012 - INFO - test loss 0.01867293469870859
2023-07-01 00:29:08,012 - INFO - test acc 0.7749999761581421
2023-07-01 00:29:09,258 - INFO - Distilling data from client: Client21
2023-07-01 00:29:09,258 - INFO - train loss: 0.00018035943197231252
2023-07-01 00:29:09,258 - INFO - train acc: 1.0
2023-07-01 00:29:09,281 - INFO - report:               precision    recall  f1-score   support

           0       0.84      0.84      0.84        62
           6       0.81      0.84      0.82        74
           7       0.72      0.69      0.70        64

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:29:09,281 - INFO - test loss 0.01841697935854539
2023-07-01 00:29:09,281 - INFO - test acc 0.7899999618530273
2023-07-01 00:29:10,523 - INFO - Distilling data from client: Client21
2023-07-01 00:29:10,523 - INFO - train loss: 0.00020058963698381407
2023-07-01 00:29:10,523 - INFO - train acc: 1.0
2023-07-01 00:29:10,547 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.82      0.86        62
           6       0.77      0.85      0.81        74
           7       0.71      0.69      0.70        64

    accuracy                           0.79       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:29:10,547 - INFO - test loss 0.01843631654393284
2023-07-01 00:29:10,548 - INFO - test acc 0.7899999618530273
2023-07-01 00:29:11,787 - INFO - Distilling data from client: Client21
2023-07-01 00:29:11,787 - INFO - train loss: 0.00016289948324025067
2023-07-01 00:29:11,787 - INFO - train acc: 1.0
2023-07-01 00:29:11,810 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.82      0.86        62
           6       0.78      0.84      0.81        74
           7       0.72      0.72      0.72        64

    accuracy                           0.80       200
   macro avg       0.80      0.79      0.80       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:29:11,811 - INFO - test loss 0.018689335223735025
2023-07-01 00:29:11,811 - INFO - test acc 0.7949999570846558
2023-07-01 00:29:13,052 - INFO - Distilling data from client: Client21
2023-07-01 00:29:13,052 - INFO - train loss: 0.0001812822532025639
2023-07-01 00:29:13,052 - INFO - train acc: 1.0
2023-07-01 00:29:13,075 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.82      0.86        62
           6       0.78      0.82      0.80        74
           7       0.70      0.72      0.71        64

    accuracy                           0.79       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:29:13,076 - INFO - test loss 0.018135202424283424
2023-07-01 00:29:13,076 - INFO - test acc 0.7899999618530273
2023-07-01 00:29:14,313 - INFO - Distilling data from client: Client21
2023-07-01 00:29:14,313 - INFO - train loss: 0.00017141379623526366
2023-07-01 00:29:14,313 - INFO - train acc: 1.0
2023-07-01 00:29:14,381 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.84      0.87        62
           6       0.79      0.85      0.82        74
           7       0.76      0.75      0.76        64

    accuracy                           0.81       200
   macro avg       0.82      0.81      0.82       200
weighted avg       0.82      0.81      0.82       200

2023-07-01 00:29:14,381 - INFO - test loss 0.018566465912789154
2023-07-01 00:29:14,381 - INFO - test acc 0.8149999976158142
2023-07-01 00:29:15,629 - INFO - Distilling data from client: Client21
2023-07-01 00:29:15,629 - INFO - train loss: 0.00017522278949025683
2023-07-01 00:29:15,629 - INFO - train acc: 1.0
2023-07-01 00:29:15,653 - INFO - report:               precision    recall  f1-score   support

           0       0.89      0.81      0.85        62
           6       0.80      0.85      0.82        74
           7       0.71      0.72      0.71        64

    accuracy                           0.80       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:29:15,653 - INFO - test loss 0.018882978166088932
2023-07-01 00:29:15,653 - INFO - test acc 0.7949999570846558
2023-07-01 00:29:16,888 - INFO - Distilling data from client: Client21
2023-07-01 00:29:16,888 - INFO - train loss: 0.00016174684129844144
2023-07-01 00:29:16,888 - INFO - train acc: 1.0
2023-07-01 00:29:16,912 - INFO - report:               precision    recall  f1-score   support

           0       0.91      0.84      0.87        62
           6       0.77      0.85      0.81        74
           7       0.72      0.69      0.70        64

    accuracy                           0.80       200
   macro avg       0.80      0.79      0.80       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:29:16,912 - INFO - test loss 0.01859022631262554
2023-07-01 00:29:16,912 - INFO - test acc 0.7949999570846558
2023-07-01 00:29:18,156 - INFO - Distilling data from client: Client21
2023-07-01 00:29:18,156 - INFO - train loss: 0.00017624895265077814
2023-07-01 00:29:18,156 - INFO - train acc: 1.0
2023-07-01 00:29:18,179 - INFO - report:               precision    recall  f1-score   support

           0       0.90      0.85      0.88        62
           6       0.79      0.85      0.82        74
           7       0.72      0.69      0.70        64

    accuracy                           0.80       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:29:18,179 - INFO - test loss 0.01865385779934481
2023-07-01 00:29:18,179 - INFO - test acc 0.7999999523162842
2023-07-01 00:29:19,419 - INFO - Distilling data from client: Client21
2023-07-01 00:29:19,419 - INFO - train loss: 0.00014696496051488562
2023-07-01 00:29:19,419 - INFO - train acc: 1.0
2023-07-01 00:29:19,443 - INFO - report:               precision    recall  f1-score   support

           0       0.88      0.84      0.86        62
           6       0.78      0.80      0.79        74
           7       0.69      0.70      0.70        64

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:29:19,443 - INFO - test loss 0.01902415740331518
2023-07-01 00:29:19,443 - INFO - test acc 0.7799999713897705
2023-07-01 00:29:19,454 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:19,463 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:19,472 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:19,480 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:19,489 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:19,497 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:19,506 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:19,515 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:19,524 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:19,899 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client21//synthetic.png
2023-07-01 00:29:19,912 - INFO - c: 9.0 and total_data_in_this_class: 799
2023-07-01 00:29:19,912 - INFO - c: 9.0 and total_data_in_this_class: 200
2023-07-01 00:29:19,932 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025653839111328125 sec
2023-07-01 00:29:19,933 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:29:19,934 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001220703125 sec
2023-07-01 00:29:19,934 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:19,945 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010643959045410156 sec
2023-07-01 00:29:19,947 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002536773681640625 sec
2023-07-01 00:29:19,947 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:29:19,948 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0010080337524414062 sec
2023-07-01 00:29:19,949 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:19,957 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008471012115478516 sec
2023-07-01 00:29:19,961 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013518333435058594 sec
2023-07-01 00:29:19,962 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012373924255371094 sec
2023-07-01 00:29:19,962 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002980232238769531 sec
2023-07-01 00:29:19,964 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020694732666015625 sec
2023-07-01 00:29:19,964 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011849403381347656 sec
2023-07-01 00:29:19,965 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002765655517578125 sec
2023-07-01 00:29:19,965 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024509429931640625 sec
2023-07-01 00:29:19,966 - WARNING - Finished tracing + transforming absolute for pjit in 0.0001628398895263672 sec
2023-07-01 00:29:19,966 - WARNING - Finished tracing + transforming fn for pjit in 0.00026917457580566406 sec
2023-07-01 00:29:19,967 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0003123283386230469 sec
2023-07-01 00:29:19,968 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000194549560546875 sec
2023-07-01 00:29:19,969 - WARNING - Finished tracing + transforming fn for pjit in 0.00022864341735839844 sec
2023-07-01 00:29:19,969 - WARNING - Finished tracing + transforming fn for pjit in 0.00026154518127441406 sec
2023-07-01 00:29:19,970 - WARNING - Finished tracing + transforming fn for pjit in 0.00021982192993164062 sec
2023-07-01 00:29:19,971 - WARNING - Finished tracing + transforming fn for pjit in 0.00025844573974609375 sec
2023-07-01 00:29:19,972 - WARNING - Finished tracing + transforming fn for pjit in 0.00022220611572265625 sec
2023-07-01 00:29:19,974 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001666545867919922 sec
2023-07-01 00:29:19,975 - WARNING - Finished tracing + transforming fn for pjit in 0.00023436546325683594 sec
2023-07-01 00:29:19,976 - WARNING - Finished tracing + transforming fn for pjit in 0.00023436546325683594 sec
2023-07-01 00:29:19,979 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003561973571777344 sec
2023-07-01 00:29:19,979 - WARNING - Finished tracing + transforming _mean for pjit in 0.000949859619140625 sec
2023-07-01 00:29:19,980 - WARNING - Finished tracing + transforming fn for pjit in 0.00022864341735839844 sec
2023-07-01 00:29:19,981 - WARNING - Finished tracing + transforming fn for pjit in 0.0002262592315673828 sec
2023-07-01 00:29:19,982 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022220611572265625 sec
2023-07-01 00:29:19,983 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00033164024353027344 sec
2023-07-01 00:29:19,983 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017571449279785156 sec
2023-07-01 00:29:19,984 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002586841583251953 sec
2023-07-01 00:29:19,985 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023436546325683594 sec
2023-07-01 00:29:19,985 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022459030151367188 sec
2023-07-01 00:29:19,986 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00026679039001464844 sec
2023-07-01 00:29:19,986 - WARNING - Finished tracing + transforming _where for pjit in 0.0009207725524902344 sec
2023-07-01 00:29:19,987 - WARNING - Finished tracing + transforming fn for pjit in 0.0002562999725341797 sec
2023-07-01 00:29:19,988 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025534629821777344 sec
2023-07-01 00:29:19,989 - WARNING - Finished tracing + transforming fn for pjit in 0.0002193450927734375 sec
2023-07-01 00:29:19,989 - WARNING - Finished tracing + transforming fn for pjit in 0.0002167224884033203 sec
2023-07-01 00:29:19,990 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000213623046875 sec
2023-07-01 00:29:19,991 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002560615539550781 sec
2023-07-01 00:29:19,991 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023937225341796875 sec
2023-07-01 00:29:19,992 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002582073211669922 sec
2023-07-01 00:29:19,993 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002276897430419922 sec
2023-07-01 00:29:19,993 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002269744873046875 sec
2023-07-01 00:29:19,994 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002570152282714844 sec
2023-07-01 00:29:19,994 - WARNING - Finished tracing + transforming _where for pjit in 0.0008313655853271484 sec
2023-07-01 00:29:19,995 - WARNING - Finished tracing + transforming fn for pjit in 0.0002570152282714844 sec
2023-07-01 00:29:19,996 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002548694610595703 sec
2023-07-01 00:29:19,997 - WARNING - Finished tracing + transforming fn for pjit in 0.0002288818359375 sec
2023-07-01 00:29:20,001 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002803802490234375 sec
2023-07-01 00:29:20,002 - WARNING - Finished tracing + transforming fn for pjit in 0.0003437995910644531 sec
2023-07-01 00:29:20,002 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002655982971191406 sec
2023-07-01 00:29:20,003 - WARNING - Finished tracing + transforming fn for pjit in 0.00022268295288085938 sec
2023-07-01 00:29:20,007 - WARNING - Finished tracing + transforming fn for pjit in 0.00021839141845703125 sec
2023-07-01 00:29:20,008 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016117095947265625 sec
2023-07-01 00:29:20,009 - WARNING - Finished tracing + transforming fn for pjit in 0.0002999305725097656 sec
2023-07-01 00:29:20,010 - WARNING - Finished tracing + transforming fn for pjit in 0.000225067138671875 sec
2023-07-01 00:29:20,027 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06738495826721191 sec
2023-07-01 00:29:20,031 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000125885009765625 sec
2023-07-01 00:29:20,031 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011205673217773438 sec
2023-07-01 00:29:20,032 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002627372741699219 sec
2023-07-01 00:29:20,034 - WARNING - Finished tracing + transforming fn for pjit in 0.00021886825561523438 sec
2023-07-01 00:29:20,034 - WARNING - Finished tracing + transforming fn for pjit in 0.00025081634521484375 sec
2023-07-01 00:29:20,036 - WARNING - Finished tracing + transforming fn for pjit in 0.00021791458129882812 sec
2023-07-01 00:29:20,042 - WARNING - Finished tracing + transforming fn for pjit in 0.00022649765014648438 sec
2023-07-01 00:29:20,043 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021600723266601562 sec
2023-07-01 00:29:20,043 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025081634521484375 sec
2023-07-01 00:29:20,044 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017547607421875 sec
2023-07-01 00:29:20,045 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003190040588378906 sec
2023-07-01 00:29:20,046 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002219676971435547 sec
2023-07-01 00:29:20,046 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021886825561523438 sec
2023-07-01 00:29:20,047 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00026297569274902344 sec
2023-07-01 00:29:20,047 - WARNING - Finished tracing + transforming _where for pjit in 0.0008406639099121094 sec
2023-07-01 00:29:20,048 - WARNING - Finished tracing + transforming fn for pjit in 0.0002543926239013672 sec
2023-07-01 00:29:20,048 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002505779266357422 sec
2023-07-01 00:29:20,049 - WARNING - Finished tracing + transforming fn for pjit in 0.0002167224884033203 sec
2023-07-01 00:29:20,050 - WARNING - Finished tracing + transforming fn for pjit in 0.0002791881561279297 sec
2023-07-01 00:29:20,062 - WARNING - Finished tracing + transforming fn for pjit in 0.00022172927856445312 sec
2023-07-01 00:29:20,081 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0529177188873291 sec
2023-07-01 00:29:20,083 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001227855682373047 sec
2023-07-01 00:29:20,084 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013375282287597656 sec
2023-07-01 00:29:20,084 - WARNING - Finished tracing + transforming _where for pjit in 0.0006182193756103516 sec
2023-07-01 00:29:20,084 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00029206275939941406 sec
2023-07-01 00:29:20,085 - WARNING - Finished tracing + transforming trace for pjit in 0.0025136470794677734 sec
2023-07-01 00:29:20,087 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010204315185546875 sec
2023-07-01 00:29:20,088 - WARNING - Finished tracing + transforming tril for pjit in 0.0006687641143798828 sec
2023-07-01 00:29:20,088 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0016818046569824219 sec
2023-07-01 00:29:20,089 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010633468627929688 sec
2023-07-01 00:29:20,090 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010395050048828125 sec
2023-07-01 00:29:20,092 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0013933181762695312 sec
2023-07-01 00:29:20,095 - WARNING - Finished tracing + transforming _solve for pjit in 0.009099245071411133 sec
2023-07-01 00:29:20,096 - WARNING - Finished tracing + transforming dot for pjit in 0.00031876564025878906 sec
2023-07-01 00:29:20,099 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.1397385597229004 sec
2023-07-01 00:29:20,101 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:29:20,134 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03321647644042969 sec
2023-07-01 00:29:20,134 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:20,258 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12409281730651855 sec
2023-07-01 00:29:20,282 - INFO - initial test loss: 0.0008100661618863896
2023-07-01 00:29:20,282 - INFO - initial test acc: 1.0
2023-07-01 00:29:20,651 - WARNING - Finished tracing + transforming dot for pjit in 0.0004189014434814453 sec
2023-07-01 00:29:20,652 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002913475036621094 sec
2023-07-01 00:29:20,653 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003705024719238281 sec
2023-07-01 00:29:20,653 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009837150573730469 sec
2023-07-01 00:29:20,654 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00018858909606933594 sec
2023-07-01 00:29:20,655 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001709461212158203 sec
2023-07-01 00:29:20,655 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023818016052246094 sec
2023-07-01 00:29:20,656 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00035500526428222656 sec
2023-07-01 00:29:20,657 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010304450988769531 sec
2023-07-01 00:29:20,657 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.3735010623931885 sec
2023-07-01 00:29:20,666 - WARNING - Finished tracing + transforming fn for pjit in 0.00024318695068359375 sec
2023-07-01 00:29:20,666 - WARNING - Finished tracing + transforming fn for pjit in 0.0002541542053222656 sec
2023-07-01 00:29:20,667 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00020575523376464844 sec
2023-07-01 00:29:20,668 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002465248107910156 sec
2023-07-01 00:29:20,668 - WARNING - Finished tracing + transforming _where for pjit in 0.0008578300476074219 sec
2023-07-01 00:29:20,676 - WARNING - Finished tracing + transforming fn for pjit in 0.00030231475830078125 sec
2023-07-01 00:29:20,676 - WARNING - Finished tracing + transforming fn for pjit in 0.0002465248107910156 sec
2023-07-01 00:29:20,677 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021219253540039062 sec
2023-07-01 00:29:20,678 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00023365020751953125 sec
2023-07-01 00:29:20,678 - WARNING - Finished tracing + transforming _where for pjit in 0.0007877349853515625 sec
2023-07-01 00:29:20,711 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002086162567138672 sec
2023-07-01 00:29:20,765 - WARNING - Finished tracing + transforming fn for pjit in 0.0002663135528564453 sec
2023-07-01 00:29:20,766 - WARNING - Finished tracing + transforming fn for pjit in 0.00022840499877929688 sec
2023-07-01 00:29:20,767 - WARNING - Finished tracing + transforming square for pjit in 0.000164031982421875 sec
2023-07-01 00:29:20,769 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021314620971679688 sec
2023-07-01 00:29:20,770 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002384185791015625 sec
2023-07-01 00:29:20,771 - WARNING - Finished tracing + transforming fn for pjit in 0.00025391578674316406 sec
2023-07-01 00:29:20,771 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002148151397705078 sec
2023-07-01 00:29:20,772 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002243518829345703 sec
2023-07-01 00:29:20,772 - WARNING - Finished tracing + transforming fn for pjit in 0.0002560615539550781 sec
2023-07-01 00:29:20,773 - WARNING - Finished tracing + transforming fn for pjit in 0.00022840499877929688 sec
2023-07-01 00:29:20,774 - WARNING - Finished tracing + transforming square for pjit in 0.0001614093780517578 sec
2023-07-01 00:29:20,776 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021314620971679688 sec
2023-07-01 00:29:20,777 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001704692840576172 sec
2023-07-01 00:29:20,778 - WARNING - Finished tracing + transforming fn for pjit in 0.00025916099548339844 sec
2023-07-01 00:29:20,778 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021219253540039062 sec
2023-07-01 00:29:20,779 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022149085998535156 sec
2023-07-01 00:29:20,780 - WARNING - Finished tracing + transforming update_fn for pjit in 0.4961974620819092 sec
2023-07-01 00:29:20,783 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,10]), ShapedArray(float32[533,10]), ShapedArray(float32[533,10]), ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,10]), ShapedArray(float32[533,3,32,32]), ShapedArray(float32[533,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:29:20,844 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.060446977615356445 sec
2023-07-01 00:29:20,844 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:21,171 - WARNING - Finished XLA compilation of jit(update_fn) in 0.326768159866333 sec
2023-07-01 00:29:22,456 - INFO - Distilling data from client: Client22
2023-07-01 00:29:22,456 - INFO - train loss: 1.9665643562924325e-05
2023-07-01 00:29:22,456 - INFO - train acc: 1.0
2023-07-01 00:29:22,516 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:22,517 - INFO - test loss 0.0005955596622774114
2023-07-01 00:29:22,517 - INFO - test acc 1.0
2023-07-01 00:29:23,792 - INFO - Distilling data from client: Client22
2023-07-01 00:29:23,792 - INFO - train loss: 2.8450900034386232e-06
2023-07-01 00:29:23,792 - INFO - train acc: 1.0
2023-07-01 00:29:23,816 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:23,816 - INFO - test loss 0.0005300773355077133
2023-07-01 00:29:23,817 - INFO - test acc 1.0
2023-07-01 00:29:25,114 - INFO - Distilling data from client: Client22
2023-07-01 00:29:25,114 - INFO - train loss: 5.881236399243044e-07
2023-07-01 00:29:25,114 - INFO - train acc: 1.0
2023-07-01 00:29:25,139 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:25,140 - INFO - test loss 0.0004933397575683736
2023-07-01 00:29:25,140 - INFO - test acc 1.0
2023-07-01 00:29:26,421 - INFO - Distilling data from client: Client22
2023-07-01 00:29:26,422 - INFO - train loss: 1.0513026828765921e-07
2023-07-01 00:29:26,422 - INFO - train acc: 1.0
2023-07-01 00:29:26,447 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:26,447 - INFO - test loss 0.0004862630815418482
2023-07-01 00:29:26,447 - INFO - test acc 1.0
2023-07-01 00:29:27,729 - INFO - Distilling data from client: Client22
2023-07-01 00:29:27,730 - INFO - train loss: 3.725877897381483e-08
2023-07-01 00:29:27,730 - INFO - train acc: 1.0
2023-07-01 00:29:27,754 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:27,754 - INFO - test loss 0.00048737742575857335
2023-07-01 00:29:27,754 - INFO - test acc 1.0
2023-07-01 00:29:29,026 - INFO - Distilling data from client: Client22
2023-07-01 00:29:29,026 - INFO - train loss: 1.0996838523117776e-08
2023-07-01 00:29:29,026 - INFO - train acc: 1.0
2023-07-01 00:29:29,050 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:29,051 - INFO - test loss 0.00048818283623852296
2023-07-01 00:29:29,051 - INFO - test acc 1.0
2023-07-01 00:29:30,350 - INFO - Distilling data from client: Client22
2023-07-01 00:29:30,350 - INFO - train loss: 4.227225695986397e-09
2023-07-01 00:29:30,350 - INFO - train acc: 1.0
2023-07-01 00:29:30,374 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:30,374 - INFO - test loss 0.0004890697907009094
2023-07-01 00:29:30,374 - INFO - test acc 1.0
2023-07-01 00:29:31,656 - INFO - Distilling data from client: Client22
2023-07-01 00:29:31,657 - INFO - train loss: 8.426336933280704e-10
2023-07-01 00:29:31,657 - INFO - train acc: 1.0
2023-07-01 00:29:31,684 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:31,684 - INFO - test loss 0.0004887421709270565
2023-07-01 00:29:31,684 - INFO - test acc 1.0
2023-07-01 00:29:32,957 - INFO - Distilling data from client: Client22
2023-07-01 00:29:32,957 - INFO - train loss: 3.0638346412823565e-10
2023-07-01 00:29:32,957 - INFO - train acc: 1.0
2023-07-01 00:29:32,981 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:32,981 - INFO - test loss 0.0004889358180832573
2023-07-01 00:29:32,981 - INFO - test acc 1.0
2023-07-01 00:29:34,263 - INFO - Distilling data from client: Client22
2023-07-01 00:29:34,263 - INFO - train loss: 8.076512078227224e-11
2023-07-01 00:29:34,263 - INFO - train acc: 1.0
2023-07-01 00:29:34,290 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:34,290 - INFO - test loss 0.0004888788058367378
2023-07-01 00:29:34,290 - INFO - test acc 1.0
2023-07-01 00:29:35,576 - INFO - Distilling data from client: Client22
2023-07-01 00:29:35,576 - INFO - train loss: 2.4272370513204987e-11
2023-07-01 00:29:35,576 - INFO - train acc: 1.0
2023-07-01 00:29:35,604 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:35,604 - INFO - test loss 0.000488907887995632
2023-07-01 00:29:35,604 - INFO - test acc 1.0
2023-07-01 00:29:36,882 - INFO - Distilling data from client: Client22
2023-07-01 00:29:36,882 - INFO - train loss: 7.68906363682744e-12
2023-07-01 00:29:36,882 - INFO - train acc: 1.0
2023-07-01 00:29:36,906 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:36,906 - INFO - test loss 0.0004889075772687562
2023-07-01 00:29:36,906 - INFO - test acc 1.0
2023-07-01 00:29:38,191 - INFO - Distilling data from client: Client22
2023-07-01 00:29:38,191 - INFO - train loss: 2.9846389191434945e-12
2023-07-01 00:29:38,191 - INFO - train acc: 1.0
2023-07-01 00:29:38,216 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:38,216 - INFO - test loss 0.000488909596570475
2023-07-01 00:29:38,216 - INFO - test acc 1.0
2023-07-01 00:29:39,495 - INFO - Distilling data from client: Client22
2023-07-01 00:29:39,495 - INFO - train loss: 9.133112100631398e-13
2023-07-01 00:29:39,495 - INFO - train acc: 1.0
2023-07-01 00:29:39,520 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:39,520 - INFO - test loss 0.0004889051454631965
2023-07-01 00:29:39,520 - INFO - test acc 1.0
2023-07-01 00:29:40,813 - INFO - Distilling data from client: Client22
2023-07-01 00:29:40,813 - INFO - train loss: 2.605192410337708e-13
2023-07-01 00:29:40,813 - INFO - train acc: 1.0
2023-07-01 00:29:40,837 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:40,837 - INFO - test loss 0.000488911748793879
2023-07-01 00:29:40,837 - INFO - test acc 1.0
2023-07-01 00:29:42,116 - INFO - Distilling data from client: Client22
2023-07-01 00:29:42,116 - INFO - train loss: 8.674629696459908e-14
2023-07-01 00:29:42,116 - INFO - train acc: 1.0
2023-07-01 00:29:42,140 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:42,140 - INFO - test loss 0.0004889101128542198
2023-07-01 00:29:42,140 - INFO - test acc 1.0
2023-07-01 00:29:43,418 - INFO - Distilling data from client: Client22
2023-07-01 00:29:43,419 - INFO - train loss: 4.8531791015239816e-14
2023-07-01 00:29:43,419 - INFO - train acc: 1.0
2023-07-01 00:29:43,444 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:43,444 - INFO - test loss 0.0004889087781148164
2023-07-01 00:29:43,444 - INFO - test acc 1.0
2023-07-01 00:29:44,716 - INFO - Distilling data from client: Client22
2023-07-01 00:29:44,717 - INFO - train loss: 1.7959122860883657e-14
2023-07-01 00:29:44,717 - INFO - train acc: 1.0
2023-07-01 00:29:44,740 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:44,740 - INFO - test loss 0.0004889103522092372
2023-07-01 00:29:44,740 - INFO - test acc 1.0
2023-07-01 00:29:46,018 - INFO - Distilling data from client: Client22
2023-07-01 00:29:46,018 - INFO - train loss: 5.37145184958839e-15
2023-07-01 00:29:46,018 - INFO - train acc: 1.0
2023-07-01 00:29:46,042 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:46,042 - INFO - test loss 0.0004889098066058337
2023-07-01 00:29:46,042 - INFO - test acc 1.0
2023-07-01 00:29:47,325 - INFO - Distilling data from client: Client22
2023-07-01 00:29:47,325 - INFO - train loss: 2.1725754796533433e-15
2023-07-01 00:29:47,325 - INFO - train acc: 1.0
2023-07-01 00:29:47,349 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:47,350 - INFO - test loss 0.0004889098820572401
2023-07-01 00:29:47,350 - INFO - test acc 1.0
2023-07-01 00:29:48,649 - INFO - Distilling data from client: Client22
2023-07-01 00:29:48,649 - INFO - train loss: 6.031361754015646e-16
2023-07-01 00:29:48,650 - INFO - train acc: 1.0
2023-07-01 00:29:48,673 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:48,673 - INFO - test loss 0.000488909912234506
2023-07-01 00:29:48,673 - INFO - test acc 1.0
2023-07-01 00:29:49,959 - INFO - Distilling data from client: Client22
2023-07-01 00:29:49,959 - INFO - train loss: 1.2446817909239222e-16
2023-07-01 00:29:49,959 - INFO - train acc: 1.0
2023-07-01 00:29:49,982 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:49,983 - INFO - test loss 0.0004889097455585118
2023-07-01 00:29:49,983 - INFO - test acc 1.0
2023-07-01 00:29:51,262 - INFO - Distilling data from client: Client22
2023-07-01 00:29:51,263 - INFO - train loss: 2.270339945053984e-17
2023-07-01 00:29:51,263 - INFO - train acc: 1.0
2023-07-01 00:29:51,288 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:51,288 - INFO - test loss 0.0004889097454753754
2023-07-01 00:29:51,288 - INFO - test acc 1.0
2023-07-01 00:29:52,579 - INFO - Distilling data from client: Client22
2023-07-01 00:29:52,579 - INFO - train loss: 4.467367802224787e-18
2023-07-01 00:29:52,579 - INFO - train acc: 1.0
2023-07-01 00:29:52,604 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:52,604 - INFO - test loss 0.0004889097232956904
2023-07-01 00:29:52,604 - INFO - test acc 1.0
2023-07-01 00:29:53,892 - INFO - Distilling data from client: Client22
2023-07-01 00:29:53,892 - INFO - train loss: 6.480952450375014e-19
2023-07-01 00:29:53,892 - INFO - train acc: 1.0
2023-07-01 00:29:53,916 - INFO - report:               precision    recall  f1-score   support

           9       1.00      1.00      1.00       200

    accuracy                           1.00       200
   macro avg       1.00      1.00      1.00       200
weighted avg       1.00      1.00      1.00       200

2023-07-01 00:29:53,916 - INFO - test loss 0.0004889097299207544
2023-07-01 00:29:53,916 - INFO - test acc 1.0
2023-07-01 00:29:53,918 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.00036716461181640625 sec
2023-07-01 00:29:53,918 - WARNING - Finished tracing + transforming fn for pjit in 0.0003235340118408203 sec
2023-07-01 00:29:53,919 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(int64[1]), ShapedArray(int64[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:29:53,920 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.001314401626586914 sec
2023-07-01 00:29:53,920 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:53,927 - WARNING - Finished XLA compilation of jit(fn) in 0.006074666976928711 sec
2023-07-01 00:29:53,927 - WARNING - Finished tracing + transforming jit(add) in 0.00023627281188964844 sec
2023-07-01 00:29:53,928 - DEBUG - Compiling add for with global shapes and types [ShapedArray(int64[1]), ShapedArray(int64[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:29:53,929 - WARNING - Finished jaxpr to MLIR module conversion jit(add) in 0.0010423660278320312 sec
2023-07-01 00:29:53,929 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:53,935 - WARNING - Finished XLA compilation of jit(add) in 0.005448102951049805 sec
2023-07-01 00:29:53,935 - WARNING - Finished tracing + transforming jit(select_n) in 0.00022649765014648438 sec
2023-07-01 00:29:53,936 - DEBUG - Compiling select_n for with global shapes and types [ShapedArray(bool[1]), ShapedArray(int64[1]), ShapedArray(int64[1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:29:53,937 - WARNING - Finished jaxpr to MLIR module conversion jit(select_n) in 0.0009882450103759766 sec
2023-07-01 00:29:53,937 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:53,943 - WARNING - Finished XLA compilation of jit(select_n) in 0.005629539489746094 sec
2023-07-01 00:29:53,944 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0001900196075439453 sec
2023-07-01 00:29:53,944 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.0001437664031982422 sec
2023-07-01 00:29:53,945 - DEBUG - Compiling convert_element_type for with global shapes and types [ShapedArray(int64[1])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:29:53,946 - WARNING - Finished jaxpr to MLIR module conversion jit(convert_element_type) in 0.0009469985961914062 sec
2023-07-01 00:29:53,946 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:53,951 - WARNING - Finished XLA compilation of jit(convert_element_type) in 0.0051593780517578125 sec
2023-07-01 00:29:53,952 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002193450927734375 sec
2023-07-01 00:29:53,952 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(int32[1])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:29:53,953 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009279251098632812 sec
2023-07-01 00:29:53,953 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:53,959 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.004921913146972656 sec
2023-07-01 00:29:53,959 - WARNING - Finished tracing + transforming jit(gather) in 0.00023293495178222656 sec
2023-07-01 00:29:53,960 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[533,3,32,32]), ShapedArray(int32[1,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:29:53,961 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0010654926300048828 sec
2023-07-01 00:29:53,961 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:53,970 - WARNING - Finished XLA compilation of jit(gather) in 0.009204864501953125 sec
2023-07-01 00:29:53,971 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00036025047302246094 sec
2023-07-01 00:29:53,972 - WARNING - Finished tracing + transforming jit(copy) in 0.00011420249938964844 sec
2023-07-01 00:29:53,972 - DEBUG - Compiling copy for with global shapes and types [ShapedArray(float32[1,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:29:53,973 - WARNING - Finished jaxpr to MLIR module conversion jit(copy) in 0.0009024143218994141 sec
2023-07-01 00:29:53,973 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:53,979 - WARNING - Finished XLA compilation of jit(copy) in 0.005070209503173828 sec
2023-07-01 00:29:53,989 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:53,998 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:53,999 - WARNING - Finished tracing + transforming _unstack for pjit in 0.00038814544677734375 sec
2023-07-01 00:29:53,999 - DEBUG - Compiling _unstack for with global shapes and types [ShapedArray(float32[1,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:29:54,000 - WARNING - Finished jaxpr to MLIR module conversion jit(_unstack) in 0.001155853271484375 sec
2023-07-01 00:29:54,001 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:54,006 - WARNING - Finished XLA compilation of jit(_unstack) in 0.00548243522644043 sec
2023-07-01 00:29:54,016 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:29:54,207 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client22//synthetic.png
2023-07-01 00:29:54,219 - INFO - c: 0.0 and total_data_in_this_class: 258
2023-07-01 00:29:54,219 - INFO - c: 6.0 and total_data_in_this_class: 277
2023-07-01 00:29:54,219 - INFO - c: 9.0 and total_data_in_this_class: 264
2023-07-01 00:29:54,219 - INFO - c: 0.0 and total_data_in_this_class: 75
2023-07-01 00:29:54,219 - INFO - c: 6.0 and total_data_in_this_class: 56
2023-07-01 00:29:54,219 - INFO - c: 9.0 and total_data_in_this_class: 69
2023-07-01 00:29:54,289 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04680824279785156 sec
2023-07-01 00:29:54,335 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04561471939086914 sec
2023-07-01 00:29:54,340 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.09926986694335938 sec
2023-07-01 00:29:54,342 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:29:54,375 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03233647346496582 sec
2023-07-01 00:29:54,375 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:54,497 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12181925773620605 sec
2023-07-01 00:29:54,519 - INFO - initial test loss: 0.020960359422923375
2023-07-01 00:29:54,519 - INFO - initial test acc: 0.75
2023-07-01 00:29:54,527 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.00561213493347168 sec
2023-07-01 00:29:54,637 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11551237106323242 sec
2023-07-01 00:29:54,640 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:29:54,703 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06225395202636719 sec
2023-07-01 00:29:54,703 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:29:55,002 - WARNING - Finished XLA compilation of jit(update_fn) in 0.29910802841186523 sec
2023-07-01 00:29:56,234 - INFO - Distilling data from client: Client23
2023-07-01 00:29:56,234 - INFO - train loss: 0.002270636225392513
2023-07-01 00:29:56,234 - INFO - train acc: 0.9941860437393188
2023-07-01 00:29:56,290 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.77      0.76        75
           6       0.74      0.86      0.79        56
           9       0.84      0.71      0.77        69

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.77       200

2023-07-01 00:29:56,290 - INFO - test loss 0.018589126381129233
2023-07-01 00:29:56,290 - INFO - test acc 0.7749999761581421
2023-07-01 00:29:57,510 - INFO - Distilling data from client: Client23
2023-07-01 00:29:57,511 - INFO - train loss: 0.0011513408444639081
2023-07-01 00:29:57,511 - INFO - train acc: 0.998062014579773
2023-07-01 00:29:57,535 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.74        75
           6       0.73      0.88      0.80        56
           9       0.85      0.72      0.78        69

    accuracy                           0.77       200
   macro avg       0.77      0.78      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-07-01 00:29:57,535 - INFO - test loss 0.018760463717077364
2023-07-01 00:29:57,535 - INFO - test acc 0.7699999809265137
2023-07-01 00:29:58,750 - INFO - Distilling data from client: Client23
2023-07-01 00:29:58,750 - INFO - train loss: 0.0007499943522554125
2023-07-01 00:29:58,750 - INFO - train acc: 1.0
2023-07-01 00:29:58,773 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.72      0.74        75
           6       0.68      0.88      0.77        56
           9       0.83      0.70      0.76        69

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.77      0.76      0.75       200

2023-07-01 00:29:58,773 - INFO - test loss 0.01907814800740331
2023-07-01 00:29:58,773 - INFO - test acc 0.7549999952316284
2023-07-01 00:29:59,984 - INFO - Distilling data from client: Client23
2023-07-01 00:29:59,984 - INFO - train loss: 0.0005424743387018596
2023-07-01 00:29:59,984 - INFO - train acc: 1.0
2023-07-01 00:30:00,008 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.74        75
           6       0.69      0.88      0.77        56
           9       0.84      0.67      0.74        69

    accuracy                           0.75       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-07-01 00:30:00,008 - INFO - test loss 0.019427907335554884
2023-07-01 00:30:00,008 - INFO - test acc 0.75
2023-07-01 00:30:01,240 - INFO - Distilling data from client: Client23
2023-07-01 00:30:01,240 - INFO - train loss: 0.0007191668648945898
2023-07-01 00:30:01,240 - INFO - train acc: 1.0
2023-07-01 00:30:01,265 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.75      0.75        75
           6       0.74      0.86      0.79        56
           9       0.84      0.74      0.78        69

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.77       200

2023-07-01 00:30:01,265 - INFO - test loss 0.019078280391723955
2023-07-01 00:30:01,265 - INFO - test acc 0.7749999761581421
2023-07-01 00:30:02,479 - INFO - Distilling data from client: Client23
2023-07-01 00:30:02,479 - INFO - train loss: 0.000490966907510512
2023-07-01 00:30:02,479 - INFO - train acc: 1.0
2023-07-01 00:30:02,502 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.72      0.72        75
           6       0.69      0.86      0.76        56
           9       0.84      0.68      0.75        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.76      0.74      0.74       200

2023-07-01 00:30:02,503 - INFO - test loss 0.01962534333780321
2023-07-01 00:30:02,503 - INFO - test acc 0.7450000047683716
2023-07-01 00:30:03,728 - INFO - Distilling data from client: Client23
2023-07-01 00:30:03,728 - INFO - train loss: 0.00038154496136986603
2023-07-01 00:30:03,728 - INFO - train acc: 1.0
2023-07-01 00:30:03,753 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        75
           6       0.66      0.86      0.74        56
           9       0.81      0.64      0.72        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.75      0.73      0.73       200

2023-07-01 00:30:03,753 - INFO - test loss 0.019523924036123776
2023-07-01 00:30:03,753 - INFO - test acc 0.73499995470047
2023-07-01 00:30:04,963 - INFO - Distilling data from client: Client23
2023-07-01 00:30:04,963 - INFO - train loss: 0.0004265655201291519
2023-07-01 00:30:04,963 - INFO - train acc: 1.0
2023-07-01 00:30:04,988 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.75      0.76        75
           6       0.66      0.82      0.73        56
           9       0.81      0.67      0.73        69

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:30:04,989 - INFO - test loss 0.019768947569701666
2023-07-01 00:30:04,989 - INFO - test acc 0.7400000095367432
2023-07-01 00:30:06,208 - INFO - Distilling data from client: Client23
2023-07-01 00:30:06,208 - INFO - train loss: 0.00039829227435062334
2023-07-01 00:30:06,208 - INFO - train acc: 1.0
2023-07-01 00:30:06,231 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.75      0.75        75
           6       0.70      0.84      0.76        56
           9       0.81      0.70      0.75        69

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.75       200

2023-07-01 00:30:06,231 - INFO - test loss 0.0196308037893385
2023-07-01 00:30:06,231 - INFO - test acc 0.7549999952316284
2023-07-01 00:30:07,454 - INFO - Distilling data from client: Client23
2023-07-01 00:30:07,454 - INFO - train loss: 0.00036624784889399735
2023-07-01 00:30:07,454 - INFO - train acc: 1.0
2023-07-01 00:30:07,478 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.75      0.74        75
           6       0.70      0.86      0.77        56
           9       0.82      0.65      0.73        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:30:07,478 - INFO - test loss 0.01991605043266653
2023-07-01 00:30:07,478 - INFO - test acc 0.7450000047683716
2023-07-01 00:30:08,701 - INFO - Distilling data from client: Client23
2023-07-01 00:30:08,701 - INFO - train loss: 0.0002672605736551237
2023-07-01 00:30:08,701 - INFO - train acc: 1.0
2023-07-01 00:30:08,724 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.72      0.73        75
           6       0.65      0.82      0.72        56
           9       0.80      0.65      0.72        69

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.74      0.72      0.72       200

2023-07-01 00:30:08,724 - INFO - test loss 0.019763755042937103
2023-07-01 00:30:08,724 - INFO - test acc 0.7249999642372131
2023-07-01 00:30:09,949 - INFO - Distilling data from client: Client23
2023-07-01 00:30:09,949 - INFO - train loss: 0.00021698093945911055
2023-07-01 00:30:09,949 - INFO - train acc: 1.0
2023-07-01 00:30:09,972 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.74        75
           6       0.66      0.88      0.75        56
           9       0.83      0.62      0.71        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.75      0.73      0.73       200

2023-07-01 00:30:09,972 - INFO - test loss 0.019657311788726466
2023-07-01 00:30:09,972 - INFO - test acc 0.73499995470047
2023-07-01 00:30:11,194 - INFO - Distilling data from client: Client23
2023-07-01 00:30:11,194 - INFO - train loss: 0.00023406735688794692
2023-07-01 00:30:11,194 - INFO - train acc: 1.0
2023-07-01 00:30:11,217 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.76      0.75        75
           6       0.70      0.86      0.77        56
           9       0.85      0.67      0.75        69

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.77      0.76      0.75       200

2023-07-01 00:30:11,218 - INFO - test loss 0.01984260003040238
2023-07-01 00:30:11,218 - INFO - test acc 0.7549999952316284
2023-07-01 00:30:12,444 - INFO - Distilling data from client: Client23
2023-07-01 00:30:12,444 - INFO - train loss: 0.000263109809051113
2023-07-01 00:30:12,444 - INFO - train acc: 1.0
2023-07-01 00:30:12,468 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        75
           6       0.70      0.86      0.77        56
           9       0.84      0.68      0.75        69

    accuracy                           0.75       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-07-01 00:30:12,468 - INFO - test loss 0.02002247037222037
2023-07-01 00:30:12,468 - INFO - test acc 0.75
2023-07-01 00:30:13,691 - INFO - Distilling data from client: Client23
2023-07-01 00:30:13,691 - INFO - train loss: 0.00022051566996663836
2023-07-01 00:30:13,691 - INFO - train acc: 1.0
2023-07-01 00:30:13,716 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.75      0.75        75
           6       0.69      0.84      0.76        56
           9       0.82      0.68      0.75        69

    accuracy                           0.75       200
   macro avg       0.75      0.76      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-07-01 00:30:13,716 - INFO - test loss 0.019879508966417967
2023-07-01 00:30:13,717 - INFO - test acc 0.75
2023-07-01 00:30:14,941 - INFO - Distilling data from client: Client23
2023-07-01 00:30:14,941 - INFO - train loss: 0.00023447125551565221
2023-07-01 00:30:14,941 - INFO - train acc: 1.0
2023-07-01 00:30:14,967 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.75      0.75        75
           6       0.67      0.84      0.75        56
           9       0.82      0.67      0.74        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.76      0.74      0.74       200

2023-07-01 00:30:14,968 - INFO - test loss 0.019477313538004738
2023-07-01 00:30:14,968 - INFO - test acc 0.7450000047683716
2023-07-01 00:30:16,205 - INFO - Distilling data from client: Client23
2023-07-01 00:30:16,206 - INFO - train loss: 0.00018624284715819468
2023-07-01 00:30:16,206 - INFO - train acc: 1.0
2023-07-01 00:30:16,233 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        75
           6       0.66      0.88      0.75        56
           9       0.85      0.65      0.74        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.76      0.74      0.74       200

2023-07-01 00:30:16,233 - INFO - test loss 0.019337614167763747
2023-07-01 00:30:16,233 - INFO - test acc 0.7450000047683716
2023-07-01 00:30:17,458 - INFO - Distilling data from client: Client23
2023-07-01 00:30:17,458 - INFO - train loss: 0.00022433817788039686
2023-07-01 00:30:17,458 - INFO - train acc: 1.0
2023-07-01 00:30:17,481 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.72      0.72        75
           6       0.67      0.88      0.76        56
           9       0.84      0.62      0.72        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.75      0.73      0.73       200

2023-07-01 00:30:17,481 - INFO - test loss 0.02007327053707504
2023-07-01 00:30:17,481 - INFO - test acc 0.7299999594688416
2023-07-01 00:30:18,690 - INFO - Distilling data from client: Client23
2023-07-01 00:30:18,690 - INFO - train loss: 0.00023602967508284535
2023-07-01 00:30:18,690 - INFO - train acc: 1.0
2023-07-01 00:30:18,713 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.69      0.71        75
           6       0.67      0.88      0.76        56
           9       0.82      0.65      0.73        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:30:18,714 - INFO - test loss 0.019760809547978354
2023-07-01 00:30:18,714 - INFO - test acc 0.7299999594688416
2023-07-01 00:30:19,933 - INFO - Distilling data from client: Client23
2023-07-01 00:30:19,933 - INFO - train loss: 0.00017351920868003123
2023-07-01 00:30:19,933 - INFO - train acc: 1.0
2023-07-01 00:30:19,956 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        75
           6       0.68      0.89      0.78        56
           9       0.85      0.64      0.73        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.76      0.74      0.74       200

2023-07-01 00:30:19,956 - INFO - test loss 0.019463019464144894
2023-07-01 00:30:19,956 - INFO - test acc 0.7450000047683716
2023-07-01 00:30:21,174 - INFO - Distilling data from client: Client23
2023-07-01 00:30:21,174 - INFO - train loss: 0.00017885919410788113
2023-07-01 00:30:21,174 - INFO - train acc: 1.0
2023-07-01 00:30:21,197 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.72      0.73        75
           6       0.66      0.86      0.74        56
           9       0.83      0.65      0.73        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.75      0.73      0.73       200

2023-07-01 00:30:21,197 - INFO - test loss 0.01969103806183036
2023-07-01 00:30:21,197 - INFO - test acc 0.73499995470047
2023-07-01 00:30:22,430 - INFO - Distilling data from client: Client23
2023-07-01 00:30:22,430 - INFO - train loss: 0.00015061457597537352
2023-07-01 00:30:22,430 - INFO - train acc: 1.0
2023-07-01 00:30:22,454 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.74        75
           6       0.69      0.86      0.76        56
           9       0.82      0.67      0.74        69

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:30:22,455 - INFO - test loss 0.019693224893583057
2023-07-01 00:30:22,455 - INFO - test acc 0.7450000047683716
2023-07-01 00:30:23,688 - INFO - Distilling data from client: Client23
2023-07-01 00:30:23,688 - INFO - train loss: 0.00016249483193004293
2023-07-01 00:30:23,688 - INFO - train acc: 1.0
2023-07-01 00:30:23,715 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.75      0.73        75
           6       0.70      0.89      0.79        56
           9       0.84      0.62      0.72        69

    accuracy                           0.74       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.74      0.74       200

2023-07-01 00:30:23,715 - INFO - test loss 0.01989291200144536
2023-07-01 00:30:23,716 - INFO - test acc 0.7450000047683716
2023-07-01 00:30:24,942 - INFO - Distilling data from client: Client23
2023-07-01 00:30:24,942 - INFO - train loss: 0.00015478344073117058
2023-07-01 00:30:24,942 - INFO - train acc: 1.0
2023-07-01 00:30:24,965 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.75      0.75        75
           6       0.68      0.86      0.76        56
           9       0.85      0.67      0.75        69

    accuracy                           0.75       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-07-01 00:30:24,965 - INFO - test loss 0.01971377964789231
2023-07-01 00:30:24,965 - INFO - test acc 0.75
2023-07-01 00:30:26,187 - INFO - Distilling data from client: Client23
2023-07-01 00:30:26,187 - INFO - train loss: 0.00015891570602165353
2023-07-01 00:30:26,187 - INFO - train acc: 1.0
2023-07-01 00:30:26,210 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.69      0.71        75
           6       0.66      0.86      0.74        56
           9       0.82      0.67      0.74        69

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.73       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:30:26,210 - INFO - test loss 0.019639735427396214
2023-07-01 00:30:26,210 - INFO - test acc 0.7299999594688416
2023-07-01 00:30:26,222 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:26,231 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:26,240 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:26,249 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:26,257 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:26,266 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:26,276 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:26,285 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:26,294 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:26,672 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client23//synthetic.png
2023-07-01 00:30:26,684 - INFO - c: 1.0 and total_data_in_this_class: 268
2023-07-01 00:30:26,684 - INFO - c: 8.0 and total_data_in_this_class: 531
2023-07-01 00:30:26,684 - INFO - c: 1.0 and total_data_in_this_class: 65
2023-07-01 00:30:26,684 - INFO - c: 8.0 and total_data_in_this_class: 135
2023-07-01 00:30:26,755 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.049010515213012695 sec
2023-07-01 00:30:26,803 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04689455032348633 sec
2023-07-01 00:30:26,808 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.10314631462097168 sec
2023-07-01 00:30:26,810 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:30:26,844 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03388524055480957 sec
2023-07-01 00:30:26,844 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:30:26,971 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12639355659484863 sec
2023-07-01 00:30:26,988 - INFO - initial test loss: 0.015404913136465725
2023-07-01 00:30:26,988 - INFO - initial test acc: 0.8399999737739563
2023-07-01 00:30:26,997 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.006828784942626953 sec
2023-07-01 00:30:27,109 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1192624568939209 sec
2023-07-01 00:30:27,113 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:30:27,175 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.062255859375 sec
2023-07-01 00:30:27,175 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:30:27,508 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3322715759277344 sec
2023-07-01 00:30:28,256 - INFO - Distilling data from client: Client24
2023-07-01 00:30:28,256 - INFO - train loss: 0.0028023812011641145
2023-07-01 00:30:28,256 - INFO - train acc: 0.9888268113136292
2023-07-01 00:30:28,299 - INFO - report:               precision    recall  f1-score   support

           1       0.67      0.78      0.72        65
           8       0.89      0.81      0.85       135

    accuracy                           0.81       200
   macro avg       0.78      0.80      0.79       200
weighted avg       0.82      0.81      0.81       200

2023-07-01 00:30:28,299 - INFO - test loss 0.014855018287647062
2023-07-01 00:30:28,299 - INFO - test acc 0.8050000071525574
2023-07-01 00:30:29,040 - INFO - Distilling data from client: Client24
2023-07-01 00:30:29,040 - INFO - train loss: 0.001722253949142355
2023-07-01 00:30:29,040 - INFO - train acc: 0.9972066879272461
2023-07-01 00:30:29,084 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.83      0.75        65
           8       0.91      0.81      0.86       135

    accuracy                           0.82       200
   macro avg       0.80      0.82      0.80       200
weighted avg       0.84      0.82      0.82       200

2023-07-01 00:30:29,085 - INFO - test loss 0.013649562502220752
2023-07-01 00:30:29,085 - INFO - test acc 0.8199999928474426
2023-07-01 00:30:29,826 - INFO - Distilling data from client: Client24
2023-07-01 00:30:29,826 - INFO - train loss: 0.00175026926568014
2023-07-01 00:30:29,826 - INFO - train acc: 0.9972066879272461
2023-07-01 00:30:29,869 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.82      0.76        65
           8       0.90      0.84      0.87       135

    accuracy                           0.83       200
   macro avg       0.81      0.83      0.82       200
weighted avg       0.84      0.83      0.84       200

2023-07-01 00:30:29,869 - INFO - test loss 0.013510097428945333
2023-07-01 00:30:29,869 - INFO - test acc 0.8349999785423279
2023-07-01 00:30:30,608 - INFO - Distilling data from client: Client24
2023-07-01 00:30:30,608 - INFO - train loss: 0.0014647983581953265
2023-07-01 00:30:30,608 - INFO - train acc: 0.9972066879272461
2023-07-01 00:30:30,626 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.82      0.76        65
           8       0.90      0.84      0.87       135

    accuracy                           0.83       200
   macro avg       0.81      0.83      0.82       200
weighted avg       0.84      0.83      0.84       200

2023-07-01 00:30:30,626 - INFO - test loss 0.013899067437451642
2023-07-01 00:30:30,626 - INFO - test acc 0.8349999785423279
2023-07-01 00:30:31,370 - INFO - Distilling data from client: Client24
2023-07-01 00:30:31,370 - INFO - train loss: 0.0013465336148862756
2023-07-01 00:30:31,371 - INFO - train acc: 1.0
2023-07-01 00:30:31,388 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.74      0.71        65
           8       0.87      0.83      0.85       135

    accuracy                           0.80       200
   macro avg       0.77      0.78      0.78       200
weighted avg       0.81      0.80      0.80       200

2023-07-01 00:30:31,389 - INFO - test loss 0.01448584581264634
2023-07-01 00:30:31,389 - INFO - test acc 0.7999999523162842
2023-07-01 00:30:32,143 - INFO - Distilling data from client: Client24
2023-07-01 00:30:32,143 - INFO - train loss: 0.001001741340507496
2023-07-01 00:30:32,143 - INFO - train acc: 1.0
2023-07-01 00:30:32,189 - INFO - report:               precision    recall  f1-score   support

           1       0.74      0.82      0.77        65
           8       0.91      0.86      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.84      0.83       200
weighted avg       0.85      0.84      0.85       200

2023-07-01 00:30:32,189 - INFO - test loss 0.013651651178498495
2023-07-01 00:30:32,189 - INFO - test acc 0.8449999690055847
2023-07-01 00:30:32,943 - INFO - Distilling data from client: Client24
2023-07-01 00:30:32,943 - INFO - train loss: 0.0011006866315648699
2023-07-01 00:30:32,943 - INFO - train acc: 0.9972066879272461
2023-07-01 00:30:32,960 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.75      0.73        65
           8       0.88      0.84      0.86       135

    accuracy                           0.81       200
   macro avg       0.79      0.80      0.79       200
weighted avg       0.82      0.81      0.82       200

2023-07-01 00:30:32,961 - INFO - test loss 0.013754401887443691
2023-07-01 00:30:32,961 - INFO - test acc 0.8149999976158142
2023-07-01 00:30:33,718 - INFO - Distilling data from client: Client24
2023-07-01 00:30:33,718 - INFO - train loss: 0.0013666719895356401
2023-07-01 00:30:33,718 - INFO - train acc: 0.9972066879272461
2023-07-01 00:30:33,736 - INFO - report:               precision    recall  f1-score   support

           1       0.73      0.80      0.76        65
           8       0.90      0.86      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.83      0.82       200
weighted avg       0.85      0.84      0.84       200

2023-07-01 00:30:33,736 - INFO - test loss 0.013948684113645428
2023-07-01 00:30:33,736 - INFO - test acc 0.8399999737739563
2023-07-01 00:30:34,488 - INFO - Distilling data from client: Client24
2023-07-01 00:30:34,488 - INFO - train loss: 0.0012106558213591375
2023-07-01 00:30:34,488 - INFO - train acc: 0.9972066879272461
2023-07-01 00:30:34,506 - INFO - report:               precision    recall  f1-score   support

           1       0.71      0.75      0.73        65
           8       0.88      0.85      0.86       135

    accuracy                           0.82       200
   macro avg       0.79      0.80      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-07-01 00:30:34,506 - INFO - test loss 0.014364692268831325
2023-07-01 00:30:34,506 - INFO - test acc 0.8199999928474426
2023-07-01 00:30:35,252 - INFO - Distilling data from client: Client24
2023-07-01 00:30:35,252 - INFO - train loss: 0.001048437240518866
2023-07-01 00:30:35,253 - INFO - train acc: 1.0
2023-07-01 00:30:35,271 - INFO - report:               precision    recall  f1-score   support

           1       0.73      0.83      0.78        65
           8       0.91      0.85      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.84      0.83       200
weighted avg       0.85      0.84      0.85       200

2023-07-01 00:30:35,271 - INFO - test loss 0.013470632010687888
2023-07-01 00:30:35,271 - INFO - test acc 0.8449999690055847
2023-07-01 00:30:36,025 - INFO - Distilling data from client: Client24
2023-07-01 00:30:36,025 - INFO - train loss: 0.0010306003615709325
2023-07-01 00:30:36,025 - INFO - train acc: 1.0
2023-07-01 00:30:36,070 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.82      0.78        65
           8       0.91      0.87      0.89       135

    accuracy                           0.85       200
   macro avg       0.83      0.84      0.83       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:30:36,070 - INFO - test loss 0.013161793628353688
2023-07-01 00:30:36,070 - INFO - test acc 0.8499999642372131
2023-07-01 00:30:36,809 - INFO - Distilling data from client: Client24
2023-07-01 00:30:36,809 - INFO - train loss: 0.0010047216562429234
2023-07-01 00:30:36,809 - INFO - train acc: 1.0
2023-07-01 00:30:36,826 - INFO - report:               precision    recall  f1-score   support

           1       0.76      0.77      0.76        65
           8       0.89      0.88      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.83      0.82       200
weighted avg       0.85      0.84      0.85       200

2023-07-01 00:30:36,826 - INFO - test loss 0.013231398005888902
2023-07-01 00:30:36,826 - INFO - test acc 0.8449999690055847
2023-07-01 00:30:37,581 - INFO - Distilling data from client: Client24
2023-07-01 00:30:37,582 - INFO - train loss: 0.0008013998834388391
2023-07-01 00:30:37,582 - INFO - train acc: 1.0
2023-07-01 00:30:37,599 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.74      0.71        65
           8       0.87      0.84      0.85       135

    accuracy                           0.81       200
   macro avg       0.78      0.79      0.78       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:30:37,599 - INFO - test loss 0.013369829881312845
2023-07-01 00:30:37,599 - INFO - test acc 0.8050000071525574
2023-07-01 00:30:38,347 - INFO - Distilling data from client: Client24
2023-07-01 00:30:38,347 - INFO - train loss: 0.0008698717327045374
2023-07-01 00:30:38,347 - INFO - train acc: 1.0
2023-07-01 00:30:38,365 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.75      0.73        65
           8       0.88      0.84      0.86       135

    accuracy                           0.81       200
   macro avg       0.79      0.80      0.79       200
weighted avg       0.82      0.81      0.82       200

2023-07-01 00:30:38,365 - INFO - test loss 0.014345316722355594
2023-07-01 00:30:38,366 - INFO - test acc 0.8149999976158142
2023-07-01 00:30:39,119 - INFO - Distilling data from client: Client24
2023-07-01 00:30:39,119 - INFO - train loss: 0.0008956731024433572
2023-07-01 00:30:39,119 - INFO - train acc: 1.0
2023-07-01 00:30:39,138 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.77      0.75        65
           8       0.89      0.86      0.87       135

    accuracy                           0.83       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.83      0.83      0.83       200

2023-07-01 00:30:39,138 - INFO - test loss 0.013681698824361764
2023-07-01 00:30:39,138 - INFO - test acc 0.8299999833106995
2023-07-01 00:30:39,890 - INFO - Distilling data from client: Client24
2023-07-01 00:30:39,890 - INFO - train loss: 0.0007692033556157319
2023-07-01 00:30:39,890 - INFO - train acc: 1.0
2023-07-01 00:30:39,907 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.74      0.73        65
           8       0.87      0.86      0.87       135

    accuracy                           0.82       200
   macro avg       0.79      0.80      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-07-01 00:30:39,908 - INFO - test loss 0.012951456008667163
2023-07-01 00:30:39,908 - INFO - test acc 0.8199999928474426
2023-07-01 00:30:40,656 - INFO - Distilling data from client: Client24
2023-07-01 00:30:40,656 - INFO - train loss: 0.0008591249345831928
2023-07-01 00:30:40,656 - INFO - train acc: 1.0
2023-07-01 00:30:40,673 - INFO - report:               precision    recall  f1-score   support

           1       0.73      0.74      0.73        65
           8       0.87      0.87      0.87       135

    accuracy                           0.82       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.83      0.82      0.83       200

2023-07-01 00:30:40,673 - INFO - test loss 0.014324271261876919
2023-07-01 00:30:40,674 - INFO - test acc 0.824999988079071
2023-07-01 00:30:41,428 - INFO - Distilling data from client: Client24
2023-07-01 00:30:41,428 - INFO - train loss: 0.0007210882493230679
2023-07-01 00:30:41,428 - INFO - train acc: 1.0
2023-07-01 00:30:41,445 - INFO - report:               precision    recall  f1-score   support

           1       0.74      0.83      0.78        65
           8       0.91      0.86      0.89       135

    accuracy                           0.85       200
   macro avg       0.83      0.85      0.83       200
weighted avg       0.86      0.85      0.85       200

2023-07-01 00:30:41,445 - INFO - test loss 0.01316503750798967
2023-07-01 00:30:41,446 - INFO - test acc 0.8499999642372131
2023-07-01 00:30:42,205 - INFO - Distilling data from client: Client24
2023-07-01 00:30:42,206 - INFO - train loss: 0.0008560946906340154
2023-07-01 00:30:42,206 - INFO - train acc: 1.0
2023-07-01 00:30:42,225 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.82      0.78        65
           8       0.91      0.87      0.89       135

    accuracy                           0.85       200
   macro avg       0.83      0.84      0.83       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:30:42,225 - INFO - test loss 0.013310003301601488
2023-07-01 00:30:42,225 - INFO - test acc 0.8499999642372131
2023-07-01 00:30:42,977 - INFO - Distilling data from client: Client24
2023-07-01 00:30:42,977 - INFO - train loss: 0.0007150579896417174
2023-07-01 00:30:42,977 - INFO - train acc: 1.0
2023-07-01 00:30:43,021 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.80      0.79        65
           8       0.90      0.89      0.90       135

    accuracy                           0.86       200
   macro avg       0.84      0.84      0.84       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:30:43,021 - INFO - test loss 0.013349678306613651
2023-07-01 00:30:43,021 - INFO - test acc 0.85999995470047
2023-07-01 00:30:43,778 - INFO - Distilling data from client: Client24
2023-07-01 00:30:43,778 - INFO - train loss: 0.000624886377439259
2023-07-01 00:30:43,778 - INFO - train acc: 1.0
2023-07-01 00:30:43,796 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.77      0.75        65
           8       0.89      0.86      0.87       135

    accuracy                           0.83       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.83      0.83      0.83       200

2023-07-01 00:30:43,796 - INFO - test loss 0.013358245592793154
2023-07-01 00:30:43,796 - INFO - test acc 0.8299999833106995
2023-07-01 00:30:44,539 - INFO - Distilling data from client: Client24
2023-07-01 00:30:44,539 - INFO - train loss: 0.0008424810303804137
2023-07-01 00:30:44,539 - INFO - train acc: 0.9972066879272461
2023-07-01 00:30:44,558 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.78      0.77        65
           8       0.89      0.87      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.83      0.83       200
weighted avg       0.85      0.84      0.85       200

2023-07-01 00:30:44,559 - INFO - test loss 0.01299614407357467
2023-07-01 00:30:44,559 - INFO - test acc 0.8449999690055847
2023-07-01 00:30:45,308 - INFO - Distilling data from client: Client24
2023-07-01 00:30:45,308 - INFO - train loss: 0.0007605871398476044
2023-07-01 00:30:45,308 - INFO - train acc: 1.0
2023-07-01 00:30:45,328 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.78      0.75        65
           8       0.89      0.85      0.87       135

    accuracy                           0.83       200
   macro avg       0.80      0.82      0.81       200
weighted avg       0.84      0.83      0.83       200

2023-07-01 00:30:45,328 - INFO - test loss 0.013699466965885584
2023-07-01 00:30:45,328 - INFO - test acc 0.8299999833106995
2023-07-01 00:30:46,077 - INFO - Distilling data from client: Client24
2023-07-01 00:30:46,077 - INFO - train loss: 0.0007074447448072038
2023-07-01 00:30:46,077 - INFO - train acc: 1.0
2023-07-01 00:30:46,094 - INFO - report:               precision    recall  f1-score   support

           1       0.76      0.74      0.75        65
           8       0.88      0.89      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.82       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:30:46,095 - INFO - test loss 0.013648056512590821
2023-07-01 00:30:46,095 - INFO - test acc 0.8399999737739563
2023-07-01 00:30:46,854 - INFO - Distilling data from client: Client24
2023-07-01 00:30:46,854 - INFO - train loss: 0.0007449297009654173
2023-07-01 00:30:46,854 - INFO - train acc: 1.0
2023-07-01 00:30:46,872 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.78      0.77        65
           8       0.89      0.87      0.88       135

    accuracy                           0.84       200
   macro avg       0.82      0.83      0.83       200
weighted avg       0.85      0.84      0.85       200

2023-07-01 00:30:46,872 - INFO - test loss 0.012951090357409354
2023-07-01 00:30:46,872 - INFO - test acc 0.8449999690055847
2023-07-01 00:30:46,883 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:46,892 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:46,901 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:46,909 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:46,918 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:46,928 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:30:47,238 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client24//synthetic.png
2023-07-01 00:30:47,251 - INFO - c: 2.0 and total_data_in_this_class: 269
2023-07-01 00:30:47,252 - INFO - c: 4.0 and total_data_in_this_class: 256
2023-07-01 00:30:47,252 - INFO - c: 7.0 and total_data_in_this_class: 274
2023-07-01 00:30:47,252 - INFO - c: 2.0 and total_data_in_this_class: 64
2023-07-01 00:30:47,252 - INFO - c: 4.0 and total_data_in_this_class: 77
2023-07-01 00:30:47,252 - INFO - c: 7.0 and total_data_in_this_class: 59
2023-07-01 00:30:47,322 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0470430850982666 sec
2023-07-01 00:30:47,369 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04529285430908203 sec
2023-07-01 00:30:47,374 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.0993509292602539 sec
2023-07-01 00:30:47,375 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:30:47,408 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03268861770629883 sec
2023-07-01 00:30:47,408 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:30:47,535 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12593674659729004 sec
2023-07-01 00:30:47,557 - INFO - initial test loss: 0.03176735245531453
2023-07-01 00:30:47,557 - INFO - initial test acc: 0.5649999976158142
2023-07-01 00:30:47,565 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.00581049919128418 sec
2023-07-01 00:30:47,674 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11493968963623047 sec
2023-07-01 00:30:47,678 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:30:47,740 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06221365928649902 sec
2023-07-01 00:30:47,740 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:30:48,073 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3328671455383301 sec
2023-07-01 00:30:49,294 - INFO - Distilling data from client: Client25
2023-07-01 00:30:49,294 - INFO - train loss: 0.0038456362734091898
2023-07-01 00:30:49,294 - INFO - train acc: 0.9902534484863281
2023-07-01 00:30:49,354 - INFO - report:               precision    recall  f1-score   support

           2       0.46      0.48      0.47        64
           4       0.60      0.62      0.61        77
           7       0.66      0.59      0.62        59

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-07-01 00:30:49,354 - INFO - test loss 0.02920802826592197
2023-07-01 00:30:49,354 - INFO - test acc 0.5699999928474426
2023-07-01 00:30:50,577 - INFO - Distilling data from client: Client25
2023-07-01 00:30:50,578 - INFO - train loss: 0.0024139143303695456
2023-07-01 00:30:50,578 - INFO - train acc: 0.9961013793945312
2023-07-01 00:30:50,643 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.59      0.56        64
           4       0.67      0.62      0.64        77
           7       0.71      0.68      0.70        59

    accuracy                           0.63       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.64      0.63      0.63       200

2023-07-01 00:30:50,643 - INFO - test loss 0.028855667114821706
2023-07-01 00:30:50,643 - INFO - test acc 0.6299999952316284
2023-07-01 00:30:51,879 - INFO - Distilling data from client: Client25
2023-07-01 00:30:51,880 - INFO - train loss: 0.002016962492681783
2023-07-01 00:30:51,880 - INFO - train acc: 0.9980506896972656
2023-07-01 00:30:51,941 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.61      0.55        64
           4       0.69      0.64      0.66        77
           7       0.75      0.66      0.70        59

    accuracy                           0.64       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.64      0.64       200

2023-07-01 00:30:51,941 - INFO - test loss 0.028923481124260512
2023-07-01 00:30:51,941 - INFO - test acc 0.6349999904632568
2023-07-01 00:30:53,167 - INFO - Distilling data from client: Client25
2023-07-01 00:30:53,167 - INFO - train loss: 0.0017868930114693328
2023-07-01 00:30:53,167 - INFO - train acc: 0.9980506896972656
2023-07-01 00:30:53,190 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.66      0.58        64
           4       0.69      0.60      0.64        77
           7       0.74      0.66      0.70        59

    accuracy                           0.64       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.64      0.64       200

2023-07-01 00:30:53,190 - INFO - test loss 0.029892615122851907
2023-07-01 00:30:53,190 - INFO - test acc 0.6349999904632568
2023-07-01 00:30:54,415 - INFO - Distilling data from client: Client25
2023-07-01 00:30:54,415 - INFO - train loss: 0.0015221695866448654
2023-07-01 00:30:54,415 - INFO - train acc: 1.0
2023-07-01 00:30:54,443 - INFO - report:               precision    recall  f1-score   support

           2       0.49      0.58      0.53        64
           4       0.69      0.64      0.66        77
           7       0.72      0.66      0.69        59

    accuracy                           0.62       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.64      0.62      0.63       200

2023-07-01 00:30:54,443 - INFO - test loss 0.029916420850208028
2023-07-01 00:30:54,443 - INFO - test acc 0.625
2023-07-01 00:30:55,674 - INFO - Distilling data from client: Client25
2023-07-01 00:30:55,674 - INFO - train loss: 0.0013890566735017757
2023-07-01 00:30:55,674 - INFO - train acc: 0.9980506896972656
2023-07-01 00:30:55,738 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.61      0.56        64
           4       0.71      0.64      0.67        77
           7       0.71      0.68      0.70        59

    accuracy                           0.64       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.64      0.64       200

2023-07-01 00:30:55,738 - INFO - test loss 0.029757417886987127
2023-07-01 00:30:55,738 - INFO - test acc 0.6399999856948853
2023-07-01 00:30:56,970 - INFO - Distilling data from client: Client25
2023-07-01 00:30:56,971 - INFO - train loss: 0.001308919664356767
2023-07-01 00:30:56,971 - INFO - train acc: 1.0
2023-07-01 00:30:56,994 - INFO - report:               precision    recall  f1-score   support

           2       0.49      0.61      0.55        64
           4       0.67      0.61      0.64        77
           7       0.78      0.68      0.73        59

    accuracy                           0.63       200
   macro avg       0.65      0.63      0.64       200
weighted avg       0.65      0.63      0.64       200

2023-07-01 00:30:56,994 - INFO - test loss 0.029624309646678823
2023-07-01 00:30:56,994 - INFO - test acc 0.6299999952316284
2023-07-01 00:30:58,219 - INFO - Distilling data from client: Client25
2023-07-01 00:30:58,219 - INFO - train loss: 0.001130029110287809
2023-07-01 00:30:58,219 - INFO - train acc: 1.0
2023-07-01 00:30:58,244 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.55      0.53        64
           4       0.68      0.64      0.66        77
           7       0.70      0.71      0.71        59

    accuracy                           0.63       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.63      0.63       200

2023-07-01 00:30:58,244 - INFO - test loss 0.029489264429513426
2023-07-01 00:30:58,245 - INFO - test acc 0.6299999952316284
2023-07-01 00:30:59,458 - INFO - Distilling data from client: Client25
2023-07-01 00:30:59,459 - INFO - train loss: 0.0012476162915291296
2023-07-01 00:30:59,459 - INFO - train acc: 1.0
2023-07-01 00:30:59,520 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.66      0.59        64
           4       0.72      0.65      0.68        77
           7       0.72      0.64      0.68        59

    accuracy                           0.65       200
   macro avg       0.66      0.65      0.65       200
weighted avg       0.66      0.65      0.65       200

2023-07-01 00:30:59,520 - INFO - test loss 0.02985390305535913
2023-07-01 00:30:59,520 - INFO - test acc 0.6499999761581421
2023-07-01 00:31:00,735 - INFO - Distilling data from client: Client25
2023-07-01 00:31:00,735 - INFO - train loss: 0.0009860969125484252
2023-07-01 00:31:00,735 - INFO - train acc: 1.0
2023-07-01 00:31:00,759 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.58      0.56        64
           4       0.70      0.66      0.68        77
           7       0.71      0.69      0.70        59

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:31:00,759 - INFO - test loss 0.030703631234061897
2023-07-01 00:31:00,759 - INFO - test acc 0.6449999809265137
2023-07-01 00:31:01,980 - INFO - Distilling data from client: Client25
2023-07-01 00:31:01,981 - INFO - train loss: 0.0011668767890043116
2023-07-01 00:31:01,981 - INFO - train acc: 1.0
2023-07-01 00:31:02,008 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.62      0.58        64
           4       0.69      0.62      0.65        77
           7       0.71      0.68      0.70        59

    accuracy                           0.64       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.64      0.64       200

2023-07-01 00:31:02,008 - INFO - test loss 0.03138534303777132
2023-07-01 00:31:02,008 - INFO - test acc 0.6399999856948853
2023-07-01 00:31:03,227 - INFO - Distilling data from client: Client25
2023-07-01 00:31:03,227 - INFO - train loss: 0.0010938488026349994
2023-07-01 00:31:03,227 - INFO - train acc: 1.0
2023-07-01 00:31:03,250 - INFO - report:               precision    recall  f1-score   support

           2       0.50      0.58      0.54        64
           4       0.65      0.61      0.63        77
           7       0.76      0.69      0.73        59

    accuracy                           0.62       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.64      0.62      0.63       200

2023-07-01 00:31:03,250 - INFO - test loss 0.030187474766877095
2023-07-01 00:31:03,250 - INFO - test acc 0.625
2023-07-01 00:31:04,484 - INFO - Distilling data from client: Client25
2023-07-01 00:31:04,484 - INFO - train loss: 0.0009944233899647106
2023-07-01 00:31:04,484 - INFO - train acc: 1.0
2023-07-01 00:31:04,507 - INFO - report:               precision    recall  f1-score   support

           2       0.47      0.56      0.51        64
           4       0.65      0.58      0.62        77
           7       0.72      0.66      0.69        59

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.61       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:31:04,508 - INFO - test loss 0.030751635263922356
2023-07-01 00:31:04,508 - INFO - test acc 0.5999999642372131
2023-07-01 00:31:05,730 - INFO - Distilling data from client: Client25
2023-07-01 00:31:05,730 - INFO - train loss: 0.001097718644467157
2023-07-01 00:31:05,730 - INFO - train acc: 1.0
2023-07-01 00:31:05,753 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.61      0.57        64
           4       0.71      0.68      0.69        77
           7       0.70      0.64      0.67        59

    accuracy                           0.65       200
   macro avg       0.65      0.64      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:31:05,753 - INFO - test loss 0.02965895920328887
2023-07-01 00:31:05,753 - INFO - test acc 0.6449999809265137
2023-07-01 00:31:06,978 - INFO - Distilling data from client: Client25
2023-07-01 00:31:06,978 - INFO - train loss: 0.0009370691387674916
2023-07-01 00:31:06,978 - INFO - train acc: 1.0
2023-07-01 00:31:07,001 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.61      0.56        64
           4       0.67      0.58      0.62        77
           7       0.68      0.66      0.67        59

    accuracy                           0.61       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.62      0.61      0.62       200

2023-07-01 00:31:07,001 - INFO - test loss 0.030871322075701004
2023-07-01 00:31:07,001 - INFO - test acc 0.6150000095367432
2023-07-01 00:31:08,219 - INFO - Distilling data from client: Client25
2023-07-01 00:31:08,219 - INFO - train loss: 0.0008715831034927579
2023-07-01 00:31:08,219 - INFO - train acc: 1.0
2023-07-01 00:31:08,242 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.59      0.55        64
           4       0.67      0.62      0.64        77
           7       0.71      0.66      0.68        59

    accuracy                           0.62       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.62      0.63       200

2023-07-01 00:31:08,242 - INFO - test loss 0.030743372883864632
2023-07-01 00:31:08,242 - INFO - test acc 0.625
2023-07-01 00:31:09,461 - INFO - Distilling data from client: Client25
2023-07-01 00:31:09,461 - INFO - train loss: 0.0007889628426579702
2023-07-01 00:31:09,461 - INFO - train acc: 1.0
2023-07-01 00:31:09,486 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.58      0.55        64
           4       0.69      0.66      0.68        77
           7       0.68      0.64      0.66        59

    accuracy                           0.63       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.63      0.63       200

2023-07-01 00:31:09,487 - INFO - test loss 0.03061790626987505
2023-07-01 00:31:09,487 - INFO - test acc 0.6299999952316284
2023-07-01 00:31:10,710 - INFO - Distilling data from client: Client25
2023-07-01 00:31:10,710 - INFO - train loss: 0.0009204165953219227
2023-07-01 00:31:10,710 - INFO - train acc: 1.0
2023-07-01 00:31:10,734 - INFO - report:               precision    recall  f1-score   support

           2       0.52      0.59      0.55        64
           4       0.69      0.62      0.65        77
           7       0.70      0.68      0.69        59

    accuracy                           0.63       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.64      0.63      0.63       200

2023-07-01 00:31:10,734 - INFO - test loss 0.030535757455173645
2023-07-01 00:31:10,734 - INFO - test acc 0.6299999952316284
2023-07-01 00:31:11,955 - INFO - Distilling data from client: Client25
2023-07-01 00:31:11,955 - INFO - train loss: 0.0007917523118441619
2023-07-01 00:31:11,955 - INFO - train acc: 1.0
2023-07-01 00:31:11,978 - INFO - report:               precision    recall  f1-score   support

           2       0.53      0.62      0.57        64
           4       0.67      0.60      0.63        77
           7       0.71      0.66      0.68        59

    accuracy                           0.62       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.62      0.63       200

2023-07-01 00:31:11,978 - INFO - test loss 0.030508528074603122
2023-07-01 00:31:11,978 - INFO - test acc 0.625
2023-07-01 00:31:13,202 - INFO - Distilling data from client: Client25
2023-07-01 00:31:13,202 - INFO - train loss: 0.0008813250711637474
2023-07-01 00:31:13,202 - INFO - train acc: 1.0
2023-07-01 00:31:13,225 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.56      0.54        64
           4       0.67      0.65      0.66        77
           7       0.71      0.66      0.68        59

    accuracy                           0.62       200
   macro avg       0.63      0.62      0.63       200
weighted avg       0.63      0.62      0.63       200

2023-07-01 00:31:13,225 - INFO - test loss 0.031891680048917674
2023-07-01 00:31:13,225 - INFO - test acc 0.625
2023-07-01 00:31:14,452 - INFO - Distilling data from client: Client25
2023-07-01 00:31:14,452 - INFO - train loss: 0.0007348144630087578
2023-07-01 00:31:14,452 - INFO - train acc: 1.0
2023-07-01 00:31:14,477 - INFO - report:               precision    recall  f1-score   support

           2       0.54      0.62      0.58        64
           4       0.70      0.62      0.66        77
           7       0.68      0.66      0.67        59

    accuracy                           0.64       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:31:14,477 - INFO - test loss 0.030600776045012285
2023-07-01 00:31:14,477 - INFO - test acc 0.6349999904632568
2023-07-01 00:31:15,694 - INFO - Distilling data from client: Client25
2023-07-01 00:31:15,695 - INFO - train loss: 0.0007784903054961388
2023-07-01 00:31:15,695 - INFO - train acc: 1.0
2023-07-01 00:31:15,717 - INFO - report:               precision    recall  f1-score   support

           2       0.50      0.55      0.52        64
           4       0.66      0.62      0.64        77
           7       0.67      0.64      0.66        59

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.61       200
weighted avg       0.61      0.60      0.61       200

2023-07-01 00:31:15,717 - INFO - test loss 0.030693188914289876
2023-07-01 00:31:15,717 - INFO - test acc 0.6049999594688416
2023-07-01 00:31:16,945 - INFO - Distilling data from client: Client25
2023-07-01 00:31:16,945 - INFO - train loss: 0.0007196143676524701
2023-07-01 00:31:16,945 - INFO - train acc: 1.0
2023-07-01 00:31:16,968 - INFO - report:               precision    recall  f1-score   support

           2       0.49      0.58      0.53        64
           4       0.67      0.62      0.64        77
           7       0.74      0.66      0.70        59

    accuracy                           0.62       200
   macro avg       0.63      0.62      0.62       200
weighted avg       0.63      0.62      0.62       200

2023-07-01 00:31:16,969 - INFO - test loss 0.030724290880383986
2023-07-01 00:31:16,969 - INFO - test acc 0.6200000047683716
2023-07-01 00:31:18,191 - INFO - Distilling data from client: Client25
2023-07-01 00:31:18,192 - INFO - train loss: 0.0006874130180367756
2023-07-01 00:31:18,192 - INFO - train acc: 1.0
2023-07-01 00:31:18,216 - INFO - report:               precision    recall  f1-score   support

           2       0.50      0.56      0.53        64
           4       0.67      0.62      0.64        77
           7       0.71      0.68      0.70        59

    accuracy                           0.62       200
   macro avg       0.63      0.62      0.62       200
weighted avg       0.63      0.62      0.62       200

2023-07-01 00:31:18,216 - INFO - test loss 0.0319936199694383
2023-07-01 00:31:18,216 - INFO - test acc 0.6200000047683716
2023-07-01 00:31:19,426 - INFO - Distilling data from client: Client25
2023-07-01 00:31:19,426 - INFO - train loss: 0.0006775170593202771
2023-07-01 00:31:19,426 - INFO - train acc: 1.0
2023-07-01 00:31:19,451 - INFO - report:               precision    recall  f1-score   support

           2       0.51      0.59      0.55        64
           4       0.68      0.58      0.63        77
           7       0.63      0.63      0.63        59

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:31:19,451 - INFO - test loss 0.03106878179763939
2023-07-01 00:31:19,451 - INFO - test acc 0.5999999642372131
2023-07-01 00:31:19,465 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:19,474 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:19,483 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:19,492 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:19,500 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:19,509 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:19,519 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:19,528 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:19,537 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:19,909 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client25//synthetic.png
2023-07-01 00:31:19,922 - INFO - c: 5.0 and total_data_in_this_class: 26
2023-07-01 00:31:19,922 - INFO - c: 6.0 and total_data_in_this_class: 246
2023-07-01 00:31:19,922 - INFO - c: 8.0 and total_data_in_this_class: 527
2023-07-01 00:31:19,922 - INFO - c: 5.0 and total_data_in_this_class: 4
2023-07-01 00:31:19,922 - INFO - c: 6.0 and total_data_in_this_class: 57
2023-07-01 00:31:19,922 - INFO - c: 8.0 and total_data_in_this_class: 139
2023-07-01 00:31:19,940 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.000255584716796875 sec
2023-07-01 00:31:19,940 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:31:19,942 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012586116790771484 sec
2023-07-01 00:31:19,942 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:31:19,953 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010807991027832031 sec
2023-07-01 00:31:19,955 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00023937225341796875 sec
2023-07-01 00:31:19,955 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:31:19,956 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009493827819824219 sec
2023-07-01 00:31:19,956 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:31:19,965 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008763551712036133 sec
2023-07-01 00:31:19,968 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001380443572998047 sec
2023-07-01 00:31:19,970 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001232624053955078 sec
2023-07-01 00:31:19,970 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003204345703125 sec
2023-07-01 00:31:19,972 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00022983551025390625 sec
2023-07-01 00:31:19,972 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011849403381347656 sec
2023-07-01 00:31:19,973 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00027561187744140625 sec
2023-07-01 00:31:19,973 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023818016052246094 sec
2023-07-01 00:31:19,974 - WARNING - Finished tracing + transforming absolute for pjit in 0.00016689300537109375 sec
2023-07-01 00:31:19,974 - WARNING - Finished tracing + transforming fn for pjit in 0.00029397010803222656 sec
2023-07-01 00:31:19,975 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00032782554626464844 sec
2023-07-01 00:31:19,976 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020575523376464844 sec
2023-07-01 00:31:19,977 - WARNING - Finished tracing + transforming fn for pjit in 0.00023174285888671875 sec
2023-07-01 00:31:19,978 - WARNING - Finished tracing + transforming fn for pjit in 0.00026702880859375 sec
2023-07-01 00:31:19,978 - WARNING - Finished tracing + transforming fn for pjit in 0.00022864341735839844 sec
2023-07-01 00:31:19,979 - WARNING - Finished tracing + transforming fn for pjit in 0.0002593994140625 sec
2023-07-01 00:31:19,980 - WARNING - Finished tracing + transforming fn for pjit in 0.00023293495178222656 sec
2023-07-01 00:31:19,982 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00018715858459472656 sec
2023-07-01 00:31:19,983 - WARNING - Finished tracing + transforming fn for pjit in 0.00023484230041503906 sec
2023-07-01 00:31:19,984 - WARNING - Finished tracing + transforming fn for pjit in 0.00022935867309570312 sec
2023-07-01 00:31:19,987 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003790855407714844 sec
2023-07-01 00:31:19,988 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009865760803222656 sec
2023-07-01 00:31:19,989 - WARNING - Finished tracing + transforming fn for pjit in 0.0002315044403076172 sec
2023-07-01 00:31:19,989 - WARNING - Finished tracing + transforming fn for pjit in 0.0002262592315673828 sec
2023-07-01 00:31:19,990 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029778480529785156 sec
2023-07-01 00:31:19,991 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002663135528564453 sec
2023-07-01 00:31:19,992 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00018072128295898438 sec
2023-07-01 00:31:19,992 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002627372741699219 sec
2023-07-01 00:31:19,993 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022292137145996094 sec
2023-07-01 00:31:19,994 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022554397583007812 sec
2023-07-01 00:31:19,995 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.000362396240234375 sec
2023-07-01 00:31:19,995 - WARNING - Finished tracing + transforming _where for pjit in 0.0009815692901611328 sec
2023-07-01 00:31:19,996 - WARNING - Finished tracing + transforming fn for pjit in 0.00026535987854003906 sec
2023-07-01 00:31:19,996 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025725364685058594 sec
2023-07-01 00:31:19,997 - WARNING - Finished tracing + transforming fn for pjit in 0.00022101402282714844 sec
2023-07-01 00:31:19,998 - WARNING - Finished tracing + transforming fn for pjit in 0.0002205371856689453 sec
2023-07-01 00:31:19,999 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022077560424804688 sec
2023-07-01 00:31:19,999 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002570152282714844 sec
2023-07-01 00:31:20,000 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025582313537597656 sec
2023-07-01 00:31:20,001 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002598762512207031 sec
2023-07-01 00:31:20,001 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022149085998535156 sec
2023-07-01 00:31:20,002 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002219676971435547 sec
2023-07-01 00:31:20,003 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00027751922607421875 sec
2023-07-01 00:31:20,003 - WARNING - Finished tracing + transforming _where for pjit in 0.0008728504180908203 sec
2023-07-01 00:31:20,004 - WARNING - Finished tracing + transforming fn for pjit in 0.0002579689025878906 sec
2023-07-01 00:31:20,005 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002627372741699219 sec
2023-07-01 00:31:20,007 - WARNING - Finished tracing + transforming fn for pjit in 0.00022459030151367188 sec
2023-07-01 00:31:20,011 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002682209014892578 sec
2023-07-01 00:31:20,011 - WARNING - Finished tracing + transforming fn for pjit in 0.00034356117248535156 sec
2023-07-01 00:31:20,012 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026702880859375 sec
2023-07-01 00:31:20,013 - WARNING - Finished tracing + transforming fn for pjit in 0.00023484230041503906 sec
2023-07-01 00:31:20,016 - WARNING - Finished tracing + transforming fn for pjit in 0.00021958351135253906 sec
2023-07-01 00:31:20,018 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00017189979553222656 sec
2023-07-01 00:31:20,019 - WARNING - Finished tracing + transforming fn for pjit in 0.00030922889709472656 sec
2023-07-01 00:31:20,020 - WARNING - Finished tracing + transforming fn for pjit in 0.0002300739288330078 sec
2023-07-01 00:31:20,038 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06947803497314453 sec
2023-07-01 00:31:20,041 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012564659118652344 sec
2023-07-01 00:31:20,041 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00010943412780761719 sec
2023-07-01 00:31:20,042 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000263214111328125 sec
2023-07-01 00:31:20,044 - WARNING - Finished tracing + transforming fn for pjit in 0.0002167224884033203 sec
2023-07-01 00:31:20,044 - WARNING - Finished tracing + transforming fn for pjit in 0.0002446174621582031 sec
2023-07-01 00:31:20,046 - WARNING - Finished tracing + transforming fn for pjit in 0.0002200603485107422 sec
2023-07-01 00:31:20,051 - WARNING - Finished tracing + transforming fn for pjit in 0.00022864341735839844 sec
2023-07-01 00:31:20,053 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002181529998779297 sec
2023-07-01 00:31:20,053 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002701282501220703 sec
2023-07-01 00:31:20,054 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00018143653869628906 sec
2023-07-01 00:31:20,055 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032782554626464844 sec
2023-07-01 00:31:20,056 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022482872009277344 sec
2023-07-01 00:31:20,056 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025200843811035156 sec
2023-07-01 00:31:20,057 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002994537353515625 sec
2023-07-01 00:31:20,058 - WARNING - Finished tracing + transforming _where for pjit in 0.0009801387786865234 sec
2023-07-01 00:31:20,059 - WARNING - Finished tracing + transforming fn for pjit in 0.0002677440643310547 sec
2023-07-01 00:31:20,059 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002651214599609375 sec
2023-07-01 00:31:20,060 - WARNING - Finished tracing + transforming fn for pjit in 0.0002295970916748047 sec
2023-07-01 00:31:20,061 - WARNING - Finished tracing + transforming fn for pjit in 0.00029587745666503906 sec
2023-07-01 00:31:20,073 - WARNING - Finished tracing + transforming fn for pjit in 0.0002231597900390625 sec
2023-07-01 00:31:20,093 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05482673645019531 sec
2023-07-01 00:31:20,095 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001609325408935547 sec
2023-07-01 00:31:20,096 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0001354217529296875 sec
2023-07-01 00:31:20,096 - WARNING - Finished tracing + transforming _where for pjit in 0.0006737709045410156 sec
2023-07-01 00:31:20,097 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003008842468261719 sec
2023-07-01 00:31:20,097 - WARNING - Finished tracing + transforming trace for pjit in 0.00275421142578125 sec
2023-07-01 00:31:20,100 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010585784912109375 sec
2023-07-01 00:31:20,101 - WARNING - Finished tracing + transforming tril for pjit in 0.0006816387176513672 sec
2023-07-01 00:31:20,101 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0019335746765136719 sec
2023-07-01 00:31:20,102 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010704994201660156 sec
2023-07-01 00:31:20,102 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00011086463928222656 sec
2023-07-01 00:31:20,104 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014259815216064453 sec
2023-07-01 00:31:20,108 - WARNING - Finished tracing + transforming _solve for pjit in 0.009329557418823242 sec
2023-07-01 00:31:20,109 - WARNING - Finished tracing + transforming dot for pjit in 0.0002956390380859375 sec
2023-07-01 00:31:20,111 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.14445900917053223 sec
2023-07-01 00:31:20,113 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:31:20,146 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03265810012817383 sec
2023-07-01 00:31:20,146 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:31:20,250 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.10374259948730469 sec
2023-07-01 00:31:20,254 - INFO - initial test loss: 0.023396245768358758
2023-07-01 00:31:20,255 - INFO - initial test acc: 0.6649999618530273
2023-07-01 00:31:20,259 - WARNING - Finished tracing + transforming dot for pjit in 0.00034618377685546875 sec
2023-07-01 00:31:20,260 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002739429473876953 sec
2023-07-01 00:31:20,261 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003781318664550781 sec
2023-07-01 00:31:20,261 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009922981262207031 sec
2023-07-01 00:31:20,262 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00020003318786621094 sec
2023-07-01 00:31:20,263 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00018358230590820312 sec
2023-07-01 00:31:20,263 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024199485778808594 sec
2023-07-01 00:31:20,264 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003762245178222656 sec
2023-07-01 00:31:20,265 - WARNING - Finished tracing + transforming _mean for pjit in 0.00110626220703125 sec
2023-07-01 00:31:20,265 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009740591049194336 sec
2023-07-01 00:31:20,275 - WARNING - Finished tracing + transforming fn for pjit in 0.0008885860443115234 sec
2023-07-01 00:31:20,275 - WARNING - Finished tracing + transforming fn for pjit in 0.0002655982971191406 sec
2023-07-01 00:31:20,276 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021648406982421875 sec
2023-07-01 00:31:20,277 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002722740173339844 sec
2023-07-01 00:31:20,277 - WARNING - Finished tracing + transforming _where for pjit in 0.0008807182312011719 sec
2023-07-01 00:31:20,285 - WARNING - Finished tracing + transforming fn for pjit in 0.00024819374084472656 sec
2023-07-01 00:31:20,286 - WARNING - Finished tracing + transforming fn for pjit in 0.0002593994140625 sec
2023-07-01 00:31:20,286 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002110004425048828 sec
2023-07-01 00:31:20,287 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002472400665283203 sec
2023-07-01 00:31:20,288 - WARNING - Finished tracing + transforming _where for pjit in 0.0008318424224853516 sec
2023-07-01 00:31:20,321 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021719932556152344 sec
2023-07-01 00:31:20,375 - WARNING - Finished tracing + transforming fn for pjit in 0.0002751350402832031 sec
2023-07-01 00:31:20,376 - WARNING - Finished tracing + transforming fn for pjit in 0.00022482872009277344 sec
2023-07-01 00:31:20,377 - WARNING - Finished tracing + transforming square for pjit in 0.00016689300537109375 sec
2023-07-01 00:31:20,379 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000217437744140625 sec
2023-07-01 00:31:20,380 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002455711364746094 sec
2023-07-01 00:31:20,381 - WARNING - Finished tracing + transforming fn for pjit in 0.0002734661102294922 sec
2023-07-01 00:31:20,382 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002200603485107422 sec
2023-07-01 00:31:20,382 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002193450927734375 sec
2023-07-01 00:31:20,383 - WARNING - Finished tracing + transforming fn for pjit in 0.0002636909484863281 sec
2023-07-01 00:31:20,384 - WARNING - Finished tracing + transforming fn for pjit in 0.0002181529998779297 sec
2023-07-01 00:31:20,384 - WARNING - Finished tracing + transforming square for pjit in 0.00016188621520996094 sec
2023-07-01 00:31:20,386 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002162456512451172 sec
2023-07-01 00:31:20,388 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017333030700683594 sec
2023-07-01 00:31:20,388 - WARNING - Finished tracing + transforming fn for pjit in 0.0002665519714355469 sec
2023-07-01 00:31:20,389 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002205371856689453 sec
2023-07-01 00:31:20,389 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021266937255859375 sec
2023-07-01 00:31:20,390 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13492250442504883 sec
2023-07-01 00:31:20,394 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,10]), ShapedArray(float32[51,10]), ShapedArray(float32[51,10]), ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,10]), ShapedArray(float32[51,3,32,32]), ShapedArray(float32[51,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:31:20,456 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06171584129333496 sec
2023-07-01 00:31:20,456 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:31:20,729 - WARNING - Finished XLA compilation of jit(update_fn) in 0.2729167938232422 sec
2023-07-01 00:31:20,799 - INFO - Distilling data from client: Client26
2023-07-01 00:31:20,799 - INFO - train loss: 0.012904934646620762
2023-07-01 00:31:20,799 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:20,810 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.73      0.89      0.80        57
           8       0.97      0.86      0.91       139

    accuracy                           0.85       200
   macro avg       0.57      0.59      0.57       200
weighted avg       0.88      0.85      0.86       200

2023-07-01 00:31:20,811 - INFO - test loss 0.015117193823288265
2023-07-01 00:31:20,811 - INFO - test acc 0.8549999594688416
2023-07-01 00:31:20,880 - INFO - Distilling data from client: Client26
2023-07-01 00:31:20,880 - INFO - train loss: 0.008967877073622304
2023-07-01 00:31:20,880 - INFO - train acc: 0.9215686321258545
2023-07-01 00:31:20,889 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.72      0.89      0.80        57
           8       0.96      0.87      0.91       139

    accuracy                           0.86       200
   macro avg       0.56      0.59      0.57       200
weighted avg       0.87      0.86      0.86       200

2023-07-01 00:31:20,889 - INFO - test loss 0.013951253630725954
2023-07-01 00:31:20,889 - INFO - test acc 0.85999995470047
2023-07-01 00:31:20,962 - INFO - Distilling data from client: Client26
2023-07-01 00:31:20,962 - INFO - train loss: 0.009817210174274919
2023-07-01 00:31:20,962 - INFO - train acc: 0.9411765336990356
2023-07-01 00:31:20,971 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.76      0.88      0.81        57
           8       0.96      0.89      0.93       139

    accuracy                           0.87       200
   macro avg       0.57      0.59      0.58       200
weighted avg       0.88      0.87      0.87       200

2023-07-01 00:31:20,971 - INFO - test loss 0.014194614965884866
2023-07-01 00:31:20,971 - INFO - test acc 0.8700000047683716
2023-07-01 00:31:21,041 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,041 - INFO - train loss: 0.008354273901287693
2023-07-01 00:31:21,041 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:21,050 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.83      0.86      0.84        57
           8       0.94      0.93      0.93       139

    accuracy                           0.89       200
   macro avg       0.59      0.60      0.59       200
weighted avg       0.89      0.89      0.89       200

2023-07-01 00:31:21,050 - INFO - test loss 0.013091809463224448
2023-07-01 00:31:21,050 - INFO - test acc 0.8899999856948853
2023-07-01 00:31:21,126 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,127 - INFO - train loss: 0.006087414069273519
2023-07-01 00:31:21,127 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:21,132 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.77      0.89      0.83        57
           8       0.95      0.91      0.93       139

    accuracy                           0.89       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.88      0.89      0.88       200

2023-07-01 00:31:21,132 - INFO - test loss 0.013101233253585847
2023-07-01 00:31:21,132 - INFO - test acc 0.8849999904632568
2023-07-01 00:31:21,204 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,204 - INFO - train loss: 0.007006486861144349
2023-07-01 00:31:21,204 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:21,210 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.77      0.86      0.81        57
           8       0.94      0.90      0.92       139

    accuracy                           0.87       200
   macro avg       0.57      0.59      0.58       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:31:21,210 - INFO - test loss 0.012990850939606698
2023-07-01 00:31:21,210 - INFO - test acc 0.8700000047683716
2023-07-01 00:31:21,282 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,282 - INFO - train loss: 0.00795880877589476
2023-07-01 00:31:21,282 - INFO - train acc: 0.960784375667572
2023-07-01 00:31:21,288 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.77      0.89      0.83        57
           8       0.96      0.90      0.93       139

    accuracy                           0.88       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.89      0.88      0.88       200

2023-07-01 00:31:21,288 - INFO - test loss 0.01349065686822933
2023-07-01 00:31:21,288 - INFO - test acc 0.8799999952316284
2023-07-01 00:31:21,362 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,362 - INFO - train loss: 0.006724519162811153
2023-07-01 00:31:21,362 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:21,367 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.83      0.84      0.83        57
           8       0.95      0.94      0.94       139

    accuracy                           0.89       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.90      0.89      0.89       200

2023-07-01 00:31:21,368 - INFO - test loss 0.012485299409761421
2023-07-01 00:31:21,368 - INFO - test acc 0.8899999856948853
2023-07-01 00:31:21,441 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,441 - INFO - train loss: 0.007144381570062968
2023-07-01 00:31:21,441 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:21,446 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.75      0.91      0.83        57
           8       0.95      0.88      0.92       139

    accuracy                           0.88       200
   macro avg       0.57      0.60      0.58       200
weighted avg       0.88      0.88      0.87       200

2023-07-01 00:31:21,446 - INFO - test loss 0.012893155413707648
2023-07-01 00:31:21,446 - INFO - test acc 0.875
2023-07-01 00:31:21,518 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,518 - INFO - train loss: 0.007195021865156912
2023-07-01 00:31:21,518 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:21,523 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.78      0.88      0.83        57
           8       0.94      0.91      0.92       139

    accuracy                           0.88       200
   macro avg       0.57      0.59      0.58       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:31:21,523 - INFO - test loss 0.01230928679165119
2023-07-01 00:31:21,523 - INFO - test acc 0.8799999952316284
2023-07-01 00:31:21,595 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,595 - INFO - train loss: 0.00774687881402867
2023-07-01 00:31:21,595 - INFO - train acc: 0.960784375667572
2023-07-01 00:31:21,600 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.80      0.82      0.81        57
           8       0.93      0.91      0.92       139

    accuracy                           0.87       200
   macro avg       0.57      0.58      0.58       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:31:21,600 - INFO - test loss 0.01358330244856111
2023-07-01 00:31:21,600 - INFO - test acc 0.8700000047683716
2023-07-01 00:31:21,674 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,674 - INFO - train loss: 0.0061577363372204335
2023-07-01 00:31:21,674 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:21,680 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.80      0.84      0.82        57
           8       0.95      0.91      0.93       139

    accuracy                           0.87       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.89      0.87      0.88       200

2023-07-01 00:31:21,680 - INFO - test loss 0.013859019936023953
2023-07-01 00:31:21,681 - INFO - test acc 0.8700000047683716
2023-07-01 00:31:21,756 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,756 - INFO - train loss: 0.004047588781535689
2023-07-01 00:31:21,756 - INFO - train acc: 1.0
2023-07-01 00:31:21,761 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.79      0.88      0.83        57
           8       0.96      0.92      0.94       139

    accuracy                           0.89       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.89      0.89      0.89       200

2023-07-01 00:31:21,761 - INFO - test loss 0.012872696516720513
2023-07-01 00:31:21,761 - INFO - test acc 0.8899999856948853
2023-07-01 00:31:21,832 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,832 - INFO - train loss: 0.004916999885674273
2023-07-01 00:31:21,833 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:21,838 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.78      0.88      0.83        57
           8       0.95      0.91      0.93       139

    accuracy                           0.88       200
   macro avg       0.58      0.59      0.59       200
weighted avg       0.89      0.88      0.88       200

2023-07-01 00:31:21,838 - INFO - test loss 0.013766090124483675
2023-07-01 00:31:21,838 - INFO - test acc 0.8799999952316284
2023-07-01 00:31:21,912 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,912 - INFO - train loss: 0.004720020564787121
2023-07-01 00:31:21,912 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:21,917 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.79      0.86      0.82        57
           8       0.94      0.91      0.92       139

    accuracy                           0.88       200
   macro avg       0.58      0.59      0.58       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:31:21,917 - INFO - test loss 0.013044160714607466
2023-07-01 00:31:21,917 - INFO - test acc 0.875
2023-07-01 00:31:21,984 - INFO - Distilling data from client: Client26
2023-07-01 00:31:21,984 - INFO - train loss: 0.006012696986492268
2023-07-01 00:31:21,984 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:21,989 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.77      0.88      0.82        57
           8       0.94      0.91      0.93       139

    accuracy                           0.89       200
   macro avg       0.57      0.60      0.58       200
weighted avg       0.87      0.89      0.88       200

2023-07-01 00:31:21,989 - INFO - test loss 0.01284005901432082
2023-07-01 00:31:21,989 - INFO - test acc 0.8849999904632568
2023-07-01 00:31:22,064 - INFO - Distilling data from client: Client26
2023-07-01 00:31:22,064 - INFO - train loss: 0.005397190210297476
2023-07-01 00:31:22,064 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:22,070 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.75      0.91      0.83        57
           8       0.95      0.88      0.92       139

    accuracy                           0.88       200
   macro avg       0.57      0.60      0.58       200
weighted avg       0.88      0.88      0.87       200

2023-07-01 00:31:22,070 - INFO - test loss 0.012558633524499077
2023-07-01 00:31:22,070 - INFO - test acc 0.875
2023-07-01 00:31:22,142 - INFO - Distilling data from client: Client26
2023-07-01 00:31:22,142 - INFO - train loss: 0.00421100118433497
2023-07-01 00:31:22,142 - INFO - train acc: 1.0
2023-07-01 00:31:22,147 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.79      0.88      0.83        57
           8       0.95      0.91      0.93       139

    accuracy                           0.89       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.89      0.89      0.89       200

2023-07-01 00:31:22,147 - INFO - test loss 0.012646061573128947
2023-07-01 00:31:22,147 - INFO - test acc 0.8849999904632568
2023-07-01 00:31:22,215 - INFO - Distilling data from client: Client26
2023-07-01 00:31:22,215 - INFO - train loss: 0.005123297897280627
2023-07-01 00:31:22,215 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:22,224 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.80      0.86      0.83        57
           8       0.95      0.94      0.94       139

    accuracy                           0.90       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.89      0.90      0.89       200

2023-07-01 00:31:22,224 - INFO - test loss 0.013016167267658382
2023-07-01 00:31:22,224 - INFO - test acc 0.8949999809265137
2023-07-01 00:31:22,294 - INFO - Distilling data from client: Client26
2023-07-01 00:31:22,295 - INFO - train loss: 0.007159875432483761
2023-07-01 00:31:22,295 - INFO - train acc: 0.9411765336990356
2023-07-01 00:31:22,300 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.80      0.89      0.84        57
           8       0.96      0.92      0.94       139

    accuracy                           0.90       200
   macro avg       0.59      0.61      0.59       200
weighted avg       0.90      0.90      0.89       200

2023-07-01 00:31:22,300 - INFO - test loss 0.012201042155758197
2023-07-01 00:31:22,300 - INFO - test acc 0.8949999809265137
2023-07-01 00:31:22,374 - INFO - Distilling data from client: Client26
2023-07-01 00:31:22,374 - INFO - train loss: 0.004790854078296964
2023-07-01 00:31:22,374 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:22,380 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.74      0.88      0.80        57
           8       0.94      0.89      0.92       139

    accuracy                           0.87       200
   macro avg       0.56      0.59      0.57       200
weighted avg       0.86      0.87      0.86       200

2023-07-01 00:31:22,380 - INFO - test loss 0.012969998101787422
2023-07-01 00:31:22,380 - INFO - test acc 0.8700000047683716
2023-07-01 00:31:22,449 - INFO - Distilling data from client: Client26
2023-07-01 00:31:22,450 - INFO - train loss: 0.005560169381705196
2023-07-01 00:31:22,450 - INFO - train acc: 0.960784375667572
2023-07-01 00:31:22,455 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.83      0.86      0.84        57
           8       0.94      0.94      0.94       139

    accuracy                           0.90       200
   macro avg       0.59      0.60      0.59       200
weighted avg       0.89      0.90      0.89       200

2023-07-01 00:31:22,455 - INFO - test loss 0.012098058687488955
2023-07-01 00:31:22,455 - INFO - test acc 0.8949999809265137
2023-07-01 00:31:22,524 - INFO - Distilling data from client: Client26
2023-07-01 00:31:22,524 - INFO - train loss: 0.0036720776677583913
2023-07-01 00:31:22,524 - INFO - train acc: 1.0
2023-07-01 00:31:22,529 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.78      0.89      0.84        57
           8       0.95      0.91      0.93       139

    accuracy                           0.89       200
   macro avg       0.58      0.60      0.59       200
weighted avg       0.89      0.89      0.89       200

2023-07-01 00:31:22,529 - INFO - test loss 0.01230024790806382
2023-07-01 00:31:22,529 - INFO - test acc 0.8899999856948853
2023-07-01 00:31:22,603 - INFO - Distilling data from client: Client26
2023-07-01 00:31:22,604 - INFO - train loss: 0.003553424021713247
2023-07-01 00:31:22,604 - INFO - train acc: 0.9803922176361084
2023-07-01 00:31:22,610 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.76      0.88      0.81        57
           8       0.95      0.90      0.92       139

    accuracy                           0.88       200
   macro avg       0.57      0.59      0.58       200
weighted avg       0.87      0.88      0.87       200

2023-07-01 00:31:22,610 - INFO - test loss 0.01273711041106951
2023-07-01 00:31:22,610 - INFO - test acc 0.875
2023-07-01 00:31:22,683 - INFO - Distilling data from client: Client26
2023-07-01 00:31:22,683 - INFO - train loss: 0.004624878202116097
2023-07-01 00:31:22,683 - INFO - train acc: 0.960784375667572
2023-07-01 00:31:22,688 - INFO - report:               precision    recall  f1-score   support

           5       0.00      0.00      0.00         4
           6       0.77      0.86      0.81        57
           8       0.95      0.91      0.93       139

    accuracy                           0.88       200
   macro avg       0.57      0.59      0.58       200
weighted avg       0.88      0.88      0.87       200

2023-07-01 00:31:22,688 - INFO - test loss 0.012672305629795851
2023-07-01 00:31:22,688 - INFO - test acc 0.875
2023-07-01 00:31:22,690 - WARNING - Finished tracing + transforming jit(gather) in 0.00023674964904785156 sec
2023-07-01 00:31:22,690 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[51,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:31:22,692 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0011930465698242188 sec
2023-07-01 00:31:22,692 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:31:22,702 - WARNING - Finished XLA compilation of jit(gather) in 0.009962320327758789 sec
2023-07-01 00:31:22,713 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:22,722 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:22,731 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:22,739 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:22,749 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:22,758 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:23,042 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client26//synthetic.png
2023-07-01 00:31:23,054 - INFO - c: 3.0 and total_data_in_this_class: 531
2023-07-01 00:31:23,054 - INFO - c: 4.0 and total_data_in_this_class: 268
2023-07-01 00:31:23,054 - INFO - c: 3.0 and total_data_in_this_class: 135
2023-07-01 00:31:23,054 - INFO - c: 4.0 and total_data_in_this_class: 65
2023-07-01 00:31:23,125 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04825186729431152 sec
2023-07-01 00:31:23,170 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04487276077270508 sec
2023-07-01 00:31:23,175 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.1001439094543457 sec
2023-07-01 00:31:23,177 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:31:23,210 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03263258934020996 sec
2023-07-01 00:31:23,210 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:31:23,335 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12453126907348633 sec
2023-07-01 00:31:23,353 - INFO - initial test loss: 0.023536299380305204
2023-07-01 00:31:23,353 - INFO - initial test acc: 0.7049999833106995
2023-07-01 00:31:23,362 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.0064241886138916016 sec
2023-07-01 00:31:23,469 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11470675468444824 sec
2023-07-01 00:31:23,473 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10]), ShapedArray(float32[358,3,32,32]), ShapedArray(float32[358,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:31:23,535 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06188702583312988 sec
2023-07-01 00:31:23,535 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:31:23,865 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3296844959259033 sec
2023-07-01 00:31:24,615 - INFO - Distilling data from client: Client27
2023-07-01 00:31:24,616 - INFO - train loss: 0.005287578130156827
2023-07-01 00:31:24,616 - INFO - train acc: 0.9720669984817505
2023-07-01 00:31:24,658 - INFO - report:               precision    recall  f1-score   support

           3       0.84      0.84      0.84       135
           4       0.67      0.66      0.67        65

    accuracy                           0.79       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.78      0.79      0.78       200

2023-07-01 00:31:24,658 - INFO - test loss 0.016583086936523543
2023-07-01 00:31:24,658 - INFO - test acc 0.7849999666213989
2023-07-01 00:31:25,399 - INFO - Distilling data from client: Client27
2023-07-01 00:31:25,400 - INFO - train loss: 0.003826067033811089
2023-07-01 00:31:25,400 - INFO - train acc: 0.9832401871681213
2023-07-01 00:31:25,417 - INFO - report:               precision    recall  f1-score   support

           3       0.82      0.83      0.83       135
           4       0.64      0.63      0.64        65

    accuracy                           0.77       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.76      0.77      0.76       200

2023-07-01 00:31:25,418 - INFO - test loss 0.017386508491588133
2023-07-01 00:31:25,418 - INFO - test acc 0.7649999856948853
2023-07-01 00:31:26,161 - INFO - Distilling data from client: Client27
2023-07-01 00:31:26,161 - INFO - train loss: 0.0035405033735380295
2023-07-01 00:31:26,161 - INFO - train acc: 0.9804468750953674
2023-07-01 00:31:26,178 - INFO - report:               precision    recall  f1-score   support

           3       0.82      0.86      0.84       135
           4       0.67      0.60      0.63        65

    accuracy                           0.78       200
   macro avg       0.74      0.73      0.74       200
weighted avg       0.77      0.78      0.77       200

2023-07-01 00:31:26,179 - INFO - test loss 0.017657386822175873
2023-07-01 00:31:26,179 - INFO - test acc 0.7749999761581421
2023-07-01 00:31:26,931 - INFO - Distilling data from client: Client27
2023-07-01 00:31:26,932 - INFO - train loss: 0.0034450581682900477
2023-07-01 00:31:26,932 - INFO - train acc: 0.9832401871681213
2023-07-01 00:31:26,950 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.60      0.60      0.60        65

    accuracy                           0.74       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:31:26,951 - INFO - test loss 0.017689932121313656
2023-07-01 00:31:26,951 - INFO - test acc 0.7400000095367432
2023-07-01 00:31:27,706 - INFO - Distilling data from client: Client27
2023-07-01 00:31:27,706 - INFO - train loss: 0.0029721720828936823
2023-07-01 00:31:27,706 - INFO - train acc: 0.9972066879272461
2023-07-01 00:31:27,725 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.82      0.81       135
           4       0.61      0.58      0.60        65

    accuracy                           0.74       200
   macro avg       0.71      0.70      0.71       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:31:27,725 - INFO - test loss 0.01827677875414165
2023-07-01 00:31:27,725 - INFO - test acc 0.7450000047683716
2023-07-01 00:31:28,484 - INFO - Distilling data from client: Client27
2023-07-01 00:31:28,485 - INFO - train loss: 0.0026546501843784003
2023-07-01 00:31:28,485 - INFO - train acc: 0.9916200637817383
2023-07-01 00:31:28,502 - INFO - report:               precision    recall  f1-score   support

           3       0.84      0.83      0.83       135
           4       0.65      0.66      0.66        65

    accuracy                           0.78       200
   macro avg       0.74      0.75      0.74       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:31:28,502 - INFO - test loss 0.017694393900604836
2023-07-01 00:31:28,502 - INFO - test acc 0.7749999761581421
2023-07-01 00:31:29,255 - INFO - Distilling data from client: Client27
2023-07-01 00:31:29,255 - INFO - train loss: 0.002380226468665736
2023-07-01 00:31:29,255 - INFO - train acc: 0.9972066879272461
2023-07-01 00:31:29,274 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.83      0.82       135
           4       0.63      0.60      0.61        65

    accuracy                           0.76       200
   macro avg       0.72      0.71      0.72       200
weighted avg       0.75      0.76      0.75       200

2023-07-01 00:31:29,274 - INFO - test loss 0.01775463058766722
2023-07-01 00:31:29,274 - INFO - test acc 0.7549999952316284
2023-07-01 00:31:30,028 - INFO - Distilling data from client: Client27
2023-07-01 00:31:30,028 - INFO - train loss: 0.002656216951393129
2023-07-01 00:31:30,028 - INFO - train acc: 0.9916200637817383
2023-07-01 00:31:30,045 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.62      0.62      0.62        65

    accuracy                           0.75       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.75      0.75      0.75       200

2023-07-01 00:31:30,045 - INFO - test loss 0.018528907094932698
2023-07-01 00:31:30,045 - INFO - test acc 0.75
2023-07-01 00:31:30,794 - INFO - Distilling data from client: Client27
2023-07-01 00:31:30,794 - INFO - train loss: 0.002320813227814892
2023-07-01 00:31:30,795 - INFO - train acc: 1.0
2023-07-01 00:31:30,811 - INFO - report:               precision    recall  f1-score   support

           3       0.85      0.79      0.82       135
           4       0.61      0.71      0.66        65

    accuracy                           0.76       200
   macro avg       0.73      0.75      0.74       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:31:30,812 - INFO - test loss 0.01802456158013805
2023-07-01 00:31:30,812 - INFO - test acc 0.7599999904632568
2023-07-01 00:31:31,564 - INFO - Distilling data from client: Client27
2023-07-01 00:31:31,564 - INFO - train loss: 0.0025691267993161794
2023-07-01 00:31:31,564 - INFO - train acc: 0.9916200637817383
2023-07-01 00:31:31,581 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.60      0.60      0.60        65

    accuracy                           0.74       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:31:31,581 - INFO - test loss 0.01855281595328873
2023-07-01 00:31:31,581 - INFO - test acc 0.7400000095367432
2023-07-01 00:31:32,331 - INFO - Distilling data from client: Client27
2023-07-01 00:31:32,331 - INFO - train loss: 0.0025418039577339477
2023-07-01 00:31:32,331 - INFO - train acc: 0.9944133758544922
2023-07-01 00:31:32,350 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.80      0.81       135
           4       0.60      0.62      0.61        65

    accuracy                           0.74       200
   macro avg       0.70      0.71      0.71       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:31:32,350 - INFO - test loss 0.01818758359815645
2023-07-01 00:31:32,350 - INFO - test acc 0.7400000095367432
2023-07-01 00:31:33,101 - INFO - Distilling data from client: Client27
2023-07-01 00:31:33,101 - INFO - train loss: 0.0024054917550209837
2023-07-01 00:31:33,101 - INFO - train acc: 1.0
2023-07-01 00:31:33,118 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.61      0.62      0.61        65

    accuracy                           0.74       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.75      0.74      0.75       200

2023-07-01 00:31:33,118 - INFO - test loss 0.01788506179053314
2023-07-01 00:31:33,118 - INFO - test acc 0.7450000047683716
2023-07-01 00:31:33,867 - INFO - Distilling data from client: Client27
2023-07-01 00:31:33,867 - INFO - train loss: 0.002899422647635169
2023-07-01 00:31:33,867 - INFO - train acc: 0.9888268113136292
2023-07-01 00:31:33,885 - INFO - report:               precision    recall  f1-score   support

           3       0.83      0.84      0.84       135
           4       0.67      0.65      0.66        65

    accuracy                           0.78       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:31:33,885 - INFO - test loss 0.017672815928240077
2023-07-01 00:31:33,885 - INFO - test acc 0.7799999713897705
2023-07-01 00:31:34,632 - INFO - Distilling data from client: Client27
2023-07-01 00:31:34,632 - INFO - train loss: 0.002638227644633711
2023-07-01 00:31:34,632 - INFO - train acc: 0.9972066879272461
2023-07-01 00:31:34,649 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.82      0.82       135
           4       0.62      0.60      0.61        65

    accuracy                           0.75       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.75      0.75      0.75       200

2023-07-01 00:31:34,649 - INFO - test loss 0.018610979549478438
2023-07-01 00:31:34,649 - INFO - test acc 0.75
2023-07-01 00:31:35,389 - INFO - Distilling data from client: Client27
2023-07-01 00:31:35,389 - INFO - train loss: 0.0023251758295553403
2023-07-01 00:31:35,389 - INFO - train acc: 1.0
2023-07-01 00:31:35,407 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.61      0.62      0.61        65

    accuracy                           0.74       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.75      0.74      0.75       200

2023-07-01 00:31:35,407 - INFO - test loss 0.01889068950466765
2023-07-01 00:31:35,407 - INFO - test acc 0.7450000047683716
2023-07-01 00:31:36,157 - INFO - Distilling data from client: Client27
2023-07-01 00:31:36,157 - INFO - train loss: 0.002178177314395689
2023-07-01 00:31:36,158 - INFO - train acc: 0.9972066879272461
2023-07-01 00:31:36,174 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.60      0.60      0.60        65

    accuracy                           0.74       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:31:36,175 - INFO - test loss 0.017927167931038015
2023-07-01 00:31:36,175 - INFO - test acc 0.7400000095367432
2023-07-01 00:31:36,919 - INFO - Distilling data from client: Client27
2023-07-01 00:31:36,919 - INFO - train loss: 0.0022221479059402194
2023-07-01 00:31:36,919 - INFO - train acc: 0.9972066879272461
2023-07-01 00:31:36,937 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.76      0.79       135
           4       0.56      0.63      0.59        65

    accuracy                           0.72       200
   macro avg       0.69      0.70      0.69       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:31:36,937 - INFO - test loss 0.018994874046220446
2023-07-01 00:31:36,938 - INFO - test acc 0.7199999690055847
2023-07-01 00:31:37,682 - INFO - Distilling data from client: Client27
2023-07-01 00:31:37,682 - INFO - train loss: 0.0016568352260591934
2023-07-01 00:31:37,682 - INFO - train acc: 1.0
2023-07-01 00:31:37,698 - INFO - report:               precision    recall  f1-score   support

           3       0.83      0.79      0.81       135
           4       0.60      0.66      0.63        65

    accuracy                           0.74       200
   macro avg       0.71      0.72      0.72       200
weighted avg       0.75      0.74      0.75       200

2023-07-01 00:31:37,699 - INFO - test loss 0.018957525683164562
2023-07-01 00:31:37,699 - INFO - test acc 0.7450000047683716
2023-07-01 00:31:38,437 - INFO - Distilling data from client: Client27
2023-07-01 00:31:38,437 - INFO - train loss: 0.0021027494398208166
2023-07-01 00:31:38,437 - INFO - train acc: 0.9972066879272461
2023-07-01 00:31:38,456 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.84      0.82       135
           4       0.63      0.57      0.60        65

    accuracy                           0.75       200
   macro avg       0.71      0.70      0.71       200
weighted avg       0.74      0.75      0.75       200

2023-07-01 00:31:38,456 - INFO - test loss 0.018766923996629374
2023-07-01 00:31:38,456 - INFO - test acc 0.75
2023-07-01 00:31:39,197 - INFO - Distilling data from client: Client27
2023-07-01 00:31:39,197 - INFO - train loss: 0.0020285850864111598
2023-07-01 00:31:39,197 - INFO - train acc: 1.0
2023-07-01 00:31:39,214 - INFO - report:               precision    recall  f1-score   support

           3       0.82      0.81      0.82       135
           4       0.62      0.63      0.63        65

    accuracy                           0.76       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:31:39,215 - INFO - test loss 0.01771770356756094
2023-07-01 00:31:39,215 - INFO - test acc 0.7549999952316284
2023-07-01 00:31:39,957 - INFO - Distilling data from client: Client27
2023-07-01 00:31:39,957 - INFO - train loss: 0.0021445388652158405
2023-07-01 00:31:39,957 - INFO - train acc: 0.9972066879272461
2023-07-01 00:31:40,002 - INFO - report:               precision    recall  f1-score   support

           3       0.83      0.87      0.85       135
           4       0.70      0.62      0.66        65

    accuracy                           0.79       200
   macro avg       0.76      0.74      0.75       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:31:40,002 - INFO - test loss 0.018326020324947942
2023-07-01 00:31:40,002 - INFO - test acc 0.7899999618530273
2023-07-01 00:31:40,744 - INFO - Distilling data from client: Client27
2023-07-01 00:31:40,745 - INFO - train loss: 0.0022739384568335806
2023-07-01 00:31:40,745 - INFO - train acc: 1.0
2023-07-01 00:31:40,762 - INFO - report:               precision    recall  f1-score   support

           3       0.83      0.81      0.82       135
           4       0.63      0.65      0.64        65

    accuracy                           0.76       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:31:40,762 - INFO - test loss 0.018792653919396652
2023-07-01 00:31:40,762 - INFO - test acc 0.7599999904632568
2023-07-01 00:31:41,510 - INFO - Distilling data from client: Client27
2023-07-01 00:31:41,510 - INFO - train loss: 0.0019855507502792176
2023-07-01 00:31:41,510 - INFO - train acc: 0.9972066879272461
2023-07-01 00:31:41,527 - INFO - report:               precision    recall  f1-score   support

           3       0.82      0.82      0.82       135
           4       0.63      0.63      0.63        65

    accuracy                           0.76       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:31:41,528 - INFO - test loss 0.018229660566823007
2023-07-01 00:31:41,528 - INFO - test acc 0.7599999904632568
2023-07-01 00:31:42,272 - INFO - Distilling data from client: Client27
2023-07-01 00:31:42,273 - INFO - train loss: 0.002057713280930043
2023-07-01 00:31:42,273 - INFO - train acc: 1.0
2023-07-01 00:31:42,290 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.80      0.80       135
           4       0.59      0.60      0.60        65

    accuracy                           0.73       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.73      0.74       200

2023-07-01 00:31:42,290 - INFO - test loss 0.020076979676258266
2023-07-01 00:31:42,290 - INFO - test acc 0.73499995470047
2023-07-01 00:31:43,037 - INFO - Distilling data from client: Client27
2023-07-01 00:31:43,037 - INFO - train loss: 0.0022714546960260662
2023-07-01 00:31:43,037 - INFO - train acc: 0.9972066879272461
2023-07-01 00:31:43,054 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.81      0.81       135
           4       0.60      0.60      0.60        65

    accuracy                           0.74       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:31:43,055 - INFO - test loss 0.01983630865942593
2023-07-01 00:31:43,055 - INFO - test acc 0.7400000095367432
2023-07-01 00:31:43,067 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:43,076 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:43,084 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:43,093 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:43,103 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:43,112 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:31:43,402 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client27//synthetic.png
2023-07-01 00:31:43,414 - INFO - c: 3.0 and total_data_in_this_class: 271
2023-07-01 00:31:43,414 - INFO - c: 5.0 and total_data_in_this_class: 264
2023-07-01 00:31:43,414 - INFO - c: 9.0 and total_data_in_this_class: 264
2023-07-01 00:31:43,414 - INFO - c: 3.0 and total_data_in_this_class: 62
2023-07-01 00:31:43,414 - INFO - c: 5.0 and total_data_in_this_class: 69
2023-07-01 00:31:43,414 - INFO - c: 9.0 and total_data_in_this_class: 69
2023-07-01 00:31:43,488 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.050618648529052734 sec
2023-07-01 00:31:43,536 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04699206352233887 sec
2023-07-01 00:31:43,541 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.10524892807006836 sec
2023-07-01 00:31:43,543 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:31:43,577 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03359627723693848 sec
2023-07-01 00:31:43,577 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:31:43,695 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1177363395690918 sec
2023-07-01 00:31:43,717 - INFO - initial test loss: 0.027723368701974564
2023-07-01 00:31:43,717 - INFO - initial test acc: 0.5600000023841858
2023-07-01 00:31:43,726 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.006039857864379883 sec
2023-07-01 00:31:43,838 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11883807182312012 sec
2023-07-01 00:31:43,842 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[528,10]), ShapedArray(float32[528,10]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10]), ShapedArray(float32[528,3,32,32]), ShapedArray(float32[528,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:31:43,905 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06278371810913086 sec
2023-07-01 00:31:43,905 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:31:44,193 - WARNING - Finished XLA compilation of jit(update_fn) in 0.2876460552215576 sec
2023-07-01 00:31:45,448 - INFO - Distilling data from client: Client28
2023-07-01 00:31:45,448 - INFO - train loss: 0.0030743036942903986
2023-07-01 00:31:45,448 - INFO - train acc: 0.9962121248245239
2023-07-01 00:31:45,510 - INFO - report:               precision    recall  f1-score   support

           3       0.47      0.50      0.48        62
           5       0.59      0.49      0.54        69
           9       0.74      0.81      0.77        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.60      0.60       200

2023-07-01 00:31:45,510 - INFO - test loss 0.026157521844471333
2023-07-01 00:31:45,511 - INFO - test acc 0.6049999594688416
2023-07-01 00:31:46,781 - INFO - Distilling data from client: Client28
2023-07-01 00:31:46,781 - INFO - train loss: 0.001577750858574539
2023-07-01 00:31:46,781 - INFO - train acc: 1.0
2023-07-01 00:31:46,804 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.50      0.47        62
           5       0.60      0.51      0.55        69
           9       0.76      0.80      0.78        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:31:46,804 - INFO - test loss 0.025939572414680038
2023-07-01 00:31:46,804 - INFO - test acc 0.6049999594688416
2023-07-01 00:31:48,074 - INFO - Distilling data from client: Client28
2023-07-01 00:31:48,074 - INFO - train loss: 0.0013231197727203816
2023-07-01 00:31:48,074 - INFO - train acc: 1.0
2023-07-01 00:31:48,100 - INFO - report:               precision    recall  f1-score   support

           3       0.47      0.52      0.49        62
           5       0.60      0.52      0.56        69
           9       0.74      0.77      0.75        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:31:48,100 - INFO - test loss 0.026035168568000647
2023-07-01 00:31:48,100 - INFO - test acc 0.6049999594688416
2023-07-01 00:31:49,370 - INFO - Distilling data from client: Client28
2023-07-01 00:31:49,370 - INFO - train loss: 0.001057679648517375
2023-07-01 00:31:49,370 - INFO - train acc: 1.0
2023-07-01 00:31:49,394 - INFO - report:               precision    recall  f1-score   support

           3       0.46      0.53      0.50        62
           5       0.60      0.49      0.54        69
           9       0.75      0.78      0.77        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:31:49,394 - INFO - test loss 0.026589711433780458
2023-07-01 00:31:49,394 - INFO - test acc 0.6049999594688416
2023-07-01 00:31:50,651 - INFO - Distilling data from client: Client28
2023-07-01 00:31:50,651 - INFO - train loss: 0.0012014638666789153
2023-07-01 00:31:50,651 - INFO - train acc: 0.998106062412262
2023-07-01 00:31:50,675 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.52      0.48        62
           5       0.61      0.52      0.56        69
           9       0.74      0.74      0.74        69

    accuracy                           0.59       200
   macro avg       0.60      0.59      0.59       200
weighted avg       0.60      0.59      0.60       200

2023-07-01 00:31:50,675 - INFO - test loss 0.026255800876766957
2023-07-01 00:31:50,675 - INFO - test acc 0.5949999690055847
2023-07-01 00:31:51,931 - INFO - Distilling data from client: Client28
2023-07-01 00:31:51,931 - INFO - train loss: 0.0009578257255137926
2023-07-01 00:31:51,932 - INFO - train acc: 1.0
2023-07-01 00:31:51,957 - INFO - report:               precision    recall  f1-score   support

           3       0.46      0.52      0.48        62
           5       0.61      0.52      0.56        69
           9       0.73      0.75      0.74        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.60      0.60       200

2023-07-01 00:31:51,957 - INFO - test loss 0.02572485019397713
2023-07-01 00:31:51,957 - INFO - test acc 0.5999999642372131
2023-07-01 00:31:53,229 - INFO - Distilling data from client: Client28
2023-07-01 00:31:53,229 - INFO - train loss: 0.0007099812788943161
2023-07-01 00:31:53,229 - INFO - train acc: 1.0
2023-07-01 00:31:53,252 - INFO - report:               precision    recall  f1-score   support

           3       0.46      0.53      0.49        62
           5       0.62      0.48      0.54        69
           9       0.72      0.78      0.75        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.59       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:31:53,252 - INFO - test loss 0.02647749438790231
2023-07-01 00:31:53,252 - INFO - test acc 0.5999999642372131
2023-07-01 00:31:54,513 - INFO - Distilling data from client: Client28
2023-07-01 00:31:54,514 - INFO - train loss: 0.0007096062781723838
2023-07-01 00:31:54,514 - INFO - train acc: 1.0
2023-07-01 00:31:54,538 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.50      0.47        62
           5       0.61      0.49      0.54        69
           9       0.75      0.80      0.77        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.59       200
weighted avg       0.60      0.60      0.60       200

2023-07-01 00:31:54,538 - INFO - test loss 0.026535948573593367
2023-07-01 00:31:54,538 - INFO - test acc 0.5999999642372131
2023-07-01 00:31:55,795 - INFO - Distilling data from client: Client28
2023-07-01 00:31:55,795 - INFO - train loss: 0.0006113732881915234
2023-07-01 00:31:55,795 - INFO - train acc: 1.0
2023-07-01 00:31:55,819 - INFO - report:               precision    recall  f1-score   support

           3       0.42      0.48      0.45        62
           5       0.56      0.48      0.52        69
           9       0.73      0.74      0.73        69

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-07-01 00:31:55,819 - INFO - test loss 0.027107600227162035
2023-07-01 00:31:55,819 - INFO - test acc 0.5699999928474426
2023-07-01 00:31:57,084 - INFO - Distilling data from client: Client28
2023-07-01 00:31:57,084 - INFO - train loss: 0.0006591206601418027
2023-07-01 00:31:57,084 - INFO - train acc: 1.0
2023-07-01 00:31:57,107 - INFO - report:               precision    recall  f1-score   support

           3       0.45      0.53      0.49        62
           5       0.61      0.51      0.56        69
           9       0.75      0.75      0.75        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:31:57,108 - INFO - test loss 0.02688093464825316
2023-07-01 00:31:57,108 - INFO - test acc 0.5999999642372131
2023-07-01 00:31:58,376 - INFO - Distilling data from client: Client28
2023-07-01 00:31:58,376 - INFO - train loss: 0.0006566589208474179
2023-07-01 00:31:58,376 - INFO - train acc: 1.0
2023-07-01 00:31:58,400 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.52      0.47        62
           5       0.62      0.48      0.54        69
           9       0.70      0.75      0.73        69

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-07-01 00:31:58,400 - INFO - test loss 0.02677228785274076
2023-07-01 00:31:58,401 - INFO - test acc 0.5849999785423279
2023-07-01 00:31:59,663 - INFO - Distilling data from client: Client28
2023-07-01 00:31:59,663 - INFO - train loss: 0.0006322889287642234
2023-07-01 00:31:59,663 - INFO - train acc: 1.0
2023-07-01 00:31:59,686 - INFO - report:               precision    recall  f1-score   support

           3       0.45      0.48      0.47        62
           5       0.62      0.54      0.57        69
           9       0.72      0.77      0.74        69

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.59       200
weighted avg       0.60      0.60      0.60       200

2023-07-01 00:31:59,686 - INFO - test loss 0.02668712660416552
2023-07-01 00:31:59,686 - INFO - test acc 0.5999999642372131
2023-07-01 00:32:00,951 - INFO - Distilling data from client: Client28
2023-07-01 00:32:00,951 - INFO - train loss: 0.0005974731252430838
2023-07-01 00:32:00,951 - INFO - train acc: 1.0
2023-07-01 00:32:00,976 - INFO - report:               precision    recall  f1-score   support

           3       0.42      0.50      0.46        62
           5       0.56      0.48      0.52        69
           9       0.72      0.71      0.72        69

    accuracy                           0.56       200
   macro avg       0.57      0.56      0.56       200
weighted avg       0.57      0.56      0.57       200

2023-07-01 00:32:00,976 - INFO - test loss 0.027245229538264425
2023-07-01 00:32:00,976 - INFO - test acc 0.5649999976158142
2023-07-01 00:32:02,254 - INFO - Distilling data from client: Client28
2023-07-01 00:32:02,254 - INFO - train loss: 0.0006741298706185977
2023-07-01 00:32:02,254 - INFO - train acc: 0.9962121248245239
2023-07-01 00:32:02,278 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.52      0.47        62
           5       0.62      0.51      0.56        69
           9       0.72      0.74      0.73        69

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.60      0.59      0.59       200

2023-07-01 00:32:02,278 - INFO - test loss 0.027242134675509597
2023-07-01 00:32:02,278 - INFO - test acc 0.5899999737739563
2023-07-01 00:32:03,545 - INFO - Distilling data from client: Client28
2023-07-01 00:32:03,546 - INFO - train loss: 0.0005564910627261893
2023-07-01 00:32:03,546 - INFO - train acc: 1.0
2023-07-01 00:32:03,570 - INFO - report:               precision    recall  f1-score   support

           3       0.43      0.48      0.46        62
           5       0.59      0.48      0.53        69
           9       0.72      0.78      0.75        69

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-07-01 00:32:03,570 - INFO - test loss 0.02751968443920593
2023-07-01 00:32:03,570 - INFO - test acc 0.5849999785423279
2023-07-01 00:32:04,830 - INFO - Distilling data from client: Client28
2023-07-01 00:32:04,830 - INFO - train loss: 0.0004571125243951159
2023-07-01 00:32:04,830 - INFO - train acc: 1.0
2023-07-01 00:32:04,854 - INFO - report:               precision    recall  f1-score   support

           3       0.46      0.52      0.49        62
           5       0.54      0.46      0.50        69
           9       0.72      0.75      0.74        69

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-07-01 00:32:04,854 - INFO - test loss 0.02747403170553172
2023-07-01 00:32:04,854 - INFO - test acc 0.5799999833106995
2023-07-01 00:32:06,118 - INFO - Distilling data from client: Client28
2023-07-01 00:32:06,118 - INFO - train loss: 0.0005983866587499935
2023-07-01 00:32:06,118 - INFO - train acc: 1.0
2023-07-01 00:32:06,143 - INFO - report:               precision    recall  f1-score   support

           3       0.45      0.55      0.49        62
           5       0.57      0.45      0.50        69
           9       0.71      0.72      0.72        69

    accuracy                           0.57       200
   macro avg       0.58      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-07-01 00:32:06,143 - INFO - test loss 0.027528013578057193
2023-07-01 00:32:06,143 - INFO - test acc 0.574999988079071
2023-07-01 00:32:07,419 - INFO - Distilling data from client: Client28
2023-07-01 00:32:07,419 - INFO - train loss: 0.0004542214858714723
2023-07-01 00:32:07,419 - INFO - train acc: 1.0
2023-07-01 00:32:07,444 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.50      0.47        62
           5       0.58      0.46      0.52        69
           9       0.73      0.78      0.76        69

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-07-01 00:32:07,445 - INFO - test loss 0.026539968072902972
2023-07-01 00:32:07,445 - INFO - test acc 0.5849999785423279
2023-07-01 00:32:08,711 - INFO - Distilling data from client: Client28
2023-07-01 00:32:08,711 - INFO - train loss: 0.000463483149273753
2023-07-01 00:32:08,711 - INFO - train acc: 1.0
2023-07-01 00:32:08,794 - INFO - report:               precision    recall  f1-score   support

           3       0.48      0.50      0.49        62
           5       0.60      0.55      0.58        69
           9       0.73      0.77      0.75        69

    accuracy                           0.61       200
   macro avg       0.60      0.61      0.60       200
weighted avg       0.61      0.61      0.61       200

2023-07-01 00:32:08,794 - INFO - test loss 0.02709917166726894
2023-07-01 00:32:08,794 - INFO - test acc 0.6100000143051147
2023-07-01 00:32:10,052 - INFO - Distilling data from client: Client28
2023-07-01 00:32:10,053 - INFO - train loss: 0.00035572921121232546
2023-07-01 00:32:10,053 - INFO - train acc: 1.0
2023-07-01 00:32:10,077 - INFO - report:               precision    recall  f1-score   support

           3       0.45      0.53      0.49        62
           5       0.61      0.49      0.54        69
           9       0.70      0.72      0.71        69

    accuracy                           0.58       200
   macro avg       0.59      0.58      0.58       200
weighted avg       0.59      0.58      0.59       200

2023-07-01 00:32:10,077 - INFO - test loss 0.02779535111733072
2023-07-01 00:32:10,077 - INFO - test acc 0.5849999785423279
2023-07-01 00:32:11,336 - INFO - Distilling data from client: Client28
2023-07-01 00:32:11,336 - INFO - train loss: 0.0004000909116611333
2023-07-01 00:32:11,336 - INFO - train acc: 1.0
2023-07-01 00:32:11,360 - INFO - report:               precision    recall  f1-score   support

           3       0.42      0.48      0.45        62
           5       0.59      0.46      0.52        69
           9       0.70      0.75      0.73        69

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-07-01 00:32:11,360 - INFO - test loss 0.02763840201936889
2023-07-01 00:32:11,361 - INFO - test acc 0.5699999928474426
2023-07-01 00:32:12,628 - INFO - Distilling data from client: Client28
2023-07-01 00:32:12,628 - INFO - train loss: 0.0004651255552729209
2023-07-01 00:32:12,628 - INFO - train acc: 1.0
2023-07-01 00:32:12,652 - INFO - report:               precision    recall  f1-score   support

           3       0.44      0.52      0.48        62
           5       0.59      0.48      0.53        69
           9       0.71      0.74      0.72        69

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-07-01 00:32:12,652 - INFO - test loss 0.027170547377237293
2023-07-01 00:32:12,652 - INFO - test acc 0.5799999833106995
2023-07-01 00:32:13,924 - INFO - Distilling data from client: Client28
2023-07-01 00:32:13,924 - INFO - train loss: 0.0004370953058404993
2023-07-01 00:32:13,924 - INFO - train acc: 1.0
2023-07-01 00:32:13,947 - INFO - report:               precision    recall  f1-score   support

           3       0.46      0.52      0.49        62
           5       0.60      0.48      0.53        69
           9       0.68      0.75      0.72        69

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.59      0.58      0.58       200

2023-07-01 00:32:13,948 - INFO - test loss 0.02702829158962698
2023-07-01 00:32:13,948 - INFO - test acc 0.5849999785423279
2023-07-01 00:32:15,205 - INFO - Distilling data from client: Client28
2023-07-01 00:32:15,205 - INFO - train loss: 0.00039128143475048073
2023-07-01 00:32:15,205 - INFO - train acc: 1.0
2023-07-01 00:32:15,229 - INFO - report:               precision    recall  f1-score   support

           3       0.45      0.52      0.48        62
           5       0.57      0.46      0.51        69
           9       0.70      0.74      0.72        69

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-07-01 00:32:15,229 - INFO - test loss 0.027838825159174442
2023-07-01 00:32:15,229 - INFO - test acc 0.574999988079071
2023-07-01 00:32:16,493 - INFO - Distilling data from client: Client28
2023-07-01 00:32:16,493 - INFO - train loss: 0.0004215709691070836
2023-07-01 00:32:16,493 - INFO - train acc: 1.0
2023-07-01 00:32:16,518 - INFO - report:               precision    recall  f1-score   support

           3       0.43      0.53      0.48        62
           5       0.58      0.45      0.51        69
           9       0.70      0.72      0.71        69

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.58      0.57      0.57       200

2023-07-01 00:32:16,518 - INFO - test loss 0.0276038619984035
2023-07-01 00:32:16,518 - INFO - test acc 0.5699999928474426
2023-07-01 00:32:16,529 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:16,539 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:16,547 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:16,556 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:16,564 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:16,573 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:16,583 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:16,592 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:16,601 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:16,974 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client28//synthetic.png
2023-07-01 00:32:16,987 - INFO - c: 2.0 and total_data_in_this_class: 270
2023-07-01 00:32:16,987 - INFO - c: 7.0 and total_data_in_this_class: 529
2023-07-01 00:32:16,987 - INFO - c: 2.0 and total_data_in_this_class: 63
2023-07-01 00:32:16,987 - INFO - c: 7.0 and total_data_in_this_class: 137
2023-07-01 00:32:17,007 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025391578674316406 sec
2023-07-01 00:32:17,007 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:32:17,008 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012226104736328125 sec
2023-07-01 00:32:17,009 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:17,019 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010659456253051758 sec
2023-07-01 00:32:17,021 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002498626708984375 sec
2023-07-01 00:32:17,022 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:32:17,023 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009703636169433594 sec
2023-07-01 00:32:17,023 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:17,031 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008390188217163086 sec
2023-07-01 00:32:17,035 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013875961303710938 sec
2023-07-01 00:32:17,036 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012612342834472656 sec
2023-07-01 00:32:17,036 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003037452697753906 sec
2023-07-01 00:32:17,038 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021004676818847656 sec
2023-07-01 00:32:17,038 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012159347534179688 sec
2023-07-01 00:32:17,039 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002818107604980469 sec
2023-07-01 00:32:17,039 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002410411834716797 sec
2023-07-01 00:32:17,040 - WARNING - Finished tracing + transforming absolute for pjit in 0.0001652240753173828 sec
2023-07-01 00:32:17,040 - WARNING - Finished tracing + transforming fn for pjit in 0.00027632713317871094 sec
2023-07-01 00:32:17,041 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0003159046173095703 sec
2023-07-01 00:32:17,042 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001971721649169922 sec
2023-07-01 00:32:17,043 - WARNING - Finished tracing + transforming fn for pjit in 0.0002262592315673828 sec
2023-07-01 00:32:17,044 - WARNING - Finished tracing + transforming fn for pjit in 0.0002655982971191406 sec
2023-07-01 00:32:17,044 - WARNING - Finished tracing + transforming fn for pjit in 0.00022411346435546875 sec
2023-07-01 00:32:17,045 - WARNING - Finished tracing + transforming fn for pjit in 0.00025916099548339844 sec
2023-07-01 00:32:17,046 - WARNING - Finished tracing + transforming fn for pjit in 0.0002243518829345703 sec
2023-07-01 00:32:17,048 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016760826110839844 sec
2023-07-01 00:32:17,048 - WARNING - Finished tracing + transforming fn for pjit in 0.00022602081298828125 sec
2023-07-01 00:32:17,049 - WARNING - Finished tracing + transforming fn for pjit in 0.00022864341735839844 sec
2023-07-01 00:32:17,053 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003590583801269531 sec
2023-07-01 00:32:17,053 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009548664093017578 sec
2023-07-01 00:32:17,054 - WARNING - Finished tracing + transforming fn for pjit in 0.0002319812774658203 sec
2023-07-01 00:32:17,054 - WARNING - Finished tracing + transforming fn for pjit in 0.00021982192993164062 sec
2023-07-01 00:32:17,055 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00028705596923828125 sec
2023-07-01 00:32:17,056 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002598762512207031 sec
2023-07-01 00:32:17,057 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017690658569335938 sec
2023-07-01 00:32:17,057 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026035308837890625 sec
2023-07-01 00:32:17,058 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022673606872558594 sec
2023-07-01 00:32:17,059 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002238750457763672 sec
2023-07-01 00:32:17,060 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0009489059448242188 sec
2023-07-01 00:32:17,061 - WARNING - Finished tracing + transforming _where for pjit in 0.001546621322631836 sec
2023-07-01 00:32:17,061 - WARNING - Finished tracing + transforming fn for pjit in 0.0002627372741699219 sec
2023-07-01 00:32:17,062 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002646446228027344 sec
2023-07-01 00:32:17,063 - WARNING - Finished tracing + transforming fn for pjit in 0.00022602081298828125 sec
2023-07-01 00:32:17,063 - WARNING - Finished tracing + transforming fn for pjit in 0.0002224445343017578 sec
2023-07-01 00:32:17,064 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002200603485107422 sec
2023-07-01 00:32:17,065 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002865791320800781 sec
2023-07-01 00:32:17,065 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025343894958496094 sec
2023-07-01 00:32:17,066 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002627372741699219 sec
2023-07-01 00:32:17,067 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002315044403076172 sec
2023-07-01 00:32:17,068 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002193450927734375 sec
2023-07-01 00:32:17,068 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025534629821777344 sec
2023-07-01 00:32:17,069 - WARNING - Finished tracing + transforming _where for pjit in 0.0008237361907958984 sec
2023-07-01 00:32:17,069 - WARNING - Finished tracing + transforming fn for pjit in 0.0002529621124267578 sec
2023-07-01 00:32:17,070 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002532005310058594 sec
2023-07-01 00:32:17,071 - WARNING - Finished tracing + transforming fn for pjit in 0.00022292137145996094 sec
2023-07-01 00:32:17,075 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002639293670654297 sec
2023-07-01 00:32:17,076 - WARNING - Finished tracing + transforming fn for pjit in 0.00033354759216308594 sec
2023-07-01 00:32:17,076 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002646446228027344 sec
2023-07-01 00:32:17,077 - WARNING - Finished tracing + transforming fn for pjit in 0.00021910667419433594 sec
2023-07-01 00:32:17,081 - WARNING - Finished tracing + transforming fn for pjit in 0.0002257823944091797 sec
2023-07-01 00:32:17,083 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016260147094726562 sec
2023-07-01 00:32:17,083 - WARNING - Finished tracing + transforming fn for pjit in 0.0002925395965576172 sec
2023-07-01 00:32:17,084 - WARNING - Finished tracing + transforming fn for pjit in 0.0002238750457763672 sec
2023-07-01 00:32:17,101 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06704306602478027 sec
2023-07-01 00:32:17,104 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012111663818359375 sec
2023-07-01 00:32:17,105 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011849403381347656 sec
2023-07-01 00:32:17,105 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00026154518127441406 sec
2023-07-01 00:32:17,107 - WARNING - Finished tracing + transforming fn for pjit in 0.0002167224884033203 sec
2023-07-01 00:32:17,108 - WARNING - Finished tracing + transforming fn for pjit in 0.00025153160095214844 sec
2023-07-01 00:32:17,109 - WARNING - Finished tracing + transforming fn for pjit in 0.00021457672119140625 sec
2023-07-01 00:32:17,115 - WARNING - Finished tracing + transforming fn for pjit in 0.00022530555725097656 sec
2023-07-01 00:32:17,116 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021409988403320312 sec
2023-07-01 00:32:17,116 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002472400665283203 sec
2023-07-01 00:32:17,117 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00016808509826660156 sec
2023-07-01 00:32:17,118 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003151893615722656 sec
2023-07-01 00:32:17,119 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021791458129882812 sec
2023-07-01 00:32:17,119 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002162456512451172 sec
2023-07-01 00:32:17,120 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025844573974609375 sec
2023-07-01 00:32:17,120 - WARNING - Finished tracing + transforming _where for pjit in 0.0008215904235839844 sec
2023-07-01 00:32:17,121 - WARNING - Finished tracing + transforming fn for pjit in 0.0002624988555908203 sec
2023-07-01 00:32:17,122 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002472400665283203 sec
2023-07-01 00:32:17,122 - WARNING - Finished tracing + transforming fn for pjit in 0.0002117156982421875 sec
2023-07-01 00:32:17,123 - WARNING - Finished tracing + transforming fn for pjit in 0.0005786418914794922 sec
2023-07-01 00:32:17,556 - WARNING - Finished tracing + transforming fn for pjit in 0.0002338886260986328 sec
2023-07-01 00:32:17,575 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.473036527633667 sec
2023-07-01 00:32:17,576 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012087821960449219 sec
2023-07-01 00:32:17,577 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013399124145507812 sec
2023-07-01 00:32:17,578 - WARNING - Finished tracing + transforming _where for pjit in 0.0006022453308105469 sec
2023-07-01 00:32:17,578 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002880096435546875 sec
2023-07-01 00:32:17,579 - WARNING - Finished tracing + transforming trace for pjit in 0.0024466514587402344 sec
2023-07-01 00:32:17,581 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010228157043457031 sec
2023-07-01 00:32:17,582 - WARNING - Finished tracing + transforming tril for pjit in 0.0006470680236816406 sec
2023-07-01 00:32:17,582 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.00165557861328125 sec
2023-07-01 00:32:17,583 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010418891906738281 sec
2023-07-01 00:32:17,583 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010156631469726562 sec
2023-07-01 00:32:17,585 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0013766288757324219 sec
2023-07-01 00:32:17,589 - WARNING - Finished tracing + transforming _solve for pjit in 0.008899927139282227 sec
2023-07-01 00:32:17,589 - WARNING - Finished tracing + transforming dot for pjit in 0.0003077983856201172 sec
2023-07-01 00:32:17,592 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.5590436458587646 sec
2023-07-01 00:32:17,594 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:32:17,626 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03170347213745117 sec
2023-07-01 00:32:17,626 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:17,747 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12123560905456543 sec
2023-07-01 00:32:17,764 - INFO - initial test loss: 0.019547657417633042
2023-07-01 00:32:17,764 - INFO - initial test acc: 0.7450000047683716
2023-07-01 00:32:17,769 - WARNING - Finished tracing + transforming dot for pjit in 0.00035881996154785156 sec
2023-07-01 00:32:17,770 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029349327087402344 sec
2023-07-01 00:32:17,771 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003631114959716797 sec
2023-07-01 00:32:17,771 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009751319885253906 sec
2023-07-01 00:32:17,772 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001926422119140625 sec
2023-07-01 00:32:17,773 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00019788742065429688 sec
2023-07-01 00:32:17,773 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024890899658203125 sec
2023-07-01 00:32:17,774 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003616809844970703 sec
2023-07-01 00:32:17,775 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010514259338378906 sec
2023-07-01 00:32:17,776 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009702682495117188 sec
2023-07-01 00:32:17,784 - WARNING - Finished tracing + transforming fn for pjit in 0.0002644062042236328 sec
2023-07-01 00:32:17,785 - WARNING - Finished tracing + transforming fn for pjit in 0.00026226043701171875 sec
2023-07-01 00:32:17,786 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021338462829589844 sec
2023-07-01 00:32:17,786 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002598762512207031 sec
2023-07-01 00:32:17,787 - WARNING - Finished tracing + transforming _where for pjit in 0.0008382797241210938 sec
2023-07-01 00:32:17,795 - WARNING - Finished tracing + transforming fn for pjit in 0.0002493858337402344 sec
2023-07-01 00:32:17,795 - WARNING - Finished tracing + transforming fn for pjit in 0.0002541542053222656 sec
2023-07-01 00:32:17,796 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002090930938720703 sec
2023-07-01 00:32:17,797 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00030684471130371094 sec
2023-07-01 00:32:17,797 - WARNING - Finished tracing + transforming _where for pjit in 0.0008683204650878906 sec
2023-07-01 00:32:17,831 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000213623046875 sec
2023-07-01 00:32:17,885 - WARNING - Finished tracing + transforming fn for pjit in 0.0002665519714355469 sec
2023-07-01 00:32:17,886 - WARNING - Finished tracing + transforming fn for pjit in 0.00022983551025390625 sec
2023-07-01 00:32:17,886 - WARNING - Finished tracing + transforming square for pjit in 0.0001633167266845703 sec
2023-07-01 00:32:17,888 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021600723266601562 sec
2023-07-01 00:32:17,890 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0005035400390625 sec
2023-07-01 00:32:17,891 - WARNING - Finished tracing + transforming fn for pjit in 0.0002677440643310547 sec
2023-07-01 00:32:17,891 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002155303955078125 sec
2023-07-01 00:32:17,892 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002238750457763672 sec
2023-07-01 00:32:17,892 - WARNING - Finished tracing + transforming fn for pjit in 0.00025916099548339844 sec
2023-07-01 00:32:17,893 - WARNING - Finished tracing + transforming fn for pjit in 0.00022482872009277344 sec
2023-07-01 00:32:17,894 - WARNING - Finished tracing + transforming square for pjit in 0.00016236305236816406 sec
2023-07-01 00:32:17,896 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021767616271972656 sec
2023-07-01 00:32:17,897 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001704692840576172 sec
2023-07-01 00:32:17,898 - WARNING - Finished tracing + transforming fn for pjit in 0.0002639293670654297 sec
2023-07-01 00:32:17,898 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021719932556152344 sec
2023-07-01 00:32:17,899 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022339820861816406 sec
2023-07-01 00:32:17,900 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13440656661987305 sec
2023-07-01 00:32:17,903 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[360,10]), ShapedArray(float32[360,10]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:32:17,964 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.0605473518371582 sec
2023-07-01 00:32:17,964 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:18,262 - WARNING - Finished XLA compilation of jit(update_fn) in 0.297680139541626 sec
2023-07-01 00:32:19,024 - INFO - Distilling data from client: Client29
2023-07-01 00:32:19,024 - INFO - train loss: 0.00530056751136313
2023-07-01 00:32:19,024 - INFO - train acc: 0.9555555582046509
2023-07-01 00:32:19,065 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.70      0.71        63
           7       0.86      0.88      0.87       137

    accuracy                           0.82       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.82      0.82      0.82       200

2023-07-01 00:32:19,065 - INFO - test loss 0.015662018219974722
2023-07-01 00:32:19,065 - INFO - test acc 0.8199999928474426
2023-07-01 00:32:19,827 - INFO - Distilling data from client: Client29
2023-07-01 00:32:19,827 - INFO - train loss: 0.0032428840024094725
2023-07-01 00:32:19,827 - INFO - train acc: 0.9916666746139526
2023-07-01 00:32:19,845 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.65      0.69        63
           7       0.85      0.90      0.87       137

    accuracy                           0.82       200
   macro avg       0.80      0.77      0.78       200
weighted avg       0.82      0.82      0.82       200

2023-07-01 00:32:19,845 - INFO - test loss 0.016284150119794984
2023-07-01 00:32:19,845 - INFO - test acc 0.8199999928474426
2023-07-01 00:32:20,605 - INFO - Distilling data from client: Client29
2023-07-01 00:32:20,606 - INFO - train loss: 0.0027225568515317684
2023-07-01 00:32:20,606 - INFO - train acc: 0.9861111640930176
2023-07-01 00:32:20,624 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.71      0.69        63
           7       0.86      0.83      0.85       137

    accuracy                           0.80       200
   macro avg       0.76      0.77      0.77       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:32:20,624 - INFO - test loss 0.016661278297594357
2023-07-01 00:32:20,624 - INFO - test acc 0.7949999570846558
2023-07-01 00:32:21,374 - INFO - Distilling data from client: Client29
2023-07-01 00:32:21,374 - INFO - train loss: 0.0023911243067842335
2023-07-01 00:32:21,374 - INFO - train acc: 0.9944444894790649
2023-07-01 00:32:21,392 - INFO - report:               precision    recall  f1-score   support

           2       0.67      0.62      0.64        63
           7       0.83      0.86      0.85       137

    accuracy                           0.79       200
   macro avg       0.75      0.74      0.75       200
weighted avg       0.78      0.79      0.78       200

2023-07-01 00:32:21,393 - INFO - test loss 0.01757843555410519
2023-07-01 00:32:21,393 - INFO - test acc 0.7849999666213989
2023-07-01 00:32:22,143 - INFO - Distilling data from client: Client29
2023-07-01 00:32:22,143 - INFO - train loss: 0.002150597605654983
2023-07-01 00:32:22,143 - INFO - train acc: 0.9972222447395325
2023-07-01 00:32:22,161 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.62      0.64        63
           7       0.83      0.85      0.84       137

    accuracy                           0.78       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:32:22,161 - INFO - test loss 0.017781019481609102
2023-07-01 00:32:22,161 - INFO - test acc 0.7799999713897705
2023-07-01 00:32:22,923 - INFO - Distilling data from client: Client29
2023-07-01 00:32:22,923 - INFO - train loss: 0.0024312456100048408
2023-07-01 00:32:22,923 - INFO - train acc: 0.9916666746139526
2023-07-01 00:32:22,941 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.63      0.68        63
           7       0.84      0.89      0.87       137

    accuracy                           0.81       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:32:22,941 - INFO - test loss 0.016815926089311928
2023-07-01 00:32:22,941 - INFO - test acc 0.8100000023841858
2023-07-01 00:32:23,695 - INFO - Distilling data from client: Client29
2023-07-01 00:32:23,695 - INFO - train loss: 0.002205289423235574
2023-07-01 00:32:23,695 - INFO - train acc: 0.9944444894790649
2023-07-01 00:32:23,713 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.63      0.68        63
           7       0.84      0.89      0.87       137

    accuracy                           0.81       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:32:23,713 - INFO - test loss 0.017488167417612062
2023-07-01 00:32:23,713 - INFO - test acc 0.8100000023841858
2023-07-01 00:32:24,458 - INFO - Distilling data from client: Client29
2023-07-01 00:32:24,458 - INFO - train loss: 0.001970997410238078
2023-07-01 00:32:24,459 - INFO - train acc: 0.9861111640930176
2023-07-01 00:32:24,476 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.63      0.68        63
           7       0.84      0.89      0.87       137

    accuracy                           0.81       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:32:24,476 - INFO - test loss 0.01666424222427046
2023-07-01 00:32:24,476 - INFO - test acc 0.8100000023841858
2023-07-01 00:32:25,219 - INFO - Distilling data from client: Client29
2023-07-01 00:32:25,219 - INFO - train loss: 0.0020179801887932206
2023-07-01 00:32:25,219 - INFO - train acc: 0.9916666746139526
2023-07-01 00:32:25,237 - INFO - report:               precision    recall  f1-score   support

           2       0.64      0.59      0.61        63
           7       0.82      0.85      0.83       137

    accuracy                           0.77       200
   macro avg       0.73      0.72      0.72       200
weighted avg       0.76      0.77      0.76       200

2023-07-01 00:32:25,237 - INFO - test loss 0.018210929306748513
2023-07-01 00:32:25,237 - INFO - test acc 0.7649999856948853
2023-07-01 00:32:25,993 - INFO - Distilling data from client: Client29
2023-07-01 00:32:25,993 - INFO - train loss: 0.0020933203674437743
2023-07-01 00:32:25,993 - INFO - train acc: 1.0
2023-07-01 00:32:26,010 - INFO - report:               precision    recall  f1-score   support

           2       0.69      0.57      0.63        63
           7       0.82      0.88      0.85       137

    accuracy                           0.79       200
   macro avg       0.75      0.73      0.74       200
weighted avg       0.78      0.79      0.78       200

2023-07-01 00:32:26,010 - INFO - test loss 0.018208717663248716
2023-07-01 00:32:26,010 - INFO - test acc 0.7849999666213989
2023-07-01 00:32:26,761 - INFO - Distilling data from client: Client29
2023-07-01 00:32:26,761 - INFO - train loss: 0.0017647571312861064
2023-07-01 00:32:26,761 - INFO - train acc: 1.0
2023-07-01 00:32:26,779 - INFO - report:               precision    recall  f1-score   support

           2       0.73      0.63      0.68        63
           7       0.84      0.89      0.87       137

    accuracy                           0.81       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:32:26,779 - INFO - test loss 0.017176147826222194
2023-07-01 00:32:26,779 - INFO - test acc 0.8100000023841858
2023-07-01 00:32:27,524 - INFO - Distilling data from client: Client29
2023-07-01 00:32:27,524 - INFO - train loss: 0.0015830603849283761
2023-07-01 00:32:27,524 - INFO - train acc: 1.0
2023-07-01 00:32:27,541 - INFO - report:               precision    recall  f1-score   support

           2       0.68      0.62      0.65        63
           7       0.83      0.87      0.85       137

    accuracy                           0.79       200
   macro avg       0.76      0.74      0.75       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:32:27,541 - INFO - test loss 0.017444666109085506
2023-07-01 00:32:27,541 - INFO - test acc 0.7899999618530273
2023-07-01 00:32:28,295 - INFO - Distilling data from client: Client29
2023-07-01 00:32:28,295 - INFO - train loss: 0.0019119191522794813
2023-07-01 00:32:28,295 - INFO - train acc: 1.0
2023-07-01 00:32:28,312 - INFO - report:               precision    recall  f1-score   support

           2       0.69      0.65      0.67        63
           7       0.84      0.87      0.86       137

    accuracy                           0.80       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:32:28,312 - INFO - test loss 0.017282872437747666
2023-07-01 00:32:28,312 - INFO - test acc 0.7999999523162842
2023-07-01 00:32:29,057 - INFO - Distilling data from client: Client29
2023-07-01 00:32:29,057 - INFO - train loss: 0.0015907471668894684
2023-07-01 00:32:29,057 - INFO - train acc: 0.9944444894790649
2023-07-01 00:32:29,075 - INFO - report:               precision    recall  f1-score   support

           2       0.71      0.65      0.68        63
           7       0.85      0.88      0.86       137

    accuracy                           0.81       200
   macro avg       0.78      0.76      0.77       200
weighted avg       0.80      0.81      0.80       200

2023-07-01 00:32:29,075 - INFO - test loss 0.017167294922458543
2023-07-01 00:32:29,075 - INFO - test acc 0.8050000071525574
2023-07-01 00:32:29,828 - INFO - Distilling data from client: Client29
2023-07-01 00:32:29,828 - INFO - train loss: 0.0016939319681255395
2023-07-01 00:32:29,828 - INFO - train acc: 1.0
2023-07-01 00:32:29,847 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.62      0.67        63
           7       0.84      0.89      0.86       137

    accuracy                           0.81       200
   macro avg       0.78      0.75      0.76       200
weighted avg       0.80      0.81      0.80       200

2023-07-01 00:32:29,847 - INFO - test loss 0.017449798085928395
2023-07-01 00:32:29,847 - INFO - test acc 0.8050000071525574
2023-07-01 00:32:30,605 - INFO - Distilling data from client: Client29
2023-07-01 00:32:30,605 - INFO - train loss: 0.001621710294447845
2023-07-01 00:32:30,605 - INFO - train acc: 0.9944444894790649
2023-07-01 00:32:30,624 - INFO - report:               precision    recall  f1-score   support

           2       0.75      0.62      0.68        63
           7       0.84      0.91      0.87       137

    accuracy                           0.81       200
   macro avg       0.79      0.76      0.77       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:32:30,624 - INFO - test loss 0.017473239274348865
2023-07-01 00:32:30,624 - INFO - test acc 0.8149999976158142
2023-07-01 00:32:31,378 - INFO - Distilling data from client: Client29
2023-07-01 00:32:31,378 - INFO - train loss: 0.0019947012558237213
2023-07-01 00:32:31,378 - INFO - train acc: 1.0
2023-07-01 00:32:31,396 - INFO - report:               precision    recall  f1-score   support

           2       0.67      0.57      0.62        63
           7       0.82      0.87      0.84       137

    accuracy                           0.78       200
   macro avg       0.74      0.72      0.73       200
weighted avg       0.77      0.78      0.77       200

2023-07-01 00:32:31,396 - INFO - test loss 0.017323823863851017
2023-07-01 00:32:31,396 - INFO - test acc 0.7749999761581421
2023-07-01 00:32:32,153 - INFO - Distilling data from client: Client29
2023-07-01 00:32:32,153 - INFO - train loss: 0.0017105780343375124
2023-07-01 00:32:32,153 - INFO - train acc: 1.0
2023-07-01 00:32:32,175 - INFO - report:               precision    recall  f1-score   support

           2       0.70      0.63      0.67        63
           7       0.84      0.88      0.86       137

    accuracy                           0.80       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:32:32,176 - INFO - test loss 0.01722118157817057
2023-07-01 00:32:32,176 - INFO - test acc 0.7999999523162842
2023-07-01 00:32:32,934 - INFO - Distilling data from client: Client29
2023-07-01 00:32:32,934 - INFO - train loss: 0.0016674956048633555
2023-07-01 00:32:32,934 - INFO - train acc: 0.9972222447395325
2023-07-01 00:32:32,951 - INFO - report:               precision    recall  f1-score   support

           2       0.72      0.60      0.66        63
           7       0.83      0.89      0.86       137

    accuracy                           0.80       200
   macro avg       0.77      0.75      0.76       200
weighted avg       0.79      0.80      0.79       200

2023-07-01 00:32:32,951 - INFO - test loss 0.017126871234404206
2023-07-01 00:32:32,951 - INFO - test acc 0.7999999523162842
2023-07-01 00:32:33,704 - INFO - Distilling data from client: Client29
2023-07-01 00:32:33,704 - INFO - train loss: 0.0017824797022480971
2023-07-01 00:32:33,704 - INFO - train acc: 0.9944444894790649
2023-07-01 00:32:33,722 - INFO - report:               precision    recall  f1-score   support

           2       0.66      0.62      0.64        63
           7       0.83      0.85      0.84       137

    accuracy                           0.78       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:32:33,722 - INFO - test loss 0.01675801743119927
2023-07-01 00:32:33,722 - INFO - test acc 0.7799999713897705
2023-07-01 00:32:34,477 - INFO - Distilling data from client: Client29
2023-07-01 00:32:34,477 - INFO - train loss: 0.0013227737981656238
2023-07-01 00:32:34,477 - INFO - train acc: 1.0
2023-07-01 00:32:34,495 - INFO - report:               precision    recall  f1-score   support

           2       0.69      0.67      0.68        63
           7       0.85      0.86      0.86       137

    accuracy                           0.80       200
   macro avg       0.77      0.76      0.77       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:32:34,495 - INFO - test loss 0.017070979808113947
2023-07-01 00:32:34,495 - INFO - test acc 0.7999999523162842
2023-07-01 00:32:35,240 - INFO - Distilling data from client: Client29
2023-07-01 00:32:35,240 - INFO - train loss: 0.001817447598805614
2023-07-01 00:32:35,240 - INFO - train acc: 0.9972222447395325
2023-07-01 00:32:35,258 - INFO - report:               precision    recall  f1-score   support

           2       0.67      0.59      0.63        63
           7       0.82      0.87      0.84       137

    accuracy                           0.78       200
   macro avg       0.75      0.73      0.74       200
weighted avg       0.77      0.78      0.78       200

2023-07-01 00:32:35,258 - INFO - test loss 0.01817814865848804
2023-07-01 00:32:35,258 - INFO - test acc 0.7799999713897705
2023-07-01 00:32:36,009 - INFO - Distilling data from client: Client29
2023-07-01 00:32:36,009 - INFO - train loss: 0.0016511469798802393
2023-07-01 00:32:36,009 - INFO - train acc: 1.0
2023-07-01 00:32:36,027 - INFO - report:               precision    recall  f1-score   support

           2       0.68      0.62      0.65        63
           7       0.83      0.87      0.85       137

    accuracy                           0.79       200
   macro avg       0.76      0.74      0.75       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:32:36,027 - INFO - test loss 0.01784644433274584
2023-07-01 00:32:36,027 - INFO - test acc 0.7899999618530273
2023-07-01 00:32:36,779 - INFO - Distilling data from client: Client29
2023-07-01 00:32:36,779 - INFO - train loss: 0.0015455880838102078
2023-07-01 00:32:36,779 - INFO - train acc: 0.9972222447395325
2023-07-01 00:32:36,796 - INFO - report:               precision    recall  f1-score   support

           2       0.68      0.57      0.62        63
           7       0.82      0.88      0.85       137

    accuracy                           0.78       200
   macro avg       0.75      0.72      0.73       200
weighted avg       0.77      0.78      0.77       200

2023-07-01 00:32:36,797 - INFO - test loss 0.017948333448358552
2023-07-01 00:32:36,797 - INFO - test acc 0.7799999713897705
2023-07-01 00:32:37,547 - INFO - Distilling data from client: Client29
2023-07-01 00:32:37,547 - INFO - train loss: 0.001237475474691732
2023-07-01 00:32:37,548 - INFO - train acc: 1.0
2023-07-01 00:32:37,565 - INFO - report:               precision    recall  f1-score   support

           2       0.69      0.63      0.66        63
           7       0.84      0.87      0.85       137

    accuracy                           0.80       200
   macro avg       0.76      0.75      0.76       200
weighted avg       0.79      0.80      0.79       200

2023-07-01 00:32:37,565 - INFO - test loss 0.016831048752894474
2023-07-01 00:32:37,565 - INFO - test acc 0.7949999570846558
2023-07-01 00:32:37,568 - WARNING - Finished tracing + transforming jit(gather) in 0.00040149688720703125 sec
2023-07-01 00:32:37,568 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[360,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:32:37,569 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0012078285217285156 sec
2023-07-01 00:32:37,570 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:37,580 - WARNING - Finished XLA compilation of jit(gather) in 0.009978771209716797 sec
2023-07-01 00:32:37,590 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:37,599 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:37,608 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:37,617 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:37,626 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:37,636 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:37,926 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client29//synthetic.png
2023-07-01 00:32:37,939 - INFO - c: 1.0 and total_data_in_this_class: 538
2023-07-01 00:32:37,939 - INFO - c: 7.0 and total_data_in_this_class: 261
2023-07-01 00:32:37,939 - INFO - c: 1.0 and total_data_in_this_class: 128
2023-07-01 00:32:37,939 - INFO - c: 7.0 and total_data_in_this_class: 72
2023-07-01 00:32:37,958 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002620220184326172 sec
2023-07-01 00:32:37,959 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:32:37,960 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012471675872802734 sec
2023-07-01 00:32:37,960 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:37,971 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010906696319580078 sec
2023-07-01 00:32:37,973 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002562999725341797 sec
2023-07-01 00:32:37,973 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:32:37,975 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009965896606445312 sec
2023-07-01 00:32:37,975 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:37,984 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008565664291381836 sec
2023-07-01 00:32:37,987 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00014543533325195312 sec
2023-07-01 00:32:37,988 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012826919555664062 sec
2023-07-01 00:32:37,989 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00031495094299316406 sec
2023-07-01 00:32:37,990 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002124309539794922 sec
2023-07-01 00:32:37,990 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012230873107910156 sec
2023-07-01 00:32:37,991 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00028896331787109375 sec
2023-07-01 00:32:37,992 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025534629821777344 sec
2023-07-01 00:32:37,992 - WARNING - Finished tracing + transforming absolute for pjit in 0.00017309188842773438 sec
2023-07-01 00:32:37,993 - WARNING - Finished tracing + transforming fn for pjit in 0.0003027915954589844 sec
2023-07-01 00:32:37,994 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00032591819763183594 sec
2023-07-01 00:32:37,994 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002079010009765625 sec
2023-07-01 00:32:37,996 - WARNING - Finished tracing + transforming fn for pjit in 0.00023317337036132812 sec
2023-07-01 00:32:37,996 - WARNING - Finished tracing + transforming fn for pjit in 0.00027489662170410156 sec
2023-07-01 00:32:37,997 - WARNING - Finished tracing + transforming fn for pjit in 0.00024247169494628906 sec
2023-07-01 00:32:37,997 - WARNING - Finished tracing + transforming fn for pjit in 0.0002720355987548828 sec
2023-07-01 00:32:37,999 - WARNING - Finished tracing + transforming fn for pjit in 0.0002307891845703125 sec
2023-07-01 00:32:38,001 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00017762184143066406 sec
2023-07-01 00:32:38,001 - WARNING - Finished tracing + transforming fn for pjit in 0.00023317337036132812 sec
2023-07-01 00:32:38,002 - WARNING - Finished tracing + transforming fn for pjit in 0.00023746490478515625 sec
2023-07-01 00:32:38,006 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003743171691894531 sec
2023-07-01 00:32:38,006 - WARNING - Finished tracing + transforming _mean for pjit in 0.000993490219116211 sec
2023-07-01 00:32:38,007 - WARNING - Finished tracing + transforming fn for pjit in 0.00024247169494628906 sec
2023-07-01 00:32:38,008 - WARNING - Finished tracing + transforming fn for pjit in 0.00023245811462402344 sec
2023-07-01 00:32:38,009 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00030231475830078125 sec
2023-07-01 00:32:38,009 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027179718017578125 sec
2023-07-01 00:32:38,010 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001819133758544922 sec
2023-07-01 00:32:38,011 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027108192443847656 sec
2023-07-01 00:32:38,012 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023436546325683594 sec
2023-07-01 00:32:38,012 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002396106719970703 sec
2023-07-01 00:32:38,013 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00036978721618652344 sec
2023-07-01 00:32:38,013 - WARNING - Finished tracing + transforming _where for pjit in 0.0009958744049072266 sec
2023-07-01 00:32:38,014 - WARNING - Finished tracing + transforming fn for pjit in 0.0002777576446533203 sec
2023-07-01 00:32:38,015 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002689361572265625 sec
2023-07-01 00:32:38,016 - WARNING - Finished tracing + transforming fn for pjit in 0.00023126602172851562 sec
2023-07-01 00:32:38,016 - WARNING - Finished tracing + transforming fn for pjit in 0.0002276897430419922 sec
2023-07-01 00:32:38,017 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022602081298828125 sec
2023-07-01 00:32:38,018 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002627372741699219 sec
2023-07-01 00:32:38,018 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002593994140625 sec
2023-07-01 00:32:38,019 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027060508728027344 sec
2023-07-01 00:32:38,020 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002300739288330078 sec
2023-07-01 00:32:38,021 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002276897430419922 sec
2023-07-01 00:32:38,021 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002613067626953125 sec
2023-07-01 00:32:38,022 - WARNING - Finished tracing + transforming _where for pjit in 0.0008585453033447266 sec
2023-07-01 00:32:38,023 - WARNING - Finished tracing + transforming fn for pjit in 0.00026679039001464844 sec
2023-07-01 00:32:38,023 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002646446228027344 sec
2023-07-01 00:32:38,024 - WARNING - Finished tracing + transforming fn for pjit in 0.00022602081298828125 sec
2023-07-01 00:32:38,029 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002741813659667969 sec
2023-07-01 00:32:38,029 - WARNING - Finished tracing + transforming fn for pjit in 0.0003414154052734375 sec
2023-07-01 00:32:38,030 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002703666687011719 sec
2023-07-01 00:32:38,031 - WARNING - Finished tracing + transforming fn for pjit in 0.0002262592315673828 sec
2023-07-01 00:32:38,035 - WARNING - Finished tracing + transforming fn for pjit in 0.0002257823944091797 sec
2023-07-01 00:32:38,036 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016951560974121094 sec
2023-07-01 00:32:38,037 - WARNING - Finished tracing + transforming fn for pjit in 0.0003135204315185547 sec
2023-07-01 00:32:38,038 - WARNING - Finished tracing + transforming fn for pjit in 0.00023293495178222656 sec
2023-07-01 00:32:38,057 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07058238983154297 sec
2023-07-01 00:32:38,060 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001304149627685547 sec
2023-07-01 00:32:38,061 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012421607971191406 sec
2023-07-01 00:32:38,061 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002741813659667969 sec
2023-07-01 00:32:38,063 - WARNING - Finished tracing + transforming fn for pjit in 0.00022983551025390625 sec
2023-07-01 00:32:38,064 - WARNING - Finished tracing + transforming fn for pjit in 0.00026488304138183594 sec
2023-07-01 00:32:38,065 - WARNING - Finished tracing + transforming fn for pjit in 0.00022411346435546875 sec
2023-07-01 00:32:38,071 - WARNING - Finished tracing + transforming fn for pjit in 0.00023484230041503906 sec
2023-07-01 00:32:38,073 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002257823944091797 sec
2023-07-01 00:32:38,073 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002636909484863281 sec
2023-07-01 00:32:38,074 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017786026000976562 sec
2023-07-01 00:32:38,075 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003352165222167969 sec
2023-07-01 00:32:38,075 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023031234741210938 sec
2023-07-01 00:32:38,076 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022840499877929688 sec
2023-07-01 00:32:38,077 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002963542938232422 sec
2023-07-01 00:32:38,077 - WARNING - Finished tracing + transforming _where for pjit in 0.0009019374847412109 sec
2023-07-01 00:32:38,078 - WARNING - Finished tracing + transforming fn for pjit in 0.00026726722717285156 sec
2023-07-01 00:32:38,079 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002613067626953125 sec
2023-07-01 00:32:38,080 - WARNING - Finished tracing + transforming fn for pjit in 0.00022411346435546875 sec
2023-07-01 00:32:38,080 - WARNING - Finished tracing + transforming fn for pjit in 0.0002849102020263672 sec
2023-07-01 00:32:38,092 - WARNING - Finished tracing + transforming fn for pjit in 0.0002238750457763672 sec
2023-07-01 00:32:38,113 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05496096611022949 sec
2023-07-01 00:32:38,114 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012564659118652344 sec
2023-07-01 00:32:38,115 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013971328735351562 sec
2023-07-01 00:32:38,116 - WARNING - Finished tracing + transforming _where for pjit in 0.0006356239318847656 sec
2023-07-01 00:32:38,116 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002968311309814453 sec
2023-07-01 00:32:38,116 - WARNING - Finished tracing + transforming trace for pjit in 0.002583742141723633 sec
2023-07-01 00:32:38,119 - WARNING - Finished tracing + transforming conjugate for pjit in 0.0001068115234375 sec
2023-07-01 00:32:38,120 - WARNING - Finished tracing + transforming tril for pjit in 0.0006830692291259766 sec
2023-07-01 00:32:38,120 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0018305778503417969 sec
2023-07-01 00:32:38,121 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010943412780761719 sec
2023-07-01 00:32:38,121 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010704994201660156 sec
2023-07-01 00:32:38,123 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014395713806152344 sec
2023-07-01 00:32:38,127 - WARNING - Finished tracing + transforming _solve for pjit in 0.009386301040649414 sec
2023-07-01 00:32:38,128 - WARNING - Finished tracing + transforming dot for pjit in 0.000324249267578125 sec
2023-07-01 00:32:38,131 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.14545774459838867 sec
2023-07-01 00:32:38,133 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:32:38,166 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03371596336364746 sec
2023-07-01 00:32:38,167 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:38,294 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12727952003479004 sec
2023-07-01 00:32:38,310 - INFO - initial test loss: 0.012270341621569113
2023-07-01 00:32:38,310 - INFO - initial test acc: 0.875
2023-07-01 00:32:38,316 - WARNING - Finished tracing + transforming dot for pjit in 0.0003681182861328125 sec
2023-07-01 00:32:38,316 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002987384796142578 sec
2023-07-01 00:32:38,317 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003781318664550781 sec
2023-07-01 00:32:38,318 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010123252868652344 sec
2023-07-01 00:32:38,319 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00019741058349609375 sec
2023-07-01 00:32:38,319 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00018596649169921875 sec
2023-07-01 00:32:38,320 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002532005310058594 sec
2023-07-01 00:32:38,321 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003790855407714844 sec
2023-07-01 00:32:38,321 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010828971862792969 sec
2023-07-01 00:32:38,322 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009992599487304688 sec
2023-07-01 00:32:38,331 - WARNING - Finished tracing + transforming fn for pjit in 0.0003190040588378906 sec
2023-07-01 00:32:38,331 - WARNING - Finished tracing + transforming fn for pjit in 0.00028324127197265625 sec
2023-07-01 00:32:38,332 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022220611572265625 sec
2023-07-01 00:32:38,333 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002777576446533203 sec
2023-07-01 00:32:38,333 - WARNING - Finished tracing + transforming _where for pjit in 0.000881195068359375 sec
2023-07-01 00:32:38,341 - WARNING - Finished tracing + transforming fn for pjit in 0.00025773048400878906 sec
2023-07-01 00:32:38,342 - WARNING - Finished tracing + transforming fn for pjit in 0.00026297569274902344 sec
2023-07-01 00:32:38,343 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000217437744140625 sec
2023-07-01 00:32:38,344 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002536773681640625 sec
2023-07-01 00:32:38,344 - WARNING - Finished tracing + transforming _where for pjit in 0.0008440017700195312 sec
2023-07-01 00:32:38,379 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00015354156494140625 sec
2023-07-01 00:32:38,436 - WARNING - Finished tracing + transforming fn for pjit in 0.0002758502960205078 sec
2023-07-01 00:32:38,437 - WARNING - Finished tracing + transforming fn for pjit in 0.00025272369384765625 sec
2023-07-01 00:32:38,438 - WARNING - Finished tracing + transforming square for pjit in 0.00017452239990234375 sec
2023-07-01 00:32:38,440 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022268295288085938 sec
2023-07-01 00:32:38,441 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002484321594238281 sec
2023-07-01 00:32:38,442 - WARNING - Finished tracing + transforming fn for pjit in 0.0002701282501220703 sec
2023-07-01 00:32:38,443 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022721290588378906 sec
2023-07-01 00:32:38,443 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002357959747314453 sec
2023-07-01 00:32:38,444 - WARNING - Finished tracing + transforming fn for pjit in 0.00027108192443847656 sec
2023-07-01 00:32:38,445 - WARNING - Finished tracing + transforming fn for pjit in 0.00023317337036132812 sec
2023-07-01 00:32:38,445 - WARNING - Finished tracing + transforming square for pjit in 0.00016689300537109375 sec
2023-07-01 00:32:38,447 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002243518829345703 sec
2023-07-01 00:32:38,449 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017690658569335938 sec
2023-07-01 00:32:38,449 - WARNING - Finished tracing + transforming fn for pjit in 0.00027108192443847656 sec
2023-07-01 00:32:38,450 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022459030151367188 sec
2023-07-01 00:32:38,450 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023555755615234375 sec
2023-07-01 00:32:38,451 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13991117477416992 sec
2023-07-01 00:32:38,455 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[348,10]), ShapedArray(float32[348,10]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:32:38,519 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06372642517089844 sec
2023-07-01 00:32:38,519 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:38,829 - WARNING - Finished XLA compilation of jit(update_fn) in 0.30913662910461426 sec
2023-07-01 00:32:39,547 - INFO - Distilling data from client: Client30
2023-07-01 00:32:39,547 - INFO - train loss: 0.002570110080897021
2023-07-01 00:32:39,547 - INFO - train acc: 1.0
2023-07-01 00:32:39,586 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.90      0.89       128
           7       0.81      0.79      0.80        72

    accuracy                           0.86       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:32:39,586 - INFO - test loss 0.01095367295790641
2023-07-01 00:32:39,586 - INFO - test acc 0.85999995470047
2023-07-01 00:32:40,314 - INFO - Distilling data from client: Client30
2023-07-01 00:32:40,314 - INFO - train loss: 0.0016674648634716267
2023-07-01 00:32:40,315 - INFO - train acc: 1.0
2023-07-01 00:32:40,332 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.89      0.89       128
           7       0.80      0.79      0.80        72

    accuracy                           0.85       200
   macro avg       0.84      0.84      0.84       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:32:40,332 - INFO - test loss 0.011169929897244406
2023-07-01 00:32:40,332 - INFO - test acc 0.8549999594688416
2023-07-01 00:32:41,050 - INFO - Distilling data from client: Client30
2023-07-01 00:32:41,051 - INFO - train loss: 0.0014758087736599782
2023-07-01 00:32:41,051 - INFO - train acc: 1.0
2023-07-01 00:32:41,098 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.91      0.90       128
           7       0.83      0.81      0.82        72

    accuracy                           0.87       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:32:41,098 - INFO - test loss 0.011259968882884682
2023-07-01 00:32:41,098 - INFO - test acc 0.8700000047683716
2023-07-01 00:32:41,821 - INFO - Distilling data from client: Client30
2023-07-01 00:32:41,821 - INFO - train loss: 0.0013940836343450905
2023-07-01 00:32:41,821 - INFO - train acc: 0.9971264600753784
2023-07-01 00:32:41,838 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.88      0.89       128
           7       0.79      0.83      0.81        72

    accuracy                           0.86       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:32:41,838 - INFO - test loss 0.011342150937805051
2023-07-01 00:32:41,838 - INFO - test acc 0.85999995470047
2023-07-01 00:32:42,549 - INFO - Distilling data from client: Client30
2023-07-01 00:32:42,549 - INFO - train loss: 0.0013352626361436803
2023-07-01 00:32:42,549 - INFO - train acc: 1.0
2023-07-01 00:32:42,566 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.90      0.89       128
           7       0.82      0.81      0.81        72

    accuracy                           0.86       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:32:42,566 - INFO - test loss 0.010722916380513748
2023-07-01 00:32:42,566 - INFO - test acc 0.8650000095367432
2023-07-01 00:32:43,285 - INFO - Distilling data from client: Client30
2023-07-01 00:32:43,285 - INFO - train loss: 0.0012365867869661372
2023-07-01 00:32:43,285 - INFO - train acc: 0.9971264600753784
2023-07-01 00:32:43,328 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.91      0.90       128
           7       0.83      0.82      0.83        72

    accuracy                           0.88       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.87      0.88      0.87       200

2023-07-01 00:32:43,328 - INFO - test loss 0.010606762280173326
2023-07-01 00:32:43,329 - INFO - test acc 0.875
2023-07-01 00:32:44,051 - INFO - Distilling data from client: Client30
2023-07-01 00:32:44,052 - INFO - train loss: 0.001144437648651747
2023-07-01 00:32:44,052 - INFO - train acc: 0.9971264600753784
2023-07-01 00:32:44,068 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.91      0.89       128
           7       0.83      0.74      0.78        72

    accuracy                           0.85       200
   macro avg       0.84      0.83      0.83       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:32:44,069 - INFO - test loss 0.011041516172401924
2023-07-01 00:32:44,069 - INFO - test acc 0.8499999642372131
2023-07-01 00:32:44,794 - INFO - Distilling data from client: Client30
2023-07-01 00:32:44,794 - INFO - train loss: 0.0008526089948098768
2023-07-01 00:32:44,794 - INFO - train acc: 1.0
2023-07-01 00:32:44,812 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.90      0.89       128
           7       0.81      0.78      0.79        72

    accuracy                           0.85       200
   macro avg       0.84      0.84      0.84       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:32:44,812 - INFO - test loss 0.011098349295514868
2023-07-01 00:32:44,812 - INFO - test acc 0.8549999594688416
2023-07-01 00:32:45,531 - INFO - Distilling data from client: Client30
2023-07-01 00:32:45,531 - INFO - train loss: 0.0010227382931276242
2023-07-01 00:32:45,531 - INFO - train acc: 1.0
2023-07-01 00:32:45,548 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.91      0.90       128
           7       0.84      0.79      0.81        72

    accuracy                           0.87       200
   macro avg       0.86      0.85      0.86       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:32:45,548 - INFO - test loss 0.010563488645843309
2023-07-01 00:32:45,548 - INFO - test acc 0.8700000047683716
2023-07-01 00:32:46,277 - INFO - Distilling data from client: Client30
2023-07-01 00:32:46,277 - INFO - train loss: 0.0008524222901732562
2023-07-01 00:32:46,278 - INFO - train acc: 1.0
2023-07-01 00:32:46,319 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.93      0.91       128
           7       0.86      0.79      0.83        72

    accuracy                           0.88       200
   macro avg       0.88      0.86      0.87       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:32:46,319 - INFO - test loss 0.01066285504610293
2023-07-01 00:32:46,320 - INFO - test acc 0.8799999952316284
2023-07-01 00:32:47,046 - INFO - Distilling data from client: Client30
2023-07-01 00:32:47,046 - INFO - train loss: 0.0011050340520553942
2023-07-01 00:32:47,046 - INFO - train acc: 1.0
2023-07-01 00:32:47,066 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.91      0.89       128
           7       0.83      0.76      0.80        72

    accuracy                           0.86       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:32:47,066 - INFO - test loss 0.010959142406765068
2023-07-01 00:32:47,066 - INFO - test acc 0.85999995470047
2023-07-01 00:32:47,775 - INFO - Distilling data from client: Client30
2023-07-01 00:32:47,775 - INFO - train loss: 0.0009135117391391448
2023-07-01 00:32:47,775 - INFO - train acc: 1.0
2023-07-01 00:32:47,791 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.91      0.90       128
           7       0.83      0.81      0.82        72

    accuracy                           0.87       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:32:47,792 - INFO - test loss 0.010914367387072275
2023-07-01 00:32:47,792 - INFO - test acc 0.8700000047683716
2023-07-01 00:32:48,512 - INFO - Distilling data from client: Client30
2023-07-01 00:32:48,512 - INFO - train loss: 0.00071870253997224
2023-07-01 00:32:48,512 - INFO - train acc: 1.0
2023-07-01 00:32:48,554 - INFO - report:               precision    recall  f1-score   support

           1       0.91      0.94      0.92       128
           7       0.88      0.83      0.86        72

    accuracy                           0.90       200
   macro avg       0.90      0.89      0.89       200
weighted avg       0.90      0.90      0.90       200

2023-07-01 00:32:48,554 - INFO - test loss 0.009489094786843772
2023-07-01 00:32:48,554 - INFO - test acc 0.8999999761581421
2023-07-01 00:32:49,275 - INFO - Distilling data from client: Client30
2023-07-01 00:32:49,275 - INFO - train loss: 0.0007465562437998579
2023-07-01 00:32:49,275 - INFO - train acc: 1.0
2023-07-01 00:32:49,291 - INFO - report:               precision    recall  f1-score   support

           1       0.85      0.91      0.88       128
           7       0.81      0.72      0.76        72

    accuracy                           0.84       200
   macro avg       0.83      0.81      0.82       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:32:49,291 - INFO - test loss 0.01143033632624033
2023-07-01 00:32:49,291 - INFO - test acc 0.8399999737739563
2023-07-01 00:32:50,016 - INFO - Distilling data from client: Client30
2023-07-01 00:32:50,016 - INFO - train loss: 0.0008574814184063635
2023-07-01 00:32:50,016 - INFO - train acc: 1.0
2023-07-01 00:32:50,033 - INFO - report:               precision    recall  f1-score   support

           1       0.92      0.88      0.90       128
           7       0.79      0.86      0.83        72

    accuracy                           0.87       200
   macro avg       0.86      0.87      0.86       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:32:50,033 - INFO - test loss 0.01034207196972157
2023-07-01 00:32:50,033 - INFO - test acc 0.8700000047683716
2023-07-01 00:32:50,765 - INFO - Distilling data from client: Client30
2023-07-01 00:32:50,765 - INFO - train loss: 0.000812046792214379
2023-07-01 00:32:50,766 - INFO - train acc: 1.0
2023-07-01 00:32:50,783 - INFO - report:               precision    recall  f1-score   support

           1       0.91      0.91      0.91       128
           7       0.83      0.83      0.83        72

    accuracy                           0.88       200
   macro avg       0.87      0.87      0.87       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:32:50,783 - INFO - test loss 0.010915942933178461
2023-07-01 00:32:50,783 - INFO - test acc 0.8799999952316284
2023-07-01 00:32:51,507 - INFO - Distilling data from client: Client30
2023-07-01 00:32:51,507 - INFO - train loss: 0.0007285988105551572
2023-07-01 00:32:51,507 - INFO - train acc: 1.0
2023-07-01 00:32:51,525 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.92      0.91       128
           7       0.86      0.82      0.84        72

    accuracy                           0.89       200
   macro avg       0.88      0.87      0.87       200
weighted avg       0.88      0.89      0.88       200

2023-07-01 00:32:51,525 - INFO - test loss 0.010598095086495448
2023-07-01 00:32:51,525 - INFO - test acc 0.8849999904632568
2023-07-01 00:32:52,244 - INFO - Distilling data from client: Client30
2023-07-01 00:32:52,245 - INFO - train loss: 0.0007673741104454258
2023-07-01 00:32:52,245 - INFO - train acc: 1.0
2023-07-01 00:32:52,263 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.91      0.89       128
           7       0.82      0.78      0.80        72

    accuracy                           0.86       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:32:52,263 - INFO - test loss 0.010608809140262248
2023-07-01 00:32:52,263 - INFO - test acc 0.85999995470047
2023-07-01 00:32:52,987 - INFO - Distilling data from client: Client30
2023-07-01 00:32:52,987 - INFO - train loss: 0.0008321234596015061
2023-07-01 00:32:52,987 - INFO - train acc: 1.0
2023-07-01 00:32:53,003 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.91      0.90       128
           7       0.83      0.79      0.81        72

    accuracy                           0.86       200
   macro avg       0.86      0.85      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:32:53,003 - INFO - test loss 0.011164454182433607
2023-07-01 00:32:53,003 - INFO - test acc 0.8650000095367432
2023-07-01 00:32:53,722 - INFO - Distilling data from client: Client30
2023-07-01 00:32:53,722 - INFO - train loss: 0.0007064465389175177
2023-07-01 00:32:53,722 - INFO - train acc: 1.0
2023-07-01 00:32:53,740 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.89      0.89       128
           7       0.81      0.81      0.81        72

    accuracy                           0.86       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:32:53,740 - INFO - test loss 0.011193996965521088
2023-07-01 00:32:53,740 - INFO - test acc 0.85999995470047
2023-07-01 00:32:54,455 - INFO - Distilling data from client: Client30
2023-07-01 00:32:54,455 - INFO - train loss: 0.0008442450520135422
2023-07-01 00:32:54,455 - INFO - train acc: 1.0
2023-07-01 00:32:54,499 - INFO - report:               precision    recall  f1-score   support

           1       0.92      0.94      0.93       128
           7       0.88      0.85      0.87        72

    accuracy                           0.91       200
   macro avg       0.90      0.89      0.90       200
weighted avg       0.90      0.91      0.90       200

2023-07-01 00:32:54,499 - INFO - test loss 0.010397595815651316
2023-07-01 00:32:54,499 - INFO - test acc 0.9049999713897705
2023-07-01 00:32:55,230 - INFO - Distilling data from client: Client30
2023-07-01 00:32:55,231 - INFO - train loss: 0.0008227601710577916
2023-07-01 00:32:55,231 - INFO - train acc: 1.0
2023-07-01 00:32:55,247 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.91      0.89       128
           7       0.82      0.76      0.79        72

    accuracy                           0.85       200
   macro avg       0.85      0.84      0.84       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:32:55,247 - INFO - test loss 0.01150709373794491
2023-07-01 00:32:55,247 - INFO - test acc 0.8549999594688416
2023-07-01 00:32:55,959 - INFO - Distilling data from client: Client30
2023-07-01 00:32:55,959 - INFO - train loss: 0.0007806726105509318
2023-07-01 00:32:55,959 - INFO - train acc: 1.0
2023-07-01 00:32:55,976 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.88      0.89       128
           7       0.79      0.81      0.80        72

    accuracy                           0.85       200
   macro avg       0.84      0.84      0.84       200
weighted avg       0.86      0.85      0.86       200

2023-07-01 00:32:55,976 - INFO - test loss 0.010362015871809578
2023-07-01 00:32:55,976 - INFO - test acc 0.8549999594688416
2023-07-01 00:32:56,690 - INFO - Distilling data from client: Client30
2023-07-01 00:32:56,690 - INFO - train loss: 0.0007970920735715202
2023-07-01 00:32:56,691 - INFO - train acc: 1.0
2023-07-01 00:32:56,707 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.92      0.89       128
           7       0.84      0.75      0.79        72

    accuracy                           0.86       200
   macro avg       0.86      0.84      0.84       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:32:56,707 - INFO - test loss 0.010403668550759328
2023-07-01 00:32:56,708 - INFO - test acc 0.85999995470047
2023-07-01 00:32:57,432 - INFO - Distilling data from client: Client30
2023-07-01 00:32:57,432 - INFO - train loss: 0.00079792771153234
2023-07-01 00:32:57,432 - INFO - train acc: 1.0
2023-07-01 00:32:57,449 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.91      0.90       128
           7       0.83      0.82      0.83        72

    accuracy                           0.88       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.87      0.88      0.87       200

2023-07-01 00:32:57,449 - INFO - test loss 0.010411419944264557
2023-07-01 00:32:57,449 - INFO - test acc 0.875
2023-07-01 00:32:57,451 - WARNING - Finished tracing + transforming jit(gather) in 0.00024819374084472656 sec
2023-07-01 00:32:57,452 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[348,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:32:57,453 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0012025833129882812 sec
2023-07-01 00:32:57,453 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:57,463 - WARNING - Finished XLA compilation of jit(gather) in 0.009883403778076172 sec
2023-07-01 00:32:57,474 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:57,483 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:57,492 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:57,500 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:57,510 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:57,519 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:32:57,804 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client30//synthetic.png
2023-07-01 00:32:57,817 - INFO - c: 8.0 and total_data_in_this_class: 261
2023-07-01 00:32:57,818 - INFO - c: 9.0 and total_data_in_this_class: 538
2023-07-01 00:32:57,818 - INFO - c: 8.0 and total_data_in_this_class: 72
2023-07-01 00:32:57,818 - INFO - c: 9.0 and total_data_in_this_class: 128
2023-07-01 00:32:57,888 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04801678657531738 sec
2023-07-01 00:32:57,934 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04525136947631836 sec
2023-07-01 00:32:57,939 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.10034847259521484 sec
2023-07-01 00:32:57,941 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:32:57,974 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.0327603816986084 sec
2023-07-01 00:32:57,974 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:58,099 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12528347969055176 sec
2023-07-01 00:32:58,117 - INFO - initial test loss: 0.016177976646153027
2023-07-01 00:32:58,117 - INFO - initial test acc: 0.824999988079071
2023-07-01 00:32:58,127 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.007882118225097656 sec
2023-07-01 00:32:58,235 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11655020713806152 sec
2023-07-01 00:32:58,239 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[348,10]), ShapedArray(float32[348,10]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10]), ShapedArray(float32[348,3,32,32]), ShapedArray(float32[348,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:32:58,301 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.0619351863861084 sec
2023-07-01 00:32:58,301 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:32:58,608 - WARNING - Finished XLA compilation of jit(update_fn) in 0.30688047409057617 sec
2023-07-01 00:32:59,337 - INFO - Distilling data from client: Client31
2023-07-01 00:32:59,337 - INFO - train loss: 0.003851158929329967
2023-07-01 00:32:59,337 - INFO - train acc: 0.9856321811676025
2023-07-01 00:32:59,375 - INFO - report:               precision    recall  f1-score   support

           8       0.72      0.75      0.73        72
           9       0.86      0.84      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:32:59,376 - INFO - test loss 0.014396134648659261
2023-07-01 00:32:59,376 - INFO - test acc 0.8050000071525574
2023-07-01 00:33:00,099 - INFO - Distilling data from client: Client31
2023-07-01 00:33:00,099 - INFO - train loss: 0.0026328467356376514
2023-07-01 00:33:00,099 - INFO - train acc: 0.9971264600753784
2023-07-01 00:33:00,142 - INFO - report:               precision    recall  f1-score   support

           8       0.76      0.74      0.75        72
           9       0.85      0.87      0.86       128

    accuracy                           0.82       200
   macro avg       0.81      0.80      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-07-01 00:33:00,142 - INFO - test loss 0.0136447422295648
2023-07-01 00:33:00,142 - INFO - test acc 0.8199999928474426
2023-07-01 00:33:00,859 - INFO - Distilling data from client: Client31
2023-07-01 00:33:00,859 - INFO - train loss: 0.0023437612919518526
2023-07-01 00:33:00,859 - INFO - train acc: 1.0
2023-07-01 00:33:00,876 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.71      0.73        72
           9       0.84      0.87      0.85       128

    accuracy                           0.81       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:33:00,876 - INFO - test loss 0.014100645347211956
2023-07-01 00:33:00,876 - INFO - test acc 0.8100000023841858
2023-07-01 00:33:01,603 - INFO - Distilling data from client: Client31
2023-07-01 00:33:01,603 - INFO - train loss: 0.0023104642229571657
2023-07-01 00:33:01,603 - INFO - train acc: 1.0
2023-07-01 00:33:01,645 - INFO - report:               precision    recall  f1-score   support

           8       0.77      0.78      0.77        72
           9       0.87      0.87      0.87       128

    accuracy                           0.83       200
   macro avg       0.82      0.82      0.82       200
weighted avg       0.84      0.83      0.84       200

2023-07-01 00:33:01,645 - INFO - test loss 0.013080481437585195
2023-07-01 00:33:01,645 - INFO - test acc 0.8349999785423279
2023-07-01 00:33:02,375 - INFO - Distilling data from client: Client31
2023-07-01 00:33:02,375 - INFO - train loss: 0.002327839731062831
2023-07-01 00:33:02,375 - INFO - train acc: 0.9942528605461121
2023-07-01 00:33:02,417 - INFO - report:               precision    recall  f1-score   support

           8       0.83      0.75      0.79        72
           9       0.87      0.91      0.89       128

    accuracy                           0.85       200
   macro avg       0.85      0.83      0.84       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:33:02,417 - INFO - test loss 0.012613051695181692
2023-07-01 00:33:02,417 - INFO - test acc 0.8549999594688416
2023-07-01 00:33:03,148 - INFO - Distilling data from client: Client31
2023-07-01 00:33:03,149 - INFO - train loss: 0.0022572788730229423
2023-07-01 00:33:03,149 - INFO - train acc: 0.9971264600753784
2023-07-01 00:33:03,165 - INFO - report:               precision    recall  f1-score   support

           8       0.76      0.69      0.72        72
           9       0.84      0.88      0.85       128

    accuracy                           0.81       200
   macro avg       0.80      0.78      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:33:03,166 - INFO - test loss 0.014439496417611568
2023-07-01 00:33:03,166 - INFO - test acc 0.8100000023841858
2023-07-01 00:33:03,906 - INFO - Distilling data from client: Client31
2023-07-01 00:33:03,906 - INFO - train loss: 0.0019695752999189897
2023-07-01 00:33:03,906 - INFO - train acc: 1.0
2023-07-01 00:33:03,923 - INFO - report:               precision    recall  f1-score   support

           8       0.73      0.72      0.73        72
           9       0.84      0.85      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.80      0.81      0.80       200

2023-07-01 00:33:03,923 - INFO - test loss 0.013920739529540626
2023-07-01 00:33:03,924 - INFO - test acc 0.8050000071525574
2023-07-01 00:33:04,650 - INFO - Distilling data from client: Client31
2023-07-01 00:33:04,650 - INFO - train loss: 0.0014547907980657301
2023-07-01 00:33:04,650 - INFO - train acc: 1.0
2023-07-01 00:33:04,667 - INFO - report:               precision    recall  f1-score   support

           8       0.80      0.74      0.77        72
           9       0.86      0.90      0.88       128

    accuracy                           0.84       200
   macro avg       0.83      0.82      0.82       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:33:04,667 - INFO - test loss 0.01385401497959883
2023-07-01 00:33:04,667 - INFO - test acc 0.8399999737739563
2023-07-01 00:33:05,391 - INFO - Distilling data from client: Client31
2023-07-01 00:33:05,391 - INFO - train loss: 0.001745084176054374
2023-07-01 00:33:05,391 - INFO - train acc: 1.0
2023-07-01 00:33:05,408 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.79      0.77        72
           9       0.88      0.85      0.87       128

    accuracy                           0.83       200
   macro avg       0.81      0.82      0.82       200
weighted avg       0.83      0.83      0.83       200

2023-07-01 00:33:05,408 - INFO - test loss 0.01354638168696788
2023-07-01 00:33:05,409 - INFO - test acc 0.8299999833106995
2023-07-01 00:33:06,134 - INFO - Distilling data from client: Client31
2023-07-01 00:33:06,134 - INFO - train loss: 0.0017563768847357055
2023-07-01 00:33:06,134 - INFO - train acc: 1.0
2023-07-01 00:33:06,150 - INFO - report:               precision    recall  f1-score   support

           8       0.73      0.72      0.73        72
           9       0.84      0.85      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.80      0.81      0.80       200

2023-07-01 00:33:06,151 - INFO - test loss 0.014179765079126813
2023-07-01 00:33:06,151 - INFO - test acc 0.8050000071525574
2023-07-01 00:33:06,875 - INFO - Distilling data from client: Client31
2023-07-01 00:33:06,875 - INFO - train loss: 0.0020376186782854718
2023-07-01 00:33:06,876 - INFO - train acc: 0.9942528605461121
2023-07-01 00:33:06,893 - INFO - report:               precision    recall  f1-score   support

           8       0.71      0.72      0.72        72
           9       0.84      0.84      0.84       128

    accuracy                           0.80       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:33:06,893 - INFO - test loss 0.013962336327248142
2023-07-01 00:33:06,893 - INFO - test acc 0.7949999570846558
2023-07-01 00:33:07,627 - INFO - Distilling data from client: Client31
2023-07-01 00:33:07,627 - INFO - train loss: 0.0017496829126118414
2023-07-01 00:33:07,627 - INFO - train acc: 1.0
2023-07-01 00:33:07,646 - INFO - report:               precision    recall  f1-score   support

           8       0.78      0.78      0.78        72
           9       0.88      0.88      0.88       128

    accuracy                           0.84       200
   macro avg       0.83      0.83      0.83       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:33:07,646 - INFO - test loss 0.013336334196100082
2023-07-01 00:33:07,646 - INFO - test acc 0.8399999737739563
2023-07-01 00:33:08,381 - INFO - Distilling data from client: Client31
2023-07-01 00:33:08,381 - INFO - train loss: 0.0019087684660631806
2023-07-01 00:33:08,381 - INFO - train acc: 0.9971264600753784
2023-07-01 00:33:08,399 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.74      0.74        72
           9       0.85      0.86      0.86       128

    accuracy                           0.81       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:33:08,399 - INFO - test loss 0.0140499686824787
2023-07-01 00:33:08,399 - INFO - test acc 0.8149999976158142
2023-07-01 00:33:09,126 - INFO - Distilling data from client: Client31
2023-07-01 00:33:09,126 - INFO - train loss: 0.0018383359190866676
2023-07-01 00:33:09,126 - INFO - train acc: 0.9971264600753784
2023-07-01 00:33:09,143 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.75      0.75        72
           9       0.86      0.86      0.86       128

    accuracy                           0.82       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.82      0.82      0.82       200

2023-07-01 00:33:09,144 - INFO - test loss 0.014222445326834034
2023-07-01 00:33:09,144 - INFO - test acc 0.8199999928474426
2023-07-01 00:33:09,861 - INFO - Distilling data from client: Client31
2023-07-01 00:33:09,861 - INFO - train loss: 0.001623707295141286
2023-07-01 00:33:09,861 - INFO - train acc: 1.0
2023-07-01 00:33:09,877 - INFO - report:               precision    recall  f1-score   support

           8       0.79      0.78      0.78        72
           9       0.88      0.88      0.88       128

    accuracy                           0.84       200
   macro avg       0.83      0.83      0.83       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:33:09,878 - INFO - test loss 0.013304911942445099
2023-07-01 00:33:09,878 - INFO - test acc 0.8449999690055847
2023-07-01 00:33:10,608 - INFO - Distilling data from client: Client31
2023-07-01 00:33:10,608 - INFO - train loss: 0.0017157505445180813
2023-07-01 00:33:10,608 - INFO - train acc: 1.0
2023-07-01 00:33:10,625 - INFO - report:               precision    recall  f1-score   support

           8       0.74      0.78      0.76        72
           9       0.87      0.84      0.86       128

    accuracy                           0.82       200
   macro avg       0.80      0.81      0.81       200
weighted avg       0.82      0.82      0.82       200

2023-07-01 00:33:10,625 - INFO - test loss 0.01314316256595333
2023-07-01 00:33:10,625 - INFO - test acc 0.8199999928474426
2023-07-01 00:33:11,352 - INFO - Distilling data from client: Client31
2023-07-01 00:33:11,352 - INFO - train loss: 0.0017244388071722479
2023-07-01 00:33:11,352 - INFO - train acc: 1.0
2023-07-01 00:33:11,370 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.71      0.73        72
           9       0.84      0.87      0.85       128

    accuracy                           0.81       200
   macro avg       0.80      0.79      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:33:11,370 - INFO - test loss 0.014489479672508673
2023-07-01 00:33:11,370 - INFO - test acc 0.8100000023841858
2023-07-01 00:33:12,086 - INFO - Distilling data from client: Client31
2023-07-01 00:33:12,087 - INFO - train loss: 0.0015405932099033192
2023-07-01 00:33:12,087 - INFO - train acc: 0.9971264600753784
2023-07-01 00:33:12,103 - INFO - report:               precision    recall  f1-score   support

           8       0.72      0.76      0.74        72
           9       0.86      0.84      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.80      0.80       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:33:12,104 - INFO - test loss 0.012701577689690026
2023-07-01 00:33:12,104 - INFO - test acc 0.8100000023841858
2023-07-01 00:33:12,824 - INFO - Distilling data from client: Client31
2023-07-01 00:33:12,824 - INFO - train loss: 0.0017262120837749698
2023-07-01 00:33:12,824 - INFO - train acc: 0.9942528605461121
2023-07-01 00:33:12,842 - INFO - report:               precision    recall  f1-score   support

           8       0.77      0.78      0.77        72
           9       0.87      0.87      0.87       128

    accuracy                           0.83       200
   macro avg       0.82      0.82      0.82       200
weighted avg       0.84      0.83      0.84       200

2023-07-01 00:33:12,842 - INFO - test loss 0.012876004650152845
2023-07-01 00:33:12,842 - INFO - test acc 0.8349999785423279
2023-07-01 00:33:13,580 - INFO - Distilling data from client: Client31
2023-07-01 00:33:13,580 - INFO - train loss: 0.0017557749033955776
2023-07-01 00:33:13,580 - INFO - train acc: 0.9971264600753784
2023-07-01 00:33:13,598 - INFO - report:               precision    recall  f1-score   support

           8       0.74      0.72      0.73        72
           9       0.85      0.86      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:33:13,598 - INFO - test loss 0.01454989941168886
2023-07-01 00:33:13,598 - INFO - test acc 0.8100000023841858
2023-07-01 00:33:14,328 - INFO - Distilling data from client: Client31
2023-07-01 00:33:14,329 - INFO - train loss: 0.001392209611157457
2023-07-01 00:33:14,329 - INFO - train acc: 1.0
2023-07-01 00:33:14,346 - INFO - report:               precision    recall  f1-score   support

           8       0.77      0.74      0.75        72
           9       0.85      0.88      0.86       128

    accuracy                           0.82       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.82      0.82      0.82       200

2023-07-01 00:33:14,347 - INFO - test loss 0.013158235612803715
2023-07-01 00:33:14,347 - INFO - test acc 0.824999988079071
2023-07-01 00:33:15,079 - INFO - Distilling data from client: Client31
2023-07-01 00:33:15,079 - INFO - train loss: 0.001768985688073364
2023-07-01 00:33:15,080 - INFO - train acc: 0.9971264600753784
2023-07-01 00:33:15,097 - INFO - report:               precision    recall  f1-score   support

           8       0.76      0.76      0.76        72
           9       0.87      0.87      0.87       128

    accuracy                           0.83       200
   macro avg       0.82      0.82      0.82       200
weighted avg       0.83      0.83      0.83       200

2023-07-01 00:33:15,097 - INFO - test loss 0.01317606823750907
2023-07-01 00:33:15,097 - INFO - test acc 0.8299999833106995
2023-07-01 00:33:15,824 - INFO - Distilling data from client: Client31
2023-07-01 00:33:15,824 - INFO - train loss: 0.0018949019586971272
2023-07-01 00:33:15,824 - INFO - train acc: 1.0
2023-07-01 00:33:15,841 - INFO - report:               precision    recall  f1-score   support

           8       0.74      0.72      0.73        72
           9       0.85      0.86      0.85       128

    accuracy                           0.81       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:33:15,842 - INFO - test loss 0.014710189111862476
2023-07-01 00:33:15,842 - INFO - test acc 0.8100000023841858
2023-07-01 00:33:16,568 - INFO - Distilling data from client: Client31
2023-07-01 00:33:16,568 - INFO - train loss: 0.0014203624352263744
2023-07-01 00:33:16,568 - INFO - train acc: 1.0
2023-07-01 00:33:16,588 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.76      0.76        72
           9       0.87      0.86      0.86       128

    accuracy                           0.82       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.83      0.82      0.83       200

2023-07-01 00:33:16,588 - INFO - test loss 0.014092637451298344
2023-07-01 00:33:16,588 - INFO - test acc 0.824999988079071
2023-07-01 00:33:17,306 - INFO - Distilling data from client: Client31
2023-07-01 00:33:17,306 - INFO - train loss: 0.0013775363066575435
2023-07-01 00:33:17,306 - INFO - train acc: 1.0
2023-07-01 00:33:17,324 - INFO - report:               precision    recall  f1-score   support

           8       0.75      0.78      0.76        72
           9       0.87      0.85      0.86       128

    accuracy                           0.82       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.83      0.82      0.83       200

2023-07-01 00:33:17,324 - INFO - test loss 0.014170146078525321
2023-07-01 00:33:17,324 - INFO - test acc 0.824999988079071
2023-07-01 00:33:17,336 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:17,345 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:17,354 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:17,363 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:17,372 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:17,381 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:17,667 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client31//synthetic.png
2023-07-01 00:33:17,678 - INFO - c: 0.0 and total_data_in_this_class: 255
2023-07-01 00:33:17,678 - INFO - c: 2.0 and total_data_in_this_class: 273
2023-07-01 00:33:17,678 - INFO - c: 3.0 and total_data_in_this_class: 271
2023-07-01 00:33:17,678 - INFO - c: 0.0 and total_data_in_this_class: 78
2023-07-01 00:33:17,678 - INFO - c: 2.0 and total_data_in_this_class: 60
2023-07-01 00:33:17,678 - INFO - c: 3.0 and total_data_in_this_class: 62
2023-07-01 00:33:17,699 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002586841583251953 sec
2023-07-01 00:33:17,699 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:33:17,700 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012195110321044922 sec
2023-07-01 00:33:17,700 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:33:17,711 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010674238204956055 sec
2023-07-01 00:33:17,713 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025010108947753906 sec
2023-07-01 00:33:17,714 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:33:17,715 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009675025939941406 sec
2023-07-01 00:33:17,715 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:33:17,724 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008518695831298828 sec
2023-07-01 00:33:17,727 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00014019012451171875 sec
2023-07-01 00:33:17,728 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011992454528808594 sec
2023-07-01 00:33:17,728 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00030231475830078125 sec
2023-07-01 00:33:17,730 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002071857452392578 sec
2023-07-01 00:33:17,730 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011777877807617188 sec
2023-07-01 00:33:17,731 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002791881561279297 sec
2023-07-01 00:33:17,731 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000244140625 sec
2023-07-01 00:33:17,732 - WARNING - Finished tracing + transforming absolute for pjit in 0.0001633167266845703 sec
2023-07-01 00:33:17,733 - WARNING - Finished tracing + transforming fn for pjit in 0.0002689361572265625 sec
2023-07-01 00:33:17,733 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0003120899200439453 sec
2023-07-01 00:33:17,734 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001926422119140625 sec
2023-07-01 00:33:17,735 - WARNING - Finished tracing + transforming fn for pjit in 0.0002238750457763672 sec
2023-07-01 00:33:17,736 - WARNING - Finished tracing + transforming fn for pjit in 0.0002613067626953125 sec
2023-07-01 00:33:17,736 - WARNING - Finished tracing + transforming fn for pjit in 0.0002200603485107422 sec
2023-07-01 00:33:17,737 - WARNING - Finished tracing + transforming fn for pjit in 0.0002765655517578125 sec
2023-07-01 00:33:17,738 - WARNING - Finished tracing + transforming fn for pjit in 0.0002307891845703125 sec
2023-07-01 00:33:17,740 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001723766326904297 sec
2023-07-01 00:33:17,740 - WARNING - Finished tracing + transforming fn for pjit in 0.00023102760314941406 sec
2023-07-01 00:33:17,741 - WARNING - Finished tracing + transforming fn for pjit in 0.00023317337036132812 sec
2023-07-01 00:33:17,745 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003693103790283203 sec
2023-07-01 00:33:17,745 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009610652923583984 sec
2023-07-01 00:33:17,746 - WARNING - Finished tracing + transforming fn for pjit in 0.00023698806762695312 sec
2023-07-01 00:33:17,747 - WARNING - Finished tracing + transforming fn for pjit in 0.00022292137145996094 sec
2023-07-01 00:33:17,748 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002887248992919922 sec
2023-07-01 00:33:17,748 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002608299255371094 sec
2023-07-01 00:33:17,749 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00018310546875 sec
2023-07-01 00:33:17,750 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026702880859375 sec
2023-07-01 00:33:17,751 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023031234741210938 sec
2023-07-01 00:33:17,751 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022840499877929688 sec
2023-07-01 00:33:17,752 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003504753112792969 sec
2023-07-01 00:33:17,752 - WARNING - Finished tracing + transforming _where for pjit in 0.0009486675262451172 sec
2023-07-01 00:33:17,753 - WARNING - Finished tracing + transforming fn for pjit in 0.0002624988555908203 sec
2023-07-01 00:33:17,754 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002589225769042969 sec
2023-07-01 00:33:17,755 - WARNING - Finished tracing + transforming fn for pjit in 0.0002224445343017578 sec
2023-07-01 00:33:17,755 - WARNING - Finished tracing + transforming fn for pjit in 0.00021982192993164062 sec
2023-07-01 00:33:17,756 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002231597900390625 sec
2023-07-01 00:33:17,757 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026345252990722656 sec
2023-07-01 00:33:17,757 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002529621124267578 sec
2023-07-01 00:33:17,758 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002696514129638672 sec
2023-07-01 00:33:17,759 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002338886260986328 sec
2023-07-01 00:33:17,759 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002319812774658203 sec
2023-07-01 00:33:17,760 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025773048400878906 sec
2023-07-01 00:33:17,761 - WARNING - Finished tracing + transforming _where for pjit in 0.0008466243743896484 sec
2023-07-01 00:33:17,761 - WARNING - Finished tracing + transforming fn for pjit in 0.0002613067626953125 sec
2023-07-01 00:33:17,762 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002598762512207031 sec
2023-07-01 00:33:17,763 - WARNING - Finished tracing + transforming fn for pjit in 0.000225067138671875 sec
2023-07-01 00:33:17,767 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002753734588623047 sec
2023-07-01 00:33:17,768 - WARNING - Finished tracing + transforming fn for pjit in 0.0003428459167480469 sec
2023-07-01 00:33:17,769 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002732276916503906 sec
2023-07-01 00:33:17,769 - WARNING - Finished tracing + transforming fn for pjit in 0.00022411346435546875 sec
2023-07-01 00:33:17,773 - WARNING - Finished tracing + transforming fn for pjit in 0.0002219676971435547 sec
2023-07-01 00:33:17,775 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016880035400390625 sec
2023-07-01 00:33:17,776 - WARNING - Finished tracing + transforming fn for pjit in 0.00029659271240234375 sec
2023-07-01 00:33:17,776 - WARNING - Finished tracing + transforming fn for pjit in 0.00022792816162109375 sec
2023-07-01 00:33:17,795 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06862998008728027 sec
2023-07-01 00:33:17,798 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000125885009765625 sec
2023-07-01 00:33:17,799 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011396408081054688 sec
2023-07-01 00:33:17,799 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002675056457519531 sec
2023-07-01 00:33:17,801 - WARNING - Finished tracing + transforming fn for pjit in 0.00022459030151367188 sec
2023-07-01 00:33:17,802 - WARNING - Finished tracing + transforming fn for pjit in 0.0002524852752685547 sec
2023-07-01 00:33:17,803 - WARNING - Finished tracing + transforming fn for pjit in 0.0002167224884033203 sec
2023-07-01 00:33:17,809 - WARNING - Finished tracing + transforming fn for pjit in 0.0002567768096923828 sec
2023-07-01 00:33:17,810 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021791458129882812 sec
2023-07-01 00:33:17,811 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002536773681640625 sec
2023-07-01 00:33:17,811 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00018024444580078125 sec
2023-07-01 00:33:17,812 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003256797790527344 sec
2023-07-01 00:33:17,813 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002295970916748047 sec
2023-07-01 00:33:17,813 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022101402282714844 sec
2023-07-01 00:33:17,814 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002789497375488281 sec
2023-07-01 00:33:17,815 - WARNING - Finished tracing + transforming _where for pjit in 0.0008623600006103516 sec
2023-07-01 00:33:17,815 - WARNING - Finished tracing + transforming fn for pjit in 0.00025844573974609375 sec
2023-07-01 00:33:17,816 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002541542053222656 sec
2023-07-01 00:33:17,817 - WARNING - Finished tracing + transforming fn for pjit in 0.00021719932556152344 sec
2023-07-01 00:33:17,817 - WARNING - Finished tracing + transforming fn for pjit in 0.0002856254577636719 sec
2023-07-01 00:33:17,829 - WARNING - Finished tracing + transforming fn for pjit in 0.00021719932556152344 sec
2023-07-01 00:33:17,849 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.053241729736328125 sec
2023-07-01 00:33:17,850 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012254714965820312 sec
2023-07-01 00:33:17,851 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013375282287597656 sec
2023-07-01 00:33:17,852 - WARNING - Finished tracing + transforming _where for pjit in 0.0006079673767089844 sec
2023-07-01 00:33:17,852 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002899169921875 sec
2023-07-01 00:33:17,853 - WARNING - Finished tracing + transforming trace for pjit in 0.0024704933166503906 sec
2023-07-01 00:33:17,855 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010514259338378906 sec
2023-07-01 00:33:17,856 - WARNING - Finished tracing + transforming tril for pjit in 0.0006568431854248047 sec
2023-07-01 00:33:17,856 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0017671585083007812 sec
2023-07-01 00:33:17,857 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00011396408081054688 sec
2023-07-01 00:33:17,857 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010585784912109375 sec
2023-07-01 00:33:17,859 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014123916625976562 sec
2023-07-01 00:33:17,863 - WARNING - Finished tracing + transforming _solve for pjit in 0.009142637252807617 sec
2023-07-01 00:33:17,864 - WARNING - Finished tracing + transforming dot for pjit in 0.00031280517578125 sec
2023-07-01 00:33:17,866 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.141160249710083 sec
2023-07-01 00:33:17,868 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:33:17,901 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03292226791381836 sec
2023-07-01 00:33:17,901 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:33:18,025 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12322664260864258 sec
2023-07-01 00:33:18,046 - INFO - initial test loss: 0.027587318950061945
2023-07-01 00:33:18,046 - INFO - initial test acc: 0.6200000047683716
2023-07-01 00:33:18,052 - WARNING - Finished tracing + transforming dot for pjit in 0.0003676414489746094 sec
2023-07-01 00:33:18,053 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031113624572753906 sec
2023-07-01 00:33:18,054 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003745555877685547 sec
2023-07-01 00:33:18,054 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010173320770263672 sec
2023-07-01 00:33:18,055 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00019884109497070312 sec
2023-07-01 00:33:18,056 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00018548965454101562 sec
2023-07-01 00:33:18,057 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024771690368652344 sec
2023-07-01 00:33:18,057 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003643035888671875 sec
2023-07-01 00:33:18,058 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010712146759033203 sec
2023-07-01 00:33:18,059 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009941816329956055 sec
2023-07-01 00:33:18,067 - WARNING - Finished tracing + transforming fn for pjit in 0.0003197193145751953 sec
2023-07-01 00:33:18,068 - WARNING - Finished tracing + transforming fn for pjit in 0.00026869773864746094 sec
2023-07-01 00:33:18,069 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022864341735839844 sec
2023-07-01 00:33:18,069 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002663135528564453 sec
2023-07-01 00:33:18,070 - WARNING - Finished tracing + transforming _where for pjit in 0.0008571147918701172 sec
2023-07-01 00:33:18,078 - WARNING - Finished tracing + transforming fn for pjit in 0.0002605915069580078 sec
2023-07-01 00:33:18,079 - WARNING - Finished tracing + transforming fn for pjit in 0.00026488304138183594 sec
2023-07-01 00:33:18,079 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021648406982421875 sec
2023-07-01 00:33:18,080 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00024819374084472656 sec
2023-07-01 00:33:18,080 - WARNING - Finished tracing + transforming _where for pjit in 0.0008182525634765625 sec
2023-07-01 00:33:18,115 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00014972686767578125 sec
2023-07-01 00:33:18,171 - WARNING - Finished tracing + transforming fn for pjit in 0.00027632713317871094 sec
2023-07-01 00:33:18,172 - WARNING - Finished tracing + transforming fn for pjit in 0.00023818016052246094 sec
2023-07-01 00:33:18,172 - WARNING - Finished tracing + transforming square for pjit in 0.0001685619354248047 sec
2023-07-01 00:33:18,174 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022339820861816406 sec
2023-07-01 00:33:18,176 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000244140625 sec
2023-07-01 00:33:18,177 - WARNING - Finished tracing + transforming fn for pjit in 0.0002646446228027344 sec
2023-07-01 00:33:18,177 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022339820861816406 sec
2023-07-01 00:33:18,178 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023365020751953125 sec
2023-07-01 00:33:18,178 - WARNING - Finished tracing + transforming fn for pjit in 0.00027298927307128906 sec
2023-07-01 00:33:18,179 - WARNING - Finished tracing + transforming fn for pjit in 0.00023412704467773438 sec
2023-07-01 00:33:18,180 - WARNING - Finished tracing + transforming square for pjit in 0.00016880035400390625 sec
2023-07-01 00:33:18,182 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002224445343017578 sec
2023-07-01 00:33:18,183 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017690658569335938 sec
2023-07-01 00:33:18,184 - WARNING - Finished tracing + transforming fn for pjit in 0.0002772808074951172 sec
2023-07-01 00:33:18,185 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002231597900390625 sec
2023-07-01 00:33:18,185 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023627281188964844 sec
2023-07-01 00:33:18,186 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1377863883972168 sec
2023-07-01 00:33:18,190 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[510,10]), ShapedArray(float32[510,10]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:33:18,253 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.0625913143157959 sec
2023-07-01 00:33:18,253 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:33:18,578 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32469987869262695 sec
2023-07-01 00:33:19,799 - INFO - Distilling data from client: Client32
2023-07-01 00:33:19,799 - INFO - train loss: 0.003223052414334615
2023-07-01 00:33:19,800 - INFO - train acc: 0.988235354423523
2023-07-01 00:33:19,859 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.74      0.75        78
           2       0.61      0.68      0.65        60
           3       0.67      0.61      0.64        62

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:33:19,859 - INFO - test loss 0.02463763803956563
2023-07-01 00:33:19,859 - INFO - test acc 0.6850000023841858
2023-07-01 00:33:21,082 - INFO - Distilling data from client: Client32
2023-07-01 00:33:21,082 - INFO - train loss: 0.001921273899524083
2023-07-01 00:33:21,082 - INFO - train acc: 0.9980392456054688
2023-07-01 00:33:21,105 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.73      0.75        78
           2       0.55      0.63      0.59        60
           3       0.62      0.56      0.59        62

    accuracy                           0.65       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.66      0.65      0.65       200

2023-07-01 00:33:21,105 - INFO - test loss 0.02589162451724793
2023-07-01 00:33:21,105 - INFO - test acc 0.6499999761581421
2023-07-01 00:33:22,324 - INFO - Distilling data from client: Client32
2023-07-01 00:33:22,324 - INFO - train loss: 0.0015530079490141831
2023-07-01 00:33:22,324 - INFO - train acc: 1.0
2023-07-01 00:33:22,347 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.68      0.71        78
           2       0.56      0.62      0.59        60
           3       0.59      0.60      0.59        62

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:33:22,347 - INFO - test loss 0.02556258779211355
2023-07-01 00:33:22,347 - INFO - test acc 0.6349999904632568
2023-07-01 00:33:23,575 - INFO - Distilling data from client: Client32
2023-07-01 00:33:23,575 - INFO - train loss: 0.0011056396793454315
2023-07-01 00:33:23,575 - INFO - train acc: 1.0
2023-07-01 00:33:23,598 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.71      0.72        78
           2       0.58      0.63      0.60        60
           3       0.63      0.61      0.62        62

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-07-01 00:33:23,598 - INFO - test loss 0.02635549404223563
2023-07-01 00:33:23,598 - INFO - test acc 0.6549999713897705
2023-07-01 00:33:24,838 - INFO - Distilling data from client: Client32
2023-07-01 00:33:24,839 - INFO - train loss: 0.0011050717009827488
2023-07-01 00:33:24,839 - INFO - train acc: 1.0
2023-07-01 00:33:24,863 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.74      0.75        78
           2       0.59      0.65      0.62        60
           3       0.63      0.58      0.61        62

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:33:24,863 - INFO - test loss 0.026081462773326906
2023-07-01 00:33:24,863 - INFO - test acc 0.6649999618530273
2023-07-01 00:33:26,086 - INFO - Distilling data from client: Client32
2023-07-01 00:33:26,086 - INFO - train loss: 0.000970558296402136
2023-07-01 00:33:26,086 - INFO - train acc: 1.0
2023-07-01 00:33:26,108 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.69      0.71        78
           2       0.57      0.60      0.59        60
           3       0.60      0.61      0.61        62

    accuracy                           0.64       200
   macro avg       0.63      0.64      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:33:26,109 - INFO - test loss 0.02641613479595781
2023-07-01 00:33:26,109 - INFO - test acc 0.6399999856948853
2023-07-01 00:33:27,336 - INFO - Distilling data from client: Client32
2023-07-01 00:33:27,336 - INFO - train loss: 0.0008848404339503235
2023-07-01 00:33:27,336 - INFO - train acc: 1.0
2023-07-01 00:33:27,359 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.73      0.75        78
           2       0.60      0.67      0.63        60
           3       0.63      0.61      0.62        62

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:33:27,359 - INFO - test loss 0.025318180016174665
2023-07-01 00:33:27,359 - INFO - test acc 0.675000011920929
2023-07-01 00:33:28,591 - INFO - Distilling data from client: Client32
2023-07-01 00:33:28,591 - INFO - train loss: 0.0007777000325736629
2023-07-01 00:33:28,591 - INFO - train acc: 1.0
2023-07-01 00:33:28,615 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.69      0.72        78
           2       0.53      0.62      0.57        60
           3       0.59      0.55      0.57        62

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.63      0.62      0.63       200

2023-07-01 00:33:28,615 - INFO - test loss 0.026944317847758307
2023-07-01 00:33:28,615 - INFO - test acc 0.625
2023-07-01 00:33:29,849 - INFO - Distilling data from client: Client32
2023-07-01 00:33:29,850 - INFO - train loss: 0.0009710680265086969
2023-07-01 00:33:29,850 - INFO - train acc: 1.0
2023-07-01 00:33:29,872 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.68      0.71        78
           2       0.54      0.63      0.58        60
           3       0.59      0.55      0.57        62

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.63      0.62      0.63       200

2023-07-01 00:33:29,872 - INFO - test loss 0.02610284776731956
2023-07-01 00:33:29,872 - INFO - test acc 0.625
2023-07-01 00:33:31,103 - INFO - Distilling data from client: Client32
2023-07-01 00:33:31,103 - INFO - train loss: 0.0007527360732808244
2023-07-01 00:33:31,103 - INFO - train acc: 1.0
2023-07-01 00:33:31,126 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        78
           2       0.58      0.65      0.61        60
           3       0.65      0.60      0.62        62

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:33:31,126 - INFO - test loss 0.02676272039703866
2023-07-01 00:33:31,127 - INFO - test acc 0.6649999618530273
2023-07-01 00:33:32,370 - INFO - Distilling data from client: Client32
2023-07-01 00:33:32,370 - INFO - train loss: 0.0007354713501198923
2023-07-01 00:33:32,370 - INFO - train acc: 1.0
2023-07-01 00:33:32,393 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.72      0.74        78
           2       0.59      0.68      0.64        60
           3       0.65      0.60      0.62        62

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:33:32,393 - INFO - test loss 0.02615691448683387
2023-07-01 00:33:32,393 - INFO - test acc 0.6699999570846558
2023-07-01 00:33:33,622 - INFO - Distilling data from client: Client32
2023-07-01 00:33:33,623 - INFO - train loss: 0.000692763627022699
2023-07-01 00:33:33,623 - INFO - train acc: 1.0
2023-07-01 00:33:33,645 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.72      0.74        78
           2       0.54      0.58      0.56        60
           3       0.59      0.58      0.59        62

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:33:33,645 - INFO - test loss 0.02657710606080198
2023-07-01 00:33:33,645 - INFO - test acc 0.6349999904632568
2023-07-01 00:33:34,882 - INFO - Distilling data from client: Client32
2023-07-01 00:33:34,883 - INFO - train loss: 0.0006117099244499574
2023-07-01 00:33:34,883 - INFO - train acc: 1.0
2023-07-01 00:33:34,906 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.69      0.72        78
           2       0.54      0.62      0.58        60
           3       0.60      0.58      0.59        62

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:33:34,906 - INFO - test loss 0.026148312602521442
2023-07-01 00:33:34,906 - INFO - test acc 0.6349999904632568
2023-07-01 00:33:36,132 - INFO - Distilling data from client: Client32
2023-07-01 00:33:36,133 - INFO - train loss: 0.0005356800160930477
2023-07-01 00:33:36,133 - INFO - train acc: 1.0
2023-07-01 00:33:36,155 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.71      0.72        78
           2       0.54      0.60      0.57        60
           3       0.58      0.55      0.56        62

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.63      0.62      0.63       200

2023-07-01 00:33:36,155 - INFO - test loss 0.026422626317992914
2023-07-01 00:33:36,156 - INFO - test acc 0.625
2023-07-01 00:33:37,385 - INFO - Distilling data from client: Client32
2023-07-01 00:33:37,385 - INFO - train loss: 0.00057901267907888
2023-07-01 00:33:37,386 - INFO - train acc: 1.0
2023-07-01 00:33:37,408 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        78
           2       0.53      0.60      0.56        60
           3       0.65      0.56      0.60        62

    accuracy                           0.64       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:33:37,409 - INFO - test loss 0.02639432304721266
2023-07-01 00:33:37,409 - INFO - test acc 0.6399999856948853
2023-07-01 00:33:38,647 - INFO - Distilling data from client: Client32
2023-07-01 00:33:38,648 - INFO - train loss: 0.0006451761229139062
2023-07-01 00:33:38,648 - INFO - train acc: 1.0
2023-07-01 00:33:38,670 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.68      0.71        78
           2       0.55      0.67      0.60        60
           3       0.61      0.55      0.58        62

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:33:38,670 - INFO - test loss 0.02665807020000044
2023-07-01 00:33:38,670 - INFO - test acc 0.6349999904632568
2023-07-01 00:33:39,908 - INFO - Distilling data from client: Client32
2023-07-01 00:33:39,908 - INFO - train loss: 0.0005110910609056618
2023-07-01 00:33:39,908 - INFO - train acc: 1.0
2023-07-01 00:33:39,931 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.71      0.73        78
           2       0.54      0.62      0.58        60
           3       0.60      0.58      0.59        62

    accuracy                           0.64       200
   macro avg       0.64      0.63      0.63       200
weighted avg       0.65      0.64      0.64       200

2023-07-01 00:33:39,931 - INFO - test loss 0.02665858123547632
2023-07-01 00:33:39,931 - INFO - test acc 0.6399999856948853
2023-07-01 00:33:41,163 - INFO - Distilling data from client: Client32
2023-07-01 00:33:41,163 - INFO - train loss: 0.0005372190983499555
2023-07-01 00:33:41,163 - INFO - train acc: 1.0
2023-07-01 00:33:41,186 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.71      0.72        78
           2       0.57      0.65      0.61        60
           3       0.64      0.60      0.62        62

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-07-01 00:33:41,186 - INFO - test loss 0.02648702260110983
2023-07-01 00:33:41,186 - INFO - test acc 0.6549999713897705
2023-07-01 00:33:42,418 - INFO - Distilling data from client: Client32
2023-07-01 00:33:42,419 - INFO - train loss: 0.0006076507055773026
2023-07-01 00:33:42,419 - INFO - train acc: 1.0
2023-07-01 00:33:42,441 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.68      0.71        78
           2       0.57      0.65      0.61        60
           3       0.61      0.60      0.60        62

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:33:42,441 - INFO - test loss 0.02663820774943094
2023-07-01 00:33:42,441 - INFO - test acc 0.6449999809265137
2023-07-01 00:33:43,670 - INFO - Distilling data from client: Client32
2023-07-01 00:33:43,670 - INFO - train loss: 0.0005662383215853797
2023-07-01 00:33:43,670 - INFO - train acc: 1.0
2023-07-01 00:33:43,693 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.72      0.73        78
           2       0.55      0.65      0.60        60
           3       0.63      0.55      0.59        62

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:33:43,693 - INFO - test loss 0.02652459658736345
2023-07-01 00:33:43,693 - INFO - test acc 0.6449999809265137
2023-07-01 00:33:44,908 - INFO - Distilling data from client: Client32
2023-07-01 00:33:44,908 - INFO - train loss: 0.0004647226899316281
2023-07-01 00:33:44,908 - INFO - train acc: 1.0
2023-07-01 00:33:44,931 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.72      0.74        78
           2       0.57      0.63      0.60        60
           3       0.63      0.60      0.61        62

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-07-01 00:33:44,931 - INFO - test loss 0.026905177583758166
2023-07-01 00:33:44,931 - INFO - test acc 0.6549999713897705
2023-07-01 00:33:46,158 - INFO - Distilling data from client: Client32
2023-07-01 00:33:46,158 - INFO - train loss: 0.0005903383277299123
2023-07-01 00:33:46,158 - INFO - train acc: 1.0
2023-07-01 00:33:46,181 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.69      0.72        78
           2       0.53      0.62      0.57        60
           3       0.60      0.56      0.58        62

    accuracy                           0.63       200
   macro avg       0.63      0.62      0.62       200
weighted avg       0.64      0.63      0.63       200

2023-07-01 00:33:46,181 - INFO - test loss 0.027018265138722002
2023-07-01 00:33:46,181 - INFO - test acc 0.6299999952316284
2023-07-01 00:33:47,411 - INFO - Distilling data from client: Client32
2023-07-01 00:33:47,412 - INFO - train loss: 0.0005399572749289689
2023-07-01 00:33:47,412 - INFO - train acc: 1.0
2023-07-01 00:33:47,434 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.71      0.72        78
           2       0.57      0.65      0.60        60
           3       0.61      0.56      0.59        62

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:33:47,434 - INFO - test loss 0.02616279292708713
2023-07-01 00:33:47,435 - INFO - test acc 0.6449999809265137
2023-07-01 00:33:48,661 - INFO - Distilling data from client: Client32
2023-07-01 00:33:48,661 - INFO - train loss: 0.00037295506645898107
2023-07-01 00:33:48,661 - INFO - train acc: 1.0
2023-07-01 00:33:48,684 - INFO - report:               precision    recall  f1-score   support

           0       0.77      0.73      0.75        78
           2       0.58      0.67      0.62        60
           3       0.61      0.56      0.59        62

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-07-01 00:33:48,685 - INFO - test loss 0.026333143014317495
2023-07-01 00:33:48,685 - INFO - test acc 0.6599999666213989
2023-07-01 00:33:49,917 - INFO - Distilling data from client: Client32
2023-07-01 00:33:49,917 - INFO - train loss: 0.0003808262317524954
2023-07-01 00:33:49,918 - INFO - train acc: 1.0
2023-07-01 00:33:49,941 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        78
           2       0.54      0.62      0.58        60
           3       0.62      0.56      0.59        62

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:33:49,941 - INFO - test loss 0.026486004329759283
2023-07-01 00:33:49,941 - INFO - test acc 0.6449999809265137
2023-07-01 00:33:49,944 - WARNING - Finished tracing + transforming jit(gather) in 0.0002391338348388672 sec
2023-07-01 00:33:49,944 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[510,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:33:49,945 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0012099742889404297 sec
2023-07-01 00:33:49,945 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:33:49,955 - WARNING - Finished XLA compilation of jit(gather) in 0.00980830192565918 sec
2023-07-01 00:33:49,966 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:49,975 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:49,984 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:49,992 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:50,001 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:50,010 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:50,019 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:50,028 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:50,037 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:33:50,414 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client32//synthetic.png
2023-07-01 00:33:50,426 - INFO - c: 3.0 and total_data_in_this_class: 259
2023-07-01 00:33:50,426 - INFO - c: 6.0 and total_data_in_this_class: 279
2023-07-01 00:33:50,426 - INFO - c: 8.0 and total_data_in_this_class: 261
2023-07-01 00:33:50,426 - INFO - c: 3.0 and total_data_in_this_class: 74
2023-07-01 00:33:50,426 - INFO - c: 6.0 and total_data_in_this_class: 54
2023-07-01 00:33:50,426 - INFO - c: 8.0 and total_data_in_this_class: 72
2023-07-01 00:33:50,497 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.047005653381347656 sec
2023-07-01 00:33:50,542 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04370903968811035 sec
2023-07-01 00:33:50,546 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.09752917289733887 sec
2023-07-01 00:33:50,548 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:33:50,580 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03193998336791992 sec
2023-07-01 00:33:50,580 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:33:50,703 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12208080291748047 sec
2023-07-01 00:33:50,725 - INFO - initial test loss: 0.021868363959568256
2023-07-01 00:33:50,726 - INFO - initial test acc: 0.7199999690055847
2023-07-01 00:33:50,733 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.005474090576171875 sec
2023-07-01 00:33:50,841 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11344408988952637 sec
2023-07-01 00:33:50,844 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10]), ShapedArray(float32[519,3,32,32]), ShapedArray(float32[519,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:33:50,907 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.062320709228515625 sec
2023-07-01 00:33:50,907 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:33:51,235 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3273472785949707 sec
2023-07-01 00:33:52,505 - INFO - Distilling data from client: Client33
2023-07-01 00:33:52,505 - INFO - train loss: 0.0028156150670185006
2023-07-01 00:33:52,505 - INFO - train acc: 0.9961464405059814
2023-07-01 00:33:52,567 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.57      0.64        74
           6       0.61      0.83      0.70        54
           8       0.93      0.89      0.91        72

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.75       200
weighted avg       0.77      0.76      0.75       200

2023-07-01 00:33:52,567 - INFO - test loss 0.019538626875825724
2023-07-01 00:33:52,567 - INFO - test acc 0.7549999952316284
2023-07-01 00:33:53,815 - INFO - Distilling data from client: Client33
2023-07-01 00:33:53,815 - INFO - train loss: 0.001530080494814841
2023-07-01 00:33:53,815 - INFO - train acc: 0.9961464405059814
2023-07-01 00:33:53,838 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.51      0.61        74
           6       0.57      0.87      0.69        54
           8       0.93      0.88      0.90        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.77      0.74      0.74       200

2023-07-01 00:33:53,838 - INFO - test loss 0.020510627363272492
2023-07-01 00:33:53,838 - INFO - test acc 0.7400000095367432
2023-07-01 00:33:55,097 - INFO - Distilling data from client: Client33
2023-07-01 00:33:55,098 - INFO - train loss: 0.0013627553054546809
2023-07-01 00:33:55,098 - INFO - train acc: 0.9980732202529907
2023-07-01 00:33:55,121 - INFO - report:               precision    recall  f1-score   support

           3       0.81      0.46      0.59        74
           6       0.56      0.91      0.70        54
           8       0.90      0.89      0.90        72

    accuracy                           0.73       200
   macro avg       0.76      0.75      0.73       200
weighted avg       0.78      0.73      0.73       200

2023-07-01 00:33:55,121 - INFO - test loss 0.020748708179041925
2023-07-01 00:33:55,121 - INFO - test acc 0.73499995470047
2023-07-01 00:33:56,384 - INFO - Distilling data from client: Client33
2023-07-01 00:33:56,384 - INFO - train loss: 0.001125372845853183
2023-07-01 00:33:56,384 - INFO - train acc: 1.0
2023-07-01 00:33:56,408 - INFO - report:               precision    recall  f1-score   support

           3       0.78      0.54      0.64        74
           6       0.59      0.87      0.71        54
           8       0.90      0.88      0.89        72

    accuracy                           0.75       200
   macro avg       0.76      0.76      0.74       200
weighted avg       0.77      0.75      0.75       200

2023-07-01 00:33:56,408 - INFO - test loss 0.020780161209799884
2023-07-01 00:33:56,408 - INFO - test acc 0.75
2023-07-01 00:33:57,666 - INFO - Distilling data from client: Client33
2023-07-01 00:33:57,666 - INFO - train loss: 0.0009750359657336006
2023-07-01 00:33:57,666 - INFO - train acc: 1.0
2023-07-01 00:33:57,689 - INFO - report:               precision    recall  f1-score   support

           3       0.79      0.51      0.62        74
           6       0.58      0.89      0.70        54
           8       0.93      0.89      0.91        72

    accuracy                           0.75       200
   macro avg       0.77      0.76      0.74       200
weighted avg       0.78      0.75      0.75       200

2023-07-01 00:33:57,689 - INFO - test loss 0.021251942845770116
2023-07-01 00:33:57,689 - INFO - test acc 0.75
2023-07-01 00:33:58,941 - INFO - Distilling data from client: Client33
2023-07-01 00:33:58,941 - INFO - train loss: 0.0008079833576725179
2023-07-01 00:33:58,941 - INFO - train acc: 1.0
2023-07-01 00:33:58,966 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.49      0.59        74
           6       0.55      0.85      0.67        54
           8       0.91      0.88      0.89        72

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.72       200
weighted avg       0.76      0.72      0.72       200

2023-07-01 00:33:58,966 - INFO - test loss 0.021720463527853562
2023-07-01 00:33:58,966 - INFO - test acc 0.7249999642372131
2023-07-01 00:34:00,215 - INFO - Distilling data from client: Client33
2023-07-01 00:34:00,216 - INFO - train loss: 0.0006478265723373094
2023-07-01 00:34:00,216 - INFO - train acc: 1.0
2023-07-01 00:34:00,239 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.49      0.60        74
           6       0.56      0.85      0.68        54
           8       0.90      0.89      0.90        72

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.72       200
weighted avg       0.76      0.73      0.73       200

2023-07-01 00:34:00,239 - INFO - test loss 0.02187631936534739
2023-07-01 00:34:00,239 - INFO - test acc 0.7299999594688416
2023-07-01 00:34:01,488 - INFO - Distilling data from client: Client33
2023-07-01 00:34:01,488 - INFO - train loss: 0.0006242104863756053
2023-07-01 00:34:01,488 - INFO - train acc: 1.0
2023-07-01 00:34:01,516 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.45      0.57        74
           6       0.56      0.91      0.69        54
           8       0.90      0.89      0.90        72

    accuracy                           0.73       200
   macro avg       0.75      0.75      0.72       200
weighted avg       0.77      0.73      0.72       200

2023-07-01 00:34:01,517 - INFO - test loss 0.021112790064353945
2023-07-01 00:34:01,517 - INFO - test acc 0.7299999594688416
2023-07-01 00:34:02,776 - INFO - Distilling data from client: Client33
2023-07-01 00:34:02,777 - INFO - train loss: 0.0005554912696716522
2023-07-01 00:34:02,777 - INFO - train acc: 1.0
2023-07-01 00:34:02,803 - INFO - report:               precision    recall  f1-score   support

           3       0.78      0.51      0.62        74
           6       0.58      0.85      0.69        54
           8       0.89      0.89      0.89        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.76      0.74      0.74       200

2023-07-01 00:34:02,803 - INFO - test loss 0.02113819210208797
2023-07-01 00:34:02,803 - INFO - test acc 0.7400000095367432
2023-07-01 00:34:04,065 - INFO - Distilling data from client: Client33
2023-07-01 00:34:04,065 - INFO - train loss: 0.0006492079032251037
2023-07-01 00:34:04,065 - INFO - train acc: 1.0
2023-07-01 00:34:04,089 - INFO - report:               precision    recall  f1-score   support

           3       0.71      0.46      0.56        74
           6       0.54      0.81      0.65        54
           8       0.90      0.89      0.90        72

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.70       200
weighted avg       0.73      0.71      0.70       200

2023-07-01 00:34:04,089 - INFO - test loss 0.021512659238596278
2023-07-01 00:34:04,089 - INFO - test acc 0.7099999785423279
2023-07-01 00:34:05,345 - INFO - Distilling data from client: Client33
2023-07-01 00:34:05,345 - INFO - train loss: 0.000596716119113267
2023-07-01 00:34:05,345 - INFO - train acc: 1.0
2023-07-01 00:34:05,369 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.54      0.63        74
           6       0.58      0.83      0.69        54
           8       0.90      0.89      0.90        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.74       200
weighted avg       0.77      0.74      0.74       200

2023-07-01 00:34:05,369 - INFO - test loss 0.021834915089248057
2023-07-01 00:34:05,369 - INFO - test acc 0.7450000047683716
2023-07-01 00:34:06,633 - INFO - Distilling data from client: Client33
2023-07-01 00:34:06,633 - INFO - train loss: 0.00045968824640045194
2023-07-01 00:34:06,633 - INFO - train acc: 1.0
2023-07-01 00:34:06,656 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.49      0.61        74
           6       0.57      0.87      0.69        54
           8       0.88      0.89      0.88        72

    accuracy                           0.73       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.77      0.73      0.73       200

2023-07-01 00:34:06,657 - INFO - test loss 0.02111055460918721
2023-07-01 00:34:06,657 - INFO - test acc 0.73499995470047
2023-07-01 00:34:07,923 - INFO - Distilling data from client: Client33
2023-07-01 00:34:07,923 - INFO - train loss: 0.0004605005002084949
2023-07-01 00:34:07,923 - INFO - train acc: 1.0
2023-07-01 00:34:07,945 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.53      0.63        74
           6       0.59      0.87      0.70        54
           8       0.90      0.89      0.90        72

    accuracy                           0.75       200
   macro avg       0.76      0.76      0.74       200
weighted avg       0.78      0.75      0.75       200

2023-07-01 00:34:07,946 - INFO - test loss 0.02129218295505549
2023-07-01 00:34:07,946 - INFO - test acc 0.75
2023-07-01 00:34:09,207 - INFO - Distilling data from client: Client33
2023-07-01 00:34:09,208 - INFO - train loss: 0.0004867946167867477
2023-07-01 00:34:09,208 - INFO - train acc: 1.0
2023-07-01 00:34:09,232 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.49      0.60        74
           6       0.57      0.87      0.69        54
           8       0.89      0.88      0.88        72

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.72       200
weighted avg       0.76      0.73      0.72       200

2023-07-01 00:34:09,233 - INFO - test loss 0.021712376203408445
2023-07-01 00:34:09,233 - INFO - test acc 0.7299999594688416
2023-07-01 00:34:10,481 - INFO - Distilling data from client: Client33
2023-07-01 00:34:10,481 - INFO - train loss: 0.0003861105577798536
2023-07-01 00:34:10,481 - INFO - train acc: 1.0
2023-07-01 00:34:10,505 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.50      0.61        74
           6       0.57      0.85      0.69        54
           8       0.88      0.88      0.88        72

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.72       200
weighted avg       0.76      0.73      0.72       200

2023-07-01 00:34:10,505 - INFO - test loss 0.02099697653611746
2023-07-01 00:34:10,505 - INFO - test acc 0.7299999594688416
2023-07-01 00:34:11,758 - INFO - Distilling data from client: Client33
2023-07-01 00:34:11,758 - INFO - train loss: 0.0003445031303922545
2023-07-01 00:34:11,758 - INFO - train acc: 1.0
2023-07-01 00:34:11,824 - INFO - report:               precision    recall  f1-score   support

           3       0.78      0.54      0.64        74
           6       0.62      0.89      0.73        54
           8       0.90      0.89      0.90        72

    accuracy                           0.76       200
   macro avg       0.77      0.77      0.75       200
weighted avg       0.78      0.76      0.76       200

2023-07-01 00:34:11,824 - INFO - test loss 0.02104523753553379
2023-07-01 00:34:11,824 - INFO - test acc 0.7599999904632568
2023-07-01 00:34:13,074 - INFO - Distilling data from client: Client33
2023-07-01 00:34:13,075 - INFO - train loss: 0.0003744502719095345
2023-07-01 00:34:13,075 - INFO - train acc: 1.0
2023-07-01 00:34:13,100 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.50      0.60        74
           6       0.59      0.87      0.70        54
           8       0.89      0.88      0.88        72

    accuracy                           0.73       200
   macro avg       0.74      0.75      0.73       200
weighted avg       0.76      0.73      0.73       200

2023-07-01 00:34:13,100 - INFO - test loss 0.022077071864832272
2023-07-01 00:34:13,100 - INFO - test acc 0.73499995470047
2023-07-01 00:34:14,365 - INFO - Distilling data from client: Client33
2023-07-01 00:34:14,365 - INFO - train loss: 0.00037712482114820624
2023-07-01 00:34:14,365 - INFO - train acc: 1.0
2023-07-01 00:34:14,391 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.51      0.61        74
           6       0.59      0.87      0.71        54
           8       0.90      0.88      0.89        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.76      0.74      0.74       200

2023-07-01 00:34:14,391 - INFO - test loss 0.021781569472843553
2023-07-01 00:34:14,391 - INFO - test acc 0.7400000095367432
2023-07-01 00:34:15,654 - INFO - Distilling data from client: Client33
2023-07-01 00:34:15,654 - INFO - train loss: 0.0004287501847702877
2023-07-01 00:34:15,654 - INFO - train acc: 1.0
2023-07-01 00:34:15,678 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.53      0.63        74
           6       0.60      0.85      0.70        54
           8       0.86      0.89      0.88        72

    accuracy                           0.74       200
   macro avg       0.75      0.76      0.74       200
weighted avg       0.77      0.74      0.74       200

2023-07-01 00:34:15,678 - INFO - test loss 0.02101476994228072
2023-07-01 00:34:15,678 - INFO - test acc 0.7450000047683716
2023-07-01 00:34:16,923 - INFO - Distilling data from client: Client33
2023-07-01 00:34:16,923 - INFO - train loss: 0.0004147259576340741
2023-07-01 00:34:16,923 - INFO - train acc: 1.0
2023-07-01 00:34:16,946 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.51      0.61        74
           6       0.58      0.85      0.69        54
           8       0.89      0.88      0.88        72

    accuracy                           0.73       200
   macro avg       0.74      0.75      0.73       200
weighted avg       0.76      0.73      0.73       200

2023-07-01 00:34:16,946 - INFO - test loss 0.021532651659995735
2023-07-01 00:34:16,946 - INFO - test acc 0.73499995470047
2023-07-01 00:34:18,198 - INFO - Distilling data from client: Client33
2023-07-01 00:34:18,199 - INFO - train loss: 0.0003987914021534282
2023-07-01 00:34:18,199 - INFO - train acc: 1.0
2023-07-01 00:34:18,223 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.50      0.60        74
           6       0.57      0.87      0.69        54
           8       0.90      0.86      0.88        72

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.72       200
weighted avg       0.76      0.73      0.73       200

2023-07-01 00:34:18,223 - INFO - test loss 0.021378248867006917
2023-07-01 00:34:18,223 - INFO - test acc 0.7299999594688416
2023-07-01 00:34:19,481 - INFO - Distilling data from client: Client33
2023-07-01 00:34:19,481 - INFO - train loss: 0.00032894358498796985
2023-07-01 00:34:19,481 - INFO - train acc: 1.0
2023-07-01 00:34:19,507 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.50      0.60        74
           6       0.58      0.87      0.70        54
           8       0.91      0.89      0.90        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.77      0.74      0.74       200

2023-07-01 00:34:19,507 - INFO - test loss 0.02172811882473499
2023-07-01 00:34:19,507 - INFO - test acc 0.7400000095367432
2023-07-01 00:34:20,769 - INFO - Distilling data from client: Client33
2023-07-01 00:34:20,769 - INFO - train loss: 0.0003536681033356311
2023-07-01 00:34:20,769 - INFO - train acc: 1.0
2023-07-01 00:34:20,793 - INFO - report:               precision    recall  f1-score   support

           3       0.78      0.47      0.59        74
           6       0.58      0.89      0.70        54
           8       0.89      0.89      0.89        72

    accuracy                           0.73       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.76      0.73      0.73       200

2023-07-01 00:34:20,793 - INFO - test loss 0.022119222811193835
2023-07-01 00:34:20,793 - INFO - test acc 0.73499995470047
2023-07-01 00:34:22,048 - INFO - Distilling data from client: Client33
2023-07-01 00:34:22,048 - INFO - train loss: 0.0003249200117610015
2023-07-01 00:34:22,048 - INFO - train acc: 1.0
2023-07-01 00:34:22,072 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.46      0.57        74
           6       0.57      0.85      0.68        54
           8       0.86      0.89      0.88        72

    accuracy                           0.72       200
   macro avg       0.73      0.73      0.71       200
weighted avg       0.74      0.72      0.71       200

2023-07-01 00:34:22,072 - INFO - test loss 0.021583323992440853
2023-07-01 00:34:22,073 - INFO - test acc 0.7199999690055847
2023-07-01 00:34:23,324 - INFO - Distilling data from client: Client33
2023-07-01 00:34:23,325 - INFO - train loss: 0.0003227204184929066
2023-07-01 00:34:23,325 - INFO - train acc: 1.0
2023-07-01 00:34:23,349 - INFO - report:               precision    recall  f1-score   support

           3       0.80      0.50      0.62        74
           6       0.58      0.87      0.70        54
           8       0.88      0.89      0.88        72

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.73       200
weighted avg       0.77      0.74      0.73       200

2023-07-01 00:34:23,349 - INFO - test loss 0.022314122794685178
2023-07-01 00:34:23,349 - INFO - test acc 0.7400000095367432
2023-07-01 00:34:23,361 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:23,370 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:23,378 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:23,387 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:23,396 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:23,405 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:23,414 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:23,423 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:23,432 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:23,805 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client33//synthetic.png
2023-07-01 00:34:23,817 - INFO - c: 0.0 and total_data_in_this_class: 267
2023-07-01 00:34:23,817 - INFO - c: 2.0 and total_data_in_this_class: 270
2023-07-01 00:34:23,817 - INFO - c: 4.0 and total_data_in_this_class: 262
2023-07-01 00:34:23,817 - INFO - c: 0.0 and total_data_in_this_class: 66
2023-07-01 00:34:23,817 - INFO - c: 2.0 and total_data_in_this_class: 63
2023-07-01 00:34:23,817 - INFO - c: 4.0 and total_data_in_this_class: 71
2023-07-01 00:34:23,887 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.047044992446899414 sec
2023-07-01 00:34:23,933 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.044816017150878906 sec
2023-07-01 00:34:23,938 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.09879183769226074 sec
2023-07-01 00:34:23,940 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:34:23,972 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.032532691955566406 sec
2023-07-01 00:34:23,973 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:34:24,096 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1227879524230957 sec
2023-07-01 00:34:24,122 - INFO - initial test loss: 0.027960679603051736
2023-07-01 00:34:24,122 - INFO - initial test acc: 0.6399999856948853
2023-07-01 00:34:24,130 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.005646944046020508 sec
2023-07-01 00:34:24,236 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1123042106628418 sec
2023-07-01 00:34:24,239 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:34:24,300 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.0607752799987793 sec
2023-07-01 00:34:24,301 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:34:24,626 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32523465156555176 sec
2023-07-01 00:34:25,890 - INFO - Distilling data from client: Client34
2023-07-01 00:34:25,890 - INFO - train loss: 0.003363568408711131
2023-07-01 00:34:25,890 - INFO - train acc: 0.9904761910438538
2023-07-01 00:34:25,951 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.68      0.69        66
           2       0.49      0.49      0.49        63
           4       0.59      0.61      0.60        71

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.60      0.59      0.60       200

2023-07-01 00:34:25,952 - INFO - test loss 0.026937360609048108
2023-07-01 00:34:25,952 - INFO - test acc 0.5949999690055847
2023-07-01 00:34:27,206 - INFO - Distilling data from client: Client34
2023-07-01 00:34:27,206 - INFO - train loss: 0.0018899188536022154
2023-07-01 00:34:27,206 - INFO - train acc: 1.0
2023-07-01 00:34:27,268 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.70      0.72        66
           2       0.57      0.49      0.53        63
           4       0.56      0.68      0.62        71

    accuracy                           0.62       200
   macro avg       0.63      0.62      0.62       200
weighted avg       0.63      0.62      0.62       200

2023-07-01 00:34:27,268 - INFO - test loss 0.026118630029810037
2023-07-01 00:34:27,268 - INFO - test acc 0.625
2023-07-01 00:34:28,532 - INFO - Distilling data from client: Client34
2023-07-01 00:34:28,532 - INFO - train loss: 0.0015755727676597198
2023-07-01 00:34:28,532 - INFO - train acc: 1.0
2023-07-01 00:34:28,556 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.76      0.74        66
           2       0.53      0.46      0.49        63
           4       0.55      0.59      0.57        71

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.60      0.60       200

2023-07-01 00:34:28,557 - INFO - test loss 0.027055056227454872
2023-07-01 00:34:28,557 - INFO - test acc 0.6049999594688416
2023-07-01 00:34:29,827 - INFO - Distilling data from client: Client34
2023-07-01 00:34:29,827 - INFO - train loss: 0.001557270798184622
2023-07-01 00:34:29,827 - INFO - train acc: 1.0
2023-07-01 00:34:29,851 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.79      0.75        66
           2       0.54      0.43      0.48        63
           4       0.56      0.62      0.59        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-07-01 00:34:29,852 - INFO - test loss 0.025942623788680164
2023-07-01 00:34:29,852 - INFO - test acc 0.6150000095367432
2023-07-01 00:34:31,121 - INFO - Distilling data from client: Client34
2023-07-01 00:34:31,122 - INFO - train loss: 0.001175660197456079
2023-07-01 00:34:31,122 - INFO - train acc: 0.9980952143669128
2023-07-01 00:34:31,145 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.76      0.73        66
           2       0.49      0.44      0.47        63
           4       0.56      0.56      0.56        71

    accuracy                           0.59       200
   macro avg       0.58      0.59      0.59       200
weighted avg       0.58      0.59      0.59       200

2023-07-01 00:34:31,145 - INFO - test loss 0.027019386642934875
2023-07-01 00:34:31,145 - INFO - test acc 0.5899999737739563
2023-07-01 00:34:32,402 - INFO - Distilling data from client: Client34
2023-07-01 00:34:32,402 - INFO - train loss: 0.0010476331256575027
2023-07-01 00:34:32,402 - INFO - train acc: 1.0
2023-07-01 00:34:32,426 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.71      0.72        66
           2       0.47      0.44      0.46        63
           4       0.52      0.55      0.53        71

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-07-01 00:34:32,426 - INFO - test loss 0.026901866679923073
2023-07-01 00:34:32,426 - INFO - test acc 0.5699999928474426
2023-07-01 00:34:33,683 - INFO - Distilling data from client: Client34
2023-07-01 00:34:33,683 - INFO - train loss: 0.0010174702206067396
2023-07-01 00:34:33,683 - INFO - train acc: 1.0
2023-07-01 00:34:33,706 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.70      0.70        66
           2       0.49      0.48      0.48        63
           4       0.51      0.54      0.52        71

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-07-01 00:34:33,706 - INFO - test loss 0.02728273314250389
2023-07-01 00:34:33,706 - INFO - test acc 0.5699999928474426
2023-07-01 00:34:34,975 - INFO - Distilling data from client: Client34
2023-07-01 00:34:34,975 - INFO - train loss: 0.0010207714837127244
2023-07-01 00:34:34,975 - INFO - train acc: 1.0
2023-07-01 00:34:34,999 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.70      0.70        66
           2       0.53      0.49      0.51        63
           4       0.55      0.58      0.56        71

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.59      0.59      0.59       200

2023-07-01 00:34:34,999 - INFO - test loss 0.027266165595097546
2023-07-01 00:34:34,999 - INFO - test acc 0.5899999737739563
2023-07-01 00:34:36,255 - INFO - Distilling data from client: Client34
2023-07-01 00:34:36,255 - INFO - train loss: 0.0008960244148663186
2023-07-01 00:34:36,255 - INFO - train acc: 0.9980952143669128
2023-07-01 00:34:36,280 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.76      0.73        66
           2       0.54      0.51      0.52        63
           4       0.57      0.56      0.57        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-07-01 00:34:36,280 - INFO - test loss 0.02691770610410717
2023-07-01 00:34:36,280 - INFO - test acc 0.6100000143051147
2023-07-01 00:34:37,544 - INFO - Distilling data from client: Client34
2023-07-01 00:34:37,544 - INFO - train loss: 0.0008584855467559428
2023-07-01 00:34:37,544 - INFO - train acc: 1.0
2023-07-01 00:34:37,567 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        66
           2       0.55      0.52      0.54        63
           4       0.54      0.56      0.55        71

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.61       200
weighted avg       0.61      0.60      0.60       200

2023-07-01 00:34:37,568 - INFO - test loss 0.02705067039552
2023-07-01 00:34:37,568 - INFO - test acc 0.6049999594688416
2023-07-01 00:34:38,833 - INFO - Distilling data from client: Client34
2023-07-01 00:34:38,833 - INFO - train loss: 0.0008909168290215449
2023-07-01 00:34:38,833 - INFO - train acc: 1.0
2023-07-01 00:34:38,857 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.74      0.73        66
           2       0.56      0.48      0.51        63
           4       0.56      0.61      0.58        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-07-01 00:34:38,857 - INFO - test loss 0.026909581376867588
2023-07-01 00:34:38,857 - INFO - test acc 0.6100000143051147
2023-07-01 00:34:40,130 - INFO - Distilling data from client: Client34
2023-07-01 00:34:40,130 - INFO - train loss: 0.0007796948143176191
2023-07-01 00:34:40,130 - INFO - train acc: 1.0
2023-07-01 00:34:40,155 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.71      0.71        66
           2       0.52      0.54      0.53        63
           4       0.59      0.56      0.58        71

    accuracy                           0.60       200
   macro avg       0.61      0.61      0.60       200
weighted avg       0.61      0.60      0.61       200

2023-07-01 00:34:40,155 - INFO - test loss 0.027716813139189236
2023-07-01 00:34:40,156 - INFO - test acc 0.6049999594688416
2023-07-01 00:34:41,414 - INFO - Distilling data from client: Client34
2023-07-01 00:34:41,415 - INFO - train loss: 0.0007499211926479443
2023-07-01 00:34:41,415 - INFO - train acc: 0.9980952143669128
2023-07-01 00:34:41,438 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.74      0.76        66
           2       0.54      0.57      0.55        63
           4       0.57      0.56      0.57        71

    accuracy                           0.62       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.62      0.63       200

2023-07-01 00:34:41,438 - INFO - test loss 0.02689696693281128
2023-07-01 00:34:41,438 - INFO - test acc 0.625
2023-07-01 00:34:42,706 - INFO - Distilling data from client: Client34
2023-07-01 00:34:42,706 - INFO - train loss: 0.0006136156434495298
2023-07-01 00:34:42,706 - INFO - train acc: 1.0
2023-07-01 00:34:42,731 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.71      0.71        66
           2       0.56      0.54      0.55        63
           4       0.56      0.58      0.57        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-07-01 00:34:42,732 - INFO - test loss 0.0263623898766173
2023-07-01 00:34:42,732 - INFO - test acc 0.6100000143051147
2023-07-01 00:34:43,988 - INFO - Distilling data from client: Client34
2023-07-01 00:34:43,989 - INFO - train loss: 0.0007314822166685648
2023-07-01 00:34:43,989 - INFO - train acc: 1.0
2023-07-01 00:34:44,012 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.71      0.72        66
           2       0.55      0.48      0.51        63
           4       0.54      0.61      0.57        71

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.60      0.60       200

2023-07-01 00:34:44,012 - INFO - test loss 0.02605334169083107
2023-07-01 00:34:44,012 - INFO - test acc 0.5999999642372131
2023-07-01 00:34:45,269 - INFO - Distilling data from client: Client34
2023-07-01 00:34:45,269 - INFO - train loss: 0.0006603723382329042
2023-07-01 00:34:45,269 - INFO - train acc: 1.0
2023-07-01 00:34:45,293 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.70      0.72        66
           2       0.56      0.54      0.55        63
           4       0.55      0.59      0.57        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-07-01 00:34:45,293 - INFO - test loss 0.027265908948962067
2023-07-01 00:34:45,293 - INFO - test acc 0.6100000143051147
2023-07-01 00:34:46,554 - INFO - Distilling data from client: Client34
2023-07-01 00:34:46,554 - INFO - train loss: 0.0006157148453353625
2023-07-01 00:34:46,554 - INFO - train acc: 1.0
2023-07-01 00:34:46,578 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.71      0.72        66
           2       0.50      0.51      0.50        63
           4       0.54      0.54      0.54        71

    accuracy                           0.58       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.59      0.58      0.59       200

2023-07-01 00:34:46,578 - INFO - test loss 0.027461684809796907
2023-07-01 00:34:46,578 - INFO - test acc 0.5849999785423279
2023-07-01 00:34:47,840 - INFO - Distilling data from client: Client34
2023-07-01 00:34:47,840 - INFO - train loss: 0.0006273882590273978
2023-07-01 00:34:47,840 - INFO - train acc: 1.0
2023-07-01 00:34:47,868 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.70      0.70        66
           2       0.46      0.48      0.47        63
           4       0.52      0.51      0.51        71

    accuracy                           0.56       200
   macro avg       0.56      0.56      0.56       200
weighted avg       0.56      0.56      0.56       200

2023-07-01 00:34:47,868 - INFO - test loss 0.028157063342003023
2023-07-01 00:34:47,868 - INFO - test acc 0.5600000023841858
2023-07-01 00:34:49,127 - INFO - Distilling data from client: Client34
2023-07-01 00:34:49,127 - INFO - train loss: 0.000695078497710561
2023-07-01 00:34:49,127 - INFO - train acc: 1.0
2023-07-01 00:34:49,151 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.74      0.74        66
           2       0.54      0.52      0.53        63
           4       0.52      0.54      0.53        71

    accuracy                           0.60       200
   macro avg       0.60      0.60      0.60       200
weighted avg       0.60      0.60      0.60       200

2023-07-01 00:34:49,151 - INFO - test loss 0.02751399878101524
2023-07-01 00:34:49,152 - INFO - test acc 0.5999999642372131
2023-07-01 00:34:50,408 - INFO - Distilling data from client: Client34
2023-07-01 00:34:50,408 - INFO - train loss: 0.0005689316134430724
2023-07-01 00:34:50,408 - INFO - train acc: 1.0
2023-07-01 00:34:50,432 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        66
           2       0.54      0.52      0.53        63
           4       0.53      0.56      0.55        71

    accuracy                           0.60       200
   macro avg       0.61      0.60      0.61       200
weighted avg       0.61      0.60      0.61       200

2023-07-01 00:34:50,432 - INFO - test loss 0.027078327659450886
2023-07-01 00:34:50,432 - INFO - test acc 0.6049999594688416
2023-07-01 00:34:51,693 - INFO - Distilling data from client: Client34
2023-07-01 00:34:51,693 - INFO - train loss: 0.0006335632370317893
2023-07-01 00:34:51,693 - INFO - train acc: 1.0
2023-07-01 00:34:51,718 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.74      0.72        66
           2       0.49      0.46      0.48        63
           4       0.52      0.52      0.52        71

    accuracy                           0.57       200
   macro avg       0.57      0.57      0.57       200
weighted avg       0.57      0.57      0.57       200

2023-07-01 00:34:51,718 - INFO - test loss 0.027070132151163766
2023-07-01 00:34:51,718 - INFO - test acc 0.574999988079071
2023-07-01 00:34:52,973 - INFO - Distilling data from client: Client34
2023-07-01 00:34:52,974 - INFO - train loss: 0.0005924013836414499
2023-07-01 00:34:52,974 - INFO - train acc: 1.0
2023-07-01 00:34:53,000 - INFO - report:               precision    recall  f1-score   support

           0       0.69      0.73      0.71        66
           2       0.54      0.54      0.54        63
           4       0.54      0.51      0.52        71

    accuracy                           0.59       200
   macro avg       0.59      0.59      0.59       200
weighted avg       0.59      0.59      0.59       200

2023-07-01 00:34:53,000 - INFO - test loss 0.026413914780455017
2023-07-01 00:34:53,001 - INFO - test acc 0.5899999737739563
2023-07-01 00:34:54,254 - INFO - Distilling data from client: Client34
2023-07-01 00:34:54,254 - INFO - train loss: 0.0005439111160790242
2023-07-01 00:34:54,254 - INFO - train acc: 1.0
2023-07-01 00:34:54,278 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.73      0.72        66
           2       0.53      0.56      0.54        63
           4       0.58      0.55      0.57        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-07-01 00:34:54,278 - INFO - test loss 0.02703232878979685
2023-07-01 00:34:54,278 - INFO - test acc 0.6100000143051147
2023-07-01 00:34:55,543 - INFO - Distilling data from client: Client34
2023-07-01 00:34:55,544 - INFO - train loss: 0.000486001670503105
2023-07-01 00:34:55,544 - INFO - train acc: 1.0
2023-07-01 00:34:55,568 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.76      0.74        66
           2       0.49      0.46      0.48        63
           4       0.51      0.52      0.52        71

    accuracy                           0.58       200
   macro avg       0.58      0.58      0.58       200
weighted avg       0.58      0.58      0.58       200

2023-07-01 00:34:55,568 - INFO - test loss 0.027454079569945777
2023-07-01 00:34:55,568 - INFO - test acc 0.5799999833106995
2023-07-01 00:34:56,830 - INFO - Distilling data from client: Client34
2023-07-01 00:34:56,830 - INFO - train loss: 0.0006004787790781018
2023-07-01 00:34:56,830 - INFO - train acc: 1.0
2023-07-01 00:34:56,854 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        66
           2       0.53      0.52      0.53        63
           4       0.58      0.59      0.59        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-07-01 00:34:56,854 - INFO - test loss 0.027113865606743965
2023-07-01 00:34:56,854 - INFO - test acc 0.6150000095367432
2023-07-01 00:34:56,866 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:56,875 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:56,884 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:56,892 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:56,901 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:56,910 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:56,919 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:56,928 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:56,937 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:34:57,325 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client34//synthetic.png
2023-07-01 00:34:57,338 - INFO - c: 1.0 and total_data_in_this_class: 261
2023-07-01 00:34:57,338 - INFO - c: 2.0 and total_data_in_this_class: 263
2023-07-01 00:34:57,338 - INFO - c: 5.0 and total_data_in_this_class: 275
2023-07-01 00:34:57,338 - INFO - c: 1.0 and total_data_in_this_class: 72
2023-07-01 00:34:57,338 - INFO - c: 2.0 and total_data_in_this_class: 70
2023-07-01 00:34:57,338 - INFO - c: 5.0 and total_data_in_this_class: 58
2023-07-01 00:34:57,359 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00026607513427734375 sec
2023-07-01 00:34:57,360 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:34:57,361 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012667179107666016 sec
2023-07-01 00:34:57,361 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:34:57,372 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010753631591796875 sec
2023-07-01 00:34:57,374 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002529621124267578 sec
2023-07-01 00:34:57,374 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:34:57,376 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009875297546386719 sec
2023-07-01 00:34:57,376 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:34:57,384 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.00853276252746582 sec
2023-07-01 00:34:57,388 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001347064971923828 sec
2023-07-01 00:34:57,389 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000125885009765625 sec
2023-07-01 00:34:57,390 - WARNING - Finished tracing + transforming true_divide for pjit in 0.000301361083984375 sec
2023-07-01 00:34:57,391 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013208389282226562 sec
2023-07-01 00:34:57,392 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020194053649902344 sec
2023-07-01 00:34:57,392 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002827644348144531 sec
2023-07-01 00:34:57,393 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002484321594238281 sec
2023-07-01 00:34:57,394 - WARNING - Finished tracing + transforming absolute for pjit in 0.00016832351684570312 sec
2023-07-01 00:34:57,394 - WARNING - Finished tracing + transforming fn for pjit in 0.0002760887145996094 sec
2023-07-01 00:34:57,395 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00031685829162597656 sec
2023-07-01 00:34:57,396 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012540817260742188 sec
2023-07-01 00:34:57,397 - WARNING - Finished tracing + transforming fn for pjit in 0.0002281665802001953 sec
2023-07-01 00:34:57,397 - WARNING - Finished tracing + transforming fn for pjit in 0.0003333091735839844 sec
2023-07-01 00:34:57,398 - WARNING - Finished tracing + transforming fn for pjit in 0.00022530555725097656 sec
2023-07-01 00:34:57,398 - WARNING - Finished tracing + transforming fn for pjit in 0.0002639293670654297 sec
2023-07-01 00:34:57,400 - WARNING - Finished tracing + transforming fn for pjit in 0.00022602081298828125 sec
2023-07-01 00:34:57,401 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00023627281188964844 sec
2023-07-01 00:34:57,402 - WARNING - Finished tracing + transforming fn for pjit in 0.00022983551025390625 sec
2023-07-01 00:34:57,403 - WARNING - Finished tracing + transforming fn for pjit in 0.00023317337036132812 sec
2023-07-01 00:34:57,406 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003643035888671875 sec
2023-07-01 00:34:57,407 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009686946868896484 sec
2023-07-01 00:34:57,408 - WARNING - Finished tracing + transforming fn for pjit in 0.00023603439331054688 sec
2023-07-01 00:34:57,408 - WARNING - Finished tracing + transforming fn for pjit in 0.000225067138671875 sec
2023-07-01 00:34:57,409 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022125244140625 sec
2023-07-01 00:34:57,410 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003409385681152344 sec
2023-07-01 00:34:57,411 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001766681671142578 sec
2023-07-01 00:34:57,411 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002689361572265625 sec
2023-07-01 00:34:57,412 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023031234741210938 sec
2023-07-01 00:34:57,413 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022792816162109375 sec
2023-07-01 00:34:57,414 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00027060508728027344 sec
2023-07-01 00:34:57,414 - WARNING - Finished tracing + transforming _where for pjit in 0.0008633136749267578 sec
2023-07-01 00:34:57,415 - WARNING - Finished tracing + transforming fn for pjit in 0.0002613067626953125 sec
2023-07-01 00:34:57,415 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032329559326171875 sec
2023-07-01 00:34:57,416 - WARNING - Finished tracing + transforming fn for pjit in 0.0002231597900390625 sec
2023-07-01 00:34:57,417 - WARNING - Finished tracing + transforming fn for pjit in 0.00022268295288085938 sec
2023-07-01 00:34:57,418 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021791458129882812 sec
2023-07-01 00:34:57,418 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025916099548339844 sec
2023-07-01 00:34:57,419 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017642974853515625 sec
2023-07-01 00:34:57,419 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026035308837890625 sec
2023-07-01 00:34:57,420 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023245811462402344 sec
2023-07-01 00:34:57,421 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024056434631347656 sec
2023-07-01 00:34:57,422 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002605915069580078 sec
2023-07-01 00:34:57,422 - WARNING - Finished tracing + transforming _where for pjit in 0.0008568763732910156 sec
2023-07-01 00:34:57,423 - WARNING - Finished tracing + transforming fn for pjit in 0.0002601146697998047 sec
2023-07-01 00:34:57,423 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025963783264160156 sec
2023-07-01 00:34:57,424 - WARNING - Finished tracing + transforming fn for pjit in 0.0002243518829345703 sec
2023-07-01 00:34:57,429 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002770423889160156 sec
2023-07-01 00:34:57,429 - WARNING - Finished tracing + transforming fn for pjit in 0.00027489662170410156 sec
2023-07-01 00:34:57,430 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002682209014892578 sec
2023-07-01 00:34:57,431 - WARNING - Finished tracing + transforming fn for pjit in 0.0002994537353515625 sec
2023-07-01 00:34:57,435 - WARNING - Finished tracing + transforming fn for pjit in 0.00022459030151367188 sec
2023-07-01 00:34:57,436 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016999244689941406 sec
2023-07-01 00:34:57,437 - WARNING - Finished tracing + transforming fn for pjit in 0.00030875205993652344 sec
2023-07-01 00:34:57,438 - WARNING - Finished tracing + transforming fn for pjit in 0.00023126602172851562 sec
2023-07-01 00:34:57,456 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0684194564819336 sec
2023-07-01 00:34:57,459 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001285076141357422 sec
2023-07-01 00:34:57,460 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001232624053955078 sec
2023-07-01 00:34:57,460 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00027298927307128906 sec
2023-07-01 00:34:57,463 - WARNING - Finished tracing + transforming fn for pjit in 0.00022649765014648438 sec
2023-07-01 00:34:57,463 - WARNING - Finished tracing + transforming fn for pjit in 0.00025963783264160156 sec
2023-07-01 00:34:57,464 - WARNING - Finished tracing + transforming fn for pjit in 0.00022125244140625 sec
2023-07-01 00:34:57,470 - WARNING - Finished tracing + transforming fn for pjit in 0.00023102760314941406 sec
2023-07-01 00:34:57,471 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002243518829345703 sec
2023-07-01 00:34:57,472 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025725364685058594 sec
2023-07-01 00:34:57,473 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001742839813232422 sec
2023-07-01 00:34:57,473 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00033020973205566406 sec
2023-07-01 00:34:57,474 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002288818359375 sec
2023-07-01 00:34:57,475 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022363662719726562 sec
2023-07-01 00:34:57,476 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00026869773864746094 sec
2023-07-01 00:34:57,476 - WARNING - Finished tracing + transforming _where for pjit in 0.0008637905120849609 sec
2023-07-01 00:34:57,477 - WARNING - Finished tracing + transforming fn for pjit in 0.0002639293670654297 sec
2023-07-01 00:34:57,477 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002574920654296875 sec
2023-07-01 00:34:57,478 - WARNING - Finished tracing + transforming fn for pjit in 0.00022172927856445312 sec
2023-07-01 00:34:57,479 - WARNING - Finished tracing + transforming fn for pjit in 0.00028228759765625 sec
2023-07-01 00:34:57,492 - WARNING - Finished tracing + transforming fn for pjit in 0.00022411346435546875 sec
2023-07-01 00:34:57,512 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05474734306335449 sec
2023-07-01 00:34:57,513 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012493133544921875 sec
2023-07-01 00:34:57,514 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013589859008789062 sec
2023-07-01 00:34:57,514 - WARNING - Finished tracing + transforming _where for pjit in 0.000621795654296875 sec
2023-07-01 00:34:57,515 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002911090850830078 sec
2023-07-01 00:34:57,515 - WARNING - Finished tracing + transforming trace for pjit in 0.0025119781494140625 sec
2023-07-01 00:34:57,518 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010609626770019531 sec
2023-07-01 00:34:57,519 - WARNING - Finished tracing + transforming tril for pjit in 0.0006768703460693359 sec
2023-07-01 00:34:57,519 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0020384788513183594 sec
2023-07-01 00:34:57,520 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010752677917480469 sec
2023-07-01 00:34:57,520 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001068115234375 sec
2023-07-01 00:34:57,522 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014238357543945312 sec
2023-07-01 00:34:57,526 - WARNING - Finished tracing + transforming _solve for pjit in 0.009469747543334961 sec
2023-07-01 00:34:57,527 - WARNING - Finished tracing + transforming dot for pjit in 0.000316619873046875 sec
2023-07-01 00:34:57,529 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.14339709281921387 sec
2023-07-01 00:34:57,531 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:34:57,565 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03291773796081543 sec
2023-07-01 00:34:57,565 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:34:57,691 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12628936767578125 sec
2023-07-01 00:34:57,714 - INFO - initial test loss: 0.023041791026617964
2023-07-01 00:34:57,714 - INFO - initial test acc: 0.7149999737739563
2023-07-01 00:34:57,720 - WARNING - Finished tracing + transforming dot for pjit in 0.0003719329833984375 sec
2023-07-01 00:34:57,721 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00030803680419921875 sec
2023-07-01 00:34:57,722 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003750324249267578 sec
2023-07-01 00:34:57,723 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010149478912353516 sec
2023-07-01 00:34:57,724 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00019860267639160156 sec
2023-07-01 00:34:57,724 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001862049102783203 sec
2023-07-01 00:34:57,725 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002524852752685547 sec
2023-07-01 00:34:57,726 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003750324249267578 sec
2023-07-01 00:34:57,726 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010838508605957031 sec
2023-07-01 00:34:57,727 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.010036230087280273 sec
2023-07-01 00:34:57,735 - WARNING - Finished tracing + transforming fn for pjit in 0.00031280517578125 sec
2023-07-01 00:34:57,736 - WARNING - Finished tracing + transforming fn for pjit in 0.0002620220184326172 sec
2023-07-01 00:34:57,737 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021576881408691406 sec
2023-07-01 00:34:57,737 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025844573974609375 sec
2023-07-01 00:34:57,738 - WARNING - Finished tracing + transforming _where for pjit in 0.0008420944213867188 sec
2023-07-01 00:34:57,746 - WARNING - Finished tracing + transforming fn for pjit in 0.0002536773681640625 sec
2023-07-01 00:34:57,746 - WARNING - Finished tracing + transforming fn for pjit in 0.00026416778564453125 sec
2023-07-01 00:34:57,747 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021505355834960938 sec
2023-07-01 00:34:57,748 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002434253692626953 sec
2023-07-01 00:34:57,748 - WARNING - Finished tracing + transforming _where for pjit in 0.0008037090301513672 sec
2023-07-01 00:34:57,783 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021219253540039062 sec
2023-07-01 00:34:57,839 - WARNING - Finished tracing + transforming fn for pjit in 0.00027823448181152344 sec
2023-07-01 00:34:57,840 - WARNING - Finished tracing + transforming fn for pjit in 0.0002346038818359375 sec
2023-07-01 00:34:57,840 - WARNING - Finished tracing + transforming square for pjit in 0.00016951560974121094 sec
2023-07-01 00:34:57,842 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022172927856445312 sec
2023-07-01 00:34:57,844 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024771690368652344 sec
2023-07-01 00:34:57,844 - WARNING - Finished tracing + transforming fn for pjit in 0.0002739429473876953 sec
2023-07-01 00:34:57,845 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00023102760314941406 sec
2023-07-01 00:34:57,846 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002334117889404297 sec
2023-07-01 00:34:57,846 - WARNING - Finished tracing + transforming fn for pjit in 0.00026488304138183594 sec
2023-07-01 00:34:57,847 - WARNING - Finished tracing + transforming fn for pjit in 0.00022983551025390625 sec
2023-07-01 00:34:57,847 - WARNING - Finished tracing + transforming square for pjit in 0.00016498565673828125 sec
2023-07-01 00:34:57,849 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022792816162109375 sec
2023-07-01 00:34:57,851 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017404556274414062 sec
2023-07-01 00:34:57,852 - WARNING - Finished tracing + transforming fn for pjit in 0.00026702880859375 sec
2023-07-01 00:34:57,852 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021505355834960938 sec
2023-07-01 00:34:57,853 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002262592315673828 sec
2023-07-01 00:34:57,854 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13727593421936035 sec
2023-07-01 00:34:57,857 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[522,10]), ShapedArray(float32[522,10]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:34:57,920 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06235551834106445 sec
2023-07-01 00:34:57,920 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:34:58,256 - WARNING - Finished XLA compilation of jit(update_fn) in 0.33552026748657227 sec
2023-07-01 00:34:59,503 - INFO - Distilling data from client: Client35
2023-07-01 00:34:59,503 - INFO - train loss: 0.0023698369826022857
2023-07-01 00:34:59,503 - INFO - train acc: 0.9961685538291931
2023-07-01 00:34:59,565 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.79      0.79        72
           2       0.65      0.67      0.66        70
           5       0.60      0.57      0.58        58

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.69      0.68       200

2023-07-01 00:34:59,565 - INFO - test loss 0.021720367867767088
2023-07-01 00:34:59,565 - INFO - test acc 0.6850000023841858
2023-07-01 00:35:00,808 - INFO - Distilling data from client: Client35
2023-07-01 00:35:00,808 - INFO - train loss: 0.0013044289229865007
2023-07-01 00:35:00,808 - INFO - train acc: 0.9980842471122742
2023-07-01 00:35:00,869 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.76      0.77        72
           2       0.67      0.73      0.70        70
           5       0.62      0.57      0.59        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:35:00,869 - INFO - test loss 0.022576002530258633
2023-07-01 00:35:00,869 - INFO - test acc 0.6949999928474426
2023-07-01 00:35:02,111 - INFO - Distilling data from client: Client35
2023-07-01 00:35:02,111 - INFO - train loss: 0.0009037116004324986
2023-07-01 00:35:02,111 - INFO - train acc: 1.0
2023-07-01 00:35:02,172 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.75      0.77        72
           2       0.68      0.73      0.70        70
           5       0.66      0.64      0.65        58

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:35:02,172 - INFO - test loss 0.022856750224892197
2023-07-01 00:35:02,172 - INFO - test acc 0.7099999785423279
2023-07-01 00:35:03,410 - INFO - Distilling data from client: Client35
2023-07-01 00:35:03,410 - INFO - train loss: 0.0008091404116977772
2023-07-01 00:35:03,410 - INFO - train acc: 1.0
2023-07-01 00:35:03,433 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.75      0.77        72
           2       0.70      0.69      0.69        70
           5       0.61      0.66      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:35:03,434 - INFO - test loss 0.02272500624362846
2023-07-01 00:35:03,434 - INFO - test acc 0.699999988079071
2023-07-01 00:35:04,682 - INFO - Distilling data from client: Client35
2023-07-01 00:35:04,682 - INFO - train loss: 0.0007437454999176227
2023-07-01 00:35:04,682 - INFO - train acc: 1.0
2023-07-01 00:35:04,705 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.72      0.75        72
           2       0.65      0.67      0.66        70
           5       0.61      0.66      0.63        58

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:35:04,706 - INFO - test loss 0.023239220753830298
2023-07-01 00:35:04,706 - INFO - test acc 0.6850000023841858
2023-07-01 00:35:05,965 - INFO - Distilling data from client: Client35
2023-07-01 00:35:05,965 - INFO - train loss: 0.0006614779270161694
2023-07-01 00:35:05,965 - INFO - train acc: 1.0
2023-07-01 00:35:05,989 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.75      0.75        72
           2       0.70      0.69      0.69        70
           5       0.63      0.64      0.63        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.70       200

2023-07-01 00:35:05,989 - INFO - test loss 0.022571097387804998
2023-07-01 00:35:05,989 - INFO - test acc 0.6949999928474426
2023-07-01 00:35:07,242 - INFO - Distilling data from client: Client35
2023-07-01 00:35:07,242 - INFO - train loss: 0.0005687559704515605
2023-07-01 00:35:07,242 - INFO - train acc: 1.0
2023-07-01 00:35:07,267 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.74      0.76        72
           2       0.69      0.73      0.71        70
           5       0.63      0.64      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-07-01 00:35:07,267 - INFO - test loss 0.022578718849160816
2023-07-01 00:35:07,267 - INFO - test acc 0.7049999833106995
2023-07-01 00:35:08,522 - INFO - Distilling data from client: Client35
2023-07-01 00:35:08,523 - INFO - train loss: 0.000529018452358783
2023-07-01 00:35:08,523 - INFO - train acc: 1.0
2023-07-01 00:35:08,547 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.75      0.77        72
           2       0.71      0.69      0.70        70
           5       0.60      0.66      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:35:08,547 - INFO - test loss 0.023298308103920563
2023-07-01 00:35:08,547 - INFO - test acc 0.699999988079071
2023-07-01 00:35:09,796 - INFO - Distilling data from client: Client35
2023-07-01 00:35:09,796 - INFO - train loss: 0.0005218740322111964
2023-07-01 00:35:09,796 - INFO - train acc: 1.0
2023-07-01 00:35:09,819 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.74      0.75        72
           2       0.67      0.70      0.69        70
           5       0.66      0.66      0.66        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:35:09,819 - INFO - test loss 0.022760395227103818
2023-07-01 00:35:09,819 - INFO - test acc 0.699999988079071
2023-07-01 00:35:11,075 - INFO - Distilling data from client: Client35
2023-07-01 00:35:11,075 - INFO - train loss: 0.0005475357494419027
2023-07-01 00:35:11,075 - INFO - train acc: 1.0
2023-07-01 00:35:11,100 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.74      0.76        72
           2       0.65      0.71      0.68        70
           5       0.61      0.59      0.60        58

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:35:11,101 - INFO - test loss 0.022765727532538457
2023-07-01 00:35:11,101 - INFO - test acc 0.6850000023841858
2023-07-01 00:35:12,348 - INFO - Distilling data from client: Client35
2023-07-01 00:35:12,348 - INFO - train loss: 0.0004152231975645473
2023-07-01 00:35:12,348 - INFO - train acc: 1.0
2023-07-01 00:35:12,372 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.74      0.75        72
           2       0.68      0.73      0.70        70
           5       0.64      0.62      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:35:12,372 - INFO - test loss 0.022814997048939942
2023-07-01 00:35:12,372 - INFO - test acc 0.699999988079071
2023-07-01 00:35:13,612 - INFO - Distilling data from client: Client35
2023-07-01 00:35:13,612 - INFO - train loss: 0.0003678881972092019
2023-07-01 00:35:13,612 - INFO - train acc: 1.0
2023-07-01 00:35:13,636 - INFO - report:               precision    recall  f1-score   support

           1       0.76      0.75      0.76        72
           2       0.71      0.74      0.73        70
           5       0.64      0.62      0.63        58

    accuracy                           0.71       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:35:13,636 - INFO - test loss 0.02211961762678712
2023-07-01 00:35:13,636 - INFO - test acc 0.7099999785423279
2023-07-01 00:35:14,883 - INFO - Distilling data from client: Client35
2023-07-01 00:35:14,883 - INFO - train loss: 0.000413273349499075
2023-07-01 00:35:14,883 - INFO - train acc: 1.0
2023-07-01 00:35:14,906 - INFO - report:               precision    recall  f1-score   support

           1       0.81      0.75      0.78        72
           2       0.68      0.71      0.70        70
           5       0.62      0.64      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-07-01 00:35:14,906 - INFO - test loss 0.023483184362527717
2023-07-01 00:35:14,906 - INFO - test acc 0.7049999833106995
2023-07-01 00:35:16,155 - INFO - Distilling data from client: Client35
2023-07-01 00:35:16,155 - INFO - train loss: 0.0005146300950642571
2023-07-01 00:35:16,155 - INFO - train acc: 1.0
2023-07-01 00:35:16,217 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.74      0.76        72
           2       0.69      0.74      0.72        70
           5       0.67      0.67      0.67        58

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:35:16,217 - INFO - test loss 0.022792175836157193
2023-07-01 00:35:16,218 - INFO - test acc 0.7199999690055847
2023-07-01 00:35:17,465 - INFO - Distilling data from client: Client35
2023-07-01 00:35:17,466 - INFO - train loss: 0.00034811307128569246
2023-07-01 00:35:17,466 - INFO - train acc: 1.0
2023-07-01 00:35:17,489 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.74      0.77        72
           2       0.66      0.73      0.69        70
           5       0.60      0.60      0.60        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.70       200

2023-07-01 00:35:17,490 - INFO - test loss 0.02284901862603348
2023-07-01 00:35:17,490 - INFO - test acc 0.6949999928474426
2023-07-01 00:35:18,742 - INFO - Distilling data from client: Client35
2023-07-01 00:35:18,742 - INFO - train loss: 0.0003870867546277552
2023-07-01 00:35:18,742 - INFO - train acc: 1.0
2023-07-01 00:35:18,766 - INFO - report:               precision    recall  f1-score   support

           1       0.76      0.71      0.73        72
           2       0.66      0.69      0.67        70
           5       0.60      0.62      0.61        58

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:35:18,766 - INFO - test loss 0.023210255464874315
2023-07-01 00:35:18,766 - INFO - test acc 0.675000011920929
2023-07-01 00:35:20,032 - INFO - Distilling data from client: Client35
2023-07-01 00:35:20,032 - INFO - train loss: 0.000353338444102347
2023-07-01 00:35:20,032 - INFO - train acc: 1.0
2023-07-01 00:35:20,057 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.75      0.77        72
           2       0.66      0.69      0.67        70
           5       0.61      0.62      0.62        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:35:20,057 - INFO - test loss 0.023643638385604574
2023-07-01 00:35:20,057 - INFO - test acc 0.6899999976158142
2023-07-01 00:35:21,313 - INFO - Distilling data from client: Client35
2023-07-01 00:35:21,313 - INFO - train loss: 0.00037334753119727394
2023-07-01 00:35:21,313 - INFO - train acc: 1.0
2023-07-01 00:35:21,336 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.75      0.77        72
           2       0.65      0.67      0.66        70
           5       0.60      0.62      0.61        58

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:35:21,336 - INFO - test loss 0.022892611465949182
2023-07-01 00:35:21,336 - INFO - test acc 0.6850000023841858
2023-07-01 00:35:22,571 - INFO - Distilling data from client: Client35
2023-07-01 00:35:22,571 - INFO - train loss: 0.0003891757488560529
2023-07-01 00:35:22,571 - INFO - train acc: 1.0
2023-07-01 00:35:22,594 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.75      0.77        72
           2       0.69      0.70      0.70        70
           5       0.62      0.64      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:35:22,594 - INFO - test loss 0.022968670981619328
2023-07-01 00:35:22,594 - INFO - test acc 0.699999988079071
2023-07-01 00:35:23,838 - INFO - Distilling data from client: Client35
2023-07-01 00:35:23,839 - INFO - train loss: 0.000417575781256467
2023-07-01 00:35:23,839 - INFO - train acc: 1.0
2023-07-01 00:35:23,862 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.74      0.77        72
           2       0.64      0.67      0.66        70
           5       0.56      0.60      0.58        58

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:35:23,862 - INFO - test loss 0.023288225652619864
2023-07-01 00:35:23,862 - INFO - test acc 0.675000011920929
2023-07-01 00:35:25,112 - INFO - Distilling data from client: Client35
2023-07-01 00:35:25,112 - INFO - train loss: 0.00035331982681671784
2023-07-01 00:35:25,112 - INFO - train acc: 1.0
2023-07-01 00:35:25,136 - INFO - report:               precision    recall  f1-score   support

           1       0.81      0.75      0.78        72
           2       0.66      0.73      0.69        70
           5       0.64      0.62      0.63        58

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.71      0.70      0.71       200

2023-07-01 00:35:25,137 - INFO - test loss 0.022725224669906334
2023-07-01 00:35:25,137 - INFO - test acc 0.7049999833106995
2023-07-01 00:35:26,388 - INFO - Distilling data from client: Client35
2023-07-01 00:35:26,388 - INFO - train loss: 0.0002522308346440778
2023-07-01 00:35:26,388 - INFO - train acc: 1.0
2023-07-01 00:35:26,412 - INFO - report:               precision    recall  f1-score   support

           1       0.78      0.71      0.74        72
           2       0.68      0.73      0.70        70
           5       0.62      0.64      0.63        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.70       200

2023-07-01 00:35:26,413 - INFO - test loss 0.02309911303448695
2023-07-01 00:35:26,413 - INFO - test acc 0.6949999928474426
2023-07-01 00:35:27,651 - INFO - Distilling data from client: Client35
2023-07-01 00:35:27,651 - INFO - train loss: 0.000286178950157288
2023-07-01 00:35:27,651 - INFO - train acc: 1.0
2023-07-01 00:35:27,675 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.72      0.76        72
           2       0.68      0.73      0.70        70
           5       0.60      0.62      0.61        58

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.70      0.69      0.70       200

2023-07-01 00:35:27,675 - INFO - test loss 0.023101951734018234
2023-07-01 00:35:27,675 - INFO - test acc 0.6949999928474426
2023-07-01 00:35:28,915 - INFO - Distilling data from client: Client35
2023-07-01 00:35:28,915 - INFO - train loss: 0.00025236092665185966
2023-07-01 00:35:28,915 - INFO - train acc: 1.0
2023-07-01 00:35:28,939 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.74      0.76        72
           2       0.64      0.70      0.67        70
           5       0.61      0.60      0.61        58

    accuracy                           0.69       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:35:28,939 - INFO - test loss 0.022738305263766054
2023-07-01 00:35:28,939 - INFO - test acc 0.6850000023841858
2023-07-01 00:35:30,188 - INFO - Distilling data from client: Client35
2023-07-01 00:35:30,189 - INFO - train loss: 0.0002850008753647646
2023-07-01 00:35:30,189 - INFO - train acc: 1.0
2023-07-01 00:35:30,211 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.78      0.78        72
           2       0.70      0.73      0.71        70
           5       0.64      0.62      0.63        58

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:35:30,212 - INFO - test loss 0.022963104183118126
2023-07-01 00:35:30,212 - INFO - test acc 0.7149999737739563
2023-07-01 00:35:30,214 - WARNING - Finished tracing + transforming jit(gather) in 0.0002448558807373047 sec
2023-07-01 00:35:30,214 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[522,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:35:30,215 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0011823177337646484 sec
2023-07-01 00:35:30,216 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:35:30,226 - WARNING - Finished XLA compilation of jit(gather) in 0.009732723236083984 sec
2023-07-01 00:35:30,236 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:35:30,245 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:35:30,254 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:35:30,263 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:35:30,271 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:35:30,280 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:35:30,290 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:35:30,298 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:35:30,308 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:35:30,681 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client35//synthetic.png
2023-07-01 00:35:30,693 - INFO - c: 1.0 and total_data_in_this_class: 255
2023-07-01 00:35:30,693 - INFO - c: 2.0 and total_data_in_this_class: 265
2023-07-01 00:35:30,693 - INFO - c: 9.0 and total_data_in_this_class: 279
2023-07-01 00:35:30,693 - INFO - c: 1.0 and total_data_in_this_class: 78
2023-07-01 00:35:30,693 - INFO - c: 2.0 and total_data_in_this_class: 68
2023-07-01 00:35:30,693 - INFO - c: 9.0 and total_data_in_this_class: 54
2023-07-01 00:35:30,764 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.047556161880493164 sec
2023-07-01 00:35:30,810 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04498910903930664 sec
2023-07-01 00:35:30,815 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.09967207908630371 sec
2023-07-01 00:35:30,816 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:35:30,850 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03296399116516113 sec
2023-07-01 00:35:30,850 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:35:30,975 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12500452995300293 sec
2023-07-01 00:35:30,997 - INFO - initial test loss: 0.023150742722134098
2023-07-01 00:35:30,998 - INFO - initial test acc: 0.6599999666213989
2023-07-01 00:35:31,006 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.005995035171508789 sec
2023-07-01 00:35:31,612 - WARNING - Finished tracing + transforming update_fn for pjit in 0.612175464630127 sec
2023-07-01 00:35:31,615 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[510,10]), ShapedArray(float32[510,10]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10]), ShapedArray(float32[510,3,32,32]), ShapedArray(float32[510,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:35:31,677 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06150937080383301 sec
2023-07-01 00:35:31,677 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:35:32,007 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3297722339630127 sec
2023-07-01 00:35:33,216 - INFO - Distilling data from client: Client36
2023-07-01 00:35:33,216 - INFO - train loss: 0.0024023597946582673
2023-07-01 00:35:33,216 - INFO - train acc: 0.9980392456054688
2023-07-01 00:35:33,271 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.63      0.69        78
           2       0.79      0.79      0.79        68
           9       0.54      0.67      0.60        54

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.71      0.69      0.70       200

2023-07-01 00:35:33,271 - INFO - test loss 0.022358681547425242
2023-07-01 00:35:33,271 - INFO - test acc 0.6949999928474426
2023-07-01 00:35:34,486 - INFO - Distilling data from client: Client36
2023-07-01 00:35:34,486 - INFO - train loss: 0.0013761625909713953
2023-07-01 00:35:34,486 - INFO - train acc: 1.0
2023-07-01 00:35:34,509 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.59      0.63        78
           2       0.77      0.79      0.78        68
           9       0.52      0.59      0.55        54

    accuracy                           0.66       200
   macro avg       0.65      0.66      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:35:34,509 - INFO - test loss 0.022623961116584462
2023-07-01 00:35:34,509 - INFO - test acc 0.6599999666213989
2023-07-01 00:35:35,714 - INFO - Distilling data from client: Client36
2023-07-01 00:35:35,715 - INFO - train loss: 0.0010118153119897963
2023-07-01 00:35:35,715 - INFO - train acc: 1.0
2023-07-01 00:35:35,739 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.65      0.67        78
           2       0.78      0.79      0.79        68
           9       0.54      0.57      0.56        54

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:35:35,739 - INFO - test loss 0.022479766051687323
2023-07-01 00:35:35,739 - INFO - test acc 0.6800000071525574
2023-07-01 00:35:36,954 - INFO - Distilling data from client: Client36
2023-07-01 00:35:36,954 - INFO - train loss: 0.0008568079363648353
2023-07-01 00:35:36,954 - INFO - train acc: 1.0
2023-07-01 00:35:36,977 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.62      0.66        78
           2       0.76      0.82      0.79        68
           9       0.54      0.59      0.57        54

    accuracy                           0.68       200
   macro avg       0.67      0.68      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:35:36,977 - INFO - test loss 0.023089972104865995
2023-07-01 00:35:36,977 - INFO - test acc 0.6800000071525574
2023-07-01 00:35:38,189 - INFO - Distilling data from client: Client36
2023-07-01 00:35:38,189 - INFO - train loss: 0.0007579114792710921
2023-07-01 00:35:38,189 - INFO - train acc: 1.0
2023-07-01 00:35:38,212 - INFO - report:               precision    recall  f1-score   support

           1       0.71      0.63      0.67        78
           2       0.75      0.76      0.76        68
           9       0.50      0.57      0.53        54

    accuracy                           0.66       200
   macro avg       0.65      0.66      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:35:38,212 - INFO - test loss 0.023243998371247285
2023-07-01 00:35:38,212 - INFO - test acc 0.6599999666213989
2023-07-01 00:35:39,419 - INFO - Distilling data from client: Client36
2023-07-01 00:35:39,419 - INFO - train loss: 0.0006052158191170644
2023-07-01 00:35:39,419 - INFO - train acc: 1.0
2023-07-01 00:35:39,443 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.59      0.64        78
           2       0.78      0.82      0.80        68
           9       0.52      0.59      0.55        54

    accuracy                           0.67       200
   macro avg       0.66      0.67      0.66       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:35:39,443 - INFO - test loss 0.023703521305034277
2023-07-01 00:35:39,444 - INFO - test acc 0.6699999570846558
2023-07-01 00:35:40,645 - INFO - Distilling data from client: Client36
2023-07-01 00:35:40,645 - INFO - train loss: 0.0005119176941033797
2023-07-01 00:35:40,645 - INFO - train acc: 1.0
2023-07-01 00:35:40,668 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.60      0.64        78
           2       0.79      0.76      0.78        68
           9       0.48      0.59      0.53        54

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:35:40,668 - INFO - test loss 0.02333888058834839
2023-07-01 00:35:40,669 - INFO - test acc 0.6549999713897705
2023-07-01 00:35:41,881 - INFO - Distilling data from client: Client36
2023-07-01 00:35:41,881 - INFO - train loss: 0.00046751491440582253
2023-07-01 00:35:41,881 - INFO - train acc: 1.0
2023-07-01 00:35:41,903 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.62      0.65        78
           2       0.77      0.78      0.77        68
           9       0.52      0.59      0.56        54

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:35:41,904 - INFO - test loss 0.022970657846712553
2023-07-01 00:35:41,904 - INFO - test acc 0.6649999618530273
2023-07-01 00:35:43,110 - INFO - Distilling data from client: Client36
2023-07-01 00:35:43,110 - INFO - train loss: 0.000421041556421723
2023-07-01 00:35:43,110 - INFO - train acc: 1.0
2023-07-01 00:35:43,132 - INFO - report:               precision    recall  f1-score   support

           1       0.64      0.58      0.61        78
           2       0.75      0.76      0.76        68
           9       0.49      0.56      0.52        54

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:35:43,132 - INFO - test loss 0.023115141573849186
2023-07-01 00:35:43,132 - INFO - test acc 0.6349999904632568
2023-07-01 00:35:44,349 - INFO - Distilling data from client: Client36
2023-07-01 00:35:44,349 - INFO - train loss: 0.0003984302701755457
2023-07-01 00:35:44,349 - INFO - train acc: 1.0
2023-07-01 00:35:44,373 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.59      0.65        78
           2       0.75      0.79      0.77        68
           9       0.50      0.59      0.54        54

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:35:44,373 - INFO - test loss 0.02349080440913424
2023-07-01 00:35:44,373 - INFO - test acc 0.6599999666213989
2023-07-01 00:35:45,584 - INFO - Distilling data from client: Client36
2023-07-01 00:35:45,584 - INFO - train loss: 0.0005350360343160331
2023-07-01 00:35:45,584 - INFO - train acc: 1.0
2023-07-01 00:35:45,606 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.60      0.64        78
           2       0.76      0.78      0.77        68
           9       0.52      0.59      0.55        54

    accuracy                           0.66       200
   macro avg       0.65      0.66      0.65       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:35:45,607 - INFO - test loss 0.02358364109180053
2023-07-01 00:35:45,607 - INFO - test acc 0.6599999666213989
2023-07-01 00:35:46,826 - INFO - Distilling data from client: Client36
2023-07-01 00:35:46,826 - INFO - train loss: 0.0004749750587077452
2023-07-01 00:35:46,826 - INFO - train acc: 1.0
2023-07-01 00:35:46,851 - INFO - report:               precision    recall  f1-score   support

           1       0.72      0.60      0.66        78
           2       0.77      0.81      0.79        68
           9       0.53      0.63      0.58        54

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.67       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:35:46,851 - INFO - test loss 0.023420323757021834
2023-07-01 00:35:46,851 - INFO - test acc 0.6800000071525574
2023-07-01 00:35:48,061 - INFO - Distilling data from client: Client36
2023-07-01 00:35:48,061 - INFO - train loss: 0.00042099645601820836
2023-07-01 00:35:48,061 - INFO - train acc: 1.0
2023-07-01 00:35:48,084 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.58      0.62        78
           2       0.76      0.78      0.77        68
           9       0.53      0.63      0.58        54

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.66      0.66       200

2023-07-01 00:35:48,084 - INFO - test loss 0.02318750869434482
2023-07-01 00:35:48,084 - INFO - test acc 0.6599999666213989
2023-07-01 00:35:49,279 - INFO - Distilling data from client: Client36
2023-07-01 00:35:49,280 - INFO - train loss: 0.0003035039264595466
2023-07-01 00:35:49,280 - INFO - train acc: 1.0
2023-07-01 00:35:49,304 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.64      0.67        78
           2       0.77      0.78      0.77        68
           9       0.53      0.59      0.56        54

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:35:49,304 - INFO - test loss 0.022991030072080566
2023-07-01 00:35:49,304 - INFO - test acc 0.675000011920929
2023-07-01 00:35:50,522 - INFO - Distilling data from client: Client36
2023-07-01 00:35:50,522 - INFO - train loss: 0.000343470729501253
2023-07-01 00:35:50,522 - INFO - train acc: 1.0
2023-07-01 00:35:50,544 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.58      0.63        78
           2       0.75      0.81      0.78        68
           9       0.50      0.57      0.53        54

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-07-01 00:35:50,544 - INFO - test loss 0.023535363121854117
2023-07-01 00:35:50,545 - INFO - test acc 0.6549999713897705
2023-07-01 00:35:51,757 - INFO - Distilling data from client: Client36
2023-07-01 00:35:51,757 - INFO - train loss: 0.0002863049339497756
2023-07-01 00:35:51,758 - INFO - train acc: 1.0
2023-07-01 00:35:51,781 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.63      0.66        78
           2       0.76      0.79      0.78        68
           9       0.52      0.56      0.54        54

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:35:51,782 - INFO - test loss 0.02332241185446967
2023-07-01 00:35:51,782 - INFO - test acc 0.6649999618530273
2023-07-01 00:35:52,982 - INFO - Distilling data from client: Client36
2023-07-01 00:35:52,982 - INFO - train loss: 0.00025505240012191815
2023-07-01 00:35:52,982 - INFO - train acc: 1.0
2023-07-01 00:35:53,004 - INFO - report:               precision    recall  f1-score   support

           1       0.73      0.63      0.68        78
           2       0.77      0.79      0.78        68
           9       0.51      0.59      0.55        54

    accuracy                           0.68       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:35:53,005 - INFO - test loss 0.023634119965941565
2023-07-01 00:35:53,005 - INFO - test acc 0.675000011920929
2023-07-01 00:35:54,219 - INFO - Distilling data from client: Client36
2023-07-01 00:35:54,219 - INFO - train loss: 0.0002601853273588293
2023-07-01 00:35:54,219 - INFO - train acc: 1.0
2023-07-01 00:35:54,242 - INFO - report:               precision    recall  f1-score   support

           1       0.67      0.60      0.64        78
           2       0.74      0.78      0.76        68
           9       0.48      0.52      0.50        54

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:35:54,242 - INFO - test loss 0.02449204845063012
2023-07-01 00:35:54,242 - INFO - test acc 0.6399999856948853
2023-07-01 00:35:55,456 - INFO - Distilling data from client: Client36
2023-07-01 00:35:55,456 - INFO - train loss: 0.00033945627313217695
2023-07-01 00:35:55,456 - INFO - train acc: 1.0
2023-07-01 00:35:55,479 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.58      0.62        78
           2       0.77      0.78      0.77        68
           9       0.49      0.57      0.53        54

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:35:55,479 - INFO - test loss 0.023789111482607945
2023-07-01 00:35:55,479 - INFO - test acc 0.6449999809265137
2023-07-01 00:35:56,698 - INFO - Distilling data from client: Client36
2023-07-01 00:35:56,698 - INFO - train loss: 0.0002567740997284087
2023-07-01 00:35:56,698 - INFO - train acc: 1.0
2023-07-01 00:35:56,721 - INFO - report:               precision    recall  f1-score   support

           1       0.69      0.59      0.63        78
           2       0.78      0.79      0.79        68
           9       0.52      0.61      0.56        54

    accuracy                           0.67       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:35:56,721 - INFO - test loss 0.023981184686416336
2023-07-01 00:35:56,721 - INFO - test acc 0.6649999618530273
2023-07-01 00:35:57,937 - INFO - Distilling data from client: Client36
2023-07-01 00:35:57,937 - INFO - train loss: 0.0002597934228787883
2023-07-01 00:35:57,938 - INFO - train acc: 1.0
2023-07-01 00:35:57,960 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.60      0.65        78
           2       0.78      0.79      0.79        68
           9       0.53      0.63      0.58        54

    accuracy                           0.68       200
   macro avg       0.67      0.68      0.67       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:35:57,960 - INFO - test loss 0.0234709148723585
2023-07-01 00:35:57,960 - INFO - test acc 0.675000011920929
2023-07-01 00:35:59,178 - INFO - Distilling data from client: Client36
2023-07-01 00:35:59,178 - INFO - train loss: 0.0002849260145730782
2023-07-01 00:35:59,178 - INFO - train acc: 1.0
2023-07-01 00:35:59,202 - INFO - report:               precision    recall  f1-score   support

           1       0.66      0.58      0.62        78
           2       0.74      0.78      0.76        68
           9       0.50      0.56      0.53        54

    accuracy                           0.64       200
   macro avg       0.63      0.64      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:35:59,202 - INFO - test loss 0.024203167785547586
2023-07-01 00:35:59,202 - INFO - test acc 0.6399999856948853
2023-07-01 00:36:00,409 - INFO - Distilling data from client: Client36
2023-07-01 00:36:00,409 - INFO - train loss: 0.00023085474700889987
2023-07-01 00:36:00,409 - INFO - train acc: 1.0
2023-07-01 00:36:00,431 - INFO - report:               precision    recall  f1-score   support

           1       0.70      0.62      0.65        78
           2       0.75      0.78      0.76        68
           9       0.47      0.52      0.49        54

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:36:00,431 - INFO - test loss 0.02389385930593275
2023-07-01 00:36:00,432 - INFO - test acc 0.6449999809265137
2023-07-01 00:36:01,637 - INFO - Distilling data from client: Client36
2023-07-01 00:36:01,638 - INFO - train loss: 0.00020370471892984125
2023-07-01 00:36:01,638 - INFO - train acc: 1.0
2023-07-01 00:36:01,660 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.63      0.65        78
           2       0.75      0.79      0.77        68
           9       0.52      0.54      0.53        54

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-07-01 00:36:01,661 - INFO - test loss 0.023732320627580378
2023-07-01 00:36:01,661 - INFO - test acc 0.6599999666213989
2023-07-01 00:36:02,870 - INFO - Distilling data from client: Client36
2023-07-01 00:36:02,870 - INFO - train loss: 0.0002025606429900197
2023-07-01 00:36:02,871 - INFO - train acc: 1.0
2023-07-01 00:36:02,892 - INFO - report:               precision    recall  f1-score   support

           1       0.68      0.60      0.64        78
           2       0.76      0.79      0.78        68
           9       0.50      0.56      0.53        54

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.66      0.66      0.66       200

2023-07-01 00:36:02,893 - INFO - test loss 0.02383453383511083
2023-07-01 00:36:02,893 - INFO - test acc 0.6549999713897705
2023-07-01 00:36:02,905 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:02,913 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:02,922 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:02,931 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:02,939 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:02,948 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:02,957 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:02,966 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:02,975 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:03,357 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client36//synthetic.png
2023-07-01 00:36:03,370 - INFO - c: 3.0 and total_data_in_this_class: 528
2023-07-01 00:36:03,370 - INFO - c: 7.0 and total_data_in_this_class: 32
2023-07-01 00:36:03,370 - INFO - c: 8.0 and total_data_in_this_class: 239
2023-07-01 00:36:03,370 - INFO - c: 3.0 and total_data_in_this_class: 138
2023-07-01 00:36:03,370 - INFO - c: 7.0 and total_data_in_this_class: 8
2023-07-01 00:36:03,370 - INFO - c: 8.0 and total_data_in_this_class: 54
2023-07-01 00:36:03,388 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00024580955505371094 sec
2023-07-01 00:36:03,388 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:36:03,390 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012485980987548828 sec
2023-07-01 00:36:03,390 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:36:03,401 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010690927505493164 sec
2023-07-01 00:36:03,402 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00021886825561523438 sec
2023-07-01 00:36:03,402 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:36:03,403 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009090900421142578 sec
2023-07-01 00:36:03,404 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:36:03,412 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008290290832519531 sec
2023-07-01 00:36:03,415 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013399124145507812 sec
2023-07-01 00:36:03,416 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011968612670898438 sec
2023-07-01 00:36:03,417 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003039836883544922 sec
2023-07-01 00:36:03,418 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020170211791992188 sec
2023-07-01 00:36:03,419 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011587142944335938 sec
2023-07-01 00:36:03,419 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002739429473876953 sec
2023-07-01 00:36:03,420 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023937225341796875 sec
2023-07-01 00:36:03,420 - WARNING - Finished tracing + transforming absolute for pjit in 0.00016236305236816406 sec
2023-07-01 00:36:03,421 - WARNING - Finished tracing + transforming fn for pjit in 0.0002760887145996094 sec
2023-07-01 00:36:03,422 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0003075599670410156 sec
2023-07-01 00:36:03,422 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019669532775878906 sec
2023-07-01 00:36:03,423 - WARNING - Finished tracing + transforming fn for pjit in 0.0002269744873046875 sec
2023-07-01 00:36:03,424 - WARNING - Finished tracing + transforming fn for pjit in 0.0002570152282714844 sec
2023-07-01 00:36:03,425 - WARNING - Finished tracing + transforming fn for pjit in 0.0002193450927734375 sec
2023-07-01 00:36:03,425 - WARNING - Finished tracing + transforming fn for pjit in 0.000255584716796875 sec
2023-07-01 00:36:03,426 - WARNING - Finished tracing + transforming fn for pjit in 0.00022077560424804688 sec
2023-07-01 00:36:03,428 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00017523765563964844 sec
2023-07-01 00:36:03,429 - WARNING - Finished tracing + transforming fn for pjit in 0.0002269744873046875 sec
2023-07-01 00:36:03,429 - WARNING - Finished tracing + transforming fn for pjit in 0.00023031234741210938 sec
2023-07-01 00:36:03,433 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00035953521728515625 sec
2023-07-01 00:36:03,433 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009462833404541016 sec
2023-07-01 00:36:03,434 - WARNING - Finished tracing + transforming fn for pjit in 0.00023293495178222656 sec
2023-07-01 00:36:03,435 - WARNING - Finished tracing + transforming fn for pjit in 0.00022220611572265625 sec
2023-07-01 00:36:03,436 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029087066650390625 sec
2023-07-01 00:36:03,436 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002593994140625 sec
2023-07-01 00:36:03,437 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001735687255859375 sec
2023-07-01 00:36:03,438 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025534629821777344 sec
2023-07-01 00:36:03,439 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022339820861816406 sec
2023-07-01 00:36:03,439 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022363662719726562 sec
2023-07-01 00:36:03,440 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00033664703369140625 sec
2023-07-01 00:36:03,440 - WARNING - Finished tracing + transforming _where for pjit in 0.0009222030639648438 sec
2023-07-01 00:36:03,441 - WARNING - Finished tracing + transforming fn for pjit in 0.000255584716796875 sec
2023-07-01 00:36:03,442 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002567768096923828 sec
2023-07-01 00:36:03,443 - WARNING - Finished tracing + transforming fn for pjit in 0.0002200603485107422 sec
2023-07-01 00:36:03,443 - WARNING - Finished tracing + transforming fn for pjit in 0.0002155303955078125 sec
2023-07-01 00:36:03,444 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021457672119140625 sec
2023-07-01 00:36:03,444 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002512931823730469 sec
2023-07-01 00:36:03,445 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024771690368652344 sec
2023-07-01 00:36:03,446 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002567768096923828 sec
2023-07-01 00:36:03,447 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022292137145996094 sec
2023-07-01 00:36:03,447 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002243518829345703 sec
2023-07-01 00:36:03,448 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025391578674316406 sec
2023-07-01 00:36:03,448 - WARNING - Finished tracing + transforming _where for pjit in 0.0008270740509033203 sec
2023-07-01 00:36:03,449 - WARNING - Finished tracing + transforming fn for pjit in 0.0002689361572265625 sec
2023-07-01 00:36:03,450 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000255584716796875 sec
2023-07-01 00:36:03,451 - WARNING - Finished tracing + transforming fn for pjit in 0.00022292137145996094 sec
2023-07-01 00:36:03,455 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002696514129638672 sec
2023-07-01 00:36:03,456 - WARNING - Finished tracing + transforming fn for pjit in 0.0003459453582763672 sec
2023-07-01 00:36:03,456 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002684593200683594 sec
2023-07-01 00:36:03,457 - WARNING - Finished tracing + transforming fn for pjit in 0.00022745132446289062 sec
2023-07-01 00:36:03,461 - WARNING - Finished tracing + transforming fn for pjit in 0.0002205371856689453 sec
2023-07-01 00:36:03,462 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001652240753173828 sec
2023-07-01 00:36:03,463 - WARNING - Finished tracing + transforming fn for pjit in 0.00029349327087402344 sec
2023-07-01 00:36:03,464 - WARNING - Finished tracing + transforming fn for pjit in 0.00022268295288085938 sec
2023-07-01 00:36:03,482 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06714105606079102 sec
2023-07-01 00:36:03,485 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001246929168701172 sec
2023-07-01 00:36:03,486 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011754035949707031 sec
2023-07-01 00:36:03,486 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00026154518127441406 sec
2023-07-01 00:36:03,488 - WARNING - Finished tracing + transforming fn for pjit in 0.00022220611572265625 sec
2023-07-01 00:36:03,489 - WARNING - Finished tracing + transforming fn for pjit in 0.00025177001953125 sec
2023-07-01 00:36:03,490 - WARNING - Finished tracing + transforming fn for pjit in 0.0002181529998779297 sec
2023-07-01 00:36:03,496 - WARNING - Finished tracing + transforming fn for pjit in 0.00022172927856445312 sec
2023-07-01 00:36:03,497 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022864341735839844 sec
2023-07-01 00:36:03,497 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000255584716796875 sec
2023-07-01 00:36:03,498 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000171661376953125 sec
2023-07-01 00:36:03,499 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032901763916015625 sec
2023-07-01 00:36:03,500 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022459030151367188 sec
2023-07-01 00:36:03,500 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002219676971435547 sec
2023-07-01 00:36:03,501 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002837181091308594 sec
2023-07-01 00:36:03,501 - WARNING - Finished tracing + transforming _where for pjit in 0.0008678436279296875 sec
2023-07-01 00:36:03,502 - WARNING - Finished tracing + transforming fn for pjit in 0.00025653839111328125 sec
2023-07-01 00:36:03,503 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002524852752685547 sec
2023-07-01 00:36:03,503 - WARNING - Finished tracing + transforming fn for pjit in 0.000217437744140625 sec
2023-07-01 00:36:03,504 - WARNING - Finished tracing + transforming fn for pjit in 0.0002777576446533203 sec
2023-07-01 00:36:03,516 - WARNING - Finished tracing + transforming fn for pjit in 0.00021886825561523438 sec
2023-07-01 00:36:03,535 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05225014686584473 sec
2023-07-01 00:36:03,536 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001232624053955078 sec
2023-07-01 00:36:03,537 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0001327991485595703 sec
2023-07-01 00:36:03,538 - WARNING - Finished tracing + transforming _where for pjit in 0.0006062984466552734 sec
2023-07-01 00:36:03,538 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00028824806213378906 sec
2023-07-01 00:36:03,539 - WARNING - Finished tracing + transforming trace for pjit in 0.0024645328521728516 sec
2023-07-01 00:36:03,541 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00011515617370605469 sec
2023-07-01 00:36:03,542 - WARNING - Finished tracing + transforming tril for pjit in 0.0006651878356933594 sec
2023-07-01 00:36:03,542 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0017840862274169922 sec
2023-07-01 00:36:03,543 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010776519775390625 sec
2023-07-01 00:36:03,543 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010538101196289062 sec
2023-07-01 00:36:03,545 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014171600341796875 sec
2023-07-01 00:36:03,549 - WARNING - Finished tracing + transforming _solve for pjit in 0.009085893630981445 sec
2023-07-01 00:36:03,550 - WARNING - Finished tracing + transforming dot for pjit in 0.0003056526184082031 sec
2023-07-01 00:36:03,552 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.13849425315856934 sec
2023-07-01 00:36:03,554 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:36:03,586 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03194451332092285 sec
2023-07-01 00:36:03,586 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:36:03,690 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.10393881797790527 sec
2023-07-01 00:36:03,695 - INFO - initial test loss: 0.0283843453393796
2023-07-01 00:36:03,695 - INFO - initial test acc: 0.5949999690055847
2023-07-01 00:36:03,700 - WARNING - Finished tracing + transforming dot for pjit in 0.00035691261291503906 sec
2023-07-01 00:36:03,701 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029778480529785156 sec
2023-07-01 00:36:03,702 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003762245178222656 sec
2023-07-01 00:36:03,702 - WARNING - Finished tracing + transforming _mean for pjit in 0.001010894775390625 sec
2023-07-01 00:36:03,703 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00019693374633789062 sec
2023-07-01 00:36:03,704 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00018835067749023438 sec
2023-07-01 00:36:03,704 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023746490478515625 sec
2023-07-01 00:36:03,705 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00037932395935058594 sec
2023-07-01 00:36:03,706 - WARNING - Finished tracing + transforming _mean for pjit in 0.0011112689971923828 sec
2023-07-01 00:36:03,706 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009891271591186523 sec
2023-07-01 00:36:03,715 - WARNING - Finished tracing + transforming fn for pjit in 0.0003192424774169922 sec
2023-07-01 00:36:03,716 - WARNING - Finished tracing + transforming fn for pjit in 0.00026917457580566406 sec
2023-07-01 00:36:03,716 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021767616271972656 sec
2023-07-01 00:36:03,717 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002655982971191406 sec
2023-07-01 00:36:03,718 - WARNING - Finished tracing + transforming _where for pjit in 0.0008692741394042969 sec
2023-07-01 00:36:03,726 - WARNING - Finished tracing + transforming fn for pjit in 0.0002570152282714844 sec
2023-07-01 00:36:03,726 - WARNING - Finished tracing + transforming fn for pjit in 0.0002694129943847656 sec
2023-07-01 00:36:03,727 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021600723266601562 sec
2023-07-01 00:36:03,728 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00025177001953125 sec
2023-07-01 00:36:03,728 - WARNING - Finished tracing + transforming _where for pjit in 0.0008230209350585938 sec
2023-07-01 00:36:03,763 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00040793418884277344 sec
2023-07-01 00:36:03,818 - WARNING - Finished tracing + transforming fn for pjit in 0.0002777576446533203 sec
2023-07-01 00:36:03,819 - WARNING - Finished tracing + transforming fn for pjit in 0.00022125244140625 sec
2023-07-01 00:36:03,819 - WARNING - Finished tracing + transforming square for pjit in 0.00017213821411132812 sec
2023-07-01 00:36:03,821 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00023102760314941406 sec
2023-07-01 00:36:03,823 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024771690368652344 sec
2023-07-01 00:36:03,823 - WARNING - Finished tracing + transforming fn for pjit in 0.0002713203430175781 sec
2023-07-01 00:36:03,824 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022745132446289062 sec
2023-07-01 00:36:03,825 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022459030151367188 sec
2023-07-01 00:36:03,825 - WARNING - Finished tracing + transforming fn for pjit in 0.0002734661102294922 sec
2023-07-01 00:36:03,826 - WARNING - Finished tracing + transforming fn for pjit in 0.00023126602172851562 sec
2023-07-01 00:36:03,827 - WARNING - Finished tracing + transforming square for pjit in 0.00017404556274414062 sec
2023-07-01 00:36:03,829 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021505355834960938 sec
2023-07-01 00:36:03,830 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017690658569335938 sec
2023-07-01 00:36:03,831 - WARNING - Finished tracing + transforming fn for pjit in 0.0002703666687011719 sec
2023-07-01 00:36:03,831 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022077560424804688 sec
2023-07-01 00:36:03,832 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021886825561523438 sec
2023-07-01 00:36:03,833 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13669037818908691 sec
2023-07-01 00:36:03,836 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,10]), ShapedArray(float32[63,10]), ShapedArray(float32[63,10]), ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,10]), ShapedArray(float32[63,3,32,32]), ShapedArray(float32[63,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:36:03,898 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.061769723892211914 sec
2023-07-01 00:36:03,899 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:36:04,168 - WARNING - Finished XLA compilation of jit(update_fn) in 0.2692101001739502 sec
2023-07-01 00:36:04,255 - INFO - Distilling data from client: Client37
2023-07-01 00:36:04,255 - INFO - train loss: 0.011848838720413786
2023-07-01 00:36:04,255 - INFO - train acc: 0.9206349849700928
2023-07-01 00:36:04,266 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.80      0.85       138
           7       0.24      0.62      0.34         8
           8       0.76      0.81      0.79        54

    accuracy                           0.80       200
   macro avg       0.64      0.75      0.66       200
weighted avg       0.84      0.80      0.81       200

2023-07-01 00:36:04,266 - INFO - test loss 0.01827594283901577
2023-07-01 00:36:04,266 - INFO - test acc 0.7949999570846558
2023-07-01 00:36:04,356 - INFO - Distilling data from client: Client37
2023-07-01 00:36:04,356 - INFO - train loss: 0.0111220026793486
2023-07-01 00:36:04,356 - INFO - train acc: 0.9365079998970032
2023-07-01 00:36:04,362 - INFO - report:               precision    recall  f1-score   support

           3       0.89      0.82      0.85       138
           7       0.31      0.50      0.38         8
           8       0.70      0.78      0.74        54

    accuracy                           0.80       200
   macro avg       0.63      0.70      0.66       200
weighted avg       0.82      0.80      0.80       200

2023-07-01 00:36:04,362 - INFO - test loss 0.017883494616933494
2023-07-01 00:36:04,362 - INFO - test acc 0.7949999570846558
2023-07-01 00:36:04,451 - INFO - Distilling data from client: Client37
2023-07-01 00:36:04,451 - INFO - train loss: 0.009136510719680637
2023-07-01 00:36:04,452 - INFO - train acc: 0.9365079998970032
2023-07-01 00:36:04,463 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.87      0.89       138
           7       0.45      0.62      0.53         8
           8       0.77      0.80      0.78        54

    accuracy                           0.84       200
   macro avg       0.71      0.76      0.73       200
weighted avg       0.85      0.84      0.84       200

2023-07-01 00:36:04,463 - INFO - test loss 0.01656956327032116
2023-07-01 00:36:04,463 - INFO - test acc 0.8399999737739563
2023-07-01 00:36:04,554 - INFO - Distilling data from client: Client37
2023-07-01 00:36:04,554 - INFO - train loss: 0.007428933680368129
2023-07-01 00:36:04,554 - INFO - train acc: 0.968254029750824
2023-07-01 00:36:04,560 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.87      0.89       138
           7       0.36      0.50      0.42         8
           8       0.77      0.81      0.79        54

    accuracy                           0.84       200
   macro avg       0.68      0.73      0.70       200
weighted avg       0.85      0.84      0.84       200

2023-07-01 00:36:04,560 - INFO - test loss 0.015882454303110645
2023-07-01 00:36:04,560 - INFO - test acc 0.8399999737739563
2023-07-01 00:36:04,648 - INFO - Distilling data from client: Client37
2023-07-01 00:36:04,649 - INFO - train loss: 0.009497751655327354
2023-07-01 00:36:04,649 - INFO - train acc: 0.9206349849700928
2023-07-01 00:36:04,654 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.86      0.88       138
           7       0.60      0.38      0.46         8
           8       0.69      0.83      0.76        54

    accuracy                           0.83       200
   macro avg       0.73      0.69      0.70       200
weighted avg       0.84      0.83      0.83       200

2023-07-01 00:36:04,654 - INFO - test loss 0.01633869560148032
2023-07-01 00:36:04,654 - INFO - test acc 0.8299999833106995
2023-07-01 00:36:04,745 - INFO - Distilling data from client: Client37
2023-07-01 00:36:04,745 - INFO - train loss: 0.007783778069864138
2023-07-01 00:36:04,745 - INFO - train acc: 0.968254029750824
2023-07-01 00:36:04,750 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.86      0.88       138
           7       0.33      0.50      0.40         8
           8       0.77      0.80      0.78        54

    accuracy                           0.83       200
   macro avg       0.67      0.72      0.69       200
weighted avg       0.84      0.83      0.84       200

2023-07-01 00:36:04,751 - INFO - test loss 0.015993340836826457
2023-07-01 00:36:04,751 - INFO - test acc 0.8299999833106995
2023-07-01 00:36:04,840 - INFO - Distilling data from client: Client37
2023-07-01 00:36:04,840 - INFO - train loss: 0.010338387675556162
2023-07-01 00:36:04,841 - INFO - train acc: 0.9047619700431824
2023-07-01 00:36:04,846 - INFO - report:               precision    recall  f1-score   support

           3       0.92      0.83      0.87       138
           7       0.27      0.50      0.35         8
           8       0.77      0.85      0.81        54

    accuracy                           0.82       200
   macro avg       0.65      0.73      0.68       200
weighted avg       0.85      0.82      0.84       200

2023-07-01 00:36:04,846 - INFO - test loss 0.01622791276652135
2023-07-01 00:36:04,847 - INFO - test acc 0.824999988079071
2023-07-01 00:36:04,936 - INFO - Distilling data from client: Client37
2023-07-01 00:36:04,936 - INFO - train loss: 0.008700524173049288
2023-07-01 00:36:04,937 - INFO - train acc: 0.9365079998970032
2023-07-01 00:36:04,942 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.88      0.89       138
           7       0.33      0.38      0.35         8
           8       0.77      0.80      0.78        54

    accuracy                           0.83       200
   macro avg       0.67      0.68      0.67       200
weighted avg       0.84      0.83      0.84       200

2023-07-01 00:36:04,942 - INFO - test loss 0.016165355364186423
2023-07-01 00:36:04,942 - INFO - test acc 0.8349999785423279
2023-07-01 00:36:05,032 - INFO - Distilling data from client: Client37
2023-07-01 00:36:05,032 - INFO - train loss: 0.006906668935893518
2023-07-01 00:36:05,032 - INFO - train acc: 0.9841270446777344
2023-07-01 00:36:05,038 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.87      0.88       138
           7       0.30      0.38      0.33         8
           8       0.79      0.81      0.80        54

    accuracy                           0.83       200
   macro avg       0.66      0.69      0.67       200
weighted avg       0.84      0.83      0.84       200

2023-07-01 00:36:05,038 - INFO - test loss 0.015438944011695993
2023-07-01 00:36:05,038 - INFO - test acc 0.8349999785423279
2023-07-01 00:36:05,128 - INFO - Distilling data from client: Client37
2023-07-01 00:36:05,128 - INFO - train loss: 0.006861779140882467
2023-07-01 00:36:05,129 - INFO - train acc: 0.9523810148239136
2023-07-01 00:36:05,139 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.91      0.91       138
           7       0.40      0.25      0.31         8
           8       0.78      0.83      0.80        54

    accuracy                           0.86       200
   macro avg       0.70      0.66      0.67       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:36:05,140 - INFO - test loss 0.014483137260626666
2023-07-01 00:36:05,140 - INFO - test acc 0.85999995470047
2023-07-01 00:36:05,232 - INFO - Distilling data from client: Client37
2023-07-01 00:36:05,232 - INFO - train loss: 0.008749701434928587
2023-07-01 00:36:05,232 - INFO - train acc: 0.888888955116272
2023-07-01 00:36:05,238 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.88      0.89       138
           7       0.44      0.50      0.47         8
           8       0.77      0.81      0.79        54

    accuracy                           0.84       200
   macro avg       0.71      0.73      0.72       200
weighted avg       0.85      0.84      0.85       200

2023-07-01 00:36:05,238 - INFO - test loss 0.015099554622716685
2023-07-01 00:36:05,238 - INFO - test acc 0.8449999690055847
2023-07-01 00:36:05,327 - INFO - Distilling data from client: Client37
2023-07-01 00:36:05,328 - INFO - train loss: 0.0075501369829354455
2023-07-01 00:36:05,328 - INFO - train acc: 0.9523810148239136
2023-07-01 00:36:05,334 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.90      0.90       138
           7       0.14      0.12      0.13         8
           8       0.80      0.83      0.82        54

    accuracy                           0.85       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:36:05,334 - INFO - test loss 0.015280548727997226
2023-07-01 00:36:05,334 - INFO - test acc 0.8499999642372131
2023-07-01 00:36:05,424 - INFO - Distilling data from client: Client37
2023-07-01 00:36:05,424 - INFO - train loss: 0.007057448328897068
2023-07-01 00:36:05,424 - INFO - train acc: 0.968254029750824
2023-07-01 00:36:05,430 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.88      0.89       138
           7       0.17      0.12      0.14         8
           8       0.78      0.83      0.80        54

    accuracy                           0.84       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:36:05,430 - INFO - test loss 0.015342996608576076
2023-07-01 00:36:05,430 - INFO - test acc 0.8399999737739563
2023-07-01 00:36:05,520 - INFO - Distilling data from client: Client37
2023-07-01 00:36:05,520 - INFO - train loss: 0.008061048384421517
2023-07-01 00:36:05,521 - INFO - train acc: 0.968254029750824
2023-07-01 00:36:05,526 - INFO - report:               precision    recall  f1-score   support

           3       0.89      0.92      0.91       138
           7       0.50      0.38      0.43         8
           8       0.81      0.78      0.79        54

    accuracy                           0.86       200
   macro avg       0.73      0.69      0.71       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:36:05,526 - INFO - test loss 0.014690402759187492
2023-07-01 00:36:05,526 - INFO - test acc 0.85999995470047
2023-07-01 00:36:05,616 - INFO - Distilling data from client: Client37
2023-07-01 00:36:05,616 - INFO - train loss: 0.010185787221258317
2023-07-01 00:36:05,616 - INFO - train acc: 0.8730159401893616
2023-07-01 00:36:05,621 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.87      0.88       138
           7       0.38      0.38      0.38         8
           8       0.76      0.81      0.79        54

    accuracy                           0.83       200
   macro avg       0.68      0.69      0.68       200
weighted avg       0.84      0.83      0.84       200

2023-07-01 00:36:05,621 - INFO - test loss 0.01483960396742307
2023-07-01 00:36:05,622 - INFO - test acc 0.8349999785423279
2023-07-01 00:36:05,711 - INFO - Distilling data from client: Client37
2023-07-01 00:36:05,711 - INFO - train loss: 0.006988194567108843
2023-07-01 00:36:05,711 - INFO - train acc: 0.968254029750824
2023-07-01 00:36:05,717 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.87      0.88       138
           7       0.36      0.50      0.42         8
           8       0.78      0.80      0.79        54

    accuracy                           0.83       200
   macro avg       0.68      0.72      0.70       200
weighted avg       0.84      0.83      0.84       200

2023-07-01 00:36:05,717 - INFO - test loss 0.01561783454194191
2023-07-01 00:36:05,717 - INFO - test acc 0.8349999785423279
2023-07-01 00:36:05,806 - INFO - Distilling data from client: Client37
2023-07-01 00:36:05,806 - INFO - train loss: 0.007823920216117494
2023-07-01 00:36:05,806 - INFO - train acc: 0.9523810148239136
2023-07-01 00:36:05,812 - INFO - report:               precision    recall  f1-score   support

           3       0.89      0.89      0.89       138
           7       0.44      0.50      0.47         8
           8       0.77      0.76      0.77        54

    accuracy                           0.84       200
   macro avg       0.70      0.72      0.71       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:36:05,812 - INFO - test loss 0.01572681009662139
2023-07-01 00:36:05,812 - INFO - test acc 0.8399999737739563
2023-07-01 00:36:05,901 - INFO - Distilling data from client: Client37
2023-07-01 00:36:05,902 - INFO - train loss: 0.007401917491848475
2023-07-01 00:36:05,902 - INFO - train acc: 0.9523810148239136
2023-07-01 00:36:05,907 - INFO - report:               precision    recall  f1-score   support

           3       0.89      0.86      0.88       138
           7       0.50      0.50      0.50         8
           8       0.72      0.78      0.75        54

    accuracy                           0.82       200
   macro avg       0.70      0.71      0.71       200
weighted avg       0.83      0.82      0.83       200

2023-07-01 00:36:05,907 - INFO - test loss 0.015490950033652333
2023-07-01 00:36:05,908 - INFO - test acc 0.824999988079071
2023-07-01 00:36:05,997 - INFO - Distilling data from client: Client37
2023-07-01 00:36:05,997 - INFO - train loss: 0.008277839758499205
2023-07-01 00:36:05,997 - INFO - train acc: 0.9523810148239136
2023-07-01 00:36:06,003 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.88      0.89       138
           7       0.38      0.38      0.38         8
           8       0.75      0.80      0.77        54

    accuracy                           0.84       200
   macro avg       0.68      0.69      0.68       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:36:06,003 - INFO - test loss 0.015437332781040371
2023-07-01 00:36:06,003 - INFO - test acc 0.8399999737739563
2023-07-01 00:36:06,093 - INFO - Distilling data from client: Client37
2023-07-01 00:36:06,093 - INFO - train loss: 0.007015730634846263
2023-07-01 00:36:06,093 - INFO - train acc: 1.0
2023-07-01 00:36:06,098 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.91      0.91       138
           7       0.38      0.38      0.38         8
           8       0.80      0.80      0.80        54

    accuracy                           0.85       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:36:06,099 - INFO - test loss 0.014347665670372744
2023-07-01 00:36:06,099 - INFO - test acc 0.8549999594688416
2023-07-01 00:36:06,188 - INFO - Distilling data from client: Client37
2023-07-01 00:36:06,188 - INFO - train loss: 0.0060292739430053135
2023-07-01 00:36:06,188 - INFO - train acc: 0.9841270446777344
2023-07-01 00:36:06,193 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.88      0.89       138
           7       0.31      0.50      0.38         8
           8       0.81      0.81      0.81        54

    accuracy                           0.84       200
   macro avg       0.68      0.73      0.70       200
weighted avg       0.86      0.84      0.85       200

2023-07-01 00:36:06,194 - INFO - test loss 0.015994472215338573
2023-07-01 00:36:06,194 - INFO - test acc 0.8449999690055847
2023-07-01 00:36:06,284 - INFO - Distilling data from client: Client37
2023-07-01 00:36:06,284 - INFO - train loss: 0.006296264910218223
2023-07-01 00:36:06,285 - INFO - train acc: 0.968254029750824
2023-07-01 00:36:06,295 - INFO - report:               precision    recall  f1-score   support

           3       0.90      0.92      0.91       138
           7       0.43      0.38      0.40         8
           8       0.85      0.81      0.83        54

    accuracy                           0.87       200
   macro avg       0.73      0.70      0.71       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:36:06,296 - INFO - test loss 0.013909255004760483
2023-07-01 00:36:06,296 - INFO - test acc 0.8700000047683716
2023-07-01 00:36:06,385 - INFO - Distilling data from client: Client37
2023-07-01 00:36:06,386 - INFO - train loss: 0.008269448190559836
2023-07-01 00:36:06,386 - INFO - train acc: 0.9365079998970032
2023-07-01 00:36:06,391 - INFO - report:               precision    recall  f1-score   support

           3       0.91      0.84      0.87       138
           7       0.31      0.50      0.38         8
           8       0.75      0.81      0.78        54

    accuracy                           0.82       200
   macro avg       0.65      0.72      0.68       200
weighted avg       0.84      0.82      0.83       200

2023-07-01 00:36:06,391 - INFO - test loss 0.016410002935522184
2023-07-01 00:36:06,392 - INFO - test acc 0.8199999928474426
2023-07-01 00:36:06,481 - INFO - Distilling data from client: Client37
2023-07-01 00:36:06,481 - INFO - train loss: 0.0062982642802939795
2023-07-01 00:36:06,481 - INFO - train acc: 0.9523810148239136
2023-07-01 00:36:06,486 - INFO - report:               precision    recall  f1-score   support

           3       0.89      0.91      0.90       138
           7       0.44      0.50      0.47         8
           8       0.84      0.78      0.81        54

    accuracy                           0.86       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:36:06,487 - INFO - test loss 0.015183770849459027
2023-07-01 00:36:06,487 - INFO - test acc 0.85999995470047
2023-07-01 00:36:06,576 - INFO - Distilling data from client: Client37
2023-07-01 00:36:06,576 - INFO - train loss: 0.006790915990031189
2023-07-01 00:36:06,576 - INFO - train acc: 0.9523810148239136
2023-07-01 00:36:06,582 - INFO - report:               precision    recall  f1-score   support

           3       0.88      0.88      0.88       138
           7       0.33      0.50      0.40         8
           8       0.82      0.76      0.79        54

    accuracy                           0.83       200
   macro avg       0.68      0.71      0.69       200
weighted avg       0.84      0.83      0.84       200

2023-07-01 00:36:06,582 - INFO - test loss 0.015581084686985141
2023-07-01 00:36:06,582 - INFO - test acc 0.8349999785423279
2023-07-01 00:36:06,585 - WARNING - Finished tracing + transforming jit(gather) in 0.0002512931823730469 sec
2023-07-01 00:36:06,585 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[63,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:36:06,586 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.001184225082397461 sec
2023-07-01 00:36:06,586 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:36:06,596 - WARNING - Finished XLA compilation of jit(gather) in 0.009820222854614258 sec
2023-07-01 00:36:06,607 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:06,616 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:06,624 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:06,633 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:06,641 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:06,650 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:06,659 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:06,668 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:06,677 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:07,050 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client37//synthetic.png
2023-07-01 00:36:07,063 - INFO - c: 0.0 and total_data_in_this_class: 268
2023-07-01 00:36:07,063 - INFO - c: 2.0 and total_data_in_this_class: 269
2023-07-01 00:36:07,063 - INFO - c: 8.0 and total_data_in_this_class: 262
2023-07-01 00:36:07,063 - INFO - c: 0.0 and total_data_in_this_class: 65
2023-07-01 00:36:07,063 - INFO - c: 2.0 and total_data_in_this_class: 64
2023-07-01 00:36:07,063 - INFO - c: 8.0 and total_data_in_this_class: 71
2023-07-01 00:36:07,134 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04793882369995117 sec
2023-07-01 00:36:07,180 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.045243024826049805 sec
2023-07-01 00:36:07,185 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.1003274917602539 sec
2023-07-01 00:36:07,187 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:36:07,220 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03244352340698242 sec
2023-07-01 00:36:07,220 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:36:07,344 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12342238426208496 sec
2023-07-01 00:36:07,365 - INFO - initial test loss: 0.02453220731190638
2023-07-01 00:36:07,366 - INFO - initial test acc: 0.6349999904632568
2023-07-01 00:36:07,373 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.005487680435180664 sec
2023-07-01 00:36:07,483 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11574292182922363 sec
2023-07-01 00:36:07,487 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:36:07,551 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06365537643432617 sec
2023-07-01 00:36:07,551 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:36:07,886 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3346397876739502 sec
2023-07-01 00:36:09,149 - INFO - Distilling data from client: Client38
2023-07-01 00:36:09,150 - INFO - train loss: 0.0027901898974845977
2023-07-01 00:36:09,150 - INFO - train acc: 0.9961904883384705
2023-07-01 00:36:09,211 - INFO - report:               precision    recall  f1-score   support

           0       0.54      0.54      0.54        65
           2       0.69      0.66      0.67        64
           8       0.72      0.75      0.73        71

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:36:09,211 - INFO - test loss 0.0252123962417408
2023-07-01 00:36:09,211 - INFO - test acc 0.6499999761581421
2023-07-01 00:36:10,468 - INFO - Distilling data from client: Client38
2023-07-01 00:36:10,468 - INFO - train loss: 0.0017284484721172247
2023-07-01 00:36:10,469 - INFO - train acc: 0.9980952143669128
2023-07-01 00:36:10,496 - INFO - report:               precision    recall  f1-score   support

           0       0.52      0.52      0.52        65
           2       0.66      0.66      0.66        64
           8       0.72      0.72      0.72        71

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:36:10,496 - INFO - test loss 0.025496345817615534
2023-07-01 00:36:10,497 - INFO - test acc 0.6349999904632568
2023-07-01 00:36:11,736 - INFO - Distilling data from client: Client38
2023-07-01 00:36:11,737 - INFO - train loss: 0.0013413708829333626
2023-07-01 00:36:11,737 - INFO - train acc: 0.9980952143669128
2023-07-01 00:36:11,761 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.55      0.55        65
           2       0.65      0.66      0.65        64
           8       0.74      0.73      0.74        71

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:36:11,762 - INFO - test loss 0.02481516449977356
2023-07-01 00:36:11,762 - INFO - test acc 0.6499999761581421
2023-07-01 00:36:13,014 - INFO - Distilling data from client: Client38
2023-07-01 00:36:13,014 - INFO - train loss: 0.0011721051721723312
2023-07-01 00:36:13,014 - INFO - train acc: 1.0
2023-07-01 00:36:13,083 - INFO - report:               precision    recall  f1-score   support

           0       0.56      0.54      0.55        65
           2       0.67      0.70      0.69        64
           8       0.73      0.72      0.72        71

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.66      0.65       200

2023-07-01 00:36:13,083 - INFO - test loss 0.02571164630000564
2023-07-01 00:36:13,083 - INFO - test acc 0.6549999713897705
2023-07-01 00:36:14,340 - INFO - Distilling data from client: Client38
2023-07-01 00:36:14,340 - INFO - train loss: 0.0008477802271976739
2023-07-01 00:36:14,340 - INFO - train acc: 1.0
2023-07-01 00:36:14,401 - INFO - report:               precision    recall  f1-score   support

           0       0.58      0.57      0.57        65
           2       0.68      0.67      0.68        64
           8       0.74      0.76      0.75        71

    accuracy                           0.67       200
   macro avg       0.67      0.67      0.67       200
weighted avg       0.67      0.67      0.67       200

2023-07-01 00:36:14,401 - INFO - test loss 0.025122953501228028
2023-07-01 00:36:14,401 - INFO - test acc 0.6699999570846558
2023-07-01 00:36:15,653 - INFO - Distilling data from client: Client38
2023-07-01 00:36:15,653 - INFO - train loss: 0.0009557494322860627
2023-07-01 00:36:15,653 - INFO - train acc: 1.0
2023-07-01 00:36:15,679 - INFO - report:               precision    recall  f1-score   support

           0       0.56      0.54      0.55        65
           2       0.63      0.67      0.65        64
           8       0.75      0.73      0.74        71

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:36:15,679 - INFO - test loss 0.025866100429149996
2023-07-01 00:36:15,679 - INFO - test acc 0.6499999761581421
2023-07-01 00:36:16,942 - INFO - Distilling data from client: Client38
2023-07-01 00:36:16,942 - INFO - train loss: 0.0008620045306618411
2023-07-01 00:36:16,943 - INFO - train acc: 1.0
2023-07-01 00:36:16,967 - INFO - report:               precision    recall  f1-score   support

           0       0.51      0.48      0.49        65
           2       0.64      0.69      0.66        64
           8       0.69      0.68      0.68        71

    accuracy                           0.61       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.61      0.61      0.61       200

2023-07-01 00:36:16,967 - INFO - test loss 0.02625784793355104
2023-07-01 00:36:16,967 - INFO - test acc 0.6150000095367432
2023-07-01 00:36:18,234 - INFO - Distilling data from client: Client38
2023-07-01 00:36:18,234 - INFO - train loss: 0.0007427760314699223
2023-07-01 00:36:18,234 - INFO - train acc: 1.0
2023-07-01 00:36:18,259 - INFO - report:               precision    recall  f1-score   support

           0       0.52      0.51      0.52        65
           2       0.62      0.66      0.64        64
           8       0.71      0.69      0.70        71

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.62      0.62      0.62       200

2023-07-01 00:36:18,260 - INFO - test loss 0.025648852125165874
2023-07-01 00:36:18,260 - INFO - test acc 0.6200000047683716
2023-07-01 00:36:19,520 - INFO - Distilling data from client: Client38
2023-07-01 00:36:19,520 - INFO - train loss: 0.0006799277419068092
2023-07-01 00:36:19,520 - INFO - train acc: 1.0
2023-07-01 00:36:19,545 - INFO - report:               precision    recall  f1-score   support

           0       0.58      0.52      0.55        65
           2       0.64      0.69      0.66        64
           8       0.74      0.75      0.74        71

    accuracy                           0.66       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.66      0.65       200

2023-07-01 00:36:19,545 - INFO - test loss 0.025380021880498847
2023-07-01 00:36:19,545 - INFO - test acc 0.6549999713897705
2023-07-01 00:36:20,812 - INFO - Distilling data from client: Client38
2023-07-01 00:36:20,813 - INFO - train loss: 0.0007608057328383934
2023-07-01 00:36:20,813 - INFO - train acc: 1.0
2023-07-01 00:36:20,839 - INFO - report:               precision    recall  f1-score   support

           0       0.51      0.46      0.48        65
           2       0.63      0.67      0.65        64
           8       0.71      0.73      0.72        71

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.62      0.62      0.62       200

2023-07-01 00:36:20,839 - INFO - test loss 0.025844020598365828
2023-07-01 00:36:20,839 - INFO - test acc 0.625
2023-07-01 00:36:22,109 - INFO - Distilling data from client: Client38
2023-07-01 00:36:22,109 - INFO - train loss: 0.0005484954962225927
2023-07-01 00:36:22,109 - INFO - train acc: 1.0
2023-07-01 00:36:22,135 - INFO - report:               precision    recall  f1-score   support

           0       0.54      0.54      0.54        65
           2       0.64      0.69      0.66        64
           8       0.74      0.69      0.72        71

    accuracy                           0.64       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:36:22,136 - INFO - test loss 0.025515839416950074
2023-07-01 00:36:22,136 - INFO - test acc 0.6399999856948853
2023-07-01 00:36:23,395 - INFO - Distilling data from client: Client38
2023-07-01 00:36:23,395 - INFO - train loss: 0.000597699943067033
2023-07-01 00:36:23,395 - INFO - train acc: 1.0
2023-07-01 00:36:23,419 - INFO - report:               precision    recall  f1-score   support

           0       0.52      0.51      0.51        65
           2       0.64      0.67      0.66        64
           8       0.72      0.70      0.71        71

    accuracy                           0.63       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.63      0.63       200

2023-07-01 00:36:23,419 - INFO - test loss 0.02537658876550059
2023-07-01 00:36:23,419 - INFO - test acc 0.6299999952316284
2023-07-01 00:36:24,668 - INFO - Distilling data from client: Client38
2023-07-01 00:36:24,669 - INFO - train loss: 0.0005906127119110909
2023-07-01 00:36:24,669 - INFO - train acc: 1.0
2023-07-01 00:36:24,692 - INFO - report:               precision    recall  f1-score   support

           0       0.56      0.54      0.55        65
           2       0.66      0.64      0.65        64
           8       0.72      0.76      0.74        71

    accuracy                           0.65       200
   macro avg       0.65      0.65      0.65       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:36:24,692 - INFO - test loss 0.026488517667655906
2023-07-01 00:36:24,692 - INFO - test acc 0.6499999761581421
2023-07-01 00:36:25,942 - INFO - Distilling data from client: Client38
2023-07-01 00:36:25,943 - INFO - train loss: 0.000499568604413889
2023-07-01 00:36:25,943 - INFO - train acc: 1.0
2023-07-01 00:36:25,969 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.51      0.53        65
           2       0.65      0.67      0.66        64
           8       0.70      0.73      0.72        71

    accuracy                           0.64       200
   macro avg       0.63      0.64      0.64       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:36:25,969 - INFO - test loss 0.025902440334994458
2023-07-01 00:36:25,969 - INFO - test acc 0.6399999856948853
2023-07-01 00:36:27,233 - INFO - Distilling data from client: Client38
2023-07-01 00:36:27,233 - INFO - train loss: 0.0005099509608632359
2023-07-01 00:36:27,233 - INFO - train acc: 1.0
2023-07-01 00:36:27,256 - INFO - report:               precision    recall  f1-score   support

           0       0.52      0.49      0.51        65
           2       0.64      0.66      0.65        64
           8       0.73      0.75      0.74        71

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.64      0.63       200

2023-07-01 00:36:27,256 - INFO - test loss 0.025864675384735487
2023-07-01 00:36:27,257 - INFO - test acc 0.6349999904632568
2023-07-01 00:36:28,514 - INFO - Distilling data from client: Client38
2023-07-01 00:36:28,514 - INFO - train loss: 0.0004956311484994997
2023-07-01 00:36:28,514 - INFO - train acc: 1.0
2023-07-01 00:36:28,540 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.55      0.55        65
           2       0.64      0.70      0.67        64
           8       0.75      0.68      0.71        71

    accuracy                           0.65       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:36:28,540 - INFO - test loss 0.025994325885176868
2023-07-01 00:36:28,540 - INFO - test acc 0.6449999809265137
2023-07-01 00:36:29,809 - INFO - Distilling data from client: Client38
2023-07-01 00:36:29,809 - INFO - train loss: 0.0004962326702331423
2023-07-01 00:36:29,809 - INFO - train acc: 1.0
2023-07-01 00:36:29,833 - INFO - report:               precision    recall  f1-score   support

           0       0.56      0.54      0.55        65
           2       0.65      0.69      0.67        64
           8       0.76      0.75      0.75        71

    accuracy                           0.66       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.66      0.66      0.66       200

2023-07-01 00:36:29,833 - INFO - test loss 0.02600008533034915
2023-07-01 00:36:29,834 - INFO - test acc 0.6599999666213989
2023-07-01 00:36:31,098 - INFO - Distilling data from client: Client38
2023-07-01 00:36:31,099 - INFO - train loss: 0.0004608182878898534
2023-07-01 00:36:31,099 - INFO - train acc: 1.0
2023-07-01 00:36:31,122 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.55      0.55        65
           2       0.64      0.67      0.66        64
           8       0.74      0.70      0.72        71

    accuracy                           0.65       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:36:31,122 - INFO - test loss 0.02569945469138
2023-07-01 00:36:31,122 - INFO - test acc 0.6449999809265137
2023-07-01 00:36:32,380 - INFO - Distilling data from client: Client38
2023-07-01 00:36:32,380 - INFO - train loss: 0.0004311773498600665
2023-07-01 00:36:32,380 - INFO - train acc: 1.0
2023-07-01 00:36:32,403 - INFO - report:               precision    recall  f1-score   support

           0       0.51      0.54      0.53        65
           2       0.62      0.67      0.65        64
           8       0.73      0.65      0.69        71

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.63      0.62      0.62       200

2023-07-01 00:36:32,404 - INFO - test loss 0.02632573671385566
2023-07-01 00:36:32,404 - INFO - test acc 0.6200000047683716
2023-07-01 00:36:33,669 - INFO - Distilling data from client: Client38
2023-07-01 00:36:33,669 - INFO - train loss: 0.00046389540971826043
2023-07-01 00:36:33,669 - INFO - train acc: 1.0
2023-07-01 00:36:33,693 - INFO - report:               precision    recall  f1-score   support

           0       0.50      0.52      0.51        65
           2       0.63      0.64      0.64        64
           8       0.72      0.68      0.70        71

    accuracy                           0.61       200
   macro avg       0.62      0.61      0.61       200
weighted avg       0.62      0.61      0.62       200

2023-07-01 00:36:33,693 - INFO - test loss 0.025690610591557608
2023-07-01 00:36:33,693 - INFO - test acc 0.6150000095367432
2023-07-01 00:36:34,946 - INFO - Distilling data from client: Client38
2023-07-01 00:36:34,946 - INFO - train loss: 0.00041077252561336425
2023-07-01 00:36:34,946 - INFO - train acc: 1.0
2023-07-01 00:36:34,970 - INFO - report:               precision    recall  f1-score   support

           0       0.52      0.51      0.52        65
           2       0.63      0.66      0.64        64
           8       0.71      0.70      0.71        71

    accuracy                           0.62       200
   macro avg       0.62      0.62      0.62       200
weighted avg       0.62      0.62      0.62       200

2023-07-01 00:36:34,970 - INFO - test loss 0.026146942142244698
2023-07-01 00:36:34,970 - INFO - test acc 0.625
2023-07-01 00:36:36,222 - INFO - Distilling data from client: Client38
2023-07-01 00:36:36,222 - INFO - train loss: 0.0003976116354619967
2023-07-01 00:36:36,222 - INFO - train acc: 1.0
2023-07-01 00:36:36,247 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.55      0.55        65
           2       0.64      0.67      0.66        64
           8       0.72      0.68      0.70        71

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:36:36,247 - INFO - test loss 0.02548923090972719
2023-07-01 00:36:36,247 - INFO - test acc 0.6349999904632568
2023-07-01 00:36:37,506 - INFO - Distilling data from client: Client38
2023-07-01 00:36:37,506 - INFO - train loss: 0.00043634337226346095
2023-07-01 00:36:37,506 - INFO - train acc: 1.0
2023-07-01 00:36:37,530 - INFO - report:               precision    recall  f1-score   support

           0       0.55      0.52      0.54        65
           2       0.63      0.67      0.65        64
           8       0.71      0.70      0.71        71

    accuracy                           0.64       200
   macro avg       0.63      0.63      0.63       200
weighted avg       0.63      0.64      0.63       200

2023-07-01 00:36:37,530 - INFO - test loss 0.025827576920722534
2023-07-01 00:36:37,530 - INFO - test acc 0.6349999904632568
2023-07-01 00:36:38,794 - INFO - Distilling data from client: Client38
2023-07-01 00:36:38,794 - INFO - train loss: 0.0003841878100207662
2023-07-01 00:36:38,794 - INFO - train acc: 1.0
2023-07-01 00:36:38,822 - INFO - report:               precision    recall  f1-score   support

           0       0.54      0.54      0.54        65
           2       0.65      0.66      0.65        64
           8       0.73      0.72      0.72        71

    accuracy                           0.64       200
   macro avg       0.64      0.64      0.64       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:36:38,822 - INFO - test loss 0.026481441889143264
2023-07-01 00:36:38,822 - INFO - test acc 0.6399999856948853
2023-07-01 00:36:40,088 - INFO - Distilling data from client: Client38
2023-07-01 00:36:40,088 - INFO - train loss: 0.0003868134579074644
2023-07-01 00:36:40,088 - INFO - train acc: 1.0
2023-07-01 00:36:40,111 - INFO - report:               precision    recall  f1-score   support

           0       0.51      0.54      0.53        65
           2       0.62      0.64      0.63        64
           8       0.71      0.66      0.69        71

    accuracy                           0.61       200
   macro avg       0.62      0.61      0.61       200
weighted avg       0.62      0.61      0.62       200

2023-07-01 00:36:40,111 - INFO - test loss 0.02636437860118083
2023-07-01 00:36:40,111 - INFO - test acc 0.6150000095367432
2023-07-01 00:36:40,123 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:40,131 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:40,140 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:40,148 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:40,157 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:40,165 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:40,174 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:40,183 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:40,193 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:36:40,592 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client38//synthetic.png
2023-07-01 00:36:40,604 - INFO - c: 5.0 and total_data_in_this_class: 529
2023-07-01 00:36:40,604 - INFO - c: 6.0 and total_data_in_this_class: 270
2023-07-01 00:36:40,605 - INFO - c: 5.0 and total_data_in_this_class: 137
2023-07-01 00:36:40,605 - INFO - c: 6.0 and total_data_in_this_class: 63
2023-07-01 00:36:40,675 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04815530776977539 sec
2023-07-01 00:36:40,721 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04499554634094238 sec
2023-07-01 00:36:40,725 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.10015320777893066 sec
2023-07-01 00:36:40,727 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:36:40,760 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03261065483093262 sec
2023-07-01 00:36:40,760 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:36:40,885 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12427425384521484 sec
2023-07-01 00:36:40,902 - INFO - initial test loss: 0.018981922823387037
2023-07-01 00:36:40,902 - INFO - initial test acc: 0.7249999642372131
2023-07-01 00:36:40,911 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.005830049514770508 sec
2023-07-01 00:36:41,021 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1169729232788086 sec
2023-07-01 00:36:41,025 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[360,10]), ShapedArray(float32[360,10]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10]), ShapedArray(float32[360,3,32,32]), ShapedArray(float32[360,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:36:41,090 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06454968452453613 sec
2023-07-01 00:36:41,090 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:36:41,397 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3065612316131592 sec
2023-07-01 00:36:42,175 - INFO - Distilling data from client: Client39
2023-07-01 00:36:42,176 - INFO - train loss: 0.004766389304327115
2023-07-01 00:36:42,176 - INFO - train acc: 0.9777777791023254
2023-07-01 00:36:42,216 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.85      0.85       137
           6       0.67      0.67      0.67        63

    accuracy                           0.79       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:36:42,216 - INFO - test loss 0.01625122943668191
2023-07-01 00:36:42,216 - INFO - test acc 0.7899999618530273
2023-07-01 00:36:42,974 - INFO - Distilling data from client: Client39
2023-07-01 00:36:42,974 - INFO - train loss: 0.0034641115348087786
2023-07-01 00:36:42,974 - INFO - train acc: 0.9805555939674377
2023-07-01 00:36:42,991 - INFO - report:               precision    recall  f1-score   support

           5       0.86      0.82      0.84       137
           6       0.65      0.70      0.67        63

    accuracy                           0.79       200
   macro avg       0.75      0.76      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:36:42,992 - INFO - test loss 0.01609420917410107
2023-07-01 00:36:42,992 - INFO - test acc 0.7849999666213989
2023-07-01 00:36:43,756 - INFO - Distilling data from client: Client39
2023-07-01 00:36:43,756 - INFO - train loss: 0.0028870629831424465
2023-07-01 00:36:43,756 - INFO - train acc: 0.9944444894790649
2023-07-01 00:36:43,802 - INFO - report:               precision    recall  f1-score   support

           5       0.88      0.83      0.86       137
           6       0.68      0.76      0.72        63

    accuracy                           0.81       200
   macro avg       0.78      0.80      0.79       200
weighted avg       0.82      0.81      0.81       200

2023-07-01 00:36:43,802 - INFO - test loss 0.016688787215960305
2023-07-01 00:36:43,802 - INFO - test acc 0.8100000023841858
2023-07-01 00:36:44,567 - INFO - Distilling data from client: Client39
2023-07-01 00:36:44,567 - INFO - train loss: 0.002974870242808004
2023-07-01 00:36:44,567 - INFO - train acc: 0.9916666746139526
2023-07-01 00:36:44,585 - INFO - report:               precision    recall  f1-score   support

           5       0.82      0.82      0.82       137
           6       0.61      0.62      0.61        63

    accuracy                           0.76       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:36:44,585 - INFO - test loss 0.017059458577087704
2023-07-01 00:36:44,585 - INFO - test acc 0.7549999952316284
2023-07-01 00:36:45,342 - INFO - Distilling data from client: Client39
2023-07-01 00:36:45,343 - INFO - train loss: 0.0028721772390752744
2023-07-01 00:36:45,343 - INFO - train acc: 0.9888889193534851
2023-07-01 00:36:45,360 - INFO - report:               precision    recall  f1-score   support

           5       0.86      0.84      0.85       137
           6       0.67      0.71      0.69        63

    accuracy                           0.80       200
   macro avg       0.77      0.78      0.77       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:36:45,360 - INFO - test loss 0.016250686995338784
2023-07-01 00:36:45,360 - INFO - test acc 0.7999999523162842
2023-07-01 00:36:46,127 - INFO - Distilling data from client: Client39
2023-07-01 00:36:46,127 - INFO - train loss: 0.0023900216431265364
2023-07-01 00:36:46,127 - INFO - train acc: 0.9972222447395325
2023-07-01 00:36:46,145 - INFO - report:               precision    recall  f1-score   support

           5       0.87      0.80      0.83       137
           6       0.63      0.73      0.68        63

    accuracy                           0.78       200
   macro avg       0.75      0.77      0.75       200
weighted avg       0.79      0.78      0.78       200

2023-07-01 00:36:46,145 - INFO - test loss 0.01724616733328877
2023-07-01 00:36:46,145 - INFO - test acc 0.7799999713897705
2023-07-01 00:36:46,914 - INFO - Distilling data from client: Client39
2023-07-01 00:36:46,914 - INFO - train loss: 0.00220236566192647
2023-07-01 00:36:46,914 - INFO - train acc: 0.9972222447395325
2023-07-01 00:36:46,930 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.84      0.85       137
           6       0.66      0.68      0.67        63

    accuracy                           0.79       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:36:46,930 - INFO - test loss 0.018064409286363332
2023-07-01 00:36:46,931 - INFO - test acc 0.7899999618530273
2023-07-01 00:36:47,701 - INFO - Distilling data from client: Client39
2023-07-01 00:36:47,701 - INFO - train loss: 0.0020270069592604615
2023-07-01 00:36:47,701 - INFO - train acc: 0.9944444894790649
2023-07-01 00:36:47,721 - INFO - report:               precision    recall  f1-score   support

           5       0.84      0.82      0.83       137
           6       0.63      0.67      0.65        63

    accuracy                           0.77       200
   macro avg       0.73      0.74      0.74       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:36:47,721 - INFO - test loss 0.017023324836720758
2023-07-01 00:36:47,721 - INFO - test acc 0.7699999809265137
2023-07-01 00:36:48,485 - INFO - Distilling data from client: Client39
2023-07-01 00:36:48,485 - INFO - train loss: 0.001977609744687404
2023-07-01 00:36:48,485 - INFO - train acc: 1.0
2023-07-01 00:36:48,502 - INFO - report:               precision    recall  f1-score   support

           5       0.87      0.85      0.86       137
           6       0.68      0.71      0.70        63

    accuracy                           0.81       200
   macro avg       0.77      0.78      0.78       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:36:48,502 - INFO - test loss 0.01645306377872155
2023-07-01 00:36:48,502 - INFO - test acc 0.8050000071525574
2023-07-01 00:36:49,263 - INFO - Distilling data from client: Client39
2023-07-01 00:36:49,263 - INFO - train loss: 0.001976852620202749
2023-07-01 00:36:49,263 - INFO - train acc: 1.0
2023-07-01 00:36:49,279 - INFO - report:               precision    recall  f1-score   support

           5       0.84      0.81      0.83       137
           6       0.62      0.67      0.64        63

    accuracy                           0.77       200
   macro avg       0.73      0.74      0.73       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:36:49,280 - INFO - test loss 0.016991138455190367
2023-07-01 00:36:49,280 - INFO - test acc 0.7649999856948853
2023-07-01 00:36:50,045 - INFO - Distilling data from client: Client39
2023-07-01 00:36:50,045 - INFO - train loss: 0.002079218052412348
2023-07-01 00:36:50,045 - INFO - train acc: 1.0
2023-07-01 00:36:50,062 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.82      0.83       137
           6       0.63      0.68      0.66        63

    accuracy                           0.78       200
   macro avg       0.74      0.75      0.74       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:36:50,062 - INFO - test loss 0.016607767643129726
2023-07-01 00:36:50,062 - INFO - test acc 0.7749999761581421
2023-07-01 00:36:50,839 - INFO - Distilling data from client: Client39
2023-07-01 00:36:50,839 - INFO - train loss: 0.0018056031116246347
2023-07-01 00:36:50,839 - INFO - train acc: 1.0
2023-07-01 00:36:50,856 - INFO - report:               precision    recall  f1-score   support

           5       0.84      0.80      0.82       137
           6       0.60      0.67      0.63        63

    accuracy                           0.76       200
   macro avg       0.72      0.73      0.72       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:36:50,856 - INFO - test loss 0.016754412895496545
2023-07-01 00:36:50,856 - INFO - test acc 0.7549999952316284
2023-07-01 00:36:51,634 - INFO - Distilling data from client: Client39
2023-07-01 00:36:51,634 - INFO - train loss: 0.0019051252794910543
2023-07-01 00:36:51,634 - INFO - train acc: 0.9944444894790649
2023-07-01 00:36:51,652 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.85      0.85       137
           6       0.68      0.67      0.67        63

    accuracy                           0.80       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.79      0.80      0.79       200

2023-07-01 00:36:51,652 - INFO - test loss 0.016528635415678977
2023-07-01 00:36:51,652 - INFO - test acc 0.7949999570846558
2023-07-01 00:36:52,421 - INFO - Distilling data from client: Client39
2023-07-01 00:36:52,421 - INFO - train loss: 0.0018599021489441002
2023-07-01 00:36:52,421 - INFO - train acc: 0.9972222447395325
2023-07-01 00:36:52,439 - INFO - report:               precision    recall  f1-score   support

           5       0.86      0.82      0.84       137
           6       0.65      0.70      0.67        63

    accuracy                           0.79       200
   macro avg       0.75      0.76      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:36:52,439 - INFO - test loss 0.017298513596891524
2023-07-01 00:36:52,439 - INFO - test acc 0.7849999666213989
2023-07-01 00:36:53,209 - INFO - Distilling data from client: Client39
2023-07-01 00:36:53,209 - INFO - train loss: 0.002024332603576203
2023-07-01 00:36:53,209 - INFO - train acc: 0.9972222447395325
2023-07-01 00:36:53,226 - INFO - report:               precision    recall  f1-score   support

           5       0.86      0.82      0.84       137
           6       0.64      0.71      0.68        63

    accuracy                           0.79       200
   macro avg       0.75      0.77      0.76       200
weighted avg       0.79      0.79      0.79       200

2023-07-01 00:36:53,226 - INFO - test loss 0.01642756425762837
2023-07-01 00:36:53,226 - INFO - test acc 0.7849999666213989
2023-07-01 00:36:53,995 - INFO - Distilling data from client: Client39
2023-07-01 00:36:53,995 - INFO - train loss: 0.0017711451604028722
2023-07-01 00:36:53,995 - INFO - train acc: 1.0
2023-07-01 00:36:54,011 - INFO - report:               precision    recall  f1-score   support

           5       0.82      0.82      0.82       137
           6       0.62      0.62      0.62        63

    accuracy                           0.76       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:36:54,012 - INFO - test loss 0.016463160843241863
2023-07-01 00:36:54,012 - INFO - test acc 0.7599999904632568
2023-07-01 00:36:54,784 - INFO - Distilling data from client: Client39
2023-07-01 00:36:54,784 - INFO - train loss: 0.0019478585324963122
2023-07-01 00:36:54,784 - INFO - train acc: 1.0
2023-07-01 00:36:54,802 - INFO - report:               precision    recall  f1-score   support

           5       0.88      0.84      0.86       137
           6       0.68      0.75      0.71        63

    accuracy                           0.81       200
   macro avg       0.78      0.79      0.79       200
weighted avg       0.82      0.81      0.81       200

2023-07-01 00:36:54,802 - INFO - test loss 0.016481643503243618
2023-07-01 00:36:54,802 - INFO - test acc 0.8100000023841858
2023-07-01 00:36:55,563 - INFO - Distilling data from client: Client39
2023-07-01 00:36:55,563 - INFO - train loss: 0.0017541568236717625
2023-07-01 00:36:55,563 - INFO - train acc: 1.0
2023-07-01 00:36:55,580 - INFO - report:               precision    recall  f1-score   support

           5       0.83      0.80      0.81       137
           6       0.59      0.65      0.62        63

    accuracy                           0.75       200
   macro avg       0.71      0.72      0.72       200
weighted avg       0.76      0.75      0.75       200

2023-07-01 00:36:55,580 - INFO - test loss 0.01698016074915452
2023-07-01 00:36:55,580 - INFO - test acc 0.75
2023-07-01 00:36:56,346 - INFO - Distilling data from client: Client39
2023-07-01 00:36:56,346 - INFO - train loss: 0.0015254042312491424
2023-07-01 00:36:56,346 - INFO - train acc: 1.0
2023-07-01 00:36:56,364 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.85      0.85       137
           6       0.67      0.68      0.68        63

    accuracy                           0.80       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:36:56,364 - INFO - test loss 0.01599851834269479
2023-07-01 00:36:56,364 - INFO - test acc 0.7949999570846558
2023-07-01 00:36:57,141 - INFO - Distilling data from client: Client39
2023-07-01 00:36:57,142 - INFO - train loss: 0.002028465787860226
2023-07-01 00:36:57,142 - INFO - train acc: 1.0
2023-07-01 00:36:57,158 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.81      0.83       137
           6       0.63      0.70      0.66        63

    accuracy                           0.78       200
   macro avg       0.74      0.75      0.75       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:36:57,158 - INFO - test loss 0.01663830796156776
2023-07-01 00:36:57,158 - INFO - test acc 0.7749999761581421
2023-07-01 00:36:57,906 - INFO - Distilling data from client: Client39
2023-07-01 00:36:57,906 - INFO - train loss: 0.002013919214294115
2023-07-01 00:36:57,906 - INFO - train acc: 0.9944444894790649
2023-07-01 00:36:57,923 - INFO - report:               precision    recall  f1-score   support

           5       0.84      0.85      0.84       137
           6       0.66      0.65      0.66        63

    accuracy                           0.79       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.78      0.79      0.78       200

2023-07-01 00:36:57,923 - INFO - test loss 0.017048212220341842
2023-07-01 00:36:57,923 - INFO - test acc 0.7849999666213989
2023-07-01 00:36:58,700 - INFO - Distilling data from client: Client39
2023-07-01 00:36:58,700 - INFO - train loss: 0.001805641871454863
2023-07-01 00:36:58,700 - INFO - train acc: 0.9972222447395325
2023-07-01 00:36:58,717 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.85      0.85       137
           6       0.68      0.67      0.67        63

    accuracy                           0.80       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.79      0.80      0.79       200

2023-07-01 00:36:58,717 - INFO - test loss 0.017011761030394708
2023-07-01 00:36:58,718 - INFO - test acc 0.7949999570846558
2023-07-01 00:36:59,472 - INFO - Distilling data from client: Client39
2023-07-01 00:36:59,472 - INFO - train loss: 0.001555626623251944
2023-07-01 00:36:59,472 - INFO - train acc: 0.9972222447395325
2023-07-01 00:36:59,489 - INFO - report:               precision    recall  f1-score   support

           5       0.88      0.82      0.85       137
           6       0.67      0.76      0.71        63

    accuracy                           0.81       200
   macro avg       0.77      0.79      0.78       200
weighted avg       0.81      0.81      0.81       200

2023-07-01 00:36:59,489 - INFO - test loss 0.016703615260582366
2023-07-01 00:36:59,489 - INFO - test acc 0.8050000071525574
2023-07-01 00:37:00,249 - INFO - Distilling data from client: Client39
2023-07-01 00:37:00,249 - INFO - train loss: 0.0017419220473315566
2023-07-01 00:37:00,249 - INFO - train acc: 0.9972222447395325
2023-07-01 00:37:00,265 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.82      0.84       137
           6       0.64      0.68      0.66        63

    accuracy                           0.78       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:37:00,266 - INFO - test loss 0.01706039661441339
2023-07-01 00:37:00,266 - INFO - test acc 0.7799999713897705
2023-07-01 00:37:01,036 - INFO - Distilling data from client: Client39
2023-07-01 00:37:01,037 - INFO - train loss: 0.0018330105498222836
2023-07-01 00:37:01,037 - INFO - train acc: 1.0
2023-07-01 00:37:01,054 - INFO - report:               precision    recall  f1-score   support

           5       0.85      0.85      0.85       137
           6       0.68      0.68      0.68        63

    accuracy                           0.80       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.80      0.80      0.80       200

2023-07-01 00:37:01,054 - INFO - test loss 0.01610556440771187
2023-07-01 00:37:01,054 - INFO - test acc 0.7999999523162842
2023-07-01 00:37:01,065 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:01,074 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:01,083 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:01,092 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:01,101 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:01,110 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:01,403 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client39//synthetic.png
2023-07-01 00:37:01,416 - INFO - c: 1.0 and total_data_in_this_class: 261
2023-07-01 00:37:01,417 - INFO - c: 4.0 and total_data_in_this_class: 267
2023-07-01 00:37:01,417 - INFO - c: 5.0 and total_data_in_this_class: 271
2023-07-01 00:37:01,417 - INFO - c: 1.0 and total_data_in_this_class: 72
2023-07-01 00:37:01,417 - INFO - c: 4.0 and total_data_in_this_class: 66
2023-07-01 00:37:01,417 - INFO - c: 5.0 and total_data_in_this_class: 62
2023-07-01 00:37:01,489 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.048392534255981445 sec
2023-07-01 00:37:01,535 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04575037956237793 sec
2023-07-01 00:37:01,540 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.10119032859802246 sec
2023-07-01 00:37:01,542 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:37:01,575 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.0330657958984375 sec
2023-07-01 00:37:01,575 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:37:01,702 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12635135650634766 sec
2023-07-01 00:37:01,729 - INFO - initial test loss: 0.022544425449885733
2023-07-01 00:37:01,729 - INFO - initial test acc: 0.6899999976158142
2023-07-01 00:37:01,737 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.005791664123535156 sec
2023-07-01 00:37:01,850 - WARNING - Finished tracing + transforming update_fn for pjit in 0.1191854476928711 sec
2023-07-01 00:37:01,854 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[522,10]), ShapedArray(float32[522,10]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10]), ShapedArray(float32[522,3,32,32]), ShapedArray(float32[522,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:37:01,918 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06461453437805176 sec
2023-07-01 00:37:01,919 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:37:02,259 - WARNING - Finished XLA compilation of jit(update_fn) in 0.34061169624328613 sec
2023-07-01 00:37:03,509 - INFO - Distilling data from client: Client40
2023-07-01 00:37:03,509 - INFO - train loss: 0.002347437698084456
2023-07-01 00:37:03,509 - INFO - train acc: 0.9961685538291931
2023-07-01 00:37:03,569 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.88      0.88        72
           4       0.73      0.77      0.75        66
           5       0.72      0.68      0.70        62

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:37:03,569 - INFO - test loss 0.019131388174071937
2023-07-01 00:37:03,569 - INFO - test acc 0.7799999713897705
2023-07-01 00:37:04,817 - INFO - Distilling data from client: Client40
2023-07-01 00:37:04,817 - INFO - train loss: 0.0011056871262873015
2023-07-01 00:37:04,817 - INFO - train acc: 1.0
2023-07-01 00:37:04,840 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.81      0.84        72
           4       0.66      0.77      0.71        66
           5       0.70      0.65      0.67        62

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.75       200

2023-07-01 00:37:04,840 - INFO - test loss 0.019685259060754256
2023-07-01 00:37:04,840 - INFO - test acc 0.7450000047683716
2023-07-01 00:37:06,099 - INFO - Distilling data from client: Client40
2023-07-01 00:37:06,099 - INFO - train loss: 0.0007714799843911538
2023-07-01 00:37:06,099 - INFO - train acc: 1.0
2023-07-01 00:37:06,123 - INFO - report:               precision    recall  f1-score   support

           1       0.89      0.81      0.85        72
           4       0.67      0.77      0.72        66
           5       0.69      0.66      0.68        62

    accuracy                           0.75       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-07-01 00:37:06,123 - INFO - test loss 0.019738823668046464
2023-07-01 00:37:06,123 - INFO - test acc 0.75
2023-07-01 00:37:07,378 - INFO - Distilling data from client: Client40
2023-07-01 00:37:07,378 - INFO - train loss: 0.0007877019196711695
2023-07-01 00:37:07,378 - INFO - train acc: 1.0
2023-07-01 00:37:07,401 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.83      0.86        72
           4       0.71      0.80      0.75        66
           5       0.72      0.66      0.69        62

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:37:07,402 - INFO - test loss 0.020338207730154045
2023-07-01 00:37:07,402 - INFO - test acc 0.7699999809265137
2023-07-01 00:37:08,650 - INFO - Distilling data from client: Client40
2023-07-01 00:37:08,650 - INFO - train loss: 0.0005070218573563651
2023-07-01 00:37:08,651 - INFO - train acc: 1.0
2023-07-01 00:37:08,673 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.75      0.81        72
           4       0.64      0.80      0.71        66
           5       0.69      0.61      0.65        62

    accuracy                           0.73       200
   macro avg       0.73      0.72      0.72       200
weighted avg       0.74      0.72      0.73       200

2023-07-01 00:37:08,674 - INFO - test loss 0.02023049321083033
2023-07-01 00:37:08,674 - INFO - test acc 0.7249999642372131
2023-07-01 00:37:09,917 - INFO - Distilling data from client: Client40
2023-07-01 00:37:09,918 - INFO - train loss: 0.00049739257543739
2023-07-01 00:37:09,918 - INFO - train acc: 1.0
2023-07-01 00:37:09,940 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.83      0.86        72
           4       0.69      0.77      0.73        66
           5       0.69      0.65      0.67        62

    accuracy                           0.76       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:37:09,941 - INFO - test loss 0.020080647036202777
2023-07-01 00:37:09,941 - INFO - test acc 0.7549999952316284
2023-07-01 00:37:11,182 - INFO - Distilling data from client: Client40
2023-07-01 00:37:11,182 - INFO - train loss: 0.0004460083488785604
2023-07-01 00:37:11,182 - INFO - train acc: 1.0
2023-07-01 00:37:11,206 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.83      0.86        72
           4       0.70      0.79      0.74        66
           5       0.72      0.68      0.70        62

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:37:11,206 - INFO - test loss 0.019650888511360556
2023-07-01 00:37:11,206 - INFO - test acc 0.7699999809265137
2023-07-01 00:37:12,465 - INFO - Distilling data from client: Client40
2023-07-01 00:37:12,466 - INFO - train loss: 0.0005320348406604615
2023-07-01 00:37:12,466 - INFO - train acc: 1.0
2023-07-01 00:37:12,489 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.82      0.84        72
           4       0.73      0.73      0.73        66
           5       0.68      0.73      0.70        62

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:37:12,489 - INFO - test loss 0.020083587134606454
2023-07-01 00:37:12,489 - INFO - test acc 0.7599999904632568
2023-07-01 00:37:13,737 - INFO - Distilling data from client: Client40
2023-07-01 00:37:13,737 - INFO - train loss: 0.000413637060868451
2023-07-01 00:37:13,737 - INFO - train acc: 1.0
2023-07-01 00:37:13,760 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.83      0.85        72
           4       0.73      0.80      0.76        66
           5       0.72      0.68      0.70        62

    accuracy                           0.78       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:37:13,760 - INFO - test loss 0.019748083053925527
2023-07-01 00:37:13,760 - INFO - test acc 0.7749999761581421
2023-07-01 00:37:15,008 - INFO - Distilling data from client: Client40
2023-07-01 00:37:15,008 - INFO - train loss: 0.00047965623123455414
2023-07-01 00:37:15,008 - INFO - train acc: 1.0
2023-07-01 00:37:15,031 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.83      0.85        72
           4       0.71      0.79      0.75        66
           5       0.74      0.69      0.72        62

    accuracy                           0.78       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:37:15,032 - INFO - test loss 0.020021468577789206
2023-07-01 00:37:15,032 - INFO - test acc 0.7749999761581421
2023-07-01 00:37:16,279 - INFO - Distilling data from client: Client40
2023-07-01 00:37:16,279 - INFO - train loss: 0.00034672816400613146
2023-07-01 00:37:16,279 - INFO - train acc: 1.0
2023-07-01 00:37:16,303 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.83      0.85        72
           4       0.72      0.77      0.74        66
           5       0.72      0.69      0.70        62

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:37:16,303 - INFO - test loss 0.019688810413320153
2023-07-01 00:37:16,303 - INFO - test acc 0.7699999809265137
2023-07-01 00:37:17,560 - INFO - Distilling data from client: Client40
2023-07-01 00:37:17,560 - INFO - train loss: 0.000421154206965985
2023-07-01 00:37:17,560 - INFO - train acc: 1.0
2023-07-01 00:37:17,584 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.85      0.87        72
           4       0.66      0.77      0.71        66
           5       0.69      0.61      0.65        62

    accuracy                           0.75       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.76      0.75      0.75       200

2023-07-01 00:37:17,585 - INFO - test loss 0.020473064333060315
2023-07-01 00:37:17,585 - INFO - test acc 0.75
2023-07-01 00:37:18,840 - INFO - Distilling data from client: Client40
2023-07-01 00:37:18,840 - INFO - train loss: 0.00037073365195918135
2023-07-01 00:37:18,840 - INFO - train acc: 1.0
2023-07-01 00:37:18,867 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.83      0.85        72
           4       0.66      0.77      0.71        66
           5       0.69      0.60      0.64        62

    accuracy                           0.74       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.74      0.74       200

2023-07-01 00:37:18,868 - INFO - test loss 0.020651439451025017
2023-07-01 00:37:18,868 - INFO - test acc 0.7400000095367432
2023-07-01 00:37:20,124 - INFO - Distilling data from client: Client40
2023-07-01 00:37:20,125 - INFO - train loss: 0.0003863832540061613
2023-07-01 00:37:20,125 - INFO - train acc: 1.0
2023-07-01 00:37:20,151 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.81      0.84        72
           4       0.67      0.77      0.72        66
           5       0.69      0.65      0.67        62

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.75       200

2023-07-01 00:37:20,151 - INFO - test loss 0.019702826288359943
2023-07-01 00:37:20,151 - INFO - test acc 0.7450000047683716
2023-07-01 00:37:21,397 - INFO - Distilling data from client: Client40
2023-07-01 00:37:21,398 - INFO - train loss: 0.0003862054069970847
2023-07-01 00:37:21,398 - INFO - train acc: 1.0
2023-07-01 00:37:21,421 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.82      0.84        72
           4       0.67      0.77      0.72        66
           5       0.70      0.63      0.66        62

    accuracy                           0.74       200
   macro avg       0.75      0.74      0.74       200
weighted avg       0.75      0.74      0.75       200

2023-07-01 00:37:21,421 - INFO - test loss 0.020210342796554513
2023-07-01 00:37:21,421 - INFO - test acc 0.7450000047683716
2023-07-01 00:37:22,676 - INFO - Distilling data from client: Client40
2023-07-01 00:37:22,676 - INFO - train loss: 0.000338685227073165
2023-07-01 00:37:22,676 - INFO - train acc: 1.0
2023-07-01 00:37:22,700 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.83      0.86        72
           4       0.67      0.79      0.72        66
           5       0.71      0.63      0.67        62

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:37:22,700 - INFO - test loss 0.02014094080957377
2023-07-01 00:37:22,700 - INFO - test acc 0.7549999952316284
2023-07-01 00:37:23,942 - INFO - Distilling data from client: Client40
2023-07-01 00:37:23,943 - INFO - train loss: 0.00029935570900284673
2023-07-01 00:37:23,943 - INFO - train acc: 1.0
2023-07-01 00:37:23,966 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.85      0.87        72
           4       0.71      0.79      0.75        66
           5       0.72      0.68      0.70        62

    accuracy                           0.78       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:37:23,967 - INFO - test loss 0.01975084692563331
2023-07-01 00:37:23,967 - INFO - test acc 0.7749999761581421
2023-07-01 00:37:25,230 - INFO - Distilling data from client: Client40
2023-07-01 00:37:25,230 - INFO - train loss: 0.00027442904048593825
2023-07-01 00:37:25,230 - INFO - train acc: 1.0
2023-07-01 00:37:25,292 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.86      0.87        72
           4       0.73      0.82      0.77        66
           5       0.75      0.66      0.70        62

    accuracy                           0.79       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.79      0.79      0.78       200

2023-07-01 00:37:25,292 - INFO - test loss 0.019626637394671298
2023-07-01 00:37:25,292 - INFO - test acc 0.7849999666213989
2023-07-01 00:37:26,546 - INFO - Distilling data from client: Client40
2023-07-01 00:37:26,547 - INFO - train loss: 0.0002206170648611304
2023-07-01 00:37:26,547 - INFO - train acc: 1.0
2023-07-01 00:37:26,571 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.81      0.84        72
           4       0.65      0.77      0.71        66
           5       0.70      0.63      0.66        62

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:37:26,571 - INFO - test loss 0.020351068972989286
2023-07-01 00:37:26,571 - INFO - test acc 0.7400000095367432
2023-07-01 00:37:27,819 - INFO - Distilling data from client: Client40
2023-07-01 00:37:27,820 - INFO - train loss: 0.00027162437514654576
2023-07-01 00:37:27,820 - INFO - train acc: 1.0
2023-07-01 00:37:27,844 - INFO - report:               precision    recall  f1-score   support

           1       0.87      0.85      0.86        72
           4       0.72      0.79      0.75        66
           5       0.71      0.66      0.68        62

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.77      0.77      0.77       200

2023-07-01 00:37:27,844 - INFO - test loss 0.019585398984001103
2023-07-01 00:37:27,844 - INFO - test acc 0.7699999809265137
2023-07-01 00:37:29,088 - INFO - Distilling data from client: Client40
2023-07-01 00:37:29,088 - INFO - train loss: 0.0002717555729115052
2023-07-01 00:37:29,088 - INFO - train acc: 1.0
2023-07-01 00:37:29,111 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.82      0.85        72
           4       0.69      0.80      0.74        66
           5       0.70      0.63      0.66        62

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:37:29,112 - INFO - test loss 0.01998290806259086
2023-07-01 00:37:29,112 - INFO - test acc 0.7549999952316284
2023-07-01 00:37:30,363 - INFO - Distilling data from client: Client40
2023-07-01 00:37:30,363 - INFO - train loss: 0.00026322206649382934
2023-07-01 00:37:30,363 - INFO - train acc: 1.0
2023-07-01 00:37:30,386 - INFO - report:               precision    recall  f1-score   support

           1       0.86      0.79      0.83        72
           4       0.65      0.77      0.71        66
           5       0.70      0.63      0.66        62

    accuracy                           0.73       200
   macro avg       0.74      0.73      0.73       200
weighted avg       0.74      0.73      0.74       200

2023-07-01 00:37:30,387 - INFO - test loss 0.020083535658861743
2023-07-01 00:37:30,387 - INFO - test acc 0.73499995470047
2023-07-01 00:37:31,647 - INFO - Distilling data from client: Client40
2023-07-01 00:37:31,648 - INFO - train loss: 0.00023293725532477595
2023-07-01 00:37:31,648 - INFO - train acc: 1.0
2023-07-01 00:37:31,671 - INFO - report:               precision    recall  f1-score   support

           1       0.90      0.83      0.86        72
           4       0.68      0.76      0.71        66
           5       0.69      0.66      0.68        62

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:37:31,671 - INFO - test loss 0.020206406031733997
2023-07-01 00:37:31,671 - INFO - test acc 0.7549999952316284
2023-07-01 00:37:32,922 - INFO - Distilling data from client: Client40
2023-07-01 00:37:32,922 - INFO - train loss: 0.0002466862439419961
2023-07-01 00:37:32,922 - INFO - train acc: 1.0
2023-07-01 00:37:32,946 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.82      0.85        72
           4       0.66      0.77      0.71        66
           5       0.68      0.61      0.64        62

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:37:32,946 - INFO - test loss 0.020245497162545385
2023-07-01 00:37:32,946 - INFO - test acc 0.7400000095367432
2023-07-01 00:37:34,195 - INFO - Distilling data from client: Client40
2023-07-01 00:37:34,195 - INFO - train loss: 0.0002180829104151599
2023-07-01 00:37:34,195 - INFO - train acc: 1.0
2023-07-01 00:37:34,219 - INFO - report:               precision    recall  f1-score   support

           1       0.88      0.85      0.87        72
           4       0.72      0.80      0.76        66
           5       0.74      0.68      0.71        62

    accuracy                           0.78       200
   macro avg       0.78      0.78      0.78       200
weighted avg       0.78      0.78      0.78       200

2023-07-01 00:37:34,219 - INFO - test loss 0.020446517431686687
2023-07-01 00:37:34,219 - INFO - test acc 0.7799999713897705
2023-07-01 00:37:34,231 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:34,240 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:34,249 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:34,258 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:34,267 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:34,275 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:34,284 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:34,293 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:34,303 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:37:34,676 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client40//synthetic.png
2023-07-01 00:37:34,688 - INFO - c: 3.0 and total_data_in_this_class: 256
2023-07-01 00:37:34,688 - INFO - c: 7.0 and total_data_in_this_class: 272
2023-07-01 00:37:34,688 - INFO - c: 9.0 and total_data_in_this_class: 271
2023-07-01 00:37:34,688 - INFO - c: 3.0 and total_data_in_this_class: 77
2023-07-01 00:37:34,688 - INFO - c: 7.0 and total_data_in_this_class: 61
2023-07-01 00:37:34,688 - INFO - c: 9.0 and total_data_in_this_class: 62
2023-07-01 00:37:34,759 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04772019386291504 sec
2023-07-01 00:37:34,805 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04511570930480957 sec
2023-07-01 00:37:34,810 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.09990239143371582 sec
2023-07-01 00:37:34,811 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:37:34,844 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03264951705932617 sec
2023-07-01 00:37:34,844 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:37:34,969 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12440085411071777 sec
2023-07-01 00:37:34,991 - INFO - initial test loss: 0.02307693739575086
2023-07-01 00:37:34,991 - INFO - initial test acc: 0.7199999690055847
2023-07-01 00:37:34,999 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.00565028190612793 sec
2023-07-01 00:37:35,108 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11520075798034668 sec
2023-07-01 00:37:35,112 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10]), ShapedArray(float32[513,3,32,32]), ShapedArray(float32[513,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:37:35,174 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06239891052246094 sec
2023-07-01 00:37:35,175 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:37:35,507 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3321986198425293 sec
2023-07-01 00:37:36,741 - INFO - Distilling data from client: Client41
2023-07-01 00:37:36,741 - INFO - train loss: 0.0027051629541202825
2023-07-01 00:37:36,741 - INFO - train acc: 0.9941520690917969
2023-07-01 00:37:36,801 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.64      0.69        77
           7       0.67      0.70      0.69        61
           9       0.76      0.85      0.80        62

    accuracy                           0.73       200
   macro avg       0.72      0.73      0.73       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:37:36,801 - INFO - test loss 0.02091893336010913
2023-07-01 00:37:36,801 - INFO - test acc 0.7249999642372131
2023-07-01 00:37:38,042 - INFO - Distilling data from client: Client41
2023-07-01 00:37:38,042 - INFO - train loss: 0.0014691404705235785
2023-07-01 00:37:38,042 - INFO - train acc: 1.0
2023-07-01 00:37:38,104 - INFO - report:               precision    recall  f1-score   support

           3       0.71      0.65      0.68        77
           7       0.68      0.74      0.71        61
           9       0.80      0.82      0.81        62

    accuracy                           0.73       200
   macro avg       0.73      0.74      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-07-01 00:37:38,104 - INFO - test loss 0.02082370035493458
2023-07-01 00:37:38,104 - INFO - test acc 0.7299999594688416
2023-07-01 00:37:39,342 - INFO - Distilling data from client: Client41
2023-07-01 00:37:39,342 - INFO - train loss: 0.00108816106553158
2023-07-01 00:37:39,342 - INFO - train acc: 1.0
2023-07-01 00:37:39,365 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.64      0.69        77
           7       0.65      0.77      0.71        61
           9       0.79      0.79      0.79        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:37:39,365 - INFO - test loss 0.02165738984868203
2023-07-01 00:37:39,365 - INFO - test acc 0.7249999642372131
2023-07-01 00:37:40,601 - INFO - Distilling data from client: Client41
2023-07-01 00:37:40,601 - INFO - train loss: 0.0008131764432283704
2023-07-01 00:37:40,601 - INFO - train acc: 1.0
2023-07-01 00:37:40,662 - INFO - report:               precision    recall  f1-score   support

           3       0.73      0.69      0.71        77
           7       0.71      0.75      0.73        61
           9       0.81      0.81      0.81        62

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:37:40,663 - INFO - test loss 0.020764482770107787
2023-07-01 00:37:40,663 - INFO - test acc 0.7450000047683716
2023-07-01 00:37:41,900 - INFO - Distilling data from client: Client41
2023-07-01 00:37:41,900 - INFO - train loss: 0.000747747666068749
2023-07-01 00:37:41,900 - INFO - train acc: 1.0
2023-07-01 00:37:41,926 - INFO - report:               precision    recall  f1-score   support

           3       0.73      0.62      0.67        77
           7       0.64      0.75      0.69        61
           9       0.79      0.79      0.79        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:37:41,926 - INFO - test loss 0.02076712797426098
2023-07-01 00:37:41,927 - INFO - test acc 0.7149999737739563
2023-07-01 00:37:43,156 - INFO - Distilling data from client: Client41
2023-07-01 00:37:43,156 - INFO - train loss: 0.0006237002520970209
2023-07-01 00:37:43,156 - INFO - train acc: 1.0
2023-07-01 00:37:43,179 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.64      0.69        77
           7       0.63      0.79      0.70        61
           9       0.78      0.74      0.76        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:37:43,179 - INFO - test loss 0.021088955292519494
2023-07-01 00:37:43,179 - INFO - test acc 0.7149999737739563
2023-07-01 00:37:44,414 - INFO - Distilling data from client: Client41
2023-07-01 00:37:44,415 - INFO - train loss: 0.0005125531756221228
2023-07-01 00:37:44,415 - INFO - train acc: 1.0
2023-07-01 00:37:44,438 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.62      0.68        77
           7       0.64      0.75      0.69        61
           9       0.73      0.74      0.74        62

    accuracy                           0.70       200
   macro avg       0.70      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:37:44,438 - INFO - test loss 0.021391540626899137
2023-07-01 00:37:44,438 - INFO - test acc 0.699999988079071
2023-07-01 00:37:45,660 - INFO - Distilling data from client: Client41
2023-07-01 00:37:45,661 - INFO - train loss: 0.0004605062357859273
2023-07-01 00:37:45,661 - INFO - train acc: 1.0
2023-07-01 00:37:45,721 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.71      0.73        77
           7       0.70      0.79      0.74        61
           9       0.83      0.77      0.80        62

    accuracy                           0.76       200
   macro avg       0.76      0.76      0.76       200
weighted avg       0.76      0.76      0.76       200

2023-07-01 00:37:45,721 - INFO - test loss 0.020958262231416787
2023-07-01 00:37:45,722 - INFO - test acc 0.7549999952316284
2023-07-01 00:37:46,951 - INFO - Distilling data from client: Client41
2023-07-01 00:37:46,951 - INFO - train loss: 0.0003927653126161064
2023-07-01 00:37:46,951 - INFO - train acc: 1.0
2023-07-01 00:37:46,976 - INFO - report:               precision    recall  f1-score   support

           3       0.71      0.64      0.67        77
           7       0.64      0.74      0.69        61
           9       0.79      0.77      0.78        62

    accuracy                           0.71       200
   macro avg       0.71      0.72      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:37:46,976 - INFO - test loss 0.02107351861560104
2023-07-01 00:37:46,976 - INFO - test acc 0.7099999785423279
2023-07-01 00:37:48,205 - INFO - Distilling data from client: Client41
2023-07-01 00:37:48,205 - INFO - train loss: 0.0003540473830141337
2023-07-01 00:37:48,205 - INFO - train acc: 1.0
2023-07-01 00:37:48,230 - INFO - report:               precision    recall  f1-score   support

           3       0.73      0.69      0.71        77
           7       0.67      0.74      0.70        61
           9       0.80      0.77      0.79        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.73      0.73       200

2023-07-01 00:37:48,231 - INFO - test loss 0.02168734866696422
2023-07-01 00:37:48,231 - INFO - test acc 0.7299999594688416
2023-07-01 00:37:49,458 - INFO - Distilling data from client: Client41
2023-07-01 00:37:49,458 - INFO - train loss: 0.00031287694441756506
2023-07-01 00:37:49,458 - INFO - train acc: 1.0
2023-07-01 00:37:49,485 - INFO - report:               precision    recall  f1-score   support

           3       0.72      0.65      0.68        77
           7       0.62      0.70      0.66        61
           9       0.79      0.79      0.79        62

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:37:49,485 - INFO - test loss 0.021836292067370842
2023-07-01 00:37:49,485 - INFO - test acc 0.7099999785423279
2023-07-01 00:37:50,715 - INFO - Distilling data from client: Client41
2023-07-01 00:37:50,716 - INFO - train loss: 0.0003629213313163515
2023-07-01 00:37:50,716 - INFO - train acc: 1.0
2023-07-01 00:37:50,739 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.68      0.71        77
           7       0.65      0.75      0.70        61
           9       0.82      0.79      0.80        62

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.73      0.74       200

2023-07-01 00:37:50,739 - INFO - test loss 0.021385823255766066
2023-07-01 00:37:50,739 - INFO - test acc 0.73499995470047
2023-07-01 00:37:51,969 - INFO - Distilling data from client: Client41
2023-07-01 00:37:51,969 - INFO - train loss: 0.00032975209078719156
2023-07-01 00:37:51,969 - INFO - train acc: 1.0
2023-07-01 00:37:51,993 - INFO - report:               precision    recall  f1-score   support

           3       0.72      0.66      0.69        77
           7       0.65      0.77      0.71        61
           9       0.82      0.76      0.79        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.73       200

2023-07-01 00:37:51,993 - INFO - test loss 0.021759875644021875
2023-07-01 00:37:51,993 - INFO - test acc 0.7249999642372131
2023-07-01 00:37:53,219 - INFO - Distilling data from client: Client41
2023-07-01 00:37:53,219 - INFO - train loss: 0.0004053103718250864
2023-07-01 00:37:53,219 - INFO - train acc: 1.0
2023-07-01 00:37:53,244 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.69      0.73        77
           7       0.67      0.72      0.69        61
           9       0.77      0.81      0.79        62

    accuracy                           0.73       200
   macro avg       0.73      0.74      0.74       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:37:53,244 - INFO - test loss 0.021231288943879177
2023-07-01 00:37:53,244 - INFO - test acc 0.73499995470047
2023-07-01 00:37:54,477 - INFO - Distilling data from client: Client41
2023-07-01 00:37:54,477 - INFO - train loss: 0.00027591412659158475
2023-07-01 00:37:54,477 - INFO - train acc: 1.0
2023-07-01 00:37:54,501 - INFO - report:               precision    recall  f1-score   support

           3       0.75      0.62      0.68        77
           7       0.64      0.77      0.70        61
           9       0.79      0.81      0.80        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:37:54,501 - INFO - test loss 0.021634719079765594
2023-07-01 00:37:54,501 - INFO - test acc 0.7249999642372131
2023-07-01 00:37:55,738 - INFO - Distilling data from client: Client41
2023-07-01 00:37:55,739 - INFO - train loss: 0.0002977368194479787
2023-07-01 00:37:55,739 - INFO - train acc: 1.0
2023-07-01 00:37:55,764 - INFO - report:               precision    recall  f1-score   support

           3       0.77      0.64      0.70        77
           7       0.66      0.80      0.73        61
           9       0.79      0.79      0.79        62

    accuracy                           0.73       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:37:55,764 - INFO - test loss 0.021355814443523954
2023-07-01 00:37:55,764 - INFO - test acc 0.73499995470047
2023-07-01 00:37:57,000 - INFO - Distilling data from client: Client41
2023-07-01 00:37:57,000 - INFO - train loss: 0.00029309460350391355
2023-07-01 00:37:57,000 - INFO - train acc: 1.0
2023-07-01 00:37:57,024 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.65      0.69        77
           7       0.64      0.70      0.67        61
           9       0.77      0.81      0.79        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:37:57,024 - INFO - test loss 0.021531935742378713
2023-07-01 00:37:57,025 - INFO - test acc 0.7149999737739563
2023-07-01 00:37:58,258 - INFO - Distilling data from client: Client41
2023-07-01 00:37:58,258 - INFO - train loss: 0.0002565758650674414
2023-07-01 00:37:58,258 - INFO - train acc: 1.0
2023-07-01 00:37:58,281 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.65      0.70        77
           7       0.63      0.74      0.68        61
           9       0.79      0.81      0.80        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.73       200

2023-07-01 00:37:58,281 - INFO - test loss 0.021464525224073244
2023-07-01 00:37:58,281 - INFO - test acc 0.7249999642372131
2023-07-01 00:37:59,511 - INFO - Distilling data from client: Client41
2023-07-01 00:37:59,511 - INFO - train loss: 0.00025255628670489466
2023-07-01 00:37:59,511 - INFO - train acc: 1.0
2023-07-01 00:37:59,534 - INFO - report:               precision    recall  f1-score   support

           3       0.71      0.65      0.68        77
           7       0.65      0.77      0.71        61
           9       0.83      0.77      0.80        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.73       200

2023-07-01 00:37:59,535 - INFO - test loss 0.021291550063709724
2023-07-01 00:37:59,535 - INFO - test acc 0.7249999642372131
2023-07-01 00:38:00,787 - INFO - Distilling data from client: Client41
2023-07-01 00:38:00,787 - INFO - train loss: 0.0002757854867519961
2023-07-01 00:38:00,787 - INFO - train acc: 1.0
2023-07-01 00:38:00,810 - INFO - report:               precision    recall  f1-score   support

           3       0.72      0.65      0.68        77
           7       0.63      0.74      0.68        61
           9       0.80      0.77      0.79        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:38:00,810 - INFO - test loss 0.02204621191696721
2023-07-01 00:38:00,810 - INFO - test acc 0.7149999737739563
2023-07-01 00:38:02,040 - INFO - Distilling data from client: Client41
2023-07-01 00:38:02,040 - INFO - train loss: 0.00020246605885227217
2023-07-01 00:38:02,040 - INFO - train acc: 1.0
2023-07-01 00:38:02,063 - INFO - report:               precision    recall  f1-score   support

           3       0.72      0.64      0.68        77
           7       0.66      0.75      0.70        61
           9       0.77      0.77      0.77        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:38:02,063 - INFO - test loss 0.021825343044628732
2023-07-01 00:38:02,063 - INFO - test acc 0.7149999737739563
2023-07-01 00:38:03,298 - INFO - Distilling data from client: Client41
2023-07-01 00:38:03,299 - INFO - train loss: 0.00020713958948916856
2023-07-01 00:38:03,299 - INFO - train acc: 1.0
2023-07-01 00:38:03,322 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.61      0.68        77
           7       0.63      0.79      0.70        61
           9       0.81      0.81      0.81        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:38:03,322 - INFO - test loss 0.021603915024002326
2023-07-01 00:38:03,322 - INFO - test acc 0.7249999642372131
2023-07-01 00:38:04,567 - INFO - Distilling data from client: Client41
2023-07-01 00:38:04,567 - INFO - train loss: 0.00027333587082031157
2023-07-01 00:38:04,567 - INFO - train acc: 1.0
2023-07-01 00:38:04,590 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.62      0.69        77
           7       0.62      0.75      0.68        61
           9       0.78      0.79      0.78        62

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:38:04,590 - INFO - test loss 0.0215632331784252
2023-07-01 00:38:04,590 - INFO - test acc 0.7149999737739563
2023-07-01 00:38:05,830 - INFO - Distilling data from client: Client41
2023-07-01 00:38:05,830 - INFO - train loss: 0.00019954891910305936
2023-07-01 00:38:05,830 - INFO - train acc: 1.0
2023-07-01 00:38:05,853 - INFO - report:               precision    recall  f1-score   support

           3       0.76      0.66      0.71        77
           7       0.66      0.77      0.71        61
           9       0.82      0.82      0.82        62

    accuracy                           0.74       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.75      0.74      0.74       200

2023-07-01 00:38:05,853 - INFO - test loss 0.021323272062645317
2023-07-01 00:38:05,853 - INFO - test acc 0.7450000047683716
2023-07-01 00:38:07,096 - INFO - Distilling data from client: Client41
2023-07-01 00:38:07,096 - INFO - train loss: 0.00018316548788815433
2023-07-01 00:38:07,096 - INFO - train acc: 1.0
2023-07-01 00:38:07,119 - INFO - report:               precision    recall  f1-score   support

           3       0.74      0.64      0.69        77
           7       0.67      0.77      0.72        61
           9       0.77      0.79      0.78        62

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.73       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:38:07,119 - INFO - test loss 0.021528513417215636
2023-07-01 00:38:07,119 - INFO - test acc 0.7249999642372131
2023-07-01 00:38:07,130 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:07,139 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:07,147 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:07,156 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:07,165 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:07,173 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:07,182 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:07,191 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:07,200 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:07,566 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client41//synthetic.png
2023-07-01 00:38:07,578 - INFO - c: 6.0 and total_data_in_this_class: 272
2023-07-01 00:38:07,579 - INFO - c: 7.0 and total_data_in_this_class: 267
2023-07-01 00:38:07,579 - INFO - c: 8.0 and total_data_in_this_class: 34
2023-07-01 00:38:07,579 - INFO - c: 9.0 and total_data_in_this_class: 226
2023-07-01 00:38:07,579 - INFO - c: 6.0 and total_data_in_this_class: 61
2023-07-01 00:38:07,579 - INFO - c: 7.0 and total_data_in_this_class: 66
2023-07-01 00:38:07,579 - INFO - c: 8.0 and total_data_in_this_class: 11
2023-07-01 00:38:07,579 - INFO - c: 9.0 and total_data_in_this_class: 62
2023-07-01 00:38:07,597 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025725364685058594 sec
2023-07-01 00:38:07,598 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:38:07,599 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001298666000366211 sec
2023-07-01 00:38:07,599 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:07,610 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010800600051879883 sec
2023-07-01 00:38:07,612 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00023031234741210938 sec
2023-07-01 00:38:07,612 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:38:07,613 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009453296661376953 sec
2023-07-01 00:38:07,613 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:07,622 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.00864863395690918 sec
2023-07-01 00:38:07,626 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00014352798461914062 sec
2023-07-01 00:38:07,627 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012373924255371094 sec
2023-07-01 00:38:07,627 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003063678741455078 sec
2023-07-01 00:38:07,629 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021505355834960938 sec
2023-07-01 00:38:07,629 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012087821960449219 sec
2023-07-01 00:38:07,630 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002837181091308594 sec
2023-07-01 00:38:07,630 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024819374084472656 sec
2023-07-01 00:38:07,631 - WARNING - Finished tracing + transforming absolute for pjit in 0.00016760826110839844 sec
2023-07-01 00:38:07,631 - WARNING - Finished tracing + transforming fn for pjit in 0.00027823448181152344 sec
2023-07-01 00:38:07,632 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0003199577331542969 sec
2023-07-01 00:38:07,633 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.000202178955078125 sec
2023-07-01 00:38:07,634 - WARNING - Finished tracing + transforming fn for pjit in 0.00022935867309570312 sec
2023-07-01 00:38:07,635 - WARNING - Finished tracing + transforming fn for pjit in 0.0002663135528564453 sec
2023-07-01 00:38:07,635 - WARNING - Finished tracing + transforming fn for pjit in 0.00022864341735839844 sec
2023-07-01 00:38:07,636 - WARNING - Finished tracing + transforming fn for pjit in 0.000263214111328125 sec
2023-07-01 00:38:07,637 - WARNING - Finished tracing + transforming fn for pjit in 0.00023031234741210938 sec
2023-07-01 00:38:07,639 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001697540283203125 sec
2023-07-01 00:38:07,640 - WARNING - Finished tracing + transforming fn for pjit in 0.00022745132446289062 sec
2023-07-01 00:38:07,640 - WARNING - Finished tracing + transforming fn for pjit in 0.0002307891845703125 sec
2023-07-01 00:38:07,644 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00038361549377441406 sec
2023-07-01 00:38:07,644 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010099411010742188 sec
2023-07-01 00:38:07,645 - WARNING - Finished tracing + transforming fn for pjit in 0.0002353191375732422 sec
2023-07-01 00:38:07,646 - WARNING - Finished tracing + transforming fn for pjit in 0.000225067138671875 sec
2023-07-01 00:38:07,647 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002970695495605469 sec
2023-07-01 00:38:07,648 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026154518127441406 sec
2023-07-01 00:38:07,648 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001780986785888672 sec
2023-07-01 00:38:07,649 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002627372741699219 sec
2023-07-01 00:38:07,650 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022935867309570312 sec
2023-07-01 00:38:07,651 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022840499877929688 sec
2023-07-01 00:38:07,652 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.000362396240234375 sec
2023-07-01 00:38:07,652 - WARNING - Finished tracing + transforming _where for pjit in 0.0009903907775878906 sec
2023-07-01 00:38:07,653 - WARNING - Finished tracing + transforming fn for pjit in 0.00026106834411621094 sec
2023-07-01 00:38:07,653 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026607513427734375 sec
2023-07-01 00:38:07,654 - WARNING - Finished tracing + transforming fn for pjit in 0.00022268295288085938 sec
2023-07-01 00:38:07,655 - WARNING - Finished tracing + transforming fn for pjit in 0.0002219676971435547 sec
2023-07-01 00:38:07,656 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021910667419433594 sec
2023-07-01 00:38:07,656 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002601146697998047 sec
2023-07-01 00:38:07,657 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002579689025878906 sec
2023-07-01 00:38:07,658 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026226043701171875 sec
2023-07-01 00:38:07,659 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022292137145996094 sec
2023-07-01 00:38:07,659 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022172927856445312 sec
2023-07-01 00:38:07,660 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00027108192443847656 sec
2023-07-01 00:38:07,660 - WARNING - Finished tracing + transforming _where for pjit in 0.0009167194366455078 sec
2023-07-01 00:38:07,661 - WARNING - Finished tracing + transforming fn for pjit in 0.00026154518127441406 sec
2023-07-01 00:38:07,662 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026345252990722656 sec
2023-07-01 00:38:07,663 - WARNING - Finished tracing + transforming fn for pjit in 0.00022125244140625 sec
2023-07-01 00:38:07,667 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000274658203125 sec
2023-07-01 00:38:07,668 - WARNING - Finished tracing + transforming fn for pjit in 0.0003418922424316406 sec
2023-07-01 00:38:07,669 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002758502960205078 sec
2023-07-01 00:38:07,669 - WARNING - Finished tracing + transforming fn for pjit in 0.0002231597900390625 sec
2023-07-01 00:38:07,673 - WARNING - Finished tracing + transforming fn for pjit in 0.0002200603485107422 sec
2023-07-01 00:38:07,675 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001742839813232422 sec
2023-07-01 00:38:07,676 - WARNING - Finished tracing + transforming fn for pjit in 0.0003209114074707031 sec
2023-07-01 00:38:07,677 - WARNING - Finished tracing + transforming fn for pjit in 0.00024437904357910156 sec
2023-07-01 00:38:07,695 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.07007312774658203 sec
2023-07-01 00:38:07,698 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001285076141357422 sec
2023-07-01 00:38:07,699 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011301040649414062 sec
2023-07-01 00:38:07,699 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00027298927307128906 sec
2023-07-01 00:38:07,702 - WARNING - Finished tracing + transforming fn for pjit in 0.00022554397583007812 sec
2023-07-01 00:38:07,702 - WARNING - Finished tracing + transforming fn for pjit in 0.00025653839111328125 sec
2023-07-01 00:38:07,704 - WARNING - Finished tracing + transforming fn for pjit in 0.00022840499877929688 sec
2023-07-01 00:38:07,710 - WARNING - Finished tracing + transforming fn for pjit in 0.00022673606872558594 sec
2023-07-01 00:38:07,711 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022912025451660156 sec
2023-07-01 00:38:07,711 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026154518127441406 sec
2023-07-01 00:38:07,712 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017976760864257812 sec
2023-07-01 00:38:07,713 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003409385681152344 sec
2023-07-01 00:38:07,714 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022935867309570312 sec
2023-07-01 00:38:07,714 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000232696533203125 sec
2023-07-01 00:38:07,715 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00027632713317871094 sec
2023-07-01 00:38:07,716 - WARNING - Finished tracing + transforming _where for pjit in 0.0009365081787109375 sec
2023-07-01 00:38:07,717 - WARNING - Finished tracing + transforming fn for pjit in 0.000263214111328125 sec
2023-07-01 00:38:07,717 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027942657470703125 sec
2023-07-01 00:38:07,718 - WARNING - Finished tracing + transforming fn for pjit in 0.00022339820861816406 sec
2023-07-01 00:38:07,719 - WARNING - Finished tracing + transforming fn for pjit in 0.00029087066650390625 sec
2023-07-01 00:38:07,731 - WARNING - Finished tracing + transforming fn for pjit in 0.00021886825561523438 sec
2023-07-01 00:38:07,751 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05490589141845703 sec
2023-07-01 00:38:07,752 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012350082397460938 sec
2023-07-01 00:38:07,753 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0001385211944580078 sec
2023-07-01 00:38:07,754 - WARNING - Finished tracing + transforming _where for pjit in 0.0006563663482666016 sec
2023-07-01 00:38:07,754 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002944469451904297 sec
2023-07-01 00:38:07,755 - WARNING - Finished tracing + transforming trace for pjit in 0.0026705265045166016 sec
2023-07-01 00:38:07,757 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010704994201660156 sec
2023-07-01 00:38:07,758 - WARNING - Finished tracing + transforming tril for pjit in 0.0006902217864990234 sec
2023-07-01 00:38:07,759 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0019311904907226562 sec
2023-07-01 00:38:07,759 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010824203491210938 sec
2023-07-01 00:38:07,760 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00011086463928222656 sec
2023-07-01 00:38:07,762 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014450550079345703 sec
2023-07-01 00:38:07,766 - WARNING - Finished tracing + transforming _solve for pjit in 0.009490489959716797 sec
2023-07-01 00:38:07,766 - WARNING - Finished tracing + transforming dot for pjit in 0.00031185150146484375 sec
2023-07-01 00:38:07,769 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.14519023895263672 sec
2023-07-01 00:38:07,771 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:38:07,804 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03265213966369629 sec
2023-07-01 00:38:07,804 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:07,904 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.09961318969726562 sec
2023-07-01 00:38:07,910 - INFO - initial test loss: 0.02696879399672054
2023-07-01 00:38:07,910 - INFO - initial test acc: 0.6449999809265137
2023-07-01 00:38:07,915 - WARNING - Finished tracing + transforming dot for pjit in 0.0003597736358642578 sec
2023-07-01 00:38:07,915 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002892017364501953 sec
2023-07-01 00:38:07,916 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003743171691894531 sec
2023-07-01 00:38:07,917 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010564327239990234 sec
2023-07-01 00:38:07,918 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001971721649169922 sec
2023-07-01 00:38:07,918 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001900196075439453 sec
2023-07-01 00:38:07,919 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002372264862060547 sec
2023-07-01 00:38:07,920 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.000385284423828125 sec
2023-07-01 00:38:07,920 - WARNING - Finished tracing + transforming _mean for pjit in 0.00115966796875 sec
2023-07-01 00:38:07,921 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.010007858276367188 sec
2023-07-01 00:38:07,930 - WARNING - Finished tracing + transforming fn for pjit in 0.00030994415283203125 sec
2023-07-01 00:38:07,930 - WARNING - Finished tracing + transforming fn for pjit in 0.0002567768096923828 sec
2023-07-01 00:38:07,931 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021386146545410156 sec
2023-07-01 00:38:07,932 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002605915069580078 sec
2023-07-01 00:38:07,932 - WARNING - Finished tracing + transforming _where for pjit in 0.0008606910705566406 sec
2023-07-01 00:38:07,940 - WARNING - Finished tracing + transforming fn for pjit in 0.00024819374084472656 sec
2023-07-01 00:38:07,941 - WARNING - Finished tracing + transforming fn for pjit in 0.00025272369384765625 sec
2023-07-01 00:38:07,941 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002200603485107422 sec
2023-07-01 00:38:07,942 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002532005310058594 sec
2023-07-01 00:38:07,942 - WARNING - Finished tracing + transforming _where for pjit in 0.0008082389831542969 sec
2023-07-01 00:38:07,976 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002110004425048828 sec
2023-07-01 00:38:08,030 - WARNING - Finished tracing + transforming fn for pjit in 0.0002658367156982422 sec
2023-07-01 00:38:08,031 - WARNING - Finished tracing + transforming fn for pjit in 0.0002167224884033203 sec
2023-07-01 00:38:08,031 - WARNING - Finished tracing + transforming square for pjit in 0.0001780986785888672 sec
2023-07-01 00:38:08,033 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002243518829345703 sec
2023-07-01 00:38:08,035 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024127960205078125 sec
2023-07-01 00:38:08,036 - WARNING - Finished tracing + transforming fn for pjit in 0.0002696514129638672 sec
2023-07-01 00:38:08,036 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002231597900390625 sec
2023-07-01 00:38:08,037 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002205371856689453 sec
2023-07-01 00:38:08,037 - WARNING - Finished tracing + transforming fn for pjit in 0.0002732276916503906 sec
2023-07-01 00:38:08,038 - WARNING - Finished tracing + transforming fn for pjit in 0.00021791458129882812 sec
2023-07-01 00:38:08,039 - WARNING - Finished tracing + transforming square for pjit in 0.00016450881958007812 sec
2023-07-01 00:38:08,041 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021886825561523438 sec
2023-07-01 00:38:08,042 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00016927719116210938 sec
2023-07-01 00:38:08,043 - WARNING - Finished tracing + transforming fn for pjit in 0.0002675056457519531 sec
2023-07-01 00:38:08,043 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002200603485107422 sec
2023-07-01 00:38:08,044 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021123886108398438 sec
2023-07-01 00:38:08,045 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13406872749328613 sec
2023-07-01 00:38:08,048 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,10]), ShapedArray(float32[92,10]), ShapedArray(float32[92,10]), ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,10]), ShapedArray(float32[92,3,32,32]), ShapedArray(float32[92,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:38:08,109 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.05996131896972656 sec
2023-07-01 00:38:08,109 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:08,351 - WARNING - Finished XLA compilation of jit(update_fn) in 0.24220824241638184 sec
2023-07-01 00:38:08,485 - INFO - Distilling data from client: Client42
2023-07-01 00:38:08,485 - INFO - train loss: 0.017263703498514194
2023-07-01 00:38:08,485 - INFO - train acc: 0.8152174353599548
2023-07-01 00:38:08,498 - INFO - report:               precision    recall  f1-score   support

           6       0.68      0.74      0.71        61
           7       0.75      0.62      0.68        66
           8       0.57      0.73      0.64        11
           9       0.75      0.79      0.77        62

    accuracy                           0.71       200
   macro avg       0.69      0.72      0.70       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:38:08,499 - INFO - test loss 0.0244271640436524
2023-07-01 00:38:08,499 - INFO - test acc 0.7149999737739563
2023-07-01 00:38:08,629 - INFO - Distilling data from client: Client42
2023-07-01 00:38:08,629 - INFO - train loss: 0.014695176412194588
2023-07-01 00:38:08,629 - INFO - train acc: 0.8913043737411499
2023-07-01 00:38:08,638 - INFO - report:               precision    recall  f1-score   support

           6       0.65      0.74      0.69        61
           7       0.70      0.71      0.71        66
           8       0.50      0.45      0.48        11
           9       0.83      0.73      0.78        62

    accuracy                           0.71       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:38:08,638 - INFO - test loss 0.023917374996930623
2023-07-01 00:38:08,638 - INFO - test acc 0.7099999785423279
2023-07-01 00:38:08,761 - INFO - Distilling data from client: Client42
2023-07-01 00:38:08,761 - INFO - train loss: 0.014980536767673025
2023-07-01 00:38:08,761 - INFO - train acc: 0.8478261232376099
2023-07-01 00:38:08,768 - INFO - report:               precision    recall  f1-score   support

           6       0.62      0.77      0.69        61
           7       0.77      0.61      0.68        66
           8       0.50      0.55      0.52        11
           9       0.75      0.73      0.74        62

    accuracy                           0.69       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:38:08,768 - INFO - test loss 0.0252854096036256
2023-07-01 00:38:08,768 - INFO - test acc 0.6899999976158142
2023-07-01 00:38:08,894 - INFO - Distilling data from client: Client42
2023-07-01 00:38:08,894 - INFO - train loss: 0.01052118542189082
2023-07-01 00:38:08,894 - INFO - train acc: 0.9347826242446899
2023-07-01 00:38:08,901 - INFO - report:               precision    recall  f1-score   support

           6       0.70      0.75      0.72        61
           7       0.70      0.74      0.72        66
           8       0.46      0.55      0.50        11
           9       0.80      0.66      0.73        62

    accuracy                           0.71       200
   macro avg       0.67      0.68      0.67       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:38:08,901 - INFO - test loss 0.025007296094463168
2023-07-01 00:38:08,901 - INFO - test acc 0.7099999785423279
2023-07-01 00:38:09,028 - INFO - Distilling data from client: Client42
2023-07-01 00:38:09,028 - INFO - train loss: 0.01264649646833279
2023-07-01 00:38:09,028 - INFO - train acc: 0.9021739363670349
2023-07-01 00:38:09,035 - INFO - report:               precision    recall  f1-score   support

           6       0.68      0.72      0.70        61
           7       0.68      0.68      0.68        66
           8       0.36      0.36      0.36        11
           9       0.72      0.68      0.70        62

    accuracy                           0.68       200
   macro avg       0.61      0.61      0.61       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:38:09,035 - INFO - test loss 0.024236074520389075
2023-07-01 00:38:09,035 - INFO - test acc 0.675000011920929
2023-07-01 00:38:09,167 - INFO - Distilling data from client: Client42
2023-07-01 00:38:09,167 - INFO - train loss: 0.010486873599250558
2023-07-01 00:38:09,167 - INFO - train acc: 0.8913043737411499
2023-07-01 00:38:09,183 - INFO - report:               precision    recall  f1-score   support

           6       0.72      0.75      0.74        61
           7       0.72      0.76      0.74        66
           8       0.45      0.45      0.45        11
           9       0.86      0.77      0.81        62

    accuracy                           0.74       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.75      0.74      0.75       200

2023-07-01 00:38:09,183 - INFO - test loss 0.02383673557166148
2023-07-01 00:38:09,183 - INFO - test acc 0.7450000047683716
2023-07-01 00:38:09,312 - INFO - Distilling data from client: Client42
2023-07-01 00:38:09,313 - INFO - train loss: 0.012918091422961242
2023-07-01 00:38:09,313 - INFO - train acc: 0.8804348111152649
2023-07-01 00:38:09,320 - INFO - report:               precision    recall  f1-score   support

           6       0.65      0.72      0.68        61
           7       0.72      0.64      0.68        66
           8       0.55      0.55      0.55        11
           9       0.73      0.74      0.74        62

    accuracy                           0.69       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:38:09,320 - INFO - test loss 0.02459258715435803
2023-07-01 00:38:09,320 - INFO - test acc 0.6899999976158142
2023-07-01 00:38:09,447 - INFO - Distilling data from client: Client42
2023-07-01 00:38:09,447 - INFO - train loss: 0.012318208532532954
2023-07-01 00:38:09,447 - INFO - train acc: 0.9130434989929199
2023-07-01 00:38:09,455 - INFO - report:               precision    recall  f1-score   support

           6       0.70      0.72      0.71        61
           7       0.70      0.73      0.71        66
           8       0.54      0.64      0.58        11
           9       0.84      0.74      0.79        62

    accuracy                           0.73       200
   macro avg       0.69      0.71      0.70       200
weighted avg       0.73      0.72      0.73       200

2023-07-01 00:38:09,455 - INFO - test loss 0.023580007358592205
2023-07-01 00:38:09,455 - INFO - test acc 0.7249999642372131
2023-07-01 00:38:09,584 - INFO - Distilling data from client: Client42
2023-07-01 00:38:09,585 - INFO - train loss: 0.01145263854346878
2023-07-01 00:38:09,585 - INFO - train acc: 0.9130434989929199
2023-07-01 00:38:09,591 - INFO - report:               precision    recall  f1-score   support

           6       0.68      0.84      0.75        61
           7       0.80      0.74      0.77        66
           8       0.40      0.36      0.38        11
           9       0.80      0.69      0.74        62

    accuracy                           0.73       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:38:09,591 - INFO - test loss 0.02316130861163305
2023-07-01 00:38:09,591 - INFO - test acc 0.73499995470047
2023-07-01 00:38:09,720 - INFO - Distilling data from client: Client42
2023-07-01 00:38:09,720 - INFO - train loss: 0.012022678924637029
2023-07-01 00:38:09,720 - INFO - train acc: 0.8913043737411499
2023-07-01 00:38:09,727 - INFO - report:               precision    recall  f1-score   support

           6       0.69      0.74      0.71        61
           7       0.72      0.70      0.71        66
           8       0.50      0.45      0.48        11
           9       0.75      0.74      0.75        62

    accuracy                           0.71       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:38:09,727 - INFO - test loss 0.023375696144558943
2023-07-01 00:38:09,727 - INFO - test acc 0.7099999785423279
2023-07-01 00:38:09,855 - INFO - Distilling data from client: Client42
2023-07-01 00:38:09,855 - INFO - train loss: 0.01100644227379729
2023-07-01 00:38:09,855 - INFO - train acc: 0.9239130616188049
2023-07-01 00:38:09,862 - INFO - report:               precision    recall  f1-score   support

           6       0.70      0.77      0.73        61
           7       0.75      0.70      0.72        66
           8       0.55      0.55      0.55        11
           9       0.79      0.77      0.78        62

    accuracy                           0.73       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:38:09,862 - INFO - test loss 0.0237669344181871
2023-07-01 00:38:09,862 - INFO - test acc 0.73499995470047
2023-07-01 00:38:09,990 - INFO - Distilling data from client: Client42
2023-07-01 00:38:09,990 - INFO - train loss: 0.011577363149608689
2023-07-01 00:38:09,991 - INFO - train acc: 0.8804348111152649
2023-07-01 00:38:09,998 - INFO - report:               precision    recall  f1-score   support

           6       0.69      0.72      0.70        61
           7       0.74      0.70      0.72        66
           8       0.50      0.55      0.52        11
           9       0.77      0.77      0.77        62

    accuracy                           0.72       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:38:09,998 - INFO - test loss 0.02371139817642309
2023-07-01 00:38:09,999 - INFO - test acc 0.7199999690055847
2023-07-01 00:38:10,129 - INFO - Distilling data from client: Client42
2023-07-01 00:38:10,129 - INFO - train loss: 0.011770327708598883
2023-07-01 00:38:10,129 - INFO - train acc: 0.9239130616188049
2023-07-01 00:38:10,137 - INFO - report:               precision    recall  f1-score   support

           6       0.62      0.77      0.69        61
           7       0.71      0.64      0.67        66
           8       0.55      0.55      0.55        11
           9       0.80      0.69      0.74        62

    accuracy                           0.69       200
   macro avg       0.67      0.66      0.66       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:38:10,137 - INFO - test loss 0.023867339758815016
2023-07-01 00:38:10,137 - INFO - test acc 0.6899999976158142
2023-07-01 00:38:10,267 - INFO - Distilling data from client: Client42
2023-07-01 00:38:10,267 - INFO - train loss: 0.010928674858493557
2023-07-01 00:38:10,268 - INFO - train acc: 0.9347826242446899
2023-07-01 00:38:10,281 - INFO - report:               precision    recall  f1-score   support

           6       0.69      0.90      0.78        61
           7       0.82      0.68      0.74        66
           8       0.50      0.45      0.48        11
           9       0.82      0.73      0.77        62

    accuracy                           0.75       200
   macro avg       0.71      0.69      0.69       200
weighted avg       0.76      0.75      0.75       200

2023-07-01 00:38:10,281 - INFO - test loss 0.023550074974052235
2023-07-01 00:38:10,281 - INFO - test acc 0.75
2023-07-01 00:38:10,409 - INFO - Distilling data from client: Client42
2023-07-01 00:38:10,409 - INFO - train loss: 0.010981581771382733
2023-07-01 00:38:10,409 - INFO - train acc: 0.9347826242446899
2023-07-01 00:38:10,416 - INFO - report:               precision    recall  f1-score   support

           6       0.69      0.70      0.70        61
           7       0.71      0.70      0.70        66
           8       0.45      0.45      0.45        11
           9       0.79      0.79      0.79        62

    accuracy                           0.71       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:38:10,416 - INFO - test loss 0.023606558674832086
2023-07-01 00:38:10,416 - INFO - test acc 0.7149999737739563
2023-07-01 00:38:10,543 - INFO - Distilling data from client: Client42
2023-07-01 00:38:10,543 - INFO - train loss: 0.011006879005407592
2023-07-01 00:38:10,543 - INFO - train acc: 0.9021739363670349
2023-07-01 00:38:10,550 - INFO - report:               precision    recall  f1-score   support

           6       0.67      0.74      0.70        61
           7       0.72      0.71      0.72        66
           8       0.41      0.64      0.50        11
           9       0.84      0.69      0.76        62

    accuracy                           0.71       200
   macro avg       0.66      0.69      0.67       200
weighted avg       0.73      0.71      0.71       200

2023-07-01 00:38:10,550 - INFO - test loss 0.02448621412181196
2023-07-01 00:38:10,550 - INFO - test acc 0.7099999785423279
2023-07-01 00:38:10,678 - INFO - Distilling data from client: Client42
2023-07-01 00:38:10,678 - INFO - train loss: 0.010566586399181078
2023-07-01 00:38:10,678 - INFO - train acc: 0.9021739363670349
2023-07-01 00:38:10,685 - INFO - report:               precision    recall  f1-score   support

           6       0.64      0.69      0.66        61
           7       0.67      0.68      0.68        66
           8       0.50      0.45      0.48        11
           9       0.77      0.71      0.74        62

    accuracy                           0.68       200
   macro avg       0.64      0.63      0.64       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:38:10,685 - INFO - test loss 0.023631143886112354
2023-07-01 00:38:10,685 - INFO - test acc 0.6800000071525574
2023-07-01 00:38:10,812 - INFO - Distilling data from client: Client42
2023-07-01 00:38:10,812 - INFO - train loss: 0.010679021545408911
2023-07-01 00:38:10,813 - INFO - train acc: 0.9347826242446899
2023-07-01 00:38:10,819 - INFO - report:               precision    recall  f1-score   support

           6       0.69      0.82      0.75        61
           7       0.76      0.68      0.72        66
           8       0.58      0.64      0.61        11
           9       0.79      0.73      0.76        62

    accuracy                           0.73       200
   macro avg       0.71      0.72      0.71       200
weighted avg       0.74      0.73      0.73       200

2023-07-01 00:38:10,819 - INFO - test loss 0.024774694485065703
2023-07-01 00:38:10,819 - INFO - test acc 0.73499995470047
2023-07-01 00:38:10,944 - INFO - Distilling data from client: Client42
2023-07-01 00:38:10,944 - INFO - train loss: 0.010198003837859158
2023-07-01 00:38:10,944 - INFO - train acc: 0.9347826242446899
2023-07-01 00:38:10,950 - INFO - report:               precision    recall  f1-score   support

           6       0.65      0.72      0.68        61
           7       0.75      0.73      0.74        66
           8       0.44      0.36      0.40        11
           9       0.80      0.76      0.78        62

    accuracy                           0.71       200
   macro avg       0.66      0.64      0.65       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:38:10,951 - INFO - test loss 0.023486429091382127
2023-07-01 00:38:10,951 - INFO - test acc 0.7149999737739563
2023-07-01 00:38:11,076 - INFO - Distilling data from client: Client42
2023-07-01 00:38:11,077 - INFO - train loss: 0.012338432949322766
2023-07-01 00:38:11,077 - INFO - train acc: 0.8586956858634949
2023-07-01 00:38:11,083 - INFO - report:               precision    recall  f1-score   support

           6       0.63      0.79      0.70        61
           7       0.70      0.64      0.67        66
           8       0.50      0.45      0.48        11
           9       0.76      0.66      0.71        62

    accuracy                           0.68       200
   macro avg       0.65      0.63      0.64       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:38:11,083 - INFO - test loss 0.024262584590936023
2023-07-01 00:38:11,083 - INFO - test acc 0.6800000071525574
2023-07-01 00:38:11,211 - INFO - Distilling data from client: Client42
2023-07-01 00:38:11,211 - INFO - train loss: 0.011264617087577493
2023-07-01 00:38:11,212 - INFO - train acc: 0.8804348111152649
2023-07-01 00:38:11,218 - INFO - report:               precision    recall  f1-score   support

           6       0.71      0.77      0.74        61
           7       0.70      0.76      0.73        66
           8       0.36      0.36      0.36        11
           9       0.81      0.68      0.74        62

    accuracy                           0.71       200
   macro avg       0.65      0.64      0.64       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:38:11,218 - INFO - test loss 0.024366692262973225
2023-07-01 00:38:11,218 - INFO - test acc 0.7149999737739563
2023-07-01 00:38:11,344 - INFO - Distilling data from client: Client42
2023-07-01 00:38:11,344 - INFO - train loss: 0.01115209020316028
2023-07-01 00:38:11,345 - INFO - train acc: 0.9239130616188049
2023-07-01 00:38:11,351 - INFO - report:               precision    recall  f1-score   support

           6       0.64      0.77      0.70        61
           7       0.68      0.67      0.67        66
           8       0.50      0.45      0.48        11
           9       0.81      0.68      0.74        62

    accuracy                           0.69       200
   macro avg       0.66      0.64      0.65       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:38:11,351 - INFO - test loss 0.02392062099456428
2023-07-01 00:38:11,351 - INFO - test acc 0.6899999976158142
2023-07-01 00:38:11,478 - INFO - Distilling data from client: Client42
2023-07-01 00:38:11,479 - INFO - train loss: 0.010639873397901104
2023-07-01 00:38:11,479 - INFO - train acc: 0.9130434989929199
2023-07-01 00:38:11,486 - INFO - report:               precision    recall  f1-score   support

           6       0.65      0.72      0.68        61
           7       0.70      0.67      0.68        66
           8       0.50      0.45      0.48        11
           9       0.76      0.73      0.74        62

    accuracy                           0.69       200
   macro avg       0.65      0.64      0.65       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:38:11,486 - INFO - test loss 0.023896677364351384
2023-07-01 00:38:11,486 - INFO - test acc 0.6899999976158142
2023-07-01 00:38:11,610 - INFO - Distilling data from client: Client42
2023-07-01 00:38:11,611 - INFO - train loss: 0.009193100128934343
2023-07-01 00:38:11,611 - INFO - train acc: 0.9239130616188049
2023-07-01 00:38:11,618 - INFO - report:               precision    recall  f1-score   support

           6       0.63      0.72      0.67        61
           7       0.73      0.70      0.71        66
           8       0.50      0.55      0.52        11
           9       0.78      0.69      0.74        62

    accuracy                           0.69       200
   macro avg       0.66      0.66      0.66       200
weighted avg       0.70      0.69      0.70       200

2023-07-01 00:38:11,618 - INFO - test loss 0.024200031596559975
2023-07-01 00:38:11,618 - INFO - test acc 0.6949999928474426
2023-07-01 00:38:11,742 - INFO - Distilling data from client: Client42
2023-07-01 00:38:11,742 - INFO - train loss: 0.00999662701023681
2023-07-01 00:38:11,742 - INFO - train acc: 0.9347826242446899
2023-07-01 00:38:11,749 - INFO - report:               precision    recall  f1-score   support

           6       0.64      0.74      0.69        61
           7       0.70      0.65      0.68        66
           8       0.67      0.55      0.60        11
           9       0.78      0.76      0.77        62

    accuracy                           0.70       200
   macro avg       0.70      0.67      0.68       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:38:11,749 - INFO - test loss 0.02413585044414135
2023-07-01 00:38:11,749 - INFO - test acc 0.7049999833106995
2023-07-01 00:38:11,750 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.00020241737365722656 sec
2023-07-01 00:38:11,751 - WARNING - Finished tracing + transforming fn for pjit in 0.0003216266632080078 sec
2023-07-01 00:38:11,751 - DEBUG - Compiling fn for with global shapes and types [ShapedArray(int64[4]), ShapedArray(int64[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:38:11,753 - WARNING - Finished jaxpr to MLIR module conversion jit(fn) in 0.0013020038604736328 sec
2023-07-01 00:38:11,753 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:11,761 - WARNING - Finished XLA compilation of jit(fn) in 0.007317543029785156 sec
2023-07-01 00:38:11,761 - WARNING - Finished tracing + transforming jit(add) in 0.0002512931823730469 sec
2023-07-01 00:38:11,762 - DEBUG - Compiling add for with global shapes and types [ShapedArray(int64[4]), ShapedArray(int64[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:38:11,763 - WARNING - Finished jaxpr to MLIR module conversion jit(add) in 0.0012996196746826172 sec
2023-07-01 00:38:11,763 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:11,770 - WARNING - Finished XLA compilation of jit(add) in 0.006591320037841797 sec
2023-07-01 00:38:11,771 - WARNING - Finished tracing + transforming jit(select_n) in 0.00024008750915527344 sec
2023-07-01 00:38:11,771 - DEBUG - Compiling select_n for with global shapes and types [ShapedArray(bool[4]), ShapedArray(int64[4]), ShapedArray(int64[4])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:38:11,772 - WARNING - Finished jaxpr to MLIR module conversion jit(select_n) in 0.0010437965393066406 sec
2023-07-01 00:38:11,773 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:11,780 - WARNING - Finished XLA compilation of jit(select_n) in 0.007688999176025391 sec
2023-07-01 00:38:11,782 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.000202178955078125 sec
2023-07-01 00:38:11,782 - WARNING - Finished tracing + transforming jit(convert_element_type) in 0.00014328956604003906 sec
2023-07-01 00:38:11,783 - DEBUG - Compiling convert_element_type for with global shapes and types [ShapedArray(int64[4])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:38:11,784 - WARNING - Finished jaxpr to MLIR module conversion jit(convert_element_type) in 0.0009770393371582031 sec
2023-07-01 00:38:11,784 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:11,790 - WARNING - Finished XLA compilation of jit(convert_element_type) in 0.006410837173461914 sec
2023-07-01 00:38:11,791 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00023031234741210938 sec
2023-07-01 00:38:11,791 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(int32[4])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:38:11,793 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009400844573974609 sec
2023-07-01 00:38:11,793 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:11,798 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.005151987075805664 sec
2023-07-01 00:38:11,799 - WARNING - Finished tracing + transforming jit(gather) in 0.0002377033233642578 sec
2023-07-01 00:38:11,799 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[92,3,32,32]), ShapedArray(int32[4,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:38:11,800 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0010478496551513672 sec
2023-07-01 00:38:11,800 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:11,810 - WARNING - Finished XLA compilation of jit(gather) in 0.009137630462646484 sec
2023-07-01 00:38:11,811 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00024628639221191406 sec
2023-07-01 00:38:11,811 - WARNING - Finished tracing + transforming jit(copy) in 0.00010251998901367188 sec
2023-07-01 00:38:11,811 - DEBUG - Compiling copy for with global shapes and types [ShapedArray(float32[4,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:38:11,812 - WARNING - Finished jaxpr to MLIR module conversion jit(copy) in 0.0008740425109863281 sec
2023-07-01 00:38:11,813 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:11,818 - WARNING - Finished XLA compilation of jit(copy) in 0.005089521408081055 sec
2023-07-01 00:38:11,827 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:11,837 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:11,845 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:11,853 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:11,861 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:11,870 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:11,878 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:11,887 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:11,888 - WARNING - Finished tracing + transforming _unstack for pjit in 0.0008945465087890625 sec
2023-07-01 00:38:11,889 - DEBUG - Compiling _unstack for with global shapes and types [ShapedArray(float32[4,3,32,32])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:38:11,891 - WARNING - Finished jaxpr to MLIR module conversion jit(_unstack) in 0.0017070770263671875 sec
2023-07-01 00:38:11,891 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:11,907 - WARNING - Finished XLA compilation of jit(_unstack) in 0.015571832656860352 sec
2023-07-01 00:38:11,916 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:11,926 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:11,935 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:11,944 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:12,412 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client42//synthetic.png
2023-07-01 00:38:12,423 - INFO - c: 0.0 and total_data_in_this_class: 260
2023-07-01 00:38:12,423 - INFO - c: 1.0 and total_data_in_this_class: 8
2023-07-01 00:38:12,423 - INFO - c: 2.0 and total_data_in_this_class: 254
2023-07-01 00:38:12,423 - INFO - c: 3.0 and total_data_in_this_class: 277
2023-07-01 00:38:12,423 - INFO - c: 0.0 and total_data_in_this_class: 73
2023-07-01 00:38:12,423 - INFO - c: 1.0 and total_data_in_this_class: 2
2023-07-01 00:38:12,423 - INFO - c: 2.0 and total_data_in_this_class: 69
2023-07-01 00:38:12,423 - INFO - c: 3.0 and total_data_in_this_class: 56
2023-07-01 00:38:12,444 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002663135528564453 sec
2023-07-01 00:38:12,444 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:38:12,446 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012607574462890625 sec
2023-07-01 00:38:12,446 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:12,457 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010916948318481445 sec
2023-07-01 00:38:12,459 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002548694610595703 sec
2023-07-01 00:38:12,459 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:38:12,460 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009679794311523438 sec
2023-07-01 00:38:12,461 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:12,469 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.00857400894165039 sec
2023-07-01 00:38:12,473 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013637542724609375 sec
2023-07-01 00:38:12,474 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012731552124023438 sec
2023-07-01 00:38:12,474 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003223419189453125 sec
2023-07-01 00:38:12,476 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00021719932556152344 sec
2023-07-01 00:38:12,476 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001251697540283203 sec
2023-07-01 00:38:12,477 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003046989440917969 sec
2023-07-01 00:38:12,478 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025010108947753906 sec
2023-07-01 00:38:12,478 - WARNING - Finished tracing + transforming absolute for pjit in 0.00017547607421875 sec
2023-07-01 00:38:12,479 - WARNING - Finished tracing + transforming fn for pjit in 0.00028133392333984375 sec
2023-07-01 00:38:12,479 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.0003247261047363281 sec
2023-07-01 00:38:12,480 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002067089080810547 sec
2023-07-01 00:38:12,481 - WARNING - Finished tracing + transforming fn for pjit in 0.0002319812774658203 sec
2023-07-01 00:38:12,482 - WARNING - Finished tracing + transforming fn for pjit in 0.0002741813659667969 sec
2023-07-01 00:38:12,483 - WARNING - Finished tracing + transforming fn for pjit in 0.00023055076599121094 sec
2023-07-01 00:38:12,483 - WARNING - Finished tracing + transforming fn for pjit in 0.0002703666687011719 sec
2023-07-01 00:38:12,484 - WARNING - Finished tracing + transforming fn for pjit in 0.00022864341735839844 sec
2023-07-01 00:38:12,486 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001723766326904297 sec
2023-07-01 00:38:12,487 - WARNING - Finished tracing + transforming fn for pjit in 0.0002315044403076172 sec
2023-07-01 00:38:12,488 - WARNING - Finished tracing + transforming fn for pjit in 0.00023293495178222656 sec
2023-07-01 00:38:12,491 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003609657287597656 sec
2023-07-01 00:38:12,491 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009505748748779297 sec
2023-07-01 00:38:12,492 - WARNING - Finished tracing + transforming fn for pjit in 0.00023055076599121094 sec
2023-07-01 00:38:12,493 - WARNING - Finished tracing + transforming fn for pjit in 0.0002219676971435547 sec
2023-07-01 00:38:12,494 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029754638671875 sec
2023-07-01 00:38:12,495 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026035308837890625 sec
2023-07-01 00:38:12,495 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001747608184814453 sec
2023-07-01 00:38:12,496 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002644062042236328 sec
2023-07-01 00:38:12,497 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023865699768066406 sec
2023-07-01 00:38:12,497 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002262592315673828 sec
2023-07-01 00:38:12,498 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003502368927001953 sec
2023-07-01 00:38:12,499 - WARNING - Finished tracing + transforming _where for pjit in 0.0009665489196777344 sec
2023-07-01 00:38:12,499 - WARNING - Finished tracing + transforming fn for pjit in 0.00026345252990722656 sec
2023-07-01 00:38:12,500 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025844573974609375 sec
2023-07-01 00:38:12,501 - WARNING - Finished tracing + transforming fn for pjit in 0.00023984909057617188 sec
2023-07-01 00:38:12,501 - WARNING - Finished tracing + transforming fn for pjit in 0.0002200603485107422 sec
2023-07-01 00:38:12,502 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021767616271972656 sec
2023-07-01 00:38:12,503 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002639293670654297 sec
2023-07-01 00:38:12,503 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002486705780029297 sec
2023-07-01 00:38:12,504 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002627372741699219 sec
2023-07-01 00:38:12,505 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002319812774658203 sec
2023-07-01 00:38:12,506 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002257823944091797 sec
2023-07-01 00:38:12,506 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00026154518127441406 sec
2023-07-01 00:38:12,507 - WARNING - Finished tracing + transforming _where for pjit in 0.0008556842803955078 sec
2023-07-01 00:38:12,508 - WARNING - Finished tracing + transforming fn for pjit in 0.00025916099548339844 sec
2023-07-01 00:38:12,508 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002570152282714844 sec
2023-07-01 00:38:12,509 - WARNING - Finished tracing + transforming fn for pjit in 0.00022482872009277344 sec
2023-07-01 00:38:12,513 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002789497375488281 sec
2023-07-01 00:38:12,514 - WARNING - Finished tracing + transforming fn for pjit in 0.0003414154052734375 sec
2023-07-01 00:38:12,515 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002734661102294922 sec
2023-07-01 00:38:12,516 - WARNING - Finished tracing + transforming fn for pjit in 0.00022459030151367188 sec
2023-07-01 00:38:12,519 - WARNING - Finished tracing + transforming fn for pjit in 0.00022363662719726562 sec
2023-07-01 00:38:12,521 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016617774963378906 sec
2023-07-01 00:38:12,522 - WARNING - Finished tracing + transforming fn for pjit in 0.0002982616424560547 sec
2023-07-01 00:38:12,523 - WARNING - Finished tracing + transforming fn for pjit in 0.00022912025451660156 sec
2023-07-01 00:38:12,541 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0688016414642334 sec
2023-07-01 00:38:12,544 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012373924255371094 sec
2023-07-01 00:38:12,545 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012111663818359375 sec
2023-07-01 00:38:12,545 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002758502960205078 sec
2023-07-01 00:38:12,547 - WARNING - Finished tracing + transforming fn for pjit in 0.00022029876708984375 sec
2023-07-01 00:38:12,548 - WARNING - Finished tracing + transforming fn for pjit in 0.00025963783264160156 sec
2023-07-01 00:38:12,549 - WARNING - Finished tracing + transforming fn for pjit in 0.00021648406982421875 sec
2023-07-01 00:38:12,555 - WARNING - Finished tracing + transforming fn for pjit in 0.00022792816162109375 sec
2023-07-01 00:38:12,556 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022602081298828125 sec
2023-07-01 00:38:12,557 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002562999725341797 sec
2023-07-01 00:38:12,557 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001728534698486328 sec
2023-07-01 00:38:12,558 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032520294189453125 sec
2023-07-01 00:38:12,559 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002269744873046875 sec
2023-07-01 00:38:12,559 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002224445343017578 sec
2023-07-01 00:38:12,560 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00027942657470703125 sec
2023-07-01 00:38:12,561 - WARNING - Finished tracing + transforming _where for pjit in 0.0008857250213623047 sec
2023-07-01 00:38:12,561 - WARNING - Finished tracing + transforming fn for pjit in 0.00026154518127441406 sec
2023-07-01 00:38:12,562 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002548694610595703 sec
2023-07-01 00:38:12,563 - WARNING - Finished tracing + transforming fn for pjit in 0.00022101402282714844 sec
2023-07-01 00:38:12,563 - WARNING - Finished tracing + transforming fn for pjit in 0.0002803802490234375 sec
2023-07-01 00:38:12,575 - WARNING - Finished tracing + transforming fn for pjit in 0.00022220611572265625 sec
2023-07-01 00:38:12,596 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05364704132080078 sec
2023-07-01 00:38:12,597 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012302398681640625 sec
2023-07-01 00:38:12,598 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013637542724609375 sec
2023-07-01 00:38:12,598 - WARNING - Finished tracing + transforming _where for pjit in 0.0006177425384521484 sec
2023-07-01 00:38:12,599 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002932548522949219 sec
2023-07-01 00:38:12,599 - WARNING - Finished tracing + transforming trace for pjit in 0.002538919448852539 sec
2023-07-01 00:38:12,601 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010609626770019531 sec
2023-07-01 00:38:12,602 - WARNING - Finished tracing + transforming tril for pjit in 0.0006732940673828125 sec
2023-07-01 00:38:12,603 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0017974376678466797 sec
2023-07-01 00:38:12,604 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010919570922851562 sec
2023-07-01 00:38:12,604 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00011038780212402344 sec
2023-07-01 00:38:12,606 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014355182647705078 sec
2023-07-01 00:38:12,610 - WARNING - Finished tracing + transforming _solve for pjit in 0.009322166442871094 sec
2023-07-01 00:38:12,611 - WARNING - Finished tracing + transforming dot for pjit in 0.00032210350036621094 sec
2023-07-01 00:38:12,613 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.14227032661437988 sec
2023-07-01 00:38:12,615 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:38:12,648 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.032926082611083984 sec
2023-07-01 00:38:12,648 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:12,774 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1255810260772705 sec
2023-07-01 00:38:12,799 - INFO - initial test loss: 0.02877518864260161
2023-07-01 00:38:12,799 - INFO - initial test acc: 0.6800000071525574
2023-07-01 00:38:12,805 - WARNING - Finished tracing + transforming dot for pjit in 0.0003807544708251953 sec
2023-07-01 00:38:12,806 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00030040740966796875 sec
2023-07-01 00:38:12,807 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00037789344787597656 sec
2023-07-01 00:38:12,807 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010051727294921875 sec
2023-07-01 00:38:12,808 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00019478797912597656 sec
2023-07-01 00:38:12,809 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00018453598022460938 sec
2023-07-01 00:38:12,809 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002486705780029297 sec
2023-07-01 00:38:12,810 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003669261932373047 sec
2023-07-01 00:38:12,811 - WARNING - Finished tracing + transforming _mean for pjit in 0.001056671142578125 sec
2023-07-01 00:38:12,811 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009948492050170898 sec
2023-07-01 00:38:12,820 - WARNING - Finished tracing + transforming fn for pjit in 0.0003123283386230469 sec
2023-07-01 00:38:12,820 - WARNING - Finished tracing + transforming fn for pjit in 0.00026607513427734375 sec
2023-07-01 00:38:12,821 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022530555725097656 sec
2023-07-01 00:38:12,822 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002593994140625 sec
2023-07-01 00:38:12,822 - WARNING - Finished tracing + transforming _where for pjit in 0.0008423328399658203 sec
2023-07-01 00:38:12,830 - WARNING - Finished tracing + transforming fn for pjit in 0.0002543926239013672 sec
2023-07-01 00:38:12,831 - WARNING - Finished tracing + transforming fn for pjit in 0.00026416778564453125 sec
2023-07-01 00:38:12,832 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002155303955078125 sec
2023-07-01 00:38:12,832 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002627372741699219 sec
2023-07-01 00:38:12,833 - WARNING - Finished tracing + transforming _where for pjit in 0.0008447170257568359 sec
2023-07-01 00:38:12,867 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00014662742614746094 sec
2023-07-01 00:38:12,922 - WARNING - Finished tracing + transforming fn for pjit in 0.0002732276916503906 sec
2023-07-01 00:38:12,923 - WARNING - Finished tracing + transforming fn for pjit in 0.0002334117889404297 sec
2023-07-01 00:38:12,924 - WARNING - Finished tracing + transforming square for pjit in 0.0001671314239501953 sec
2023-07-01 00:38:12,926 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022292137145996094 sec
2023-07-01 00:38:12,928 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00028014183044433594 sec
2023-07-01 00:38:12,928 - WARNING - Finished tracing + transforming fn for pjit in 0.0002644062042236328 sec
2023-07-01 00:38:12,929 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002243518829345703 sec
2023-07-01 00:38:12,929 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024175643920898438 sec
2023-07-01 00:38:12,930 - WARNING - Finished tracing + transforming fn for pjit in 0.00026416778564453125 sec
2023-07-01 00:38:12,931 - WARNING - Finished tracing + transforming fn for pjit in 0.00022721290588378906 sec
2023-07-01 00:38:12,931 - WARNING - Finished tracing + transforming square for pjit in 0.00017333030700683594 sec
2023-07-01 00:38:12,933 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021791458129882812 sec
2023-07-01 00:38:12,935 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00019884109497070312 sec
2023-07-01 00:38:12,935 - WARNING - Finished tracing + transforming fn for pjit in 0.0002646446228027344 sec
2023-07-01 00:38:12,936 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021529197692871094 sec
2023-07-01 00:38:12,937 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002474784851074219 sec
2023-07-01 00:38:12,937 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13674044609069824 sec
2023-07-01 00:38:12,941 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,10]), ShapedArray(float32[507,10]), ShapedArray(float32[507,10]), ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,10]), ShapedArray(float32[507,3,32,32]), ShapedArray(float32[507,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:38:13,004 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06301259994506836 sec
2023-07-01 00:38:13,005 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:13,336 - WARNING - Finished XLA compilation of jit(update_fn) in 0.33096742630004883 sec
2023-07-01 00:38:14,555 - INFO - Distilling data from client: Client43
2023-07-01 00:38:14,555 - INFO - train loss: 0.0032530716950191934
2023-07-01 00:38:14,555 - INFO - train acc: 0.9822485446929932
2023-07-01 00:38:14,617 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        73
           1       0.00      0.00      0.00         2
           2       0.63      0.62      0.63        69
           3       0.57      0.62      0.60        56

    accuracy                           0.66       200
   macro avg       0.49      0.49      0.49       200
weighted avg       0.65      0.66      0.65       200

2023-07-01 00:38:14,617 - INFO - test loss 0.026518904286963192
2023-07-01 00:38:14,617 - INFO - test acc 0.6549999713897705
2023-07-01 00:38:15,837 - INFO - Distilling data from client: Client43
2023-07-01 00:38:15,837 - INFO - train loss: 0.0014983460482405322
2023-07-01 00:38:15,837 - INFO - train acc: 1.0
2023-07-01 00:38:15,899 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.78      0.77        73
           1       0.00      0.00      0.00         2
           2       0.62      0.59      0.61        69
           3       0.59      0.61      0.60        56

    accuracy                           0.66       200
   macro avg       0.49      0.50      0.49       200
weighted avg       0.65      0.66      0.66       200

2023-07-01 00:38:15,899 - INFO - test loss 0.02720173402095343
2023-07-01 00:38:15,899 - INFO - test acc 0.6599999666213989
2023-07-01 00:38:17,112 - INFO - Distilling data from client: Client43
2023-07-01 00:38:17,112 - INFO - train loss: 0.0011514236631465517
2023-07-01 00:38:17,112 - INFO - train acc: 1.0
2023-07-01 00:38:17,172 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.79      0.76        73
           1       0.00      0.00      0.00         2
           2       0.65      0.61      0.63        69
           3       0.62      0.62      0.62        56

    accuracy                           0.68       200
   macro avg       0.50      0.51      0.50       200
weighted avg       0.67      0.68      0.67       200

2023-07-01 00:38:17,173 - INFO - test loss 0.027396099201083702
2023-07-01 00:38:17,173 - INFO - test acc 0.675000011920929
2023-07-01 00:38:18,384 - INFO - Distilling data from client: Client43
2023-07-01 00:38:18,384 - INFO - train loss: 0.0010099381871126088
2023-07-01 00:38:18,384 - INFO - train acc: 1.0
2023-07-01 00:38:18,407 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.81      0.77        73
           1       0.00      0.00      0.00         2
           2       0.65      0.59      0.62        69
           3       0.61      0.62      0.62        56

    accuracy                           0.68       200
   macro avg       0.50      0.51      0.50       200
weighted avg       0.67      0.68      0.67       200

2023-07-01 00:38:18,408 - INFO - test loss 0.027921059647386677
2023-07-01 00:38:18,408 - INFO - test acc 0.675000011920929
2023-07-01 00:38:19,622 - INFO - Distilling data from client: Client43
2023-07-01 00:38:19,622 - INFO - train loss: 0.0008987066540585481
2023-07-01 00:38:19,622 - INFO - train acc: 1.0
2023-07-01 00:38:19,645 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.74      0.74        73
           1       0.00      0.00      0.00         2
           2       0.59      0.58      0.58        69
           3       0.58      0.61      0.59        56

    accuracy                           0.64       200
   macro avg       0.48      0.48      0.48       200
weighted avg       0.63      0.64      0.64       200

2023-07-01 00:38:19,645 - INFO - test loss 0.028026302400934552
2023-07-01 00:38:19,645 - INFO - test acc 0.6399999856948853
2023-07-01 00:38:20,851 - INFO - Distilling data from client: Client43
2023-07-01 00:38:20,851 - INFO - train loss: 0.0007417513213819393
2023-07-01 00:38:20,851 - INFO - train acc: 1.0
2023-07-01 00:38:20,875 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.73      0.73        73
           1       0.00      0.00      0.00         2
           2       0.61      0.57      0.59        69
           3       0.59      0.68      0.63        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.49       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:38:20,875 - INFO - test loss 0.0278551693918387
2023-07-01 00:38:20,875 - INFO - test acc 0.6499999761581421
2023-07-01 00:38:22,084 - INFO - Distilling data from client: Client43
2023-07-01 00:38:22,084 - INFO - train loss: 0.0009081198279105161
2023-07-01 00:38:22,084 - INFO - train acc: 1.0
2023-07-01 00:38:22,107 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.77      0.76        73
           1       0.00      0.00      0.00         2
           2       0.60      0.55      0.58        69
           3       0.60      0.66      0.63        56

    accuracy                           0.66       200
   macro avg       0.49      0.49      0.49       200
weighted avg       0.65      0.66      0.65       200

2023-07-01 00:38:22,108 - INFO - test loss 0.028066554443523815
2023-07-01 00:38:22,108 - INFO - test acc 0.6549999713897705
2023-07-01 00:38:23,326 - INFO - Distilling data from client: Client43
2023-07-01 00:38:23,326 - INFO - train loss: 0.0007054018915795668
2023-07-01 00:38:23,326 - INFO - train acc: 1.0
2023-07-01 00:38:23,351 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.73      0.74        73
           1       0.00      0.00      0.00         2
           2       0.58      0.55      0.57        69
           3       0.55      0.64      0.60        56

    accuracy                           0.64       200
   macro avg       0.47      0.48      0.48       200
weighted avg       0.63      0.64      0.63       200

2023-07-01 00:38:23,351 - INFO - test loss 0.02854584592359347
2023-07-01 00:38:23,352 - INFO - test acc 0.6349999904632568
2023-07-01 00:38:24,580 - INFO - Distilling data from client: Client43
2023-07-01 00:38:24,580 - INFO - train loss: 0.0005393669574705392
2023-07-01 00:38:24,580 - INFO - train acc: 1.0
2023-07-01 00:38:24,604 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.71      0.72        73
           1       0.00      0.00      0.00         2
           2       0.58      0.62      0.60        69
           3       0.58      0.57      0.58        56

    accuracy                           0.64       200
   macro avg       0.47      0.48      0.48       200
weighted avg       0.63      0.64      0.63       200

2023-07-01 00:38:24,604 - INFO - test loss 0.028777033409688763
2023-07-01 00:38:24,604 - INFO - test acc 0.6349999904632568
2023-07-01 00:38:25,830 - INFO - Distilling data from client: Client43
2023-07-01 00:38:25,830 - INFO - train loss: 0.0006626970239039107
2023-07-01 00:38:25,830 - INFO - train acc: 1.0
2023-07-01 00:38:25,853 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.75      0.73        73
           1       0.00      0.00      0.00         2
           2       0.60      0.55      0.58        69
           3       0.58      0.62      0.60        56

    accuracy                           0.64       200
   macro avg       0.48      0.48      0.48       200
weighted avg       0.63      0.64      0.64       200

2023-07-01 00:38:25,853 - INFO - test loss 0.02769024278179768
2023-07-01 00:38:25,853 - INFO - test acc 0.6399999856948853
2023-07-01 00:38:27,075 - INFO - Distilling data from client: Client43
2023-07-01 00:38:27,075 - INFO - train loss: 0.0005821153643543674
2023-07-01 00:38:27,075 - INFO - train acc: 1.0
2023-07-01 00:38:27,100 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.78      0.76        73
           1       0.00      0.00      0.00         2
           2       0.62      0.57      0.59        69
           3       0.60      0.64      0.62        56

    accuracy                           0.66       200
   macro avg       0.49      0.50      0.49       200
weighted avg       0.65      0.66      0.66       200

2023-07-01 00:38:27,100 - INFO - test loss 0.028166403651325205
2023-07-01 00:38:27,100 - INFO - test acc 0.6599999666213989
2023-07-01 00:38:28,308 - INFO - Distilling data from client: Client43
2023-07-01 00:38:28,308 - INFO - train loss: 0.0005197772806166855
2023-07-01 00:38:28,308 - INFO - train acc: 1.0
2023-07-01 00:38:28,332 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.75      0.75        73
           1       0.00      0.00      0.00         2
           2       0.59      0.59      0.59        69
           3       0.60      0.61      0.60        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.49       200
weighted avg       0.64      0.65      0.65       200

2023-07-01 00:38:28,332 - INFO - test loss 0.028636924319745603
2023-07-01 00:38:28,332 - INFO - test acc 0.6499999761581421
2023-07-01 00:38:29,543 - INFO - Distilling data from client: Client43
2023-07-01 00:38:29,543 - INFO - train loss: 0.0004952227721684854
2023-07-01 00:38:29,543 - INFO - train acc: 1.0
2023-07-01 00:38:29,566 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.75      0.76        73
           1       0.00      0.00      0.00         2
           2       0.60      0.55      0.58        69
           3       0.57      0.66      0.61        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.49       200
weighted avg       0.65      0.65      0.65       200

2023-07-01 00:38:29,566 - INFO - test loss 0.02848466433483027
2023-07-01 00:38:29,566 - INFO - test acc 0.6499999761581421
2023-07-01 00:38:30,790 - INFO - Distilling data from client: Client43
2023-07-01 00:38:30,790 - INFO - train loss: 0.00046405373508802937
2023-07-01 00:38:30,790 - INFO - train acc: 1.0
2023-07-01 00:38:30,813 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.77      0.76        73
           1       0.00      0.00      0.00         2
           2       0.60      0.55      0.58        69
           3       0.58      0.64      0.61        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.49       200
weighted avg       0.64      0.65      0.65       200

2023-07-01 00:38:30,813 - INFO - test loss 0.02802004097134343
2023-07-01 00:38:30,813 - INFO - test acc 0.6499999761581421
2023-07-01 00:38:32,030 - INFO - Distilling data from client: Client43
2023-07-01 00:38:32,030 - INFO - train loss: 0.0004972211068425713
2023-07-01 00:38:32,030 - INFO - train acc: 1.0
2023-07-01 00:38:32,052 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.77      0.75        73
           1       0.00      0.00      0.00         2
           2       0.61      0.55      0.58        69
           3       0.57      0.62      0.60        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.48       200
weighted avg       0.64      0.65      0.64       200

2023-07-01 00:38:32,053 - INFO - test loss 0.028509382678599534
2023-07-01 00:38:32,053 - INFO - test acc 0.6449999809265137
2023-07-01 00:38:33,275 - INFO - Distilling data from client: Client43
2023-07-01 00:38:33,276 - INFO - train loss: 0.00047899284742806324
2023-07-01 00:38:33,276 - INFO - train acc: 1.0
2023-07-01 00:38:33,299 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.73      0.72        73
           1       0.00      0.00      0.00         2
           2       0.62      0.57      0.59        69
           3       0.59      0.66      0.62        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.48       200
weighted avg       0.64      0.65      0.64       200

2023-07-01 00:38:33,300 - INFO - test loss 0.027674640058994244
2023-07-01 00:38:33,300 - INFO - test acc 0.6449999809265137
2023-07-01 00:38:34,510 - INFO - Distilling data from client: Client43
2023-07-01 00:38:34,510 - INFO - train loss: 0.00036048968907583537
2023-07-01 00:38:34,510 - INFO - train acc: 1.0
2023-07-01 00:38:34,533 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        73
           1       0.00      0.00      0.00         2
           2       0.60      0.55      0.58        69
           3       0.58      0.68      0.62        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.48       200
weighted avg       0.64      0.65      0.64       200

2023-07-01 00:38:34,533 - INFO - test loss 0.028556544296446683
2023-07-01 00:38:34,533 - INFO - test acc 0.6449999809265137
2023-07-01 00:38:35,754 - INFO - Distilling data from client: Client43
2023-07-01 00:38:35,754 - INFO - train loss: 0.0004003782322595346
2023-07-01 00:38:35,754 - INFO - train acc: 1.0
2023-07-01 00:38:35,777 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.68      0.70        73
           1       0.00      0.00      0.00         2
           2       0.59      0.59      0.59        69
           3       0.60      0.66      0.63        56

    accuracy                           0.64       200
   macro avg       0.48      0.48      0.48       200
weighted avg       0.64      0.64      0.64       200

2023-07-01 00:38:35,777 - INFO - test loss 0.028203821623872038
2023-07-01 00:38:35,777 - INFO - test acc 0.6399999856948853
2023-07-01 00:38:36,991 - INFO - Distilling data from client: Client43
2023-07-01 00:38:36,991 - INFO - train loss: 0.0004125390461450129
2023-07-01 00:38:36,991 - INFO - train acc: 1.0
2023-07-01 00:38:37,015 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.75      0.75        73
           1       0.00      0.00      0.00         2
           2       0.62      0.59      0.61        69
           3       0.58      0.62      0.60        56

    accuracy                           0.66       200
   macro avg       0.49      0.49      0.49       200
weighted avg       0.65      0.66      0.65       200

2023-07-01 00:38:37,015 - INFO - test loss 0.028111567404421358
2023-07-01 00:38:37,015 - INFO - test acc 0.6549999713897705
2023-07-01 00:38:38,241 - INFO - Distilling data from client: Client43
2023-07-01 00:38:38,241 - INFO - train loss: 0.00044501121945075286
2023-07-01 00:38:38,241 - INFO - train acc: 1.0
2023-07-01 00:38:38,265 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.74      0.73        73
           1       0.00      0.00      0.00         2
           2       0.60      0.61      0.60        69
           3       0.61      0.61      0.61        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.49       200
weighted avg       0.64      0.65      0.65       200

2023-07-01 00:38:38,265 - INFO - test loss 0.028169616184853852
2023-07-01 00:38:38,265 - INFO - test acc 0.6499999761581421
2023-07-01 00:38:39,475 - INFO - Distilling data from client: Client43
2023-07-01 00:38:39,475 - INFO - train loss: 0.0005110789361121822
2023-07-01 00:38:39,475 - INFO - train acc: 1.0
2023-07-01 00:38:39,499 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.74      0.72        73
           1       0.00      0.00      0.00         2
           2       0.63      0.55      0.59        69
           3       0.58      0.66      0.62        56

    accuracy                           0.65       200
   macro avg       0.48      0.49      0.48       200
weighted avg       0.64      0.65      0.64       200

2023-07-01 00:38:39,499 - INFO - test loss 0.02825561654523313
2023-07-01 00:38:39,499 - INFO - test acc 0.6449999809265137
2023-07-01 00:38:40,711 - INFO - Distilling data from client: Client43
2023-07-01 00:38:40,711 - INFO - train loss: 0.00043221826647121304
2023-07-01 00:38:40,711 - INFO - train acc: 1.0
2023-07-01 00:38:40,737 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.75      0.74        73
           1       0.00      0.00      0.00         2
           2       0.62      0.58      0.60        69
           3       0.59      0.64      0.62        56

    accuracy                           0.66       200
   macro avg       0.49      0.49      0.49       200
weighted avg       0.65      0.66      0.65       200

2023-07-01 00:38:40,737 - INFO - test loss 0.02816652583453306
2023-07-01 00:38:40,737 - INFO - test acc 0.6549999713897705
2023-07-01 00:38:41,948 - INFO - Distilling data from client: Client43
2023-07-01 00:38:41,948 - INFO - train loss: 0.0004222369313307684
2023-07-01 00:38:41,948 - INFO - train acc: 1.0
2023-07-01 00:38:41,973 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.74      0.74        73
           1       0.00      0.00      0.00         2
           2       0.64      0.61      0.62        69
           3       0.62      0.68      0.65        56

    accuracy                           0.67       200
   macro avg       0.50      0.51      0.50       200
weighted avg       0.66      0.67      0.67       200

2023-07-01 00:38:41,973 - INFO - test loss 0.028368683501606356
2023-07-01 00:38:41,973 - INFO - test acc 0.6699999570846558
2023-07-01 00:38:43,185 - INFO - Distilling data from client: Client43
2023-07-01 00:38:43,185 - INFO - train loss: 0.0004609794492480004
2023-07-01 00:38:43,185 - INFO - train acc: 1.0
2023-07-01 00:38:43,210 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.75      0.75        73
           1       0.00      0.00      0.00         2
           2       0.63      0.58      0.61        69
           3       0.57      0.64      0.61        56

    accuracy                           0.66       200
   macro avg       0.49      0.49      0.49       200
weighted avg       0.65      0.66      0.65       200

2023-07-01 00:38:43,210 - INFO - test loss 0.028574101870943763
2023-07-01 00:38:43,210 - INFO - test acc 0.6549999713897705
2023-07-01 00:38:44,416 - INFO - Distilling data from client: Client43
2023-07-01 00:38:44,416 - INFO - train loss: 0.00035412961845747985
2023-07-01 00:38:44,416 - INFO - train acc: 1.0
2023-07-01 00:38:44,439 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.70      0.71        73
           1       0.00      0.00      0.00         2
           2       0.57      0.58      0.58        69
           3       0.59      0.62      0.61        56

    accuracy                           0.63       200
   macro avg       0.47      0.48      0.47       200
weighted avg       0.63      0.63      0.63       200

2023-07-01 00:38:44,439 - INFO - test loss 0.028548055473504967
2023-07-01 00:38:44,439 - INFO - test acc 0.6299999952316284
2023-07-01 00:38:44,441 - WARNING - Finished tracing + transforming jit(gather) in 0.0002446174621582031 sec
2023-07-01 00:38:44,441 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[507,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:38:44,443 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0011680126190185547 sec
2023-07-01 00:38:44,443 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:44,453 - WARNING - Finished XLA compilation of jit(gather) in 0.009678363800048828 sec
2023-07-01 00:38:44,463 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:44,472 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:44,480 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:44,489 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:44,497 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:44,506 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:44,515 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:44,524 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:44,533 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:38:45,487 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client43//synthetic.png
2023-07-01 00:38:45,498 - INFO - c: 0.0 and total_data_in_this_class: 263
2023-07-01 00:38:45,498 - INFO - c: 5.0 and total_data_in_this_class: 270
2023-07-01 00:38:45,499 - INFO - c: 9.0 and total_data_in_this_class: 266
2023-07-01 00:38:45,499 - INFO - c: 0.0 and total_data_in_this_class: 70
2023-07-01 00:38:45,499 - INFO - c: 5.0 and total_data_in_this_class: 63
2023-07-01 00:38:45,499 - INFO - c: 9.0 and total_data_in_this_class: 67
2023-07-01 00:38:45,569 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04758143424987793 sec
2023-07-01 00:38:45,615 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.045334815979003906 sec
2023-07-01 00:38:45,621 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.10029077529907227 sec
2023-07-01 00:38:45,622 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:38:45,656 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.033223628997802734 sec
2023-07-01 00:38:45,656 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:45,780 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1241309642791748 sec
2023-07-01 00:38:45,805 - INFO - initial test loss: 0.02151620937668398
2023-07-01 00:38:45,805 - INFO - initial test acc: 0.7299999594688416
2023-07-01 00:38:45,815 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.007379293441772461 sec
2023-07-01 00:38:45,925 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11841154098510742 sec
2023-07-01 00:38:45,929 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10]), ShapedArray(float32[525,3,32,32]), ShapedArray(float32[525,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:38:45,992 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06263113021850586 sec
2023-07-01 00:38:45,992 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:38:46,320 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32761526107788086 sec
2023-07-01 00:38:47,598 - INFO - Distilling data from client: Client44
2023-07-01 00:38:47,598 - INFO - train loss: 0.0017533485633781816
2023-07-01 00:38:47,598 - INFO - train acc: 1.0
2023-07-01 00:38:47,662 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.71      0.75        70
           5       0.72      0.81      0.76        63
           9       0.68      0.66      0.67        67

    accuracy                           0.73       200
   macro avg       0.73      0.73      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:38:47,662 - INFO - test loss 0.02163872164344757
2023-07-01 00:38:47,662 - INFO - test acc 0.7249999642372131
2023-07-01 00:38:48,911 - INFO - Distilling data from client: Client44
2023-07-01 00:38:48,911 - INFO - train loss: 0.0008228041231986766
2023-07-01 00:38:48,911 - INFO - train acc: 1.0
2023-07-01 00:38:48,935 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.71      0.74        70
           5       0.69      0.79      0.74        63
           9       0.69      0.64      0.67        67

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:38:48,935 - INFO - test loss 0.02103049084089913
2023-07-01 00:38:48,935 - INFO - test acc 0.7149999737739563
2023-07-01 00:38:50,186 - INFO - Distilling data from client: Client44
2023-07-01 00:38:50,186 - INFO - train loss: 0.000702020138002718
2023-07-01 00:38:50,186 - INFO - train acc: 1.0
2023-07-01 00:38:50,210 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.69      0.72        70
           5       0.69      0.79      0.74        63
           9       0.69      0.67      0.68        67

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:38:50,210 - INFO - test loss 0.0214560282808019
2023-07-01 00:38:50,210 - INFO - test acc 0.7149999737739563
2023-07-01 00:38:51,478 - INFO - Distilling data from client: Client44
2023-07-01 00:38:51,478 - INFO - train loss: 0.0004841851512386098
2023-07-01 00:38:51,478 - INFO - train acc: 1.0
2023-07-01 00:38:51,503 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.67      0.71        70
           5       0.68      0.78      0.73        63
           9       0.68      0.66      0.67        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:38:51,503 - INFO - test loss 0.021862380879990605
2023-07-01 00:38:51,503 - INFO - test acc 0.699999988079071
2023-07-01 00:38:52,762 - INFO - Distilling data from client: Client44
2023-07-01 00:38:52,763 - INFO - train loss: 0.00047304363589154323
2023-07-01 00:38:52,763 - INFO - train acc: 1.0
2023-07-01 00:38:52,786 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.66      0.69        70
           5       0.72      0.79      0.76        63
           9       0.63      0.64      0.64        67

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:38:52,786 - INFO - test loss 0.022357052508782737
2023-07-01 00:38:52,786 - INFO - test acc 0.6949999928474426
2023-07-01 00:38:54,038 - INFO - Distilling data from client: Client44
2023-07-01 00:38:54,038 - INFO - train loss: 0.0004320944640904262
2023-07-01 00:38:54,038 - INFO - train acc: 1.0
2023-07-01 00:38:54,062 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.67      0.71        70
           5       0.70      0.81      0.75        63
           9       0.68      0.66      0.67        67

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:38:54,063 - INFO - test loss 0.021452761998670754
2023-07-01 00:38:54,063 - INFO - test acc 0.7099999785423279
2023-07-01 00:38:55,324 - INFO - Distilling data from client: Client44
2023-07-01 00:38:55,324 - INFO - train loss: 0.00026372530053397333
2023-07-01 00:38:55,324 - INFO - train acc: 1.0
2023-07-01 00:38:55,348 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.67      0.71        70
           5       0.71      0.81      0.76        63
           9       0.69      0.67      0.68        67

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:38:55,349 - INFO - test loss 0.02201954495109919
2023-07-01 00:38:55,349 - INFO - test acc 0.7149999737739563
2023-07-01 00:38:56,598 - INFO - Distilling data from client: Client44
2023-07-01 00:38:56,598 - INFO - train loss: 0.00032318417074810957
2023-07-01 00:38:56,598 - INFO - train acc: 1.0
2023-07-01 00:38:56,622 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.66      0.69        70
           5       0.69      0.78      0.73        63
           9       0.68      0.67      0.68        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:38:56,622 - INFO - test loss 0.022380271542849342
2023-07-01 00:38:56,623 - INFO - test acc 0.699999988079071
2023-07-01 00:38:57,871 - INFO - Distilling data from client: Client44
2023-07-01 00:38:57,871 - INFO - train loss: 0.0002879958897852816
2023-07-01 00:38:57,871 - INFO - train acc: 1.0
2023-07-01 00:38:57,894 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.66      0.69        70
           5       0.70      0.79      0.75        63
           9       0.68      0.67      0.68        67

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:38:57,894 - INFO - test loss 0.021818738756381427
2023-07-01 00:38:57,894 - INFO - test acc 0.7049999833106995
2023-07-01 00:38:59,150 - INFO - Distilling data from client: Client44
2023-07-01 00:38:59,150 - INFO - train loss: 0.00024716760728477657
2023-07-01 00:38:59,150 - INFO - train acc: 1.0
2023-07-01 00:38:59,175 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.69      0.72        70
           5       0.71      0.79      0.75        63
           9       0.68      0.67      0.68        67

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:38:59,175 - INFO - test loss 0.021830300996233058
2023-07-01 00:38:59,175 - INFO - test acc 0.7149999737739563
2023-07-01 00:39:00,423 - INFO - Distilling data from client: Client44
2023-07-01 00:39:00,424 - INFO - train loss: 0.0002537900968358327
2023-07-01 00:39:00,424 - INFO - train acc: 1.0
2023-07-01 00:39:00,448 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.67      0.70        70
           5       0.72      0.81      0.76        63
           9       0.71      0.69      0.70        67

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:39:00,448 - INFO - test loss 0.02201529775023585
2023-07-01 00:39:00,448 - INFO - test acc 0.7199999690055847
2023-07-01 00:39:01,696 - INFO - Distilling data from client: Client44
2023-07-01 00:39:01,696 - INFO - train loss: 0.00022493820226405663
2023-07-01 00:39:01,696 - INFO - train acc: 1.0
2023-07-01 00:39:01,720 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.69      0.70        70
           5       0.70      0.78      0.74        63
           9       0.69      0.64      0.67        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:39:01,720 - INFO - test loss 0.022153718988724286
2023-07-01 00:39:01,720 - INFO - test acc 0.699999988079071
2023-07-01 00:39:02,971 - INFO - Distilling data from client: Client44
2023-07-01 00:39:02,971 - INFO - train loss: 0.0001908828446317785
2023-07-01 00:39:02,971 - INFO - train acc: 1.0
2023-07-01 00:39:02,995 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.67      0.71        70
           5       0.68      0.79      0.73        63
           9       0.68      0.64      0.66        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:39:02,995 - INFO - test loss 0.022441321070012878
2023-07-01 00:39:02,995 - INFO - test acc 0.699999988079071
2023-07-01 00:39:04,249 - INFO - Distilling data from client: Client44
2023-07-01 00:39:04,249 - INFO - train loss: 0.00014433327717205857
2023-07-01 00:39:04,249 - INFO - train acc: 1.0
2023-07-01 00:39:04,273 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.67      0.70        70
           5       0.74      0.79      0.76        63
           9       0.69      0.70      0.70        67

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:39:04,273 - INFO - test loss 0.022043869263984838
2023-07-01 00:39:04,273 - INFO - test acc 0.7199999690055847
2023-07-01 00:39:05,515 - INFO - Distilling data from client: Client44
2023-07-01 00:39:05,516 - INFO - train loss: 0.00015754712483910736
2023-07-01 00:39:05,516 - INFO - train acc: 1.0
2023-07-01 00:39:05,543 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.61      0.67        70
           5       0.68      0.81      0.74        63
           9       0.67      0.66      0.66        67

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:39:05,543 - INFO - test loss 0.02223039617186773
2023-07-01 00:39:05,543 - INFO - test acc 0.6899999976158142
2023-07-01 00:39:06,792 - INFO - Distilling data from client: Client44
2023-07-01 00:39:06,793 - INFO - train loss: 0.0001557042952507693
2023-07-01 00:39:06,793 - INFO - train acc: 1.0
2023-07-01 00:39:06,817 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.67      0.70        70
           5       0.69      0.78      0.73        63
           9       0.68      0.66      0.67        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:39:06,818 - INFO - test loss 0.022102567504971467
2023-07-01 00:39:06,818 - INFO - test acc 0.699999988079071
2023-07-01 00:39:08,074 - INFO - Distilling data from client: Client44
2023-07-01 00:39:08,075 - INFO - train loss: 0.00016115492631690843
2023-07-01 00:39:08,075 - INFO - train acc: 1.0
2023-07-01 00:39:08,098 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.67      0.70        70
           5       0.70      0.78      0.74        63
           9       0.68      0.67      0.68        67

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:39:08,098 - INFO - test loss 0.02172069595756484
2023-07-01 00:39:08,098 - INFO - test acc 0.7049999833106995
2023-07-01 00:39:09,353 - INFO - Distilling data from client: Client44
2023-07-01 00:39:09,353 - INFO - train loss: 0.00016864119926860119
2023-07-01 00:39:09,353 - INFO - train acc: 1.0
2023-07-01 00:39:09,376 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.67      0.69        70
           5       0.68      0.75      0.71        63
           9       0.66      0.64      0.65        67

    accuracy                           0.69       200
   macro avg       0.68      0.69      0.68       200
weighted avg       0.69      0.69      0.68       200

2023-07-01 00:39:09,377 - INFO - test loss 0.02238837022249274
2023-07-01 00:39:09,377 - INFO - test acc 0.6850000023841858
2023-07-01 00:39:10,629 - INFO - Distilling data from client: Client44
2023-07-01 00:39:10,629 - INFO - train loss: 0.0001496689813419893
2023-07-01 00:39:10,629 - INFO - train acc: 1.0
2023-07-01 00:39:10,654 - INFO - report:               precision    recall  f1-score   support

           0       0.72      0.67      0.70        70
           5       0.67      0.76      0.71        63
           9       0.68      0.64      0.66        67

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:39:10,655 - INFO - test loss 0.022249370326954686
2023-07-01 00:39:10,655 - INFO - test acc 0.6899999976158142
2023-07-01 00:39:11,904 - INFO - Distilling data from client: Client44
2023-07-01 00:39:11,904 - INFO - train loss: 0.00020546076030327802
2023-07-01 00:39:11,904 - INFO - train acc: 1.0
2023-07-01 00:39:11,927 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.67      0.71        70
           5       0.70      0.78      0.74        63
           9       0.69      0.69      0.69        67

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:39:11,927 - INFO - test loss 0.022262921500457597
2023-07-01 00:39:11,928 - INFO - test acc 0.7099999785423279
2023-07-01 00:39:13,180 - INFO - Distilling data from client: Client44
2023-07-01 00:39:13,180 - INFO - train loss: 0.0001756799345811806
2023-07-01 00:39:13,180 - INFO - train acc: 1.0
2023-07-01 00:39:13,204 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.63      0.66        70
           5       0.70      0.79      0.75        63
           9       0.70      0.69      0.69        67

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:39:13,205 - INFO - test loss 0.02191247188184173
2023-07-01 00:39:13,205 - INFO - test acc 0.699999988079071
2023-07-01 00:39:14,458 - INFO - Distilling data from client: Client44
2023-07-01 00:39:14,458 - INFO - train loss: 0.00015698869094650918
2023-07-01 00:39:14,458 - INFO - train acc: 1.0
2023-07-01 00:39:14,483 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.66      0.70        70
           5       0.71      0.79      0.75        63
           9       0.69      0.70      0.70        67

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:39:14,483 - INFO - test loss 0.021982244486649784
2023-07-01 00:39:14,483 - INFO - test acc 0.7149999737739563
2023-07-01 00:39:15,739 - INFO - Distilling data from client: Client44
2023-07-01 00:39:15,739 - INFO - train loss: 0.00014817882493549267
2023-07-01 00:39:15,740 - INFO - train acc: 1.0
2023-07-01 00:39:15,764 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.67      0.71        70
           5       0.69      0.79      0.74        63
           9       0.67      0.66      0.66        67

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:39:15,764 - INFO - test loss 0.02212319177889438
2023-07-01 00:39:15,764 - INFO - test acc 0.7049999833106995
2023-07-01 00:39:17,023 - INFO - Distilling data from client: Client44
2023-07-01 00:39:17,023 - INFO - train loss: 0.00015887176218471968
2023-07-01 00:39:17,023 - INFO - train acc: 1.0
2023-07-01 00:39:17,048 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.66      0.70        70
           5       0.69      0.81      0.74        63
           9       0.68      0.66      0.67        67

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:39:17,048 - INFO - test loss 0.022230171107739168
2023-07-01 00:39:17,048 - INFO - test acc 0.7049999833106995
2023-07-01 00:39:18,303 - INFO - Distilling data from client: Client44
2023-07-01 00:39:18,303 - INFO - train loss: 0.00014412673610314893
2023-07-01 00:39:18,303 - INFO - train acc: 1.0
2023-07-01 00:39:18,328 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.66      0.70        70
           5       0.68      0.79      0.74        63
           9       0.65      0.64      0.65        67

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.69       200
weighted avg       0.70      0.69      0.69       200

2023-07-01 00:39:18,329 - INFO - test loss 0.022380424204926797
2023-07-01 00:39:18,329 - INFO - test acc 0.6949999928474426
2023-07-01 00:39:18,341 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:18,349 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:18,358 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:18,367 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:18,376 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:18,385 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:18,394 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:18,403 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:18,413 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:18,788 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client44//synthetic.png
2023-07-01 00:39:18,800 - INFO - c: 3.0 and total_data_in_this_class: 15
2023-07-01 00:39:18,800 - INFO - c: 4.0 and total_data_in_this_class: 243
2023-07-01 00:39:18,800 - INFO - c: 5.0 and total_data_in_this_class: 264
2023-07-01 00:39:18,800 - INFO - c: 7.0 and total_data_in_this_class: 277
2023-07-01 00:39:18,800 - INFO - c: 3.0 and total_data_in_this_class: 5
2023-07-01 00:39:18,800 - INFO - c: 4.0 and total_data_in_this_class: 70
2023-07-01 00:39:18,800 - INFO - c: 5.0 and total_data_in_this_class: 69
2023-07-01 00:39:18,800 - INFO - c: 7.0 and total_data_in_this_class: 56
2023-07-01 00:39:18,820 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002627372741699219 sec
2023-07-01 00:39:18,821 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:39:18,822 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012373924255371094 sec
2023-07-01 00:39:18,822 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:39:18,833 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010708093643188477 sec
2023-07-01 00:39:18,835 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002529621124267578 sec
2023-07-01 00:39:18,835 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:39:18,837 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009496212005615234 sec
2023-07-01 00:39:18,837 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:39:18,845 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008552074432373047 sec
2023-07-01 00:39:18,849 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00015401840209960938 sec
2023-07-01 00:39:18,850 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012445449829101562 sec
2023-07-01 00:39:18,850 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003058910369873047 sec
2023-07-01 00:39:18,852 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020813941955566406 sec
2023-07-01 00:39:18,852 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011992454528808594 sec
2023-07-01 00:39:18,853 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002913475036621094 sec
2023-07-01 00:39:18,854 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000247955322265625 sec
2023-07-01 00:39:18,854 - WARNING - Finished tracing + transforming absolute for pjit in 0.00016951560974121094 sec
2023-07-01 00:39:18,855 - WARNING - Finished tracing + transforming fn for pjit in 0.0002739429473876953 sec
2023-07-01 00:39:18,855 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00032019615173339844 sec
2023-07-01 00:39:18,856 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001983642578125 sec
2023-07-01 00:39:18,857 - WARNING - Finished tracing + transforming fn for pjit in 0.0002319812774658203 sec
2023-07-01 00:39:18,858 - WARNING - Finished tracing + transforming fn for pjit in 0.0002682209014892578 sec
2023-07-01 00:39:18,859 - WARNING - Finished tracing + transforming fn for pjit in 0.00022721290588378906 sec
2023-07-01 00:39:18,859 - WARNING - Finished tracing + transforming fn for pjit in 0.00027942657470703125 sec
2023-07-01 00:39:18,861 - WARNING - Finished tracing + transforming fn for pjit in 0.0002357959747314453 sec
2023-07-01 00:39:18,862 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00017333030700683594 sec
2023-07-01 00:39:18,863 - WARNING - Finished tracing + transforming fn for pjit in 0.00023221969604492188 sec
2023-07-01 00:39:18,864 - WARNING - Finished tracing + transforming fn for pjit in 0.0002353191375732422 sec
2023-07-01 00:39:18,867 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003695487976074219 sec
2023-07-01 00:39:18,868 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009732246398925781 sec
2023-07-01 00:39:18,869 - WARNING - Finished tracing + transforming fn for pjit in 0.00023627281188964844 sec
2023-07-01 00:39:18,869 - WARNING - Finished tracing + transforming fn for pjit in 0.00022482872009277344 sec
2023-07-01 00:39:18,870 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00029349327087402344 sec
2023-07-01 00:39:18,871 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026106834411621094 sec
2023-07-01 00:39:18,871 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001819133758544922 sec
2023-07-01 00:39:18,872 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002646446228027344 sec
2023-07-01 00:39:18,873 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022983551025390625 sec
2023-07-01 00:39:18,874 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023555755615234375 sec
2023-07-01 00:39:18,875 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00035071372985839844 sec
2023-07-01 00:39:18,875 - WARNING - Finished tracing + transforming _where for pjit in 0.0009629726409912109 sec
2023-07-01 00:39:18,876 - WARNING - Finished tracing + transforming fn for pjit in 0.0002651214599609375 sec
2023-07-01 00:39:18,876 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002605915069580078 sec
2023-07-01 00:39:18,877 - WARNING - Finished tracing + transforming fn for pjit in 0.00022745132446289062 sec
2023-07-01 00:39:18,878 - WARNING - Finished tracing + transforming fn for pjit in 0.0002257823944091797 sec
2023-07-01 00:39:18,879 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021958351135253906 sec
2023-07-01 00:39:18,879 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002617835998535156 sec
2023-07-01 00:39:18,880 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025916099548339844 sec
2023-07-01 00:39:18,881 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002663135528564453 sec
2023-07-01 00:39:18,881 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023865699768066406 sec
2023-07-01 00:39:18,882 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002334117889404297 sec
2023-07-01 00:39:18,883 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00026726722717285156 sec
2023-07-01 00:39:18,883 - WARNING - Finished tracing + transforming _where for pjit in 0.0008707046508789062 sec
2023-07-01 00:39:18,884 - WARNING - Finished tracing + transforming fn for pjit in 0.00026607513427734375 sec
2023-07-01 00:39:18,885 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026535987854003906 sec
2023-07-01 00:39:18,886 - WARNING - Finished tracing + transforming fn for pjit in 0.0002269744873046875 sec
2023-07-01 00:39:18,890 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002808570861816406 sec
2023-07-01 00:39:18,891 - WARNING - Finished tracing + transforming fn for pjit in 0.00034427642822265625 sec
2023-07-01 00:39:18,891 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027060508728027344 sec
2023-07-01 00:39:18,892 - WARNING - Finished tracing + transforming fn for pjit in 0.0002257823944091797 sec
2023-07-01 00:39:18,896 - WARNING - Finished tracing + transforming fn for pjit in 0.00021696090698242188 sec
2023-07-01 00:39:18,898 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00017404556274414062 sec
2023-07-01 00:39:18,898 - WARNING - Finished tracing + transforming fn for pjit in 0.0003008842468261719 sec
2023-07-01 00:39:18,899 - WARNING - Finished tracing + transforming fn for pjit in 0.00023031234741210938 sec
2023-07-01 00:39:18,918 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06932520866394043 sec
2023-07-01 00:39:18,921 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012969970703125 sec
2023-07-01 00:39:18,921 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012350082397460938 sec
2023-07-01 00:39:18,922 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002765655517578125 sec
2023-07-01 00:39:18,924 - WARNING - Finished tracing + transforming fn for pjit in 0.00022482872009277344 sec
2023-07-01 00:39:18,925 - WARNING - Finished tracing + transforming fn for pjit in 0.00026535987854003906 sec
2023-07-01 00:39:18,926 - WARNING - Finished tracing + transforming fn for pjit in 0.00022149085998535156 sec
2023-07-01 00:39:18,932 - WARNING - Finished tracing + transforming fn for pjit in 0.00022411346435546875 sec
2023-07-01 00:39:18,933 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002262592315673828 sec
2023-07-01 00:39:18,933 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025153160095214844 sec
2023-07-01 00:39:18,934 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001735687255859375 sec
2023-07-01 00:39:18,935 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032806396484375 sec
2023-07-01 00:39:18,936 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023674964904785156 sec
2023-07-01 00:39:18,936 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.000225067138671875 sec
2023-07-01 00:39:18,937 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002751350402832031 sec
2023-07-01 00:39:18,937 - WARNING - Finished tracing + transforming _where for pjit in 0.0008823871612548828 sec
2023-07-01 00:39:18,938 - WARNING - Finished tracing + transforming fn for pjit in 0.00025963783264160156 sec
2023-07-01 00:39:18,939 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002586841583251953 sec
2023-07-01 00:39:18,940 - WARNING - Finished tracing + transforming fn for pjit in 0.00022268295288085938 sec
2023-07-01 00:39:18,940 - WARNING - Finished tracing + transforming fn for pjit in 0.00028061866760253906 sec
2023-07-01 00:39:18,952 - WARNING - Finished tracing + transforming fn for pjit in 0.00021696090698242188 sec
2023-07-01 00:39:18,972 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.05338144302368164 sec
2023-07-01 00:39:18,973 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012373924255371094 sec
2023-07-01 00:39:18,974 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00013375282287597656 sec
2023-07-01 00:39:18,975 - WARNING - Finished tracing + transforming _where for pjit in 0.0006208419799804688 sec
2023-07-01 00:39:18,975 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00029587745666503906 sec
2023-07-01 00:39:18,976 - WARNING - Finished tracing + transforming trace for pjit in 0.00254058837890625 sec
2023-07-01 00:39:18,978 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010728836059570312 sec
2023-07-01 00:39:18,979 - WARNING - Finished tracing + transforming tril for pjit in 0.0006797313690185547 sec
2023-07-01 00:39:18,979 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0018465518951416016 sec
2023-07-01 00:39:18,980 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010895729064941406 sec
2023-07-01 00:39:18,980 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010728836059570312 sec
2023-07-01 00:39:18,982 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014574527740478516 sec
2023-07-01 00:39:18,986 - WARNING - Finished tracing + transforming _solve for pjit in 0.009390592575073242 sec
2023-07-01 00:39:18,987 - WARNING - Finished tracing + transforming dot for pjit in 0.00031304359436035156 sec
2023-07-01 00:39:18,990 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.14254450798034668 sec
2023-07-01 00:39:18,992 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:39:19,024 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03232264518737793 sec
2023-07-01 00:39:19,024 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:39:19,148 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12315917015075684 sec
2023-07-01 00:39:19,168 - INFO - initial test loss: 0.03239411183600733
2023-07-01 00:39:19,169 - INFO - initial test acc: 0.5399999618530273
2023-07-01 00:39:19,174 - WARNING - Finished tracing + transforming dot for pjit in 0.0003666877746582031 sec
2023-07-01 00:39:19,175 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002925395965576172 sec
2023-07-01 00:39:19,176 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00036406517028808594 sec
2023-07-01 00:39:19,177 - WARNING - Finished tracing + transforming _mean for pjit in 0.000993967056274414 sec
2023-07-01 00:39:19,177 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00019288063049316406 sec
2023-07-01 00:39:19,178 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001785755157470703 sec
2023-07-01 00:39:19,179 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002429485321044922 sec
2023-07-01 00:39:19,179 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003631114959716797 sec
2023-07-01 00:39:19,180 - WARNING - Finished tracing + transforming _mean for pjit in 0.001043558120727539 sec
2023-07-01 00:39:19,181 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009739398956298828 sec
2023-07-01 00:39:19,189 - WARNING - Finished tracing + transforming fn for pjit in 0.00032138824462890625 sec
2023-07-01 00:39:19,190 - WARNING - Finished tracing + transforming fn for pjit in 0.0002682209014892578 sec
2023-07-01 00:39:19,190 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021719932556152344 sec
2023-07-01 00:39:19,191 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00026106834411621094 sec
2023-07-01 00:39:19,191 - WARNING - Finished tracing + transforming _where for pjit in 0.0008375644683837891 sec
2023-07-01 00:39:19,199 - WARNING - Finished tracing + transforming fn for pjit in 0.00025153160095214844 sec
2023-07-01 00:39:19,200 - WARNING - Finished tracing + transforming fn for pjit in 0.00026035308837890625 sec
2023-07-01 00:39:19,201 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021195411682128906 sec
2023-07-01 00:39:19,201 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002415180206298828 sec
2023-07-01 00:39:19,202 - WARNING - Finished tracing + transforming _where for pjit in 0.0008034706115722656 sec
2023-07-01 00:39:19,236 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021457672119140625 sec
2023-07-01 00:39:19,291 - WARNING - Finished tracing + transforming fn for pjit in 0.0002665519714355469 sec
2023-07-01 00:39:19,292 - WARNING - Finished tracing + transforming fn for pjit in 0.00023627281188964844 sec
2023-07-01 00:39:19,292 - WARNING - Finished tracing + transforming square for pjit in 0.0001666545867919922 sec
2023-07-01 00:39:19,294 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002200603485107422 sec
2023-07-01 00:39:19,296 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002465248107910156 sec
2023-07-01 00:39:19,296 - WARNING - Finished tracing + transforming fn for pjit in 0.0002703666687011719 sec
2023-07-01 00:39:19,297 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002300739288330078 sec
2023-07-01 00:39:19,298 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00023174285888671875 sec
2023-07-01 00:39:19,298 - WARNING - Finished tracing + transforming fn for pjit in 0.0002601146697998047 sec
2023-07-01 00:39:19,299 - WARNING - Finished tracing + transforming fn for pjit in 0.00022840499877929688 sec
2023-07-01 00:39:19,299 - WARNING - Finished tracing + transforming square for pjit in 0.00016164779663085938 sec
2023-07-01 00:39:19,301 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022029876708984375 sec
2023-07-01 00:39:19,303 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017333030700683594 sec
2023-07-01 00:39:19,304 - WARNING - Finished tracing + transforming fn for pjit in 0.0002601146697998047 sec
2023-07-01 00:39:19,304 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021195411682128906 sec
2023-07-01 00:39:19,305 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022554397583007812 sec
2023-07-01 00:39:19,306 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13509750366210938 sec
2023-07-01 00:39:19,309 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,10]), ShapedArray(float32[486,10]), ShapedArray(float32[486,10]), ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,10]), ShapedArray(float32[486,3,32,32]), ShapedArray(float32[486,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:39:19,371 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06122231483459473 sec
2023-07-01 00:39:19,371 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:39:19,696 - WARNING - Finished XLA compilation of jit(update_fn) in 0.32520556449890137 sec
2023-07-01 00:39:20,832 - INFO - Distilling data from client: Client45
2023-07-01 00:39:20,832 - INFO - train loss: 0.0033602686912872
2023-07-01 00:39:20,832 - INFO - train acc: 0.9958847165107727
2023-07-01 00:39:20,884 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.59      0.71      0.65        70
           5       0.58      0.54      0.56        69
           7       0.49      0.45      0.47        56

    accuracy                           0.56       200
   macro avg       0.41      0.42      0.42       200
weighted avg       0.54      0.56      0.55       200

2023-07-01 00:39:20,884 - INFO - test loss 0.030694121325451667
2023-07-01 00:39:20,884 - INFO - test acc 0.5600000023841858
2023-07-01 00:39:22,009 - INFO - Distilling data from client: Client45
2023-07-01 00:39:22,009 - INFO - train loss: 0.0019647025942775903
2023-07-01 00:39:22,009 - INFO - train acc: 0.9958847165107727
2023-07-01 00:39:22,033 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.56      0.70      0.62        70
           5       0.53      0.51      0.52        69
           7       0.51      0.43      0.47        56

    accuracy                           0.54       200
   macro avg       0.40      0.41      0.40       200
weighted avg       0.52      0.54      0.53       200

2023-07-01 00:39:22,034 - INFO - test loss 0.031192534285979536
2023-07-01 00:39:22,034 - INFO - test acc 0.5399999618530273
2023-07-01 00:39:23,162 - INFO - Distilling data from client: Client45
2023-07-01 00:39:23,162 - INFO - train loss: 0.0013487989142406153
2023-07-01 00:39:23,162 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:23,185 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.66      0.60        70
           5       0.56      0.55      0.55        69
           7       0.51      0.45      0.48        56

    accuracy                           0.55       200
   macro avg       0.41      0.41      0.41       200
weighted avg       0.53      0.55      0.54       200

2023-07-01 00:39:23,185 - INFO - test loss 0.030981017746199633
2023-07-01 00:39:23,185 - INFO - test acc 0.5450000166893005
2023-07-01 00:39:24,310 - INFO - Distilling data from client: Client45
2023-07-01 00:39:24,311 - INFO - train loss: 0.0011856925113644093
2023-07-01 00:39:24,311 - INFO - train acc: 0.997942328453064
2023-07-01 00:39:24,332 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.67      0.60        70
           5       0.54      0.54      0.54        69
           7       0.51      0.41      0.46        56

    accuracy                           0.54       200
   macro avg       0.40      0.40      0.40       200
weighted avg       0.52      0.54      0.52       200

2023-07-01 00:39:24,333 - INFO - test loss 0.03127755600785444
2023-07-01 00:39:24,333 - INFO - test acc 0.5349999666213989
2023-07-01 00:39:25,450 - INFO - Distilling data from client: Client45
2023-07-01 00:39:25,450 - INFO - train loss: 0.0011324139019810748
2023-07-01 00:39:25,450 - INFO - train acc: 0.997942328453064
2023-07-01 00:39:25,511 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.59      0.73      0.65        70
           5       0.56      0.57      0.56        69
           7       0.52      0.41      0.46        56

    accuracy                           0.56       200
   macro avg       0.42      0.43      0.42       200
weighted avg       0.55      0.56      0.55       200

2023-07-01 00:39:25,511 - INFO - test loss 0.03049842999737742
2023-07-01 00:39:25,511 - INFO - test acc 0.5649999976158142
2023-07-01 00:39:26,639 - INFO - Distilling data from client: Client45
2023-07-01 00:39:26,639 - INFO - train loss: 0.0010470356929520792
2023-07-01 00:39:26,639 - INFO - train acc: 0.997942328453064
2023-07-01 00:39:26,661 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.69      0.61        70
           5       0.53      0.51      0.52        69
           7       0.49      0.41      0.45        56

    accuracy                           0.53       200
   macro avg       0.39      0.40      0.39       200
weighted avg       0.51      0.53      0.52       200

2023-07-01 00:39:26,661 - INFO - test loss 0.03201423340041946
2023-07-01 00:39:26,661 - INFO - test acc 0.5299999713897705
2023-07-01 00:39:27,781 - INFO - Distilling data from client: Client45
2023-07-01 00:39:27,781 - INFO - train loss: 0.0009481860192996254
2023-07-01 00:39:27,781 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:27,803 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.53      0.64      0.58        70
           5       0.54      0.52      0.53        69
           7       0.46      0.39      0.42        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.38       200
weighted avg       0.50      0.52      0.50       200

2023-07-01 00:39:27,803 - INFO - test loss 0.03231032270344398
2023-07-01 00:39:27,803 - INFO - test acc 0.5149999856948853
2023-07-01 00:39:28,926 - INFO - Distilling data from client: Client45
2023-07-01 00:39:28,926 - INFO - train loss: 0.0008982463980794968
2023-07-01 00:39:28,926 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:28,947 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.54      0.64      0.58        70
           5       0.54      0.54      0.54        69
           7       0.46      0.39      0.42        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-07-01 00:39:28,947 - INFO - test loss 0.03184741758342469
2023-07-01 00:39:28,947 - INFO - test acc 0.5199999809265137
2023-07-01 00:39:30,078 - INFO - Distilling data from client: Client45
2023-07-01 00:39:30,078 - INFO - train loss: 0.0007451199615623739
2023-07-01 00:39:30,079 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:30,104 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.53      0.66      0.59        70
           5       0.53      0.51      0.52        69
           7       0.46      0.39      0.42        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.38       200
weighted avg       0.50      0.52      0.50       200

2023-07-01 00:39:30,104 - INFO - test loss 0.032109418113314005
2023-07-01 00:39:30,104 - INFO - test acc 0.5149999856948853
2023-07-01 00:39:31,235 - INFO - Distilling data from client: Client45
2023-07-01 00:39:31,235 - INFO - train loss: 0.0006899300615610972
2023-07-01 00:39:31,235 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:31,259 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.59      0.74      0.66        70
           5       0.55      0.51      0.53        69
           7       0.48      0.41      0.44        56

    accuracy                           0.55       200
   macro avg       0.40      0.42      0.41       200
weighted avg       0.53      0.55      0.54       200

2023-07-01 00:39:31,259 - INFO - test loss 0.03149800304064749
2023-07-01 00:39:31,259 - INFO - test acc 0.550000011920929
2023-07-01 00:39:32,393 - INFO - Distilling data from client: Client45
2023-07-01 00:39:32,394 - INFO - train loss: 0.0007294035008650885
2023-07-01 00:39:32,394 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:32,417 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.54      0.63      0.58        70
           5       0.52      0.52      0.52        69
           7       0.49      0.43      0.46        56

    accuracy                           0.52       200
   macro avg       0.39      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-07-01 00:39:32,417 - INFO - test loss 0.0321447604951273
2023-07-01 00:39:32,417 - INFO - test acc 0.5199999809265137
2023-07-01 00:39:33,545 - INFO - Distilling data from client: Client45
2023-07-01 00:39:33,546 - INFO - train loss: 0.0006736532802643565
2023-07-01 00:39:33,546 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:33,567 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.56      0.67      0.61        70
           5       0.52      0.54      0.53        69
           7       0.53      0.43      0.48        56

    accuracy                           0.54       200
   macro avg       0.40      0.41      0.40       200
weighted avg       0.52      0.54      0.53       200

2023-07-01 00:39:33,567 - INFO - test loss 0.03184773009402947
2023-07-01 00:39:33,568 - INFO - test acc 0.5399999618530273
2023-07-01 00:39:34,694 - INFO - Distilling data from client: Client45
2023-07-01 00:39:34,694 - INFO - train loss: 0.000724012905591709
2023-07-01 00:39:34,695 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:34,716 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.53      0.66      0.59        70
           5       0.55      0.55      0.55        69
           7       0.47      0.38      0.42        56

    accuracy                           0.53       200
   macro avg       0.39      0.40      0.39       200
weighted avg       0.51      0.53      0.51       200

2023-07-01 00:39:34,716 - INFO - test loss 0.031630094648855085
2023-07-01 00:39:34,716 - INFO - test acc 0.5249999761581421
2023-07-01 00:39:35,848 - INFO - Distilling data from client: Client45
2023-07-01 00:39:35,848 - INFO - train loss: 0.0005762800973141839
2023-07-01 00:39:35,848 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:35,870 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.70      0.62        70
           5       0.55      0.52      0.53        69
           7       0.47      0.38      0.42        56

    accuracy                           0.53       200
   macro avg       0.39      0.40      0.39       200
weighted avg       0.51      0.53      0.52       200

2023-07-01 00:39:35,870 - INFO - test loss 0.03214840108990741
2023-07-01 00:39:35,870 - INFO - test acc 0.5299999713897705
2023-07-01 00:39:37,009 - INFO - Distilling data from client: Client45
2023-07-01 00:39:37,009 - INFO - train loss: 0.0006338941428031347
2023-07-01 00:39:37,009 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:37,034 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.66      0.60        70
           5       0.55      0.52      0.54        69
           7       0.45      0.41      0.43        56

    accuracy                           0.53       200
   macro avg       0.39      0.40      0.39       200
weighted avg       0.51      0.53      0.51       200

2023-07-01 00:39:37,034 - INFO - test loss 0.031334925362374935
2023-07-01 00:39:37,034 - INFO - test acc 0.5249999761581421
2023-07-01 00:39:38,168 - INFO - Distilling data from client: Client45
2023-07-01 00:39:38,168 - INFO - train loss: 0.0004995196722705626
2023-07-01 00:39:38,168 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:38,191 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.54      0.63      0.58        70
           5       0.51      0.51      0.51        69
           7       0.48      0.43      0.45        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-07-01 00:39:38,191 - INFO - test loss 0.03300293824798274
2023-07-01 00:39:38,191 - INFO - test acc 0.5149999856948853
2023-07-01 00:39:39,322 - INFO - Distilling data from client: Client45
2023-07-01 00:39:39,322 - INFO - train loss: 0.0005569944872866935
2023-07-01 00:39:39,322 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:39,345 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.69      0.61        70
           5       0.52      0.49      0.50        69
           7       0.48      0.39      0.43        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-07-01 00:39:39,345 - INFO - test loss 0.032486299344497475
2023-07-01 00:39:39,345 - INFO - test acc 0.5199999809265137
2023-07-01 00:39:40,467 - INFO - Distilling data from client: Client45
2023-07-01 00:39:40,468 - INFO - train loss: 0.0005390054018331883
2023-07-01 00:39:40,468 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:40,490 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.66      0.60        70
           5       0.52      0.51      0.51        69
           7       0.47      0.41      0.44        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-07-01 00:39:40,490 - INFO - test loss 0.031696761312646274
2023-07-01 00:39:40,490 - INFO - test acc 0.5199999809265137
2023-07-01 00:39:41,612 - INFO - Distilling data from client: Client45
2023-07-01 00:39:41,612 - INFO - train loss: 0.0005132150927749262
2023-07-01 00:39:41,612 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:41,635 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.52      0.61      0.57        70
           5       0.49      0.51      0.50        69
           7       0.43      0.36      0.39        56

    accuracy                           0.49       200
   macro avg       0.36      0.37      0.36       200
weighted avg       0.47      0.49      0.48       200

2023-07-01 00:39:41,635 - INFO - test loss 0.03278808687231377
2023-07-01 00:39:41,635 - INFO - test acc 0.4899999797344208
2023-07-01 00:39:42,768 - INFO - Distilling data from client: Client45
2023-07-01 00:39:42,768 - INFO - train loss: 0.0004998828050663533
2023-07-01 00:39:42,768 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:42,791 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.53      0.67      0.59        70
           5       0.53      0.51      0.52        69
           7       0.46      0.38      0.41        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.38       200
weighted avg       0.50      0.52      0.50       200

2023-07-01 00:39:42,792 - INFO - test loss 0.032042690885119456
2023-07-01 00:39:42,792 - INFO - test acc 0.5149999856948853
2023-07-01 00:39:43,919 - INFO - Distilling data from client: Client45
2023-07-01 00:39:43,919 - INFO - train loss: 0.0004397597887728628
2023-07-01 00:39:43,919 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:43,940 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.53      0.66      0.59        70
           5       0.54      0.51      0.52        69
           7       0.46      0.39      0.42        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.38       200
weighted avg       0.50      0.52      0.50       200

2023-07-01 00:39:43,941 - INFO - test loss 0.032515540006647047
2023-07-01 00:39:43,941 - INFO - test acc 0.5149999856948853
2023-07-01 00:39:45,075 - INFO - Distilling data from client: Client45
2023-07-01 00:39:45,075 - INFO - train loss: 0.0004636690638728264
2023-07-01 00:39:45,075 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:45,097 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.52      0.66      0.58        70
           5       0.52      0.51      0.51        69
           7       0.43      0.34      0.38        56

    accuracy                           0.50       200
   macro avg       0.37      0.38      0.37       200
weighted avg       0.48      0.50      0.49       200

2023-07-01 00:39:45,098 - INFO - test loss 0.03249602711963269
2023-07-01 00:39:45,098 - INFO - test acc 0.5
2023-07-01 00:39:46,224 - INFO - Distilling data from client: Client45
2023-07-01 00:39:46,224 - INFO - train loss: 0.000498294369582918
2023-07-01 00:39:46,224 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:46,247 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.66      0.60        70
           5       0.51      0.51      0.51        69
           7       0.48      0.41      0.44        56

    accuracy                           0.52       200
   macro avg       0.39      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-07-01 00:39:46,248 - INFO - test loss 0.03183182517907422
2023-07-01 00:39:46,248 - INFO - test acc 0.5199999809265137
2023-07-01 00:39:47,376 - INFO - Distilling data from client: Client45
2023-07-01 00:39:47,376 - INFO - train loss: 0.00045077483306039685
2023-07-01 00:39:47,376 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:47,400 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.54      0.61      0.57        70
           5       0.51      0.51      0.51        69
           7       0.49      0.45      0.47        56

    accuracy                           0.52       200
   macro avg       0.38      0.39      0.39       200
weighted avg       0.50      0.52      0.51       200

2023-07-01 00:39:47,400 - INFO - test loss 0.03260658148669352
2023-07-01 00:39:47,400 - INFO - test acc 0.5149999856948853
2023-07-01 00:39:48,529 - INFO - Distilling data from client: Client45
2023-07-01 00:39:48,529 - INFO - train loss: 0.00044439081289526783
2023-07-01 00:39:48,529 - INFO - train acc: 0.9999999403953552
2023-07-01 00:39:48,555 - INFO - report:               precision    recall  f1-score   support

           3       0.00      0.00      0.00         5
           4       0.55      0.60      0.57        70
           5       0.51      0.54      0.52        69
           7       0.45      0.41      0.43        56

    accuracy                           0.51       200
   macro avg       0.38      0.39      0.38       200
weighted avg       0.49      0.51      0.50       200

2023-07-01 00:39:48,555 - INFO - test loss 0.03198769952571766
2023-07-01 00:39:48,555 - INFO - test acc 0.5099999904632568
2023-07-01 00:39:48,557 - WARNING - Finished tracing + transforming jit(gather) in 0.0002498626708984375 sec
2023-07-01 00:39:48,557 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[486,3,32,32]), ShapedArray(int32[3,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:39:48,558 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0011754035949707031 sec
2023-07-01 00:39:48,559 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:39:48,568 - WARNING - Finished XLA compilation of jit(gather) in 0.009610891342163086 sec
2023-07-01 00:39:48,578 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:48,587 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:48,596 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:48,604 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:48,612 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:48,621 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:48,630 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:48,639 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:48,648 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:39:49,014 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client45//synthetic.png
2023-07-01 00:39:49,026 - INFO - c: 0.0 and total_data_in_this_class: 273
2023-07-01 00:39:49,026 - INFO - c: 1.0 and total_data_in_this_class: 526
2023-07-01 00:39:49,026 - INFO - c: 0.0 and total_data_in_this_class: 60
2023-07-01 00:39:49,026 - INFO - c: 1.0 and total_data_in_this_class: 140
2023-07-01 00:39:49,047 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002510547637939453 sec
2023-07-01 00:39:49,047 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:39:49,048 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0012135505676269531 sec
2023-07-01 00:39:49,048 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:39:49,059 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010593652725219727 sec
2023-07-01 00:39:49,061 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002598762512207031 sec
2023-07-01 00:39:49,061 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:39:49,062 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.000942230224609375 sec
2023-07-01 00:39:49,062 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:39:49,071 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008428812026977539 sec
2023-07-01 00:39:49,074 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013566017150878906 sec
2023-07-01 00:39:49,075 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001246929168701172 sec
2023-07-01 00:39:49,076 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00030112266540527344 sec
2023-07-01 00:39:49,077 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00020313262939453125 sec
2023-07-01 00:39:49,078 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011944770812988281 sec
2023-07-01 00:39:49,078 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00027823448181152344 sec
2023-07-01 00:39:49,079 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024771690368652344 sec
2023-07-01 00:39:49,080 - WARNING - Finished tracing + transforming absolute for pjit in 0.00016808509826660156 sec
2023-07-01 00:39:49,080 - WARNING - Finished tracing + transforming fn for pjit in 0.00027489662170410156 sec
2023-07-01 00:39:49,081 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00032019615173339844 sec
2023-07-01 00:39:49,082 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00019669532775878906 sec
2023-07-01 00:39:49,083 - WARNING - Finished tracing + transforming fn for pjit in 0.0002269744873046875 sec
2023-07-01 00:39:49,083 - WARNING - Finished tracing + transforming fn for pjit in 0.0002663135528564453 sec
2023-07-01 00:39:49,084 - WARNING - Finished tracing + transforming fn for pjit in 0.00022220611572265625 sec
2023-07-01 00:39:49,085 - WARNING - Finished tracing + transforming fn for pjit in 0.00026106834411621094 sec
2023-07-01 00:39:49,086 - WARNING - Finished tracing + transforming fn for pjit in 0.0002231597900390625 sec
2023-07-01 00:39:49,088 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001697540283203125 sec
2023-07-01 00:39:49,088 - WARNING - Finished tracing + transforming fn for pjit in 0.0002295970916748047 sec
2023-07-01 00:39:49,089 - WARNING - Finished tracing + transforming fn for pjit in 0.0002357959747314453 sec
2023-07-01 00:39:49,092 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003533363342285156 sec
2023-07-01 00:39:49,093 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009586811065673828 sec
2023-07-01 00:39:49,094 - WARNING - Finished tracing + transforming fn for pjit in 0.00022602081298828125 sec
2023-07-01 00:39:49,094 - WARNING - Finished tracing + transforming fn for pjit in 0.0002193450927734375 sec
2023-07-01 00:39:49,096 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0010001659393310547 sec
2023-07-01 00:39:49,097 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026702880859375 sec
2023-07-01 00:39:49,097 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001785755157470703 sec
2023-07-01 00:39:49,098 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026226043701171875 sec
2023-07-01 00:39:49,099 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002262592315673828 sec
2023-07-01 00:39:49,099 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022125244140625 sec
2023-07-01 00:39:49,100 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0003604888916015625 sec
2023-07-01 00:39:49,101 - WARNING - Finished tracing + transforming _where for pjit in 0.0009665489196777344 sec
2023-07-01 00:39:49,101 - WARNING - Finished tracing + transforming fn for pjit in 0.0002617835998535156 sec
2023-07-01 00:39:49,102 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025725364685058594 sec
2023-07-01 00:39:49,103 - WARNING - Finished tracing + transforming fn for pjit in 0.0002243518829345703 sec
2023-07-01 00:39:49,104 - WARNING - Finished tracing + transforming fn for pjit in 0.00022339820861816406 sec
2023-07-01 00:39:49,104 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002181529998779297 sec
2023-07-01 00:39:49,105 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002627372741699219 sec
2023-07-01 00:39:49,106 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002484321594238281 sec
2023-07-01 00:39:49,106 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025844573974609375 sec
2023-07-01 00:39:49,107 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002238750457763672 sec
2023-07-01 00:39:49,108 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002269744873046875 sec
2023-07-01 00:39:49,109 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002684593200683594 sec
2023-07-01 00:39:49,109 - WARNING - Finished tracing + transforming _where for pjit in 0.000881195068359375 sec
2023-07-01 00:39:49,110 - WARNING - Finished tracing + transforming fn for pjit in 0.0002510547637939453 sec
2023-07-01 00:39:49,110 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002474784851074219 sec
2023-07-01 00:39:49,111 - WARNING - Finished tracing + transforming fn for pjit in 0.00021600723266601562 sec
2023-07-01 00:39:49,115 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00027108192443847656 sec
2023-07-01 00:39:49,116 - WARNING - Finished tracing + transforming fn for pjit in 0.00033402442932128906 sec
2023-07-01 00:39:49,117 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002605915069580078 sec
2023-07-01 00:39:49,118 - WARNING - Finished tracing + transforming fn for pjit in 0.00021958351135253906 sec
2023-07-01 00:39:49,121 - WARNING - Finished tracing + transforming fn for pjit in 0.00022029876708984375 sec
2023-07-01 00:39:49,123 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016736984252929688 sec
2023-07-01 00:39:49,124 - WARNING - Finished tracing + transforming fn for pjit in 0.0003020763397216797 sec
2023-07-01 00:39:49,124 - WARNING - Finished tracing + transforming fn for pjit in 0.00022554397583007812 sec
2023-07-01 00:39:49,142 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.0678858757019043 sec
2023-07-01 00:39:49,145 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001246929168701172 sec
2023-07-01 00:39:49,145 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00010943412780761719 sec
2023-07-01 00:39:49,146 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00026106834411621094 sec
2023-07-01 00:39:49,148 - WARNING - Finished tracing + transforming fn for pjit in 0.00021386146545410156 sec
2023-07-01 00:39:49,149 - WARNING - Finished tracing + transforming fn for pjit in 0.000247955322265625 sec
2023-07-01 00:39:49,150 - WARNING - Finished tracing + transforming fn for pjit in 0.00021123886108398438 sec
2023-07-01 00:39:49,156 - WARNING - Finished tracing + transforming fn for pjit in 0.00022459030151367188 sec
2023-07-01 00:39:49,157 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021266937255859375 sec
2023-07-01 00:39:49,157 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002474784851074219 sec
2023-07-01 00:39:49,158 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00016927719116210938 sec
2023-07-01 00:39:49,159 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0006804466247558594 sec
2023-07-01 00:39:49,160 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002224445343017578 sec
2023-07-01 00:39:49,160 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002181529998779297 sec
2023-07-01 00:39:49,161 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00027370452880859375 sec
2023-07-01 00:39:49,161 - WARNING - Finished tracing + transforming _where for pjit in 0.0008554458618164062 sec
2023-07-01 00:39:49,162 - WARNING - Finished tracing + transforming fn for pjit in 0.00025010108947753906 sec
2023-07-01 00:39:49,163 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002465248107910156 sec
2023-07-01 00:39:49,164 - WARNING - Finished tracing + transforming fn for pjit in 0.0002143383026123047 sec
2023-07-01 00:39:49,164 - WARNING - Finished tracing + transforming fn for pjit in 0.000270843505859375 sec
2023-07-01 00:39:49,176 - WARNING - Finished tracing + transforming fn for pjit in 0.00021266937255859375 sec
2023-07-01 00:39:49,195 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.051920413970947266 sec
2023-07-01 00:39:49,196 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012493133544921875 sec
2023-07-01 00:39:49,197 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00012993812561035156 sec
2023-07-01 00:39:49,197 - WARNING - Finished tracing + transforming _where for pjit in 0.0006074905395507812 sec
2023-07-01 00:39:49,198 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002818107604980469 sec
2023-07-01 00:39:49,198 - WARNING - Finished tracing + transforming trace for pjit in 0.0024466514587402344 sec
2023-07-01 00:39:49,200 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010633468627929688 sec
2023-07-01 00:39:49,201 - WARNING - Finished tracing + transforming tril for pjit in 0.0006618499755859375 sec
2023-07-01 00:39:49,202 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0017933845520019531 sec
2023-07-01 00:39:49,202 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.0001049041748046875 sec
2023-07-01 00:39:49,203 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010180473327636719 sec
2023-07-01 00:39:49,205 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0014312267303466797 sec
2023-07-01 00:39:49,209 - WARNING - Finished tracing + transforming _solve for pjit in 0.009088993072509766 sec
2023-07-01 00:39:49,209 - WARNING - Finished tracing + transforming dot for pjit in 0.0003211498260498047 sec
2023-07-01 00:39:49,212 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.13909530639648438 sec
2023-07-01 00:39:49,214 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:39:49,246 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.031800270080566406 sec
2023-07-01 00:39:49,246 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:39:49,368 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12148046493530273 sec
2023-07-01 00:39:49,384 - INFO - initial test loss: 0.017922337145063947
2023-07-01 00:39:49,384 - INFO - initial test acc: 0.7649999856948853
2023-07-01 00:39:49,390 - WARNING - Finished tracing + transforming dot for pjit in 0.0003638267517089844 sec
2023-07-01 00:39:49,391 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002968311309814453 sec
2023-07-01 00:39:49,392 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0009241104125976562 sec
2023-07-01 00:39:49,393 - WARNING - Finished tracing + transforming _mean for pjit in 0.001552581787109375 sec
2023-07-01 00:39:49,394 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0002040863037109375 sec
2023-07-01 00:39:49,394 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00018596649169921875 sec
2023-07-01 00:39:49,395 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002453327178955078 sec
2023-07-01 00:39:49,396 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003724098205566406 sec
2023-07-01 00:39:49,396 - WARNING - Finished tracing + transforming _mean for pjit in 0.001115560531616211 sec
2023-07-01 00:39:49,397 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.010591745376586914 sec
2023-07-01 00:39:49,406 - WARNING - Finished tracing + transforming fn for pjit in 0.00030875205993652344 sec
2023-07-01 00:39:49,406 - WARNING - Finished tracing + transforming fn for pjit in 0.0002617835998535156 sec
2023-07-01 00:39:49,407 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002155303955078125 sec
2023-07-01 00:39:49,408 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002684593200683594 sec
2023-07-01 00:39:49,408 - WARNING - Finished tracing + transforming _where for pjit in 0.0008919239044189453 sec
2023-07-01 00:39:49,416 - WARNING - Finished tracing + transforming fn for pjit in 0.0002453327178955078 sec
2023-07-01 00:39:49,417 - WARNING - Finished tracing + transforming fn for pjit in 0.00026726722717285156 sec
2023-07-01 00:39:49,418 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021219253540039062 sec
2023-07-01 00:39:49,418 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002562999725341797 sec
2023-07-01 00:39:49,419 - WARNING - Finished tracing + transforming _where for pjit in 0.000865936279296875 sec
2023-07-01 00:39:49,452 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002110004425048828 sec
2023-07-01 00:39:49,507 - WARNING - Finished tracing + transforming fn for pjit in 0.0002722740173339844 sec
2023-07-01 00:39:49,508 - WARNING - Finished tracing + transforming fn for pjit in 0.00023412704467773438 sec
2023-07-01 00:39:49,509 - WARNING - Finished tracing + transforming square for pjit in 0.00016832351684570312 sec
2023-07-01 00:39:49,511 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022077560424804688 sec
2023-07-01 00:39:49,512 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002410411834716797 sec
2023-07-01 00:39:49,513 - WARNING - Finished tracing + transforming fn for pjit in 0.00027751922607421875 sec
2023-07-01 00:39:49,514 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022864341735839844 sec
2023-07-01 00:39:49,514 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025844573974609375 sec
2023-07-01 00:39:49,515 - WARNING - Finished tracing + transforming fn for pjit in 0.00027298927307128906 sec
2023-07-01 00:39:49,516 - WARNING - Finished tracing + transforming fn for pjit in 0.00023245811462402344 sec
2023-07-01 00:39:49,516 - WARNING - Finished tracing + transforming square for pjit in 0.00018310546875 sec
2023-07-01 00:39:49,518 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021982192993164062 sec
2023-07-01 00:39:49,520 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001857280731201172 sec
2023-07-01 00:39:49,521 - WARNING - Finished tracing + transforming fn for pjit in 0.0002682209014892578 sec
2023-07-01 00:39:49,521 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00022077560424804688 sec
2023-07-01 00:39:49,522 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022745132446289062 sec
2023-07-01 00:39:49,523 - WARNING - Finished tracing + transforming update_fn for pjit in 0.137007474899292 sec
2023-07-01 00:39:49,527 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,10]), ShapedArray(float32[364,10]), ShapedArray(float32[364,10]), ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,10]), ShapedArray(float32[364,3,32,32]), ShapedArray(float32[364,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:39:49,590 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06327509880065918 sec
2023-07-01 00:39:49,591 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:39:49,890 - WARNING - Finished XLA compilation of jit(update_fn) in 0.29955577850341797 sec
2023-07-01 00:39:50,645 - INFO - Distilling data from client: Client46
2023-07-01 00:39:50,646 - INFO - train loss: 0.003030505762393046
2023-07-01 00:39:50,646 - INFO - train acc: 0.9917582869529724
2023-07-01 00:39:50,687 - INFO - report:               precision    recall  f1-score   support

           0       0.74      0.72      0.73        60
           1       0.88      0.89      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:39:50,688 - INFO - test loss 0.012686659496339202
2023-07-01 00:39:50,688 - INFO - test acc 0.8399999737739563
2023-07-01 00:39:51,442 - INFO - Distilling data from client: Client46
2023-07-01 00:39:51,442 - INFO - train loss: 0.0024882853300784626
2023-07-01 00:39:51,442 - INFO - train acc: 0.9917582869529724
2023-07-01 00:39:51,487 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.73      0.76        60
           1       0.89      0.91      0.90       140

    accuracy                           0.86       200
   macro avg       0.84      0.82      0.83       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:39:51,487 - INFO - test loss 0.01377887357732609
2023-07-01 00:39:51,487 - INFO - test acc 0.85999995470047
2023-07-01 00:39:52,239 - INFO - Distilling data from client: Client46
2023-07-01 00:39:52,240 - INFO - train loss: 0.0020463190549153505
2023-07-01 00:39:52,240 - INFO - train acc: 0.9945055246353149
2023-07-01 00:39:52,258 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.70      0.74        60
           1       0.88      0.91      0.90       140

    accuracy                           0.85       200
   macro avg       0.83      0.81      0.82       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:39:52,258 - INFO - test loss 0.013404219752221029
2023-07-01 00:39:52,258 - INFO - test acc 0.8499999642372131
2023-07-01 00:39:53,007 - INFO - Distilling data from client: Client46
2023-07-01 00:39:53,007 - INFO - train loss: 0.001688406243438792
2023-07-01 00:39:53,007 - INFO - train acc: 0.9945055246353149
2023-07-01 00:39:53,024 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.72      0.71        60
           1       0.88      0.87      0.87       140

    accuracy                           0.82       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.83      0.82      0.83       200

2023-07-01 00:39:53,024 - INFO - test loss 0.014170569559731627
2023-07-01 00:39:53,024 - INFO - test acc 0.824999988079071
2023-07-01 00:39:53,772 - INFO - Distilling data from client: Client46
2023-07-01 00:39:53,772 - INFO - train loss: 0.0014085715297799317
2023-07-01 00:39:53,772 - INFO - train acc: 1.0
2023-07-01 00:39:53,790 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.70      0.72        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:39:53,791 - INFO - test loss 0.013897355272583258
2023-07-01 00:39:53,791 - INFO - test acc 0.8399999737739563
2023-07-01 00:39:54,548 - INFO - Distilling data from client: Client46
2023-07-01 00:39:54,548 - INFO - train loss: 0.001501616161420505
2023-07-01 00:39:54,548 - INFO - train acc: 0.9972527623176575
2023-07-01 00:39:54,596 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.80      0.79        60
           1       0.91      0.91      0.91       140

    accuracy                           0.88       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:39:54,596 - INFO - test loss 0.012208917069429621
2023-07-01 00:39:54,596 - INFO - test acc 0.875
2023-07-01 00:39:55,356 - INFO - Distilling data from client: Client46
2023-07-01 00:39:55,356 - INFO - train loss: 0.0016151286107891922
2023-07-01 00:39:55,356 - INFO - train acc: 0.9972527623176575
2023-07-01 00:39:55,373 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.72      0.72        60
           1       0.88      0.89      0.88       140

    accuracy                           0.83       200
   macro avg       0.80      0.80      0.80       200
weighted avg       0.83      0.83      0.83       200

2023-07-01 00:39:55,374 - INFO - test loss 0.013911480285596092
2023-07-01 00:39:55,374 - INFO - test acc 0.8349999785423279
2023-07-01 00:39:56,131 - INFO - Distilling data from client: Client46
2023-07-01 00:39:56,131 - INFO - train loss: 0.0014684972942769973
2023-07-01 00:39:56,131 - INFO - train acc: 0.9972527623176575
2023-07-01 00:39:56,149 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        60
           1       0.89      0.89      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:39:56,149 - INFO - test loss 0.014583938327801435
2023-07-01 00:39:56,149 - INFO - test acc 0.8449999690055847
2023-07-01 00:39:56,900 - INFO - Distilling data from client: Client46
2023-07-01 00:39:56,900 - INFO - train loss: 0.0018053361901025507
2023-07-01 00:39:56,900 - INFO - train acc: 0.9972527623176575
2023-07-01 00:39:56,919 - INFO - report:               precision    recall  f1-score   support

           0       0.79      0.73      0.76        60
           1       0.89      0.91      0.90       140

    accuracy                           0.86       200
   macro avg       0.84      0.82      0.83       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:39:56,919 - INFO - test loss 0.013334063907934806
2023-07-01 00:39:56,919 - INFO - test acc 0.85999995470047
2023-07-01 00:39:57,670 - INFO - Distilling data from client: Client46
2023-07-01 00:39:57,670 - INFO - train loss: 0.0013222353438884157
2023-07-01 00:39:57,670 - INFO - train acc: 1.0
2023-07-01 00:39:57,690 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.68      0.74        60
           1       0.87      0.93      0.90       140

    accuracy                           0.85       200
   macro avg       0.84      0.81      0.82       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:39:57,690 - INFO - test loss 0.013672154510766756
2023-07-01 00:39:57,690 - INFO - test acc 0.8549999594688416
2023-07-01 00:39:58,440 - INFO - Distilling data from client: Client46
2023-07-01 00:39:58,440 - INFO - train loss: 0.0012029184649946995
2023-07-01 00:39:58,440 - INFO - train acc: 1.0
2023-07-01 00:39:58,459 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.68      0.71        60
           1       0.87      0.90      0.88       140

    accuracy                           0.83       200
   macro avg       0.81      0.79      0.80       200
weighted avg       0.83      0.83      0.83       200

2023-07-01 00:39:58,459 - INFO - test loss 0.013233715530446662
2023-07-01 00:39:58,459 - INFO - test acc 0.8349999785423279
2023-07-01 00:39:59,208 - INFO - Distilling data from client: Client46
2023-07-01 00:39:59,208 - INFO - train loss: 0.001450639293269121
2023-07-01 00:39:59,209 - INFO - train acc: 0.9972527623176575
2023-07-01 00:39:59,226 - INFO - report:               precision    recall  f1-score   support

           0       0.82      0.67      0.73        60
           1       0.87      0.94      0.90       140

    accuracy                           0.85       200
   macro avg       0.84      0.80      0.82       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:39:59,226 - INFO - test loss 0.013634459018661726
2023-07-01 00:39:59,226 - INFO - test acc 0.8549999594688416
2023-07-01 00:39:59,977 - INFO - Distilling data from client: Client46
2023-07-01 00:39:59,977 - INFO - train loss: 0.0013401352887462404
2023-07-01 00:39:59,977 - INFO - train acc: 1.0
2023-07-01 00:39:59,995 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.70      0.72        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:39:59,995 - INFO - test loss 0.013204578175116822
2023-07-01 00:39:59,995 - INFO - test acc 0.8399999737739563
2023-07-01 00:40:00,763 - INFO - Distilling data from client: Client46
2023-07-01 00:40:00,763 - INFO - train loss: 0.0012370908339270161
2023-07-01 00:40:00,763 - INFO - train acc: 1.0
2023-07-01 00:40:00,781 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.73      0.74        60
           1       0.89      0.89      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:40:00,781 - INFO - test loss 0.014047138724932434
2023-07-01 00:40:00,782 - INFO - test acc 0.8449999690055847
2023-07-01 00:40:01,531 - INFO - Distilling data from client: Client46
2023-07-01 00:40:01,532 - INFO - train loss: 0.001207459300360025
2023-07-01 00:40:01,532 - INFO - train acc: 1.0
2023-07-01 00:40:01,549 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.72      0.74        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:40:01,549 - INFO - test loss 0.014518066989130511
2023-07-01 00:40:01,549 - INFO - test acc 0.8449999690055847
2023-07-01 00:40:02,312 - INFO - Distilling data from client: Client46
2023-07-01 00:40:02,313 - INFO - train loss: 0.0011775215416986195
2023-07-01 00:40:02,313 - INFO - train acc: 1.0
2023-07-01 00:40:02,332 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.70      0.72        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:40:02,332 - INFO - test loss 0.013318784597603946
2023-07-01 00:40:02,332 - INFO - test acc 0.8399999737739563
2023-07-01 00:40:03,084 - INFO - Distilling data from client: Client46
2023-07-01 00:40:03,084 - INFO - train loss: 0.0010583477924398135
2023-07-01 00:40:03,084 - INFO - train acc: 0.9972527623176575
2023-07-01 00:40:03,102 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.70      0.73        60
           1       0.88      0.91      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:40:03,102 - INFO - test loss 0.013507121181803822
2023-07-01 00:40:03,102 - INFO - test acc 0.8449999690055847
2023-07-01 00:40:03,861 - INFO - Distilling data from client: Client46
2023-07-01 00:40:03,861 - INFO - train loss: 0.0010989142923174954
2023-07-01 00:40:03,861 - INFO - train acc: 0.9972527623176575
2023-07-01 00:40:03,879 - INFO - report:               precision    recall  f1-score   support

           0       0.73      0.73      0.73        60
           1       0.89      0.89      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.81      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:40:03,879 - INFO - test loss 0.013810910491132406
2023-07-01 00:40:03,879 - INFO - test acc 0.8399999737739563
2023-07-01 00:40:04,633 - INFO - Distilling data from client: Client46
2023-07-01 00:40:04,633 - INFO - train loss: 0.0010753209063008
2023-07-01 00:40:04,633 - INFO - train acc: 1.0
2023-07-01 00:40:04,651 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.65      0.71        60
           1       0.86      0.92      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.79      0.80       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:40:04,652 - INFO - test loss 0.013680712042399156
2023-07-01 00:40:04,652 - INFO - test acc 0.8399999737739563
2023-07-01 00:40:05,400 - INFO - Distilling data from client: Client46
2023-07-01 00:40:05,400 - INFO - train loss: 0.001029680934055981
2023-07-01 00:40:05,400 - INFO - train acc: 1.0
2023-07-01 00:40:05,419 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.70      0.72        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.81      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:40:05,419 - INFO - test loss 0.013642442611932757
2023-07-01 00:40:05,419 - INFO - test acc 0.8399999737739563
2023-07-01 00:40:06,180 - INFO - Distilling data from client: Client46
2023-07-01 00:40:06,180 - INFO - train loss: 0.001047041946157654
2023-07-01 00:40:06,180 - INFO - train acc: 1.0
2023-07-01 00:40:06,198 - INFO - report:               precision    recall  f1-score   support

           0       0.81      0.73      0.77        60
           1       0.89      0.93      0.91       140

    accuracy                           0.87       200
   macro avg       0.85      0.83      0.84       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:40:06,199 - INFO - test loss 0.014136223241947524
2023-07-01 00:40:06,199 - INFO - test acc 0.8700000047683716
2023-07-01 00:40:06,954 - INFO - Distilling data from client: Client46
2023-07-01 00:40:06,954 - INFO - train loss: 0.0010791779657759034
2023-07-01 00:40:06,954 - INFO - train acc: 1.0
2023-07-01 00:40:06,973 - INFO - report:               precision    recall  f1-score   support

           0       0.80      0.73      0.77        60
           1       0.89      0.92      0.91       140

    accuracy                           0.86       200
   macro avg       0.84      0.83      0.84       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:40:06,973 - INFO - test loss 0.013573070710348727
2023-07-01 00:40:06,973 - INFO - test acc 0.8650000095367432
2023-07-01 00:40:07,736 - INFO - Distilling data from client: Client46
2023-07-01 00:40:07,736 - INFO - train loss: 0.0008732239876554966
2023-07-01 00:40:07,736 - INFO - train acc: 1.0
2023-07-01 00:40:07,753 - INFO - report:               precision    recall  f1-score   support

           0       0.76      0.70      0.73        60
           1       0.88      0.91      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.80      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:40:07,753 - INFO - test loss 0.013227929545753586
2023-07-01 00:40:07,754 - INFO - test acc 0.8449999690055847
2023-07-01 00:40:08,520 - INFO - Distilling data from client: Client46
2023-07-01 00:40:08,520 - INFO - train loss: 0.0010564096137576136
2023-07-01 00:40:08,520 - INFO - train acc: 1.0
2023-07-01 00:40:08,538 - INFO - report:               precision    recall  f1-score   support

           0       0.75      0.72      0.74        60
           1       0.88      0.90      0.89       140

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.81       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:40:08,538 - INFO - test loss 0.013941681453040583
2023-07-01 00:40:08,538 - INFO - test acc 0.8449999690055847
2023-07-01 00:40:09,287 - INFO - Distilling data from client: Client46
2023-07-01 00:40:09,287 - INFO - train loss: 0.0008911733521480635
2023-07-01 00:40:09,287 - INFO - train acc: 1.0
2023-07-01 00:40:09,305 - INFO - report:               precision    recall  f1-score   support

           0       0.78      0.72      0.75        60
           1       0.88      0.91      0.90       140

    accuracy                           0.85       200
   macro avg       0.83      0.82      0.82       200
weighted avg       0.85      0.85      0.85       200

2023-07-01 00:40:09,305 - INFO - test loss 0.013114417281392473
2023-07-01 00:40:09,305 - INFO - test acc 0.8549999594688416
2023-07-01 00:40:09,308 - WARNING - Finished tracing + transforming jit(gather) in 0.00023889541625976562 sec
2023-07-01 00:40:09,308 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[364,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:40:09,309 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0012023448944091797 sec
2023-07-01 00:40:09,309 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:40:09,320 - WARNING - Finished XLA compilation of jit(gather) in 0.009939908981323242 sec
2023-07-01 00:40:09,330 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:09,339 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:09,348 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:09,357 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:09,366 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:09,375 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:09,674 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client46//synthetic.png
2023-07-01 00:40:09,686 - INFO - c: 0.0 and total_data_in_this_class: 258
2023-07-01 00:40:09,686 - INFO - c: 3.0 and total_data_in_this_class: 273
2023-07-01 00:40:09,686 - INFO - c: 6.0 and total_data_in_this_class: 268
2023-07-01 00:40:09,686 - INFO - c: 0.0 and total_data_in_this_class: 75
2023-07-01 00:40:09,686 - INFO - c: 3.0 and total_data_in_this_class: 60
2023-07-01 00:40:09,686 - INFO - c: 6.0 and total_data_in_this_class: 65
2023-07-01 00:40:09,756 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04757833480834961 sec
2023-07-01 00:40:09,802 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04511523246765137 sec
2023-07-01 00:40:09,807 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.09984827041625977 sec
2023-07-01 00:40:09,809 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:40:09,841 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03230166435241699 sec
2023-07-01 00:40:09,842 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:40:09,966 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12408566474914551 sec
2023-07-01 00:40:09,989 - INFO - initial test loss: 0.02198435581061444
2023-07-01 00:40:09,989 - INFO - initial test acc: 0.7149999737739563
2023-07-01 00:40:09,997 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.0058171749114990234 sec
2023-07-01 00:40:10,105 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11448383331298828 sec
2023-07-01 00:40:10,108 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10]), ShapedArray(float32[516,3,32,32]), ShapedArray(float32[516,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:40:10,169 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06064438819885254 sec
2023-07-01 00:40:10,169 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:40:10,468 - WARNING - Finished XLA compilation of jit(update_fn) in 0.29857754707336426 sec
2023-07-01 00:40:11,688 - INFO - Distilling data from client: Client47
2023-07-01 00:40:11,689 - INFO - train loss: 0.0026851425889047624
2023-07-01 00:40:11,689 - INFO - train acc: 0.9961240291595459
2023-07-01 00:40:11,750 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.79      0.86        75
           3       0.71      0.70      0.71        60
           6       0.69      0.83      0.76        65

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.79      0.78      0.78       200

2023-07-01 00:40:11,751 - INFO - test loss 0.020075554879194768
2023-07-01 00:40:11,751 - INFO - test acc 0.7749999761581421
2023-07-01 00:40:12,976 - INFO - Distilling data from client: Client47
2023-07-01 00:40:12,976 - INFO - train loss: 0.0016654562030299718
2023-07-01 00:40:12,976 - INFO - train acc: 1.0
2023-07-01 00:40:13,001 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.83      0.89        75
           3       0.66      0.72      0.69        60
           6       0.70      0.77      0.74        65

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.79      0.78      0.78       200

2023-07-01 00:40:13,001 - INFO - test loss 0.019893639868732254
2023-07-01 00:40:13,001 - INFO - test acc 0.7749999761581421
2023-07-01 00:40:14,231 - INFO - Distilling data from client: Client47
2023-07-01 00:40:14,231 - INFO - train loss: 0.001020315821163071
2023-07-01 00:40:14,231 - INFO - train acc: 1.0
2023-07-01 00:40:14,261 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.80      0.88        75
           3       0.67      0.62      0.64        60
           6       0.64      0.82      0.72        65

    accuracy                           0.75       200
   macro avg       0.76      0.74      0.75       200
weighted avg       0.77      0.75      0.75       200

2023-07-01 00:40:14,261 - INFO - test loss 0.02019941436475184
2023-07-01 00:40:14,261 - INFO - test acc 0.75
2023-07-01 00:40:15,495 - INFO - Distilling data from client: Client47
2023-07-01 00:40:15,495 - INFO - train loss: 0.0008200310414049295
2023-07-01 00:40:15,495 - INFO - train acc: 1.0
2023-07-01 00:40:15,521 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.83      0.88        75
           3       0.67      0.67      0.67        60
           6       0.69      0.78      0.73        65

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.78      0.77      0.77       200

2023-07-01 00:40:15,521 - INFO - test loss 0.020728802145731577
2023-07-01 00:40:15,521 - INFO - test acc 0.7649999856948853
2023-07-01 00:40:16,744 - INFO - Distilling data from client: Client47
2023-07-01 00:40:16,744 - INFO - train loss: 0.0006822576955037233
2023-07-01 00:40:16,744 - INFO - train acc: 1.0
2023-07-01 00:40:16,767 - INFO - report:               precision    recall  f1-score   support

           0       0.95      0.81      0.88        75
           3       0.67      0.65      0.66        60
           6       0.65      0.78      0.71        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:40:16,767 - INFO - test loss 0.02055376132337795
2023-07-01 00:40:16,767 - INFO - test acc 0.7549999952316284
2023-07-01 00:40:18,001 - INFO - Distilling data from client: Client47
2023-07-01 00:40:18,001 - INFO - train loss: 0.0007016836780230137
2023-07-01 00:40:18,001 - INFO - train acc: 1.0
2023-07-01 00:40:18,026 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.81      0.88        75
           3       0.63      0.65      0.64        60
           6       0.68      0.78      0.73        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:40:18,026 - INFO - test loss 0.020906790348982213
2023-07-01 00:40:18,026 - INFO - test acc 0.7549999952316284
2023-07-01 00:40:19,253 - INFO - Distilling data from client: Client47
2023-07-01 00:40:19,254 - INFO - train loss: 0.0006560499265967892
2023-07-01 00:40:19,254 - INFO - train acc: 1.0
2023-07-01 00:40:19,277 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.77      0.86        75
           3       0.68      0.72      0.70        60
           6       0.70      0.83      0.76        65

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.80      0.78      0.78       200

2023-07-01 00:40:19,278 - INFO - test loss 0.020757009040262432
2023-07-01 00:40:19,278 - INFO - test acc 0.7749999761581421
2023-07-01 00:40:20,509 - INFO - Distilling data from client: Client47
2023-07-01 00:40:20,509 - INFO - train loss: 0.0007158090623818417
2023-07-01 00:40:20,509 - INFO - train acc: 1.0
2023-07-01 00:40:20,535 - INFO - report:               precision    recall  f1-score   support

           0       0.95      0.84      0.89        75
           3       0.71      0.67      0.69        60
           6       0.67      0.80      0.73        65

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.79      0.78      0.78       200

2023-07-01 00:40:20,535 - INFO - test loss 0.02089217149265381
2023-07-01 00:40:20,535 - INFO - test acc 0.7749999761581421
2023-07-01 00:40:21,763 - INFO - Distilling data from client: Client47
2023-07-01 00:40:21,763 - INFO - train loss: 0.00045347034451632376
2023-07-01 00:40:21,763 - INFO - train acc: 1.0
2023-07-01 00:40:21,787 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.83      0.88        75
           3       0.63      0.63      0.63        60
           6       0.69      0.78      0.73        65

    accuracy                           0.76       200
   macro avg       0.75      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:40:21,788 - INFO - test loss 0.020681581060736424
2023-07-01 00:40:21,788 - INFO - test acc 0.7549999952316284
2023-07-01 00:40:23,006 - INFO - Distilling data from client: Client47
2023-07-01 00:40:23,006 - INFO - train loss: 0.00039229792799056737
2023-07-01 00:40:23,006 - INFO - train acc: 1.0
2023-07-01 00:40:23,071 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.83      0.89        75
           3       0.70      0.72      0.71        60
           6       0.71      0.82      0.76        65

    accuracy                           0.79       200
   macro avg       0.79      0.79      0.79       200
weighted avg       0.80      0.79      0.79       200

2023-07-01 00:40:23,072 - INFO - test loss 0.02061346784648678
2023-07-01 00:40:23,072 - INFO - test acc 0.7899999618530273
2023-07-01 00:40:24,299 - INFO - Distilling data from client: Client47
2023-07-01 00:40:24,299 - INFO - train loss: 0.000395896453541872
2023-07-01 00:40:24,299 - INFO - train acc: 1.0
2023-07-01 00:40:24,322 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.80      0.86        75
           3       0.66      0.67      0.66        60
           6       0.68      0.78      0.73        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:40:24,322 - INFO - test loss 0.020979504946782145
2023-07-01 00:40:24,322 - INFO - test acc 0.7549999952316284
2023-07-01 00:40:25,546 - INFO - Distilling data from client: Client47
2023-07-01 00:40:25,546 - INFO - train loss: 0.0004802589275229225
2023-07-01 00:40:25,546 - INFO - train acc: 1.0
2023-07-01 00:40:25,569 - INFO - report:               precision    recall  f1-score   support

           0       0.95      0.81      0.88        75
           3       0.68      0.72      0.70        60
           6       0.68      0.77      0.72        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-07-01 00:40:25,569 - INFO - test loss 0.020750989263245062
2023-07-01 00:40:25,569 - INFO - test acc 0.7699999809265137
2023-07-01 00:40:26,798 - INFO - Distilling data from client: Client47
2023-07-01 00:40:26,798 - INFO - train loss: 0.00042844236473648246
2023-07-01 00:40:26,798 - INFO - train acc: 1.0
2023-07-01 00:40:26,822 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.81      0.88        75
           3       0.70      0.72      0.71        60
           6       0.70      0.82      0.75        65

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.80      0.79      0.79       200

2023-07-01 00:40:26,822 - INFO - test loss 0.020655949009671788
2023-07-01 00:40:26,822 - INFO - test acc 0.7849999666213989
2023-07-01 00:40:28,047 - INFO - Distilling data from client: Client47
2023-07-01 00:40:28,047 - INFO - train loss: 0.0004615223653338185
2023-07-01 00:40:28,048 - INFO - train acc: 1.0
2023-07-01 00:40:28,070 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.77      0.85        75
           3       0.69      0.68      0.69        60
           6       0.68      0.83      0.75        65

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.76       200
weighted avg       0.78      0.77      0.77       200

2023-07-01 00:40:28,070 - INFO - test loss 0.020664744461072495
2023-07-01 00:40:28,070 - INFO - test acc 0.7649999856948853
2023-07-01 00:40:29,284 - INFO - Distilling data from client: Client47
2023-07-01 00:40:29,284 - INFO - train loss: 0.0004211263466153652
2023-07-01 00:40:29,284 - INFO - train acc: 1.0
2023-07-01 00:40:29,307 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.79      0.86        75
           3       0.67      0.67      0.67        60
           6       0.68      0.80      0.73        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:40:29,308 - INFO - test loss 0.020593858574789757
2023-07-01 00:40:29,308 - INFO - test acc 0.7549999952316284
2023-07-01 00:40:30,536 - INFO - Distilling data from client: Client47
2023-07-01 00:40:30,536 - INFO - train loss: 0.0004220246774465707
2023-07-01 00:40:30,536 - INFO - train acc: 1.0
2023-07-01 00:40:30,560 - INFO - report:               precision    recall  f1-score   support

           0       0.95      0.83      0.89        75
           3       0.69      0.70      0.69        60
           6       0.72      0.82      0.76        65

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.80      0.79      0.79       200

2023-07-01 00:40:30,560 - INFO - test loss 0.02046893417737619
2023-07-01 00:40:30,560 - INFO - test acc 0.7849999666213989
2023-07-01 00:40:31,782 - INFO - Distilling data from client: Client47
2023-07-01 00:40:31,782 - INFO - train loss: 0.00032471616214854753
2023-07-01 00:40:31,782 - INFO - train acc: 1.0
2023-07-01 00:40:31,806 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.80      0.86        75
           3       0.64      0.65      0.64        60
           6       0.68      0.78      0.73        65

    accuracy                           0.75       200
   macro avg       0.75      0.74      0.75       200
weighted avg       0.76      0.75      0.75       200

2023-07-01 00:40:31,806 - INFO - test loss 0.021457577627181135
2023-07-01 00:40:31,806 - INFO - test acc 0.75
2023-07-01 00:40:33,035 - INFO - Distilling data from client: Client47
2023-07-01 00:40:33,035 - INFO - train loss: 0.000326307726359599
2023-07-01 00:40:33,035 - INFO - train acc: 1.0
2023-07-01 00:40:33,060 - INFO - report:               precision    recall  f1-score   support

           0       0.92      0.81      0.87        75
           3       0.63      0.60      0.62        60
           6       0.68      0.80      0.73        65

    accuracy                           0.74       200
   macro avg       0.74      0.74      0.74       200
weighted avg       0.76      0.74      0.75       200

2023-07-01 00:40:33,060 - INFO - test loss 0.020864257886851967
2023-07-01 00:40:33,060 - INFO - test acc 0.7450000047683716
2023-07-01 00:40:34,276 - INFO - Distilling data from client: Client47
2023-07-01 00:40:34,276 - INFO - train loss: 0.00032157871130093996
2023-07-01 00:40:34,276 - INFO - train acc: 1.0
2023-07-01 00:40:34,300 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.84      0.90        75
           3       0.67      0.72      0.69        60
           6       0.72      0.78      0.75        65

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.80      0.79      0.79       200

2023-07-01 00:40:34,301 - INFO - test loss 0.020764813223031944
2023-07-01 00:40:34,301 - INFO - test acc 0.7849999666213989
2023-07-01 00:40:35,530 - INFO - Distilling data from client: Client47
2023-07-01 00:40:35,530 - INFO - train loss: 0.00039712747506172205
2023-07-01 00:40:35,530 - INFO - train acc: 1.0
2023-07-01 00:40:35,553 - INFO - report:               precision    recall  f1-score   support

           0       0.92      0.81      0.87        75
           3       0.67      0.67      0.67        60
           6       0.68      0.77      0.72        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:40:35,553 - INFO - test loss 0.02055992388780065
2023-07-01 00:40:35,553 - INFO - test acc 0.7549999952316284
2023-07-01 00:40:36,783 - INFO - Distilling data from client: Client47
2023-07-01 00:40:36,783 - INFO - train loss: 0.0003044345866898494
2023-07-01 00:40:36,783 - INFO - train acc: 1.0
2023-07-01 00:40:36,807 - INFO - report:               precision    recall  f1-score   support

           0       0.97      0.81      0.88        75
           3       0.70      0.73      0.72        60
           6       0.70      0.80      0.75        65

    accuracy                           0.79       200
   macro avg       0.79      0.78      0.78       200
weighted avg       0.80      0.79      0.79       200

2023-07-01 00:40:36,808 - INFO - test loss 0.02156896004569672
2023-07-01 00:40:36,808 - INFO - test acc 0.7849999666213989
2023-07-01 00:40:38,035 - INFO - Distilling data from client: Client47
2023-07-01 00:40:38,035 - INFO - train loss: 0.00026494646106201025
2023-07-01 00:40:38,035 - INFO - train acc: 1.0
2023-07-01 00:40:38,059 - INFO - report:               precision    recall  f1-score   support

           0       0.95      0.83      0.89        75
           3       0.68      0.68      0.68        60
           6       0.68      0.78      0.73        65

    accuracy                           0.77       200
   macro avg       0.77      0.76      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-07-01 00:40:38,059 - INFO - test loss 0.020371373876954892
2023-07-01 00:40:38,059 - INFO - test acc 0.7699999809265137
2023-07-01 00:40:39,285 - INFO - Distilling data from client: Client47
2023-07-01 00:40:39,285 - INFO - train loss: 0.0003047934144076304
2023-07-01 00:40:39,285 - INFO - train acc: 1.0
2023-07-01 00:40:39,307 - INFO - report:               precision    recall  f1-score   support

           0       0.93      0.83      0.87        75
           3       0.67      0.63      0.65        60
           6       0.68      0.80      0.74        65

    accuracy                           0.76       200
   macro avg       0.76      0.75      0.75       200
weighted avg       0.77      0.76      0.76       200

2023-07-01 00:40:39,308 - INFO - test loss 0.02092095152598416
2023-07-01 00:40:39,308 - INFO - test acc 0.7599999904632568
2023-07-01 00:40:40,537 - INFO - Distilling data from client: Client47
2023-07-01 00:40:40,538 - INFO - train loss: 0.0003433474877296119
2023-07-01 00:40:40,538 - INFO - train acc: 1.0
2023-07-01 00:40:40,561 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.83      0.88        75
           3       0.67      0.70      0.68        60
           6       0.70      0.77      0.74        65

    accuracy                           0.77       200
   macro avg       0.77      0.77      0.77       200
weighted avg       0.78      0.77      0.77       200

2023-07-01 00:40:40,562 - INFO - test loss 0.02095800281221132
2023-07-01 00:40:40,562 - INFO - test acc 0.7699999809265137
2023-07-01 00:40:41,786 - INFO - Distilling data from client: Client47
2023-07-01 00:40:41,786 - INFO - train loss: 0.00029407693026822746
2023-07-01 00:40:41,786 - INFO - train acc: 1.0
2023-07-01 00:40:41,811 - INFO - report:               precision    recall  f1-score   support

           0       0.94      0.83      0.88        75
           3       0.69      0.72      0.70        60
           6       0.69      0.77      0.73        65

    accuracy                           0.78       200
   macro avg       0.78      0.77      0.77       200
weighted avg       0.79      0.78      0.78       200

2023-07-01 00:40:41,811 - INFO - test loss 0.020568941447598264
2023-07-01 00:40:41,811 - INFO - test acc 0.7749999761581421
2023-07-01 00:40:41,823 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:41,832 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:41,841 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:41,849 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:41,859 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:41,867 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:41,876 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:41,885 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:41,895 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:40:42,275 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client47//synthetic.png
2023-07-01 00:40:42,288 - INFO - c: 1.0 and total_data_in_this_class: 272
2023-07-01 00:40:42,288 - INFO - c: 5.0 and total_data_in_this_class: 527
2023-07-01 00:40:42,288 - INFO - c: 1.0 and total_data_in_this_class: 61
2023-07-01 00:40:42,288 - INFO - c: 5.0 and total_data_in_this_class: 139
2023-07-01 00:40:42,308 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.00025773048400878906 sec
2023-07-01 00:40:42,308 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:40:42,309 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.001224517822265625 sec
2023-07-01 00:40:42,310 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:40:42,320 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.010709524154663086 sec
2023-07-01 00:40:42,322 - WARNING - Finished tracing + transforming jit(broadcast_in_dim) in 0.0002472400665283203 sec
2023-07-01 00:40:42,322 - DEBUG - Compiling broadcast_in_dim for with global shapes and types [ShapedArray(float32[])]. Argument mapping: (GSPMDSharding({replicated}),).
2023-07-01 00:40:42,324 - WARNING - Finished jaxpr to MLIR module conversion jit(broadcast_in_dim) in 0.0009722709655761719 sec
2023-07-01 00:40:42,324 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:40:42,332 - WARNING - Finished XLA compilation of jit(broadcast_in_dim) in 0.008326530456542969 sec
2023-07-01 00:40:42,336 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012946128845214844 sec
2023-07-01 00:40:42,337 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001227855682373047 sec
2023-07-01 00:40:42,338 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0003025531768798828 sec
2023-07-01 00:40:42,339 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00013065338134765625 sec
2023-07-01 00:40:42,339 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002079010009765625 sec
2023-07-01 00:40:42,340 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00027489662170410156 sec
2023-07-01 00:40:42,341 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002396106719970703 sec
2023-07-01 00:40:42,341 - WARNING - Finished tracing + transforming absolute for pjit in 0.0001647472381591797 sec
2023-07-01 00:40:42,342 - WARNING - Finished tracing + transforming fn for pjit in 0.0002682209014892578 sec
2023-07-01 00:40:42,342 - WARNING - Finished tracing + transforming _reduce_all for pjit in 0.00031256675720214844 sec
2023-07-01 00:40:42,343 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00012493133544921875 sec
2023-07-01 00:40:42,344 - WARNING - Finished tracing + transforming fn for pjit in 0.00022363662719726562 sec
2023-07-01 00:40:42,345 - WARNING - Finished tracing + transforming fn for pjit in 0.0003383159637451172 sec
2023-07-01 00:40:42,345 - WARNING - Finished tracing + transforming fn for pjit in 0.00022220611572265625 sec
2023-07-01 00:40:42,346 - WARNING - Finished tracing + transforming fn for pjit in 0.0002589225769042969 sec
2023-07-01 00:40:42,347 - WARNING - Finished tracing + transforming fn for pjit in 0.00022029876708984375 sec
2023-07-01 00:40:42,349 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0002472400665283203 sec
2023-07-01 00:40:42,349 - WARNING - Finished tracing + transforming fn for pjit in 0.000225067138671875 sec
2023-07-01 00:40:42,350 - WARNING - Finished tracing + transforming fn for pjit in 0.0002300739288330078 sec
2023-07-01 00:40:42,354 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003654956817626953 sec
2023-07-01 00:40:42,354 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009491443634033203 sec
2023-07-01 00:40:42,355 - WARNING - Finished tracing + transforming fn for pjit in 0.0002269744873046875 sec
2023-07-01 00:40:42,356 - WARNING - Finished tracing + transforming fn for pjit in 0.00021529197692871094 sec
2023-07-01 00:40:42,357 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021648406982421875 sec
2023-07-01 00:40:42,357 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0003311634063720703 sec
2023-07-01 00:40:42,358 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00017380714416503906 sec
2023-07-01 00:40:42,359 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025653839111328125 sec
2023-07-01 00:40:42,359 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022125244140625 sec
2023-07-01 00:40:42,360 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021982192993164062 sec
2023-07-01 00:40:42,361 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002658367156982422 sec
2023-07-01 00:40:42,361 - WARNING - Finished tracing + transforming _where for pjit in 0.0008573532104492188 sec
2023-07-01 00:40:42,362 - WARNING - Finished tracing + transforming fn for pjit in 0.0002620220184326172 sec
2023-07-01 00:40:42,362 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00032329559326171875 sec
2023-07-01 00:40:42,363 - WARNING - Finished tracing + transforming fn for pjit in 0.00021696090698242188 sec
2023-07-01 00:40:42,364 - WARNING - Finished tracing + transforming fn for pjit in 0.0002143383026123047 sec
2023-07-01 00:40:42,365 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00020933151245117188 sec
2023-07-01 00:40:42,365 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024962425231933594 sec
2023-07-01 00:40:42,366 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00016689300537109375 sec
2023-07-01 00:40:42,366 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025534629821777344 sec
2023-07-01 00:40:42,367 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002238750457763672 sec
2023-07-01 00:40:42,368 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022292137145996094 sec
2023-07-01 00:40:42,369 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00024962425231933594 sec
2023-07-01 00:40:42,369 - WARNING - Finished tracing + transforming _where for pjit in 0.0008220672607421875 sec
2023-07-01 00:40:42,370 - WARNING - Finished tracing + transforming fn for pjit in 0.00025081634521484375 sec
2023-07-01 00:40:42,370 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002503395080566406 sec
2023-07-01 00:40:42,371 - WARNING - Finished tracing + transforming fn for pjit in 0.0002193450927734375 sec
2023-07-01 00:40:42,375 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026416778564453125 sec
2023-07-01 00:40:42,376 - WARNING - Finished tracing + transforming fn for pjit in 0.00026106834411621094 sec
2023-07-01 00:40:42,377 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00025582313537597656 sec
2023-07-01 00:40:42,378 - WARNING - Finished tracing + transforming fn for pjit in 0.0002849102020263672 sec
2023-07-01 00:40:42,381 - WARNING - Finished tracing + transforming fn for pjit in 0.00021505355834960938 sec
2023-07-01 00:40:42,383 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00016021728515625 sec
2023-07-01 00:40:42,383 - WARNING - Finished tracing + transforming fn for pjit in 0.00028634071350097656 sec
2023-07-01 00:40:42,384 - WARNING - Finished tracing + transforming fn for pjit in 0.00021958351135253906 sec
2023-07-01 00:40:42,402 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.06617164611816406 sec
2023-07-01 00:40:42,405 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001361370086669922 sec
2023-07-01 00:40:42,405 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.00011014938354492188 sec
2023-07-01 00:40:42,406 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002589225769042969 sec
2023-07-01 00:40:42,408 - WARNING - Finished tracing + transforming fn for pjit in 0.00021219253540039062 sec
2023-07-01 00:40:42,408 - WARNING - Finished tracing + transforming fn for pjit in 0.0002474784851074219 sec
2023-07-01 00:40:42,410 - WARNING - Finished tracing + transforming fn for pjit in 0.00021576881408691406 sec
2023-07-01 00:40:42,415 - WARNING - Finished tracing + transforming fn for pjit in 0.00022101402282714844 sec
2023-07-01 00:40:42,416 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021195411682128906 sec
2023-07-01 00:40:42,417 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00026154518127441406 sec
2023-07-01 00:40:42,417 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0001659393310546875 sec
2023-07-01 00:40:42,418 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00031304359436035156 sec
2023-07-01 00:40:42,419 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022125244140625 sec
2023-07-01 00:40:42,420 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00021386146545410156 sec
2023-07-01 00:40:42,420 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002605915069580078 sec
2023-07-01 00:40:42,421 - WARNING - Finished tracing + transforming _where for pjit in 0.0008273124694824219 sec
2023-07-01 00:40:42,421 - WARNING - Finished tracing + transforming fn for pjit in 0.0002486705780029297 sec
2023-07-01 00:40:42,422 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002448558807373047 sec
2023-07-01 00:40:42,423 - WARNING - Finished tracing + transforming fn for pjit in 0.00020933151245117188 sec
2023-07-01 00:40:42,423 - WARNING - Finished tracing + transforming fn for pjit in 0.00026726722717285156 sec
2023-07-01 00:40:42,435 - WARNING - Finished tracing + transforming fn for pjit in 0.0002124309539794922 sec
2023-07-01 00:40:42,454 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.051152944564819336 sec
2023-07-01 00:40:42,455 - WARNING - Finished tracing + transforming _moveaxis for pjit in 0.0001201629638671875 sec
2023-07-01 00:40:42,456 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00012946128845214844 sec
2023-07-01 00:40:42,456 - WARNING - Finished tracing + transforming _where for pjit in 0.0005877017974853516 sec
2023-07-01 00:40:42,457 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0002894401550292969 sec
2023-07-01 00:40:42,457 - WARNING - Finished tracing + transforming trace for pjit in 0.0024192333221435547 sec
2023-07-01 00:40:42,459 - WARNING - Finished tracing + transforming conjugate for pjit in 0.00010204315185546875 sec
2023-07-01 00:40:42,460 - WARNING - Finished tracing + transforming tril for pjit in 0.0006494522094726562 sec
2023-07-01 00:40:42,461 - WARNING - Finished tracing + transforming _cholesky for pjit in 0.0019376277923583984 sec
2023-07-01 00:40:42,462 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010466575622558594 sec
2023-07-01 00:40:42,462 - WARNING - Finished tracing + transforming _squeeze for pjit in 0.00010371208190917969 sec
2023-07-01 00:40:42,464 - WARNING - Finished tracing + transforming _cho_solve for pjit in 0.0013742446899414062 sec
2023-07-01 00:40:42,468 - WARNING - Finished tracing + transforming _solve for pjit in 0.00908207893371582 sec
2023-07-01 00:40:42,468 - WARNING - Finished tracing + transforming dot for pjit in 0.0003027915954589844 sec
2023-07-01 00:40:42,471 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.13681626319885254 sec
2023-07-01 00:40:42,473 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:40:42,505 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03220772743225098 sec
2023-07-01 00:40:42,505 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:40:42,626 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.1205129623413086 sec
2023-07-01 00:40:42,642 - INFO - initial test loss: 0.01290919235743765
2023-07-01 00:40:42,642 - INFO - initial test acc: 0.8449999690055847
2023-07-01 00:40:42,648 - WARNING - Finished tracing + transforming dot for pjit in 0.00037026405334472656 sec
2023-07-01 00:40:42,649 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.0002913475036621094 sec
2023-07-01 00:40:42,650 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.0003628730773925781 sec
2023-07-01 00:40:42,650 - WARNING - Finished tracing + transforming _mean for pjit in 0.0009822845458984375 sec
2023-07-01 00:40:42,651 - WARNING - Finished tracing + transforming _argmax for pjit in 0.0001900196075439453 sec
2023-07-01 00:40:42,652 - WARNING - Finished tracing + transforming _argmax for pjit in 0.00018215179443359375 sec
2023-07-01 00:40:42,652 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024437904357910156 sec
2023-07-01 00:40:42,653 - WARNING - Finished tracing + transforming _reduce_sum for pjit in 0.00038743019104003906 sec
2023-07-01 00:40:42,653 - WARNING - Finished tracing + transforming _mean for pjit in 0.0010786056518554688 sec
2023-07-01 00:40:42,654 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.009731769561767578 sec
2023-07-01 00:40:42,663 - WARNING - Finished tracing + transforming fn for pjit in 0.00030994415283203125 sec
2023-07-01 00:40:42,663 - WARNING - Finished tracing + transforming fn for pjit in 0.000255584716796875 sec
2023-07-01 00:40:42,664 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021505355834960938 sec
2023-07-01 00:40:42,665 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.0002582073211669922 sec
2023-07-01 00:40:42,665 - WARNING - Finished tracing + transforming _where for pjit in 0.0008559226989746094 sec
2023-07-01 00:40:42,673 - WARNING - Finished tracing + transforming fn for pjit in 0.00025343894958496094 sec
2023-07-01 00:40:42,674 - WARNING - Finished tracing + transforming fn for pjit in 0.0002613067626953125 sec
2023-07-01 00:40:42,674 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021147727966308594 sec
2023-07-01 00:40:42,675 - WARNING - Finished tracing + transforming _broadcast_arrays for pjit in 0.00024271011352539062 sec
2023-07-01 00:40:42,675 - WARNING - Finished tracing + transforming _where for pjit in 0.0008001327514648438 sec
2023-07-01 00:40:42,709 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00020623207092285156 sec
2023-07-01 00:40:42,763 - WARNING - Finished tracing + transforming fn for pjit in 0.00026798248291015625 sec
2023-07-01 00:40:42,764 - WARNING - Finished tracing + transforming fn for pjit in 0.00022864341735839844 sec
2023-07-01 00:40:42,764 - WARNING - Finished tracing + transforming square for pjit in 0.00016379356384277344 sec
2023-07-01 00:40:42,766 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021839141845703125 sec
2023-07-01 00:40:42,768 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00024127960205078125 sec
2023-07-01 00:40:42,768 - WARNING - Finished tracing + transforming fn for pjit in 0.00026226043701171875 sec
2023-07-01 00:40:42,769 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002338886260986328 sec
2023-07-01 00:40:42,769 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022029876708984375 sec
2023-07-01 00:40:42,770 - WARNING - Finished tracing + transforming fn for pjit in 0.0002560615539550781 sec
2023-07-01 00:40:42,771 - WARNING - Finished tracing + transforming fn for pjit in 0.0002231597900390625 sec
2023-07-01 00:40:42,771 - WARNING - Finished tracing + transforming square for pjit in 0.00016069412231445312 sec
2023-07-01 00:40:42,773 - WARNING - Finished tracing + transforming true_divide for pjit in 0.00021338462829589844 sec
2023-07-01 00:40:42,775 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00016832351684570312 sec
2023-07-01 00:40:42,775 - WARNING - Finished tracing + transforming fn for pjit in 0.00025653839111328125 sec
2023-07-01 00:40:42,776 - WARNING - Finished tracing + transforming true_divide for pjit in 0.0002086162567138672 sec
2023-07-01 00:40:42,776 - WARNING - Finished tracing + transforming <lambda> for pjit in 0.00022029876708984375 sec
2023-07-01 00:40:42,777 - WARNING - Finished tracing + transforming update_fn for pjit in 0.13327765464782715 sec
2023-07-01 00:40:42,781 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,10]), ShapedArray(float32[362,10]), ShapedArray(float32[362,10]), ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,10]), ShapedArray(float32[362,3,32,32]), ShapedArray(float32[362,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:40:42,841 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06042909622192383 sec
2023-07-01 00:40:42,842 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:40:43,163 - WARNING - Finished XLA compilation of jit(update_fn) in 0.321063756942749 sec
2023-07-01 00:40:43,926 - INFO - Distilling data from client: Client48
2023-07-01 00:40:43,927 - INFO - train loss: 0.0023374003255101547
2023-07-01 00:40:43,927 - INFO - train acc: 0.9944751858711243
2023-07-01 00:40:43,967 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.80      0.78        61
           5       0.91      0.89      0.90       139

    accuracy                           0.86       200
   macro avg       0.84      0.85      0.84       200
weighted avg       0.87      0.86      0.87       200

2023-07-01 00:40:43,967 - INFO - test loss 0.011073102578644026
2023-07-01 00:40:43,967 - INFO - test acc 0.8650000095367432
2023-07-01 00:40:44,709 - INFO - Distilling data from client: Client48
2023-07-01 00:40:44,710 - INFO - train loss: 0.0015640730528603396
2023-07-01 00:40:44,710 - INFO - train acc: 0.9972376227378845
2023-07-01 00:40:44,754 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.79      0.79        61
           5       0.91      0.91      0.91       139

    accuracy                           0.88       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.88      0.87       200

2023-07-01 00:40:44,755 - INFO - test loss 0.01073707503664602
2023-07-01 00:40:44,755 - INFO - test acc 0.875
2023-07-01 00:40:45,522 - INFO - Distilling data from client: Client48
2023-07-01 00:40:45,523 - INFO - train loss: 0.0012726623116973367
2023-07-01 00:40:45,523 - INFO - train acc: 1.0
2023-07-01 00:40:45,542 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.79      0.79        61
           5       0.91      0.91      0.91       139

    accuracy                           0.88       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.88      0.87       200

2023-07-01 00:40:45,542 - INFO - test loss 0.010710910293301148
2023-07-01 00:40:45,542 - INFO - test acc 0.875
2023-07-01 00:40:46,305 - INFO - Distilling data from client: Client48
2023-07-01 00:40:46,305 - INFO - train loss: 0.0010895165415466851
2023-07-01 00:40:46,305 - INFO - train acc: 1.0
2023-07-01 00:40:46,351 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.82      0.83        61
           5       0.92      0.93      0.92       139

    accuracy                           0.90       200
   macro avg       0.88      0.87      0.88       200
weighted avg       0.89      0.90      0.89       200

2023-07-01 00:40:46,351 - INFO - test loss 0.009992694603314358
2023-07-01 00:40:46,351 - INFO - test acc 0.8949999809265137
2023-07-01 00:40:47,110 - INFO - Distilling data from client: Client48
2023-07-01 00:40:47,110 - INFO - train loss: 0.0009512853357640284
2023-07-01 00:40:47,110 - INFO - train acc: 1.0
2023-07-01 00:40:47,129 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.77      0.78        61
           5       0.90      0.91      0.91       139

    accuracy                           0.87       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:40:47,129 - INFO - test loss 0.011160662232711548
2023-07-01 00:40:47,129 - INFO - test acc 0.8700000047683716
2023-07-01 00:40:47,887 - INFO - Distilling data from client: Client48
2023-07-01 00:40:47,887 - INFO - train loss: 0.0009538143652258842
2023-07-01 00:40:47,888 - INFO - train acc: 1.0
2023-07-01 00:40:47,905 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.80      0.80        61
           5       0.91      0.91      0.91       139

    accuracy                           0.88       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:40:47,905 - INFO - test loss 0.010833163170839953
2023-07-01 00:40:47,905 - INFO - test acc 0.875
2023-07-01 00:40:48,667 - INFO - Distilling data from client: Client48
2023-07-01 00:40:48,667 - INFO - train loss: 0.0008819489437953085
2023-07-01 00:40:48,667 - INFO - train acc: 1.0
2023-07-01 00:40:48,685 - INFO - report:               precision    recall  f1-score   support

           1       0.81      0.79      0.80        61
           5       0.91      0.92      0.91       139

    accuracy                           0.88       200
   macro avg       0.86      0.85      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:40:48,685 - INFO - test loss 0.01113271212997894
2023-07-01 00:40:48,685 - INFO - test acc 0.8799999952316284
2023-07-01 00:40:49,457 - INFO - Distilling data from client: Client48
2023-07-01 00:40:49,457 - INFO - train loss: 0.0009313691183169982
2023-07-01 00:40:49,457 - INFO - train acc: 0.9972376227378845
2023-07-01 00:40:49,474 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.74      0.76        61
           5       0.89      0.91      0.90       139

    accuracy                           0.86       200
   macro avg       0.84      0.83      0.83       200
weighted avg       0.86      0.86      0.86       200

2023-07-01 00:40:49,475 - INFO - test loss 0.011369338389446261
2023-07-01 00:40:49,475 - INFO - test acc 0.85999995470047
2023-07-01 00:40:50,239 - INFO - Distilling data from client: Client48
2023-07-01 00:40:50,239 - INFO - train loss: 0.0008670027107508673
2023-07-01 00:40:50,239 - INFO - train acc: 1.0
2023-07-01 00:40:50,257 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.80      0.81        61
           5       0.91      0.92      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-07-01 00:40:50,257 - INFO - test loss 0.010482192558087085
2023-07-01 00:40:50,257 - INFO - test acc 0.8849999904632568
2023-07-01 00:40:51,016 - INFO - Distilling data from client: Client48
2023-07-01 00:40:51,017 - INFO - train loss: 0.0007166746440100315
2023-07-01 00:40:51,017 - INFO - train acc: 0.9972376227378845
2023-07-01 00:40:51,035 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.79      0.81        61
           5       0.91      0.93      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-07-01 00:40:51,035 - INFO - test loss 0.01075240524362683
2023-07-01 00:40:51,035 - INFO - test acc 0.8849999904632568
2023-07-01 00:40:51,796 - INFO - Distilling data from client: Client48
2023-07-01 00:40:51,796 - INFO - train loss: 0.0007491768672133815
2023-07-01 00:40:51,796 - INFO - train acc: 1.0
2023-07-01 00:40:51,815 - INFO - report:               precision    recall  f1-score   support

           1       0.81      0.79      0.80        61
           5       0.91      0.92      0.91       139

    accuracy                           0.88       200
   macro avg       0.86      0.85      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:40:51,816 - INFO - test loss 0.010145315345769465
2023-07-01 00:40:51,816 - INFO - test acc 0.8799999952316284
2023-07-01 00:40:52,578 - INFO - Distilling data from client: Client48
2023-07-01 00:40:52,578 - INFO - train loss: 0.0007469067437886985
2023-07-01 00:40:52,579 - INFO - train acc: 1.0
2023-07-01 00:40:52,597 - INFO - report:               precision    recall  f1-score   support

           1       0.75      0.74      0.74        61
           5       0.89      0.89      0.89       139

    accuracy                           0.84       200
   macro avg       0.82      0.81      0.82       200
weighted avg       0.84      0.84      0.84       200

2023-07-01 00:40:52,597 - INFO - test loss 0.01119512606936523
2023-07-01 00:40:52,597 - INFO - test acc 0.8449999690055847
2023-07-01 00:40:53,352 - INFO - Distilling data from client: Client48
2023-07-01 00:40:53,353 - INFO - train loss: 0.0007022675629593054
2023-07-01 00:40:53,353 - INFO - train acc: 1.0
2023-07-01 00:40:53,370 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.80      0.80        61
           5       0.91      0.91      0.91       139

    accuracy                           0.88       200
   macro avg       0.86      0.86      0.86       200
weighted avg       0.88      0.88      0.88       200

2023-07-01 00:40:53,370 - INFO - test loss 0.01035756083528706
2023-07-01 00:40:53,370 - INFO - test acc 0.8799999952316284
2023-07-01 00:40:54,120 - INFO - Distilling data from client: Client48
2023-07-01 00:40:54,120 - INFO - train loss: 0.0009108226542410202
2023-07-01 00:40:54,121 - INFO - train acc: 1.0
2023-07-01 00:40:54,137 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.80      0.81        61
           5       0.91      0.92      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-07-01 00:40:54,137 - INFO - test loss 0.010145432109157684
2023-07-01 00:40:54,137 - INFO - test acc 0.8849999904632568
2023-07-01 00:40:54,898 - INFO - Distilling data from client: Client48
2023-07-01 00:40:54,898 - INFO - train loss: 0.0005420574776433864
2023-07-01 00:40:54,898 - INFO - train acc: 1.0
2023-07-01 00:40:54,916 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.77      0.78        61
           5       0.90      0.91      0.91       139

    accuracy                           0.87       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:40:54,916 - INFO - test loss 0.011101430000761612
2023-07-01 00:40:54,916 - INFO - test acc 0.8700000047683716
2023-07-01 00:40:55,685 - INFO - Distilling data from client: Client48
2023-07-01 00:40:55,685 - INFO - train loss: 0.000632465589995432
2023-07-01 00:40:55,685 - INFO - train acc: 1.0
2023-07-01 00:40:55,702 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.80      0.82        61
           5       0.91      0.93      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.87      0.87       200
weighted avg       0.89      0.89      0.89       200

2023-07-01 00:40:55,702 - INFO - test loss 0.01040582035332064
2023-07-01 00:40:55,702 - INFO - test acc 0.8899999856948853
2023-07-01 00:40:56,468 - INFO - Distilling data from client: Client48
2023-07-01 00:40:56,468 - INFO - train loss: 0.0005522779955147754
2023-07-01 00:40:56,468 - INFO - train acc: 1.0
2023-07-01 00:40:56,485 - INFO - report:               precision    recall  f1-score   support

           1       0.79      0.79      0.79        61
           5       0.91      0.91      0.91       139

    accuracy                           0.87       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:40:56,486 - INFO - test loss 0.01142986879179177
2023-07-01 00:40:56,486 - INFO - test acc 0.8700000047683716
2023-07-01 00:40:57,253 - INFO - Distilling data from client: Client48
2023-07-01 00:40:57,253 - INFO - train loss: 0.0005363252328908093
2023-07-01 00:40:57,253 - INFO - train acc: 1.0
2023-07-01 00:40:57,270 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.80      0.81        61
           5       0.91      0.92      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-07-01 00:40:57,271 - INFO - test loss 0.010728864810772343
2023-07-01 00:40:57,271 - INFO - test acc 0.8849999904632568
2023-07-01 00:40:58,038 - INFO - Distilling data from client: Client48
2023-07-01 00:40:58,038 - INFO - train loss: 0.0004595553050748707
2023-07-01 00:40:58,038 - INFO - train acc: 1.0
2023-07-01 00:40:58,056 - INFO - report:               precision    recall  f1-score   support

           1       0.84      0.79      0.81        61
           5       0.91      0.94      0.92       139

    accuracy                           0.89       200
   macro avg       0.88      0.86      0.87       200
weighted avg       0.89      0.89      0.89       200

2023-07-01 00:40:58,056 - INFO - test loss 0.010514012909999437
2023-07-01 00:40:58,056 - INFO - test acc 0.8899999856948853
2023-07-01 00:40:58,823 - INFO - Distilling data from client: Client48
2023-07-01 00:40:58,823 - INFO - train loss: 0.0005481395334710982
2023-07-01 00:40:58,823 - INFO - train acc: 1.0
2023-07-01 00:40:58,840 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.79      0.81        61
           5       0.91      0.93      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-07-01 00:40:58,840 - INFO - test loss 0.010500657548892298
2023-07-01 00:40:58,840 - INFO - test acc 0.8849999904632568
2023-07-01 00:40:59,597 - INFO - Distilling data from client: Client48
2023-07-01 00:40:59,597 - INFO - train loss: 0.00047942809727799827
2023-07-01 00:40:59,597 - INFO - train acc: 1.0
2023-07-01 00:40:59,615 - INFO - report:               precision    recall  f1-score   support

           1       0.83      0.79      0.81        61
           5       0.91      0.93      0.92       139

    accuracy                           0.89       200
   macro avg       0.87      0.86      0.86       200
weighted avg       0.88      0.89      0.88       200

2023-07-01 00:40:59,615 - INFO - test loss 0.010694601793609022
2023-07-01 00:40:59,615 - INFO - test acc 0.8849999904632568
2023-07-01 00:41:00,376 - INFO - Distilling data from client: Client48
2023-07-01 00:41:00,376 - INFO - train loss: 0.00048455530354825783
2023-07-01 00:41:00,376 - INFO - train acc: 1.0
2023-07-01 00:41:00,395 - INFO - report:               precision    recall  f1-score   support

           1       0.82      0.75      0.79        61
           5       0.90      0.93      0.91       139

    accuracy                           0.88       200
   macro avg       0.86      0.84      0.85       200
weighted avg       0.87      0.88      0.87       200

2023-07-01 00:41:00,395 - INFO - test loss 0.010494079922205394
2023-07-01 00:41:00,395 - INFO - test acc 0.875
2023-07-01 00:41:01,159 - INFO - Distilling data from client: Client48
2023-07-01 00:41:01,159 - INFO - train loss: 0.0004966475706794152
2023-07-01 00:41:01,159 - INFO - train acc: 1.0
2023-07-01 00:41:01,177 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.79      0.79        61
           5       0.91      0.91      0.91       139

    accuracy                           0.88       200
   macro avg       0.85      0.85      0.85       200
weighted avg       0.87      0.88      0.87       200

2023-07-01 00:41:01,177 - INFO - test loss 0.011235612334311305
2023-07-01 00:41:01,177 - INFO - test acc 0.875
2023-07-01 00:41:01,943 - INFO - Distilling data from client: Client48
2023-07-01 00:41:01,943 - INFO - train loss: 0.0006114270991262336
2023-07-01 00:41:01,943 - INFO - train acc: 1.0
2023-07-01 00:41:01,960 - INFO - report:               precision    recall  f1-score   support

           1       0.77      0.80      0.78        61
           5       0.91      0.89      0.90       139

    accuracy                           0.86       200
   macro avg       0.84      0.85      0.84       200
weighted avg       0.87      0.86      0.87       200

2023-07-01 00:41:01,960 - INFO - test loss 0.010874893744269823
2023-07-01 00:41:01,961 - INFO - test acc 0.8650000095367432
2023-07-01 00:41:02,720 - INFO - Distilling data from client: Client48
2023-07-01 00:41:02,720 - INFO - train loss: 0.0004867987315362321
2023-07-01 00:41:02,720 - INFO - train acc: 1.0
2023-07-01 00:41:02,737 - INFO - report:               precision    recall  f1-score   support

           1       0.80      0.77      0.78        61
           5       0.90      0.91      0.91       139

    accuracy                           0.87       200
   macro avg       0.85      0.84      0.85       200
weighted avg       0.87      0.87      0.87       200

2023-07-01 00:41:02,737 - INFO - test loss 0.010447406372125212
2023-07-01 00:41:02,737 - INFO - test acc 0.8700000047683716
2023-07-01 00:41:02,740 - WARNING - Finished tracing + transforming jit(gather) in 0.0002338886260986328 sec
2023-07-01 00:41:02,740 - DEBUG - Compiling gather for with global shapes and types [ShapedArray(float32[362,3,32,32]), ShapedArray(int32[2,1])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:41:02,741 - WARNING - Finished jaxpr to MLIR module conversion jit(gather) in 0.0011968612670898438 sec
2023-07-01 00:41:02,741 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:41:02,751 - WARNING - Finished XLA compilation of jit(gather) in 0.009769916534423828 sec
2023-07-01 00:41:02,761 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:02,771 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:02,779 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:02,787 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:02,796 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:02,806 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:03,085 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client48//synthetic.png
2023-07-01 00:41:03,096 - INFO - c: 0.0 and total_data_in_this_class: 268
2023-07-01 00:41:03,096 - INFO - c: 8.0 and total_data_in_this_class: 266
2023-07-01 00:41:03,096 - INFO - c: 9.0 and total_data_in_this_class: 265
2023-07-01 00:41:03,096 - INFO - c: 0.0 and total_data_in_this_class: 65
2023-07-01 00:41:03,096 - INFO - c: 8.0 and total_data_in_this_class: 67
2023-07-01 00:41:03,096 - INFO - c: 9.0 and total_data_in_this_class: 68
2023-07-01 00:41:03,167 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04779314994812012 sec
2023-07-01 00:41:03,213 - WARNING - Finished tracing + transforming <unnamed wrapped function> for pjit in 0.04520845413208008 sec
2023-07-01 00:41:03,218 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.10013437271118164 sec
2023-07-01 00:41:03,220 - DEBUG - Compiling loss_acc_fn for with global shapes and types [ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[200,3,32,32]), ShapedArray(float32[200,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:41:03,252 - WARNING - Finished jaxpr to MLIR module conversion jit(loss_acc_fn) in 0.03207063674926758 sec
2023-07-01 00:41:03,252 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:41:03,375 - WARNING - Finished XLA compilation of jit(loss_acc_fn) in 0.12281250953674316 sec
2023-07-01 00:41:03,398 - INFO - initial test loss: 0.02533138369812557
2023-07-01 00:41:03,398 - INFO - initial test acc: 0.6800000071525574
2023-07-01 00:41:03,407 - WARNING - Finished tracing + transforming loss_acc_fn for pjit in 0.0061931610107421875 sec
2023-07-01 00:41:03,516 - WARNING - Finished tracing + transforming update_fn for pjit in 0.11612367630004883 sec
2023-07-01 00:41:03,520 - DEBUG - Compiling update_fn for with global shapes and types [ShapedArray(int64[], weak_type=True), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[531,10]), ShapedArray(float32[531,10]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10]), ShapedArray(float32[531,3,32,32]), ShapedArray(float32[531,10])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).
2023-07-01 00:41:03,582 - WARNING - Finished jaxpr to MLIR module conversion jit(update_fn) in 0.06213831901550293 sec
2023-07-01 00:41:03,582 - DEBUG - get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[CpuDevice(id=0)]]
2023-07-01 00:41:03,910 - WARNING - Finished XLA compilation of jit(update_fn) in 0.3274056911468506 sec
2023-07-01 00:41:05,205 - INFO - Distilling data from client: Client49
2023-07-01 00:41:05,205 - INFO - train loss: 0.002512490578037919
2023-07-01 00:41:05,205 - INFO - train acc: 0.994350254535675
2023-07-01 00:41:05,266 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.72      0.72        65
           8       0.66      0.73      0.70        67
           9       0.80      0.71      0.75        68

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:41:05,267 - INFO - test loss 0.02217077367850114
2023-07-01 00:41:05,267 - INFO - test acc 0.7199999690055847
2023-07-01 00:41:06,539 - INFO - Distilling data from client: Client49
2023-07-01 00:41:06,539 - INFO - train loss: 0.001501776620169183
2023-07-01 00:41:06,539 - INFO - train acc: 0.9981167316436768
2023-07-01 00:41:06,563 - INFO - report:               precision    recall  f1-score   support

           0       0.62      0.71      0.66        65
           8       0.64      0.66      0.65        67
           9       0.79      0.66      0.72        68

    accuracy                           0.68       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.68      0.68      0.68       200

2023-07-01 00:41:06,564 - INFO - test loss 0.023369743198947034
2023-07-01 00:41:06,564 - INFO - test acc 0.675000011920929
2023-07-01 00:41:07,850 - INFO - Distilling data from client: Client49
2023-07-01 00:41:07,850 - INFO - train loss: 0.001209486122166434
2023-07-01 00:41:07,850 - INFO - train acc: 1.0
2023-07-01 00:41:07,874 - INFO - report:               precision    recall  f1-score   support

           0       0.69      0.71      0.70        65
           8       0.67      0.72      0.69        67
           9       0.80      0.72      0.76        68

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.72       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:41:07,874 - INFO - test loss 0.022278305925563652
2023-07-01 00:41:07,874 - INFO - test acc 0.7149999737739563
2023-07-01 00:41:09,156 - INFO - Distilling data from client: Client49
2023-07-01 00:41:09,156 - INFO - train loss: 0.0009582177431316705
2023-07-01 00:41:09,156 - INFO - train acc: 1.0
2023-07-01 00:41:09,180 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.74      0.72        65
           8       0.68      0.75      0.71        67
           9       0.74      0.63      0.68        68

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:41:09,180 - INFO - test loss 0.022716515188989605
2023-07-01 00:41:09,180 - INFO - test acc 0.7049999833106995
2023-07-01 00:41:10,457 - INFO - Distilling data from client: Client49
2023-07-01 00:41:10,457 - INFO - train loss: 0.0008823862590877861
2023-07-01 00:41:10,457 - INFO - train acc: 1.0
2023-07-01 00:41:10,480 - INFO - report:               precision    recall  f1-score   support

           0       0.68      0.72      0.70        65
           8       0.66      0.73      0.70        67
           9       0.81      0.68      0.74        68

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:41:10,480 - INFO - test loss 0.02296564004050592
2023-07-01 00:41:10,480 - INFO - test acc 0.7099999785423279
2023-07-01 00:41:11,766 - INFO - Distilling data from client: Client49
2023-07-01 00:41:11,766 - INFO - train loss: 0.0009525195030577323
2023-07-01 00:41:11,766 - INFO - train acc: 1.0
2023-07-01 00:41:11,793 - INFO - report:               precision    recall  f1-score   support

           0       0.65      0.71      0.68        65
           8       0.62      0.67      0.65        67
           9       0.79      0.66      0.72        68

    accuracy                           0.68       200
   macro avg       0.69      0.68      0.68       200
weighted avg       0.69      0.68      0.68       200

2023-07-01 00:41:11,793 - INFO - test loss 0.023108862343013573
2023-07-01 00:41:11,793 - INFO - test acc 0.6800000071525574
2023-07-01 00:41:13,082 - INFO - Distilling data from client: Client49
2023-07-01 00:41:13,082 - INFO - train loss: 0.000736023614466991
2023-07-01 00:41:13,082 - INFO - train acc: 1.0
2023-07-01 00:41:13,107 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.72      0.70        65
           8       0.68      0.72      0.70        67
           9       0.81      0.71      0.76        68

    accuracy                           0.71       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:41:13,107 - INFO - test loss 0.02345093247994948
2023-07-01 00:41:13,107 - INFO - test acc 0.7149999737739563
2023-07-01 00:41:14,381 - INFO - Distilling data from client: Client49
2023-07-01 00:41:14,381 - INFO - train loss: 0.0008956291433783516
2023-07-01 00:41:14,381 - INFO - train acc: 1.0
2023-07-01 00:41:14,408 - INFO - report:               precision    recall  f1-score   support

           0       0.63      0.69      0.66        65
           8       0.63      0.67      0.65        67
           9       0.76      0.65      0.70        68

    accuracy                           0.67       200
   macro avg       0.68      0.67      0.67       200
weighted avg       0.68      0.67      0.67       200

2023-07-01 00:41:14,408 - INFO - test loss 0.024347211448899394
2023-07-01 00:41:14,408 - INFO - test acc 0.6699999570846558
2023-07-01 00:41:15,688 - INFO - Distilling data from client: Client49
2023-07-01 00:41:15,688 - INFO - train loss: 0.0006238228107130723
2023-07-01 00:41:15,688 - INFO - train acc: 1.0
2023-07-01 00:41:15,714 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.69      0.68        65
           8       0.63      0.69      0.66        67
           9       0.77      0.68      0.72        68

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:41:15,714 - INFO - test loss 0.02339766943361553
2023-07-01 00:41:15,714 - INFO - test acc 0.6850000023841858
2023-07-01 00:41:17,011 - INFO - Distilling data from client: Client49
2023-07-01 00:41:17,012 - INFO - train loss: 0.000753585283844308
2023-07-01 00:41:17,012 - INFO - train acc: 1.0
2023-07-01 00:41:17,036 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.68      0.67        65
           8       0.64      0.73      0.69        67
           9       0.76      0.65      0.70        68

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:41:17,036 - INFO - test loss 0.023503865381308334
2023-07-01 00:41:17,036 - INFO - test acc 0.6850000023841858
2023-07-01 00:41:18,323 - INFO - Distilling data from client: Client49
2023-07-01 00:41:18,323 - INFO - train loss: 0.0006949766761939907
2023-07-01 00:41:18,323 - INFO - train acc: 1.0
2023-07-01 00:41:18,348 - INFO - report:               precision    recall  f1-score   support

           0       0.69      0.74      0.71        65
           8       0.67      0.70      0.69        67
           9       0.78      0.69      0.73        68

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:41:18,348 - INFO - test loss 0.023143231475190016
2023-07-01 00:41:18,348 - INFO - test acc 0.7099999785423279
2023-07-01 00:41:19,627 - INFO - Distilling data from client: Client49
2023-07-01 00:41:19,628 - INFO - train loss: 0.0006807081511075439
2023-07-01 00:41:19,628 - INFO - train acc: 1.0
2023-07-01 00:41:19,652 - INFO - report:               precision    recall  f1-score   support

           0       0.68      0.69      0.69        65
           8       0.65      0.69      0.67        67
           9       0.76      0.71      0.73        68

    accuracy                           0.69       200
   macro avg       0.70      0.69      0.70       200
weighted avg       0.70      0.69      0.70       200

2023-07-01 00:41:19,652 - INFO - test loss 0.023858456206996604
2023-07-01 00:41:19,653 - INFO - test acc 0.6949999928474426
2023-07-01 00:41:20,937 - INFO - Distilling data from client: Client49
2023-07-01 00:41:20,937 - INFO - train loss: 0.0005098979786166875
2023-07-01 00:41:20,937 - INFO - train acc: 1.0
2023-07-01 00:41:20,961 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.69      0.68        65
           8       0.65      0.72      0.68        67
           9       0.80      0.69      0.74        68

    accuracy                           0.70       200
   macro avg       0.71      0.70      0.70       200
weighted avg       0.71      0.70      0.70       200

2023-07-01 00:41:20,961 - INFO - test loss 0.023301195149840765
2023-07-01 00:41:20,961 - INFO - test acc 0.699999988079071
2023-07-01 00:41:22,251 - INFO - Distilling data from client: Client49
2023-07-01 00:41:22,251 - INFO - train loss: 0.0005317172730890132
2023-07-01 00:41:22,251 - INFO - train acc: 1.0
2023-07-01 00:41:22,275 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.68      0.67        65
           8       0.64      0.70      0.67        67
           9       0.77      0.69      0.73        68

    accuracy                           0.69       200
   macro avg       0.69      0.69      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:41:22,275 - INFO - test loss 0.023891990543191972
2023-07-01 00:41:22,275 - INFO - test acc 0.6899999976158142
2023-07-01 00:41:23,561 - INFO - Distilling data from client: Client49
2023-07-01 00:41:23,562 - INFO - train loss: 0.00046410588102002655
2023-07-01 00:41:23,562 - INFO - train acc: 1.0
2023-07-01 00:41:23,586 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.71      0.71        65
           8       0.66      0.72      0.69        67
           9       0.77      0.71      0.74        68

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.71      0.71       200

2023-07-01 00:41:23,587 - INFO - test loss 0.02397371367376098
2023-07-01 00:41:23,587 - INFO - test acc 0.7099999785423279
2023-07-01 00:41:24,883 - INFO - Distilling data from client: Client49
2023-07-01 00:41:24,883 - INFO - train loss: 0.00047938826470656713
2023-07-01 00:41:24,883 - INFO - train acc: 1.0
2023-07-01 00:41:24,907 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.72      0.70        65
           8       0.67      0.73      0.70        67
           9       0.79      0.66      0.72        68

    accuracy                           0.70       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.71      0.70      0.71       200

2023-07-01 00:41:24,907 - INFO - test loss 0.023938323220590585
2023-07-01 00:41:24,907 - INFO - test acc 0.7049999833106995
2023-07-01 00:41:26,187 - INFO - Distilling data from client: Client49
2023-07-01 00:41:26,187 - INFO - train loss: 0.0004926778414853244
2023-07-01 00:41:26,187 - INFO - train acc: 1.0
2023-07-01 00:41:26,215 - INFO - report:               precision    recall  f1-score   support

           0       0.70      0.71      0.70        65
           8       0.67      0.73      0.70        67
           9       0.80      0.72      0.76        68

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:41:26,215 - INFO - test loss 0.023572627802596145
2023-07-01 00:41:26,215 - INFO - test acc 0.7199999690055847
2023-07-01 00:41:27,492 - INFO - Distilling data from client: Client49
2023-07-01 00:41:27,492 - INFO - train loss: 0.0003632856696569688
2023-07-01 00:41:27,492 - INFO - train acc: 1.0
2023-07-01 00:41:27,517 - INFO - report:               precision    recall  f1-score   support

           0       0.65      0.71      0.68        65
           8       0.66      0.70      0.68        67
           9       0.79      0.68      0.73        68

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.69      0.70       200

2023-07-01 00:41:27,517 - INFO - test loss 0.024445045183134256
2023-07-01 00:41:27,517 - INFO - test acc 0.6949999928474426
2023-07-01 00:41:28,795 - INFO - Distilling data from client: Client49
2023-07-01 00:41:28,796 - INFO - train loss: 0.00040317585637813705
2023-07-01 00:41:28,796 - INFO - train acc: 1.0
2023-07-01 00:41:28,823 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.74      0.72        65
           8       0.68      0.73      0.71        67
           9       0.78      0.69      0.73        68

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.72      0.72      0.72       200

2023-07-01 00:41:28,823 - INFO - test loss 0.023309548334544728
2023-07-01 00:41:28,823 - INFO - test acc 0.7199999690055847
2023-07-01 00:41:30,097 - INFO - Distilling data from client: Client49
2023-07-01 00:41:30,097 - INFO - train loss: 0.00046376968201229614
2023-07-01 00:41:30,097 - INFO - train acc: 1.0
2023-07-01 00:41:30,120 - INFO - report:               precision    recall  f1-score   support

           0       0.66      0.71      0.68        65
           8       0.70      0.72      0.71        67
           9       0.80      0.72      0.76        68

    accuracy                           0.71       200
   macro avg       0.72      0.71      0.72       200
weighted avg       0.72      0.71      0.72       200

2023-07-01 00:41:30,121 - INFO - test loss 0.023765962236163687
2023-07-01 00:41:30,121 - INFO - test acc 0.7149999737739563
2023-07-01 00:41:31,407 - INFO - Distilling data from client: Client49
2023-07-01 00:41:31,407 - INFO - train loss: 0.00036958974858639527
2023-07-01 00:41:31,407 - INFO - train acc: 1.0
2023-07-01 00:41:31,432 - INFO - report:               precision    recall  f1-score   support

           0       0.63      0.66      0.65        65
           8       0.66      0.70      0.68        67
           9       0.77      0.69      0.73        68

    accuracy                           0.69       200
   macro avg       0.69      0.68      0.69       200
weighted avg       0.69      0.69      0.69       200

2023-07-01 00:41:31,432 - INFO - test loss 0.02348816870286951
2023-07-01 00:41:31,432 - INFO - test acc 0.6850000023841858
2023-07-01 00:41:32,713 - INFO - Distilling data from client: Client49
2023-07-01 00:41:32,713 - INFO - train loss: 0.00033954266186462666
2023-07-01 00:41:32,713 - INFO - train acc: 1.0
2023-07-01 00:41:32,736 - INFO - report:               precision    recall  f1-score   support

           0       0.65      0.71      0.68        65
           8       0.70      0.72      0.71        67
           9       0.77      0.68      0.72        68

    accuracy                           0.70       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.70      0.70       200

2023-07-01 00:41:32,736 - INFO - test loss 0.023924826111712726
2023-07-01 00:41:32,736 - INFO - test acc 0.699999988079071
2023-07-01 00:41:34,012 - INFO - Distilling data from client: Client49
2023-07-01 00:41:34,012 - INFO - train loss: 0.00041130106005050747
2023-07-01 00:41:34,013 - INFO - train acc: 1.0
2023-07-01 00:41:34,036 - INFO - report:               precision    recall  f1-score   support

           0       0.71      0.74      0.72        65
           8       0.67      0.73      0.70        67
           9       0.80      0.69      0.74        68

    accuracy                           0.72       200
   macro avg       0.72      0.72      0.72       200
weighted avg       0.73      0.72      0.72       200

2023-07-01 00:41:34,036 - INFO - test loss 0.023818747042825255
2023-07-01 00:41:34,036 - INFO - test acc 0.7199999690055847
2023-07-01 00:41:35,310 - INFO - Distilling data from client: Client49
2023-07-01 00:41:35,310 - INFO - train loss: 0.00041764546550464787
2023-07-01 00:41:35,310 - INFO - train acc: 1.0
2023-07-01 00:41:35,336 - INFO - report:               precision    recall  f1-score   support

           0       0.66      0.71      0.68        65
           8       0.69      0.73      0.71        67
           9       0.80      0.69      0.74        68

    accuracy                           0.71       200
   macro avg       0.71      0.71      0.71       200
weighted avg       0.72      0.71      0.71       200

2023-07-01 00:41:35,336 - INFO - test loss 0.023696845188663693
2023-07-01 00:41:35,337 - INFO - test acc 0.7099999785423279
2023-07-01 00:41:36,617 - INFO - Distilling data from client: Client49
2023-07-01 00:41:36,617 - INFO - train loss: 0.0004275588924125791
2023-07-01 00:41:36,617 - INFO - train acc: 1.0
2023-07-01 00:41:36,642 - INFO - report:               precision    recall  f1-score   support

           0       0.67      0.71      0.69        65
           8       0.68      0.72      0.70        67
           9       0.75      0.66      0.70        68

    accuracy                           0.69       200
   macro avg       0.70      0.70      0.70       200
weighted avg       0.70      0.69      0.70       200

2023-07-01 00:41:36,642 - INFO - test loss 0.02338252492492902
2023-07-01 00:41:36,642 - INFO - test acc 0.6949999928474426
2023-07-01 00:41:36,653 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:36,662 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:36,672 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:36,681 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:36,689 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:36,698 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:36,708 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:36,717 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:36,726 - WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
2023-07-01 00:41:37,106 - INFO - Done save image to fedtask/cifar10_cnum50_dist3_skew0.8_seed0/distill_data_kip/Client49//synthetic.png
